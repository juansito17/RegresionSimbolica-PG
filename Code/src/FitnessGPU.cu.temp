
// ============================================================
// LEXICASE SELECTION SUPPORT
// ============================================================

__global__ void calculate_errors_kernel(
    const LinearGpuNode* __restrict__ d_all_nodes,
    const int* __restrict__ d_offsets,
    const int* __restrict__ d_sizes,
    int total_trees,
    const double* __restrict__ d_targets,
    const double* __restrict__ d_x_values,
    int num_points,
    int num_vars,
    double* __restrict__ d_error_matrix // Output: [total_trees * num_points]
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x; // Tree index

    if (idx < total_trees) {
        int offset = d_offsets[idx];
        int size = d_sizes[idx];
        bool valid = true;

        for (int p = 0; p < num_points; ++p) {
            // If tree was already invalid for previous points, fill remaining with MAX
            if (!valid) {
                 d_error_matrix[idx * num_points + p] = GPU_MAX_DOUBLE;
                 continue;
            }

            double stack[64]; 
            int stack_top = -1;

            for (int i = 0; i < size; ++i) {
                LinearGpuNode node = d_all_nodes[offset + i];
                // Reuse interpretation logic (simplified for brevity, should use device func if possible)
                // Note: Copy-pasting logic is bad practice, but CUDA device function extraction 
                // requires refactoring existing kernels. For now, we duplicate for safety/speed.
                
                if (node.type == NodeType::Constant) {
                    stack[++stack_top] = node.value;
                } else if (node.type == NodeType::Variable) {
                    int var_idx = node.var_index;
                    if (var_idx >= num_vars) var_idx = 0;
                    stack[++stack_top] = d_x_values[p * num_vars + var_idx];
                } else if (node.type == NodeType::Operator) {
                     // ... same ops ...
                     bool is_unary = (node.op == 's' || node.op == 'c' || node.op == 'l' || node.op == 'e' || node.op == '!' || node.op == '_' || node.op == 'g');
                     if (is_unary) {
                        if (stack_top < 0) { valid = false; break; }
                        double val = stack[stack_top--];
                        double res = 0.0;
                        switch (node.op) {
                            case 's': res = sin(val); break;
                            case 'c': res = cos(val); break;
                            case 'l': res = (val <= 1e-9) ? GPU_MAX_DOUBLE : log(val); break;
                            case 'e': res = (val > 700.0) ? GPU_MAX_DOUBLE : exp(val); break;
                            case '!': res = (val < 0 || val > 170.0) ? GPU_MAX_DOUBLE : tgamma(val + 1.0); break;
                            case '_': res = floor(val); break;
                            case 'g': res = (val <= -1.0) ? GPU_MAX_DOUBLE : lgamma(val + 1.0); break;
                            default: res = NAN; break;
                        }
                        stack[++stack_top] = res;
                     } else {
                        if (stack_top < 1) { valid = false; break; }
                        double r = stack[stack_top--];
                        double l = stack[stack_top--];
                        double res;
                        switch (node.op) {
                            case '+': res = l + r; break;
                            case '-': res = l - r; break;
                            case '*': res = l * r; break;
                            case '/': res = (fabs(r) < 1e-9) ? GPU_MAX_DOUBLE : l / r; break;
                            case '^': res = pow(l, r); break;
                            case '%': res = (fabs(r) < 1e-9) ? GPU_MAX_DOUBLE : fmod(l, r); break;
                            default: res = NAN; break;
                        }
                        stack[++stack_top] = res;
                     }
                }
            }
            
            if (!valid || stack_top != 0) {
                d_error_matrix[idx * num_points + p] = GPU_MAX_DOUBLE;
                valid = false;
            } else {
                double pred = stack[0];
                if (isnan(pred) || isinf(pred)) {
                    d_error_matrix[idx * num_points + p] = GPU_MAX_DOUBLE;
                    valid = false;
                } else {
                    double diff = pred - d_targets[p];
                    // Lexicase needs Absolute Error, not Squared, usually. 
                    // But standard is Absolute Error.
                    d_error_matrix[idx * num_points + p] = fabs(diff);
                }
            }
        }
    }
}

void get_population_errors_gpu(
    const std::vector<LinearGpuNode>& all_nodes,
    const std::vector<int>& tree_offsets,
    const std::vector<int>& tree_sizes,
    const std::vector<double>& targets,
    const std::vector<std::vector<double>>& x_values,
    std::vector<double>& flat_errors, 
    double* d_targets, double* d_x_values,
    GlobalGpuBuffers& buffers)
{
    int total_trees = tree_offsets.size();
    if (total_trees == 0) return;
    
    int num_points = x_values.size();
    int num_vars = (num_points > 0) ? x_values[0].size() : 0;
    
    // Resize Buffers
    size_t total_nodes = all_nodes.size();
    if (total_nodes > buffers.d_nodes_capacity) {
        if (buffers.d_nodes) cudaFree(buffers.d_nodes);
        size_t new_cap = total_nodes * 1.5;
        cudaMalloc(&buffers.d_nodes, new_cap * sizeof(LinearGpuNode));
        buffers.d_nodes_capacity = new_cap;
    }
    
    // We need d_results to be big enough for the MATRIX (Pop * Points)
    size_t needed_res_size = (size_t)total_trees * num_points;
    if (needed_res_size > buffers.d_pop_capacity) { // Reusing d_pop_capacity logic but for matrix size?
        // Note: d_pop_capacity tracks d_results size. But usually d_results is just PopSize.
        // We probably need a separate buffer for the matrix or resize d_results temporarily.
        // Let's check buffers definition. d_results is double*. 
        // We can just resize it to needed_res_size. 
        if (buffers.d_results) cudaFree(buffers.d_results);
        cudaMalloc(&buffers.d_results, needed_res_size * sizeof(double));
        buffers.d_pop_capacity = needed_res_size; 
    }
    
    // Copy Inputs
    if ((size_t)total_trees * sizeof(int) > buffers.d_pop_capacity * sizeof(double) / 8) { 
        // Offsets/sizes might need resize too if population grew huge, but usually covered.
        // Let's assume safely reallocated above if d_pop_capacity was small.
    }
    // Actually we need to be careful. d_pop_capacity was intended for "PopSize".
    // If we simple use it for "PopSize * Points", it's fine as long as we treat it as "doubles capacity".
    // But d_offsets is "int". If we resize d_results to 10M doubles, d_pop_capacity = 10M.
    // Then we try to malloc d_offsets for 10M ints? That's wasteful but safe.
    
    // However, we need to ensure d_offsets is properly allocated for total_trees FIRST.
    // The previous check might have expanded it for 10M trees.
    
    // Let's just be explicit:
     LinearGpuNode* d_all_nodes = (LinearGpuNode*)buffers.d_nodes;
     // Note: buffers.d_offsets/sizes/results are void* or typed? In struct they are typed.
     // We need to recast or use them.
     
     // RE-VERIFY buffers structure in code view above.
     // buffers.d_offsets is allocated using d_pop_capacity. 
     // If we increase d_pop_capacity for the matrix, we allocate big offsets array too.
     // It's acceptable overhead.
    
    cudaMemcpyAsync(d_all_nodes, all_nodes.data(), total_nodes * sizeof(LinearGpuNode), cudaMemcpyHostToDevice, (cudaStream_t)buffers.cuda_stream);
    cudaMemcpyAsync(buffers.d_offsets, tree_offsets.data(), total_trees * sizeof(int), cudaMemcpyHostToDevice, (cudaStream_t)buffers.cuda_stream);
    cudaMemcpyAsync(buffers.d_sizes, tree_sizes.data(), total_trees * sizeof(int), cudaMemcpyHostToDevice, (cudaStream_t)buffers.cuda_stream);
    
    int threadsPerBlock = 256;
    int blocksPerGrid = (total_trees + threadsPerBlock - 1) / threadsPerBlock;
    
    calculate_errors_kernel<<<blocksPerGrid, threadsPerBlock, 0, (cudaStream_t)buffers.cuda_stream>>>(
        d_all_nodes, buffers.d_offsets, buffers.d_sizes, total_trees,
        d_targets, d_x_values, num_points, num_vars, 
        buffers.d_results // Output matrix here
    );
    
    // Copy back
    flat_errors.resize(needed_res_size);
    cudaMemcpyAsync(flat_errors.data(), buffers.d_results, needed_res_size * sizeof(double), cudaMemcpyDeviceToHost, (cudaStream_t)buffers.cuda_stream);
    
    cudaStreamSynchronize((cudaStream_t)buffers.cuda_stream);
}
