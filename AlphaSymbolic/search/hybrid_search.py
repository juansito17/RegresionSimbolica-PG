import time
import torch
import numpy as np
from typing import List, Dict, Any, Optional
import concurrent.futures
import os

from AlphaSymbolic.core.gp_bridge import GPEngine
from search.beam_search import BeamSearch, beam_solve
from AlphaSymbolic.core.grammar import ExpressionTree
try:
    from AlphaSymbolic.core.gpu import TensorGeneticEngine
except ImportError:
    TensorGeneticEngine = None
    print("Warning: Could not import TensorGeneticEngine (PyTorch/CUDA missing?)")

def _run_gp_worker(args):
    """
    Worker function for Parallel GP execution.
    args: (x_list, y_list, seeds_chunk, gp_timeout, gp_binary_path)
    """
    x_list, y_list, seeds, timeout, binary_path = args
    import numpy as np
    from AlphaSymbolic.core.grammar import ExpressionTree
    engine = GPEngine(binary_path=binary_path)
    # Give each worker a slight timeout variance to avoid file lock collisions if using temp files
    # or just to spread load. But GPEngine handles unique tmp files so it should be fine.
    
    # Run GP
    result = engine.run(x_list, y_list, seeds, timeout_sec=timeout)
    
    # Evaluate immediately if result found to return RMSE for comparison
    if result:
        try:
             # Basic RMSE check for the worker's champion
            tree = ExpressionTree.from_infix(result)
            y_pred = tree.evaluate(np.array(x_list))
            mse = np.mean((np.array(y_list) - y_pred)**2)
            rmse = np.sqrt(mse)
            return {'formula': result, 'rmse': rmse, 'status': 'success'}
        except:
            return {'formula': result, 'rmse': 999.0, 'status': 'eval_error'}
    
    return {'formula': None, 'rmse': 1e9, 'status': 'failed'}
# Optimized Cache for GPU Engine to avoid startup overhead
_GPU_ENGINE_CACHE = {}

def hybrid_solve(
    x_values: np.ndarray,
    y_values: np.ndarray,
    model: torch.nn.Module,
    device: torch.device,
    beam_width: int = 50,
    gp_timeout: int = 10,
    gp_binary_path: Optional[str] = None,
    max_workers: int = 4,
    num_variables: int = 1,
    extra_seeds: Optional[List[str]] = None,
    max_neural_seeds: Optional[int] = None,
    random_seed_selection: bool = False,
    use_gpu_gp: bool = True,
    pop_size: int = None,
    use_log: bool = None,
    use_engine_cache: bool = True
) -> Dict[str, Any]:
    """
    Solves Symbolic Regression using a Hybrid Neuro-Evolutionary approach with Parallel GP.
    """
    
    # print(f"--- Starting Alpha-GP Hybrid Search (Parallel Workers={max_workers}, Vars={num_variables}) ---")
    start_time = time.time()
    
    # 1. Neural Beam Search (Phase 1)
    # print(f"[Phase 1] Neural Beam Search (Width={beam_width})...")
    if model is not None:
        neural_results = beam_solve(x_values, y_values, model, device, beam_width=beam_width, num_variables=num_variables)
    else:
        neural_results = None
    
    seeds = []
    
    # Inject Extra Seeds (Feedback Loop)
    if extra_seeds:
        pass # print(f"[Phase 1] Injecting {len(extra_seeds)} external seeds (Feedback Loop).")
        seeds.extend(extra_seeds)
        
    if neural_results:
        pass # print(f"[Phase 1] Found {len(neural_results)} candidates.")
        seen_formulas = set()
        
        # 1. Collect ALL unique valid candidates first
        all_candidates = []
        for res in neural_results:
            f_str = res['formula']
            if f_str.startswith("Partial"): continue
            if f_str not in seen_formulas:
                all_candidates.append(f_str)
                seen_formulas.add(f_str)
        
        # 2. Select Seeds (Top K or Random)
        selected_seeds = []
        if max_neural_seeds is not None and len(all_candidates) > 0:
            k = min(len(all_candidates), max_neural_seeds)
            if random_seed_selection:
                import random
                # User request: "use a random one from everything generated by NN"
                selected_seeds = random.sample(all_candidates, k)
            else:
                # PRIORITIZE TOP CANDIDATES (They are sorted by RMSE)
                selected_seeds = all_candidates[:k]
            seeds.extend(selected_seeds)
        else:
            # Default behavior: take all
            selected_seeds = all_candidates
            seeds.extend(all_candidates)
        
        # print(f"[Phase 1] Generated {len(seeds)} unique seeds for GP.")
        if len(selected_seeds) > 0:
             label = "Random Seed NN" if random_seed_selection else "Top Seed NN"
             # Just print the last added one
             print(f"{label}: {selected_seeds[-1]}")
    else:
        print("[Phase 1] No valid candidates found. Falling back to pure GP.")

    # 2. GP Refinement (Phase 2 - Heterogeneous CPU + GPU)
    # print(f"[Phase 2] Genetic Refinement (Timeout={gp_timeout}s)...")
    
    x_list = x_values.tolist() if hasattr(x_values, 'tolist') else list(x_values)
    y_list = y_values.tolist() if hasattr(y_values, 'tolist') else list(y_values)
    
    # Resources managed dynamically by max_workers

    results = []
    futures = []

    # A. Launch GPU Engine
    # ---------------------------------
    if TensorGeneticEngine and use_gpu_gp:
        try:
            # Use cached engine if available
            from AlphaSymbolic.core.gpu.config import GpuGlobals
            
            # Default to Globals if not provided
            tgt_pop = pop_size if pop_size is not None else GpuGlobals.POP_SIZE
            
            cache_key = (str(device), tgt_pop, num_variables) 
            if use_engine_cache and cache_key in _GPU_ENGINE_CACHE:
                gpu_engine = _GPU_ENGINE_CACHE[cache_key]
            else:
                # Initialize Engine once
                gpu_engine = TensorGeneticEngine(pop_size=tgt_pop, n_islands=GpuGlobals.NUM_ISLANDS, device=device, num_variables=num_variables)
                if use_engine_cache:
                    _GPU_ENGINE_CACHE[cache_key] = gpu_engine
                
            print(f"[Phase 2] Launching TensorGeneticEngine (GPU) with {len(seeds)} seeds (Pop={tgt_pop})...")
            
            # Run Evolution
            best_formula_str = gpu_engine.run(x_values, y_values, seeds=seeds, timeout_sec=gp_timeout, use_log=use_log)
            
            if best_formula_str:
                # Calculate RMSE for consistency
                # (Engine already checked target compliance, but we need the number for return dict)
                try:
                    tree = ExpressionTree.from_infix(best_formula_str)
                    
                    # STABILITY FIX: Clamp inputs on CPU too to match GPU stability
                    x_safe = np.clip(x_values, -100, 100)
                    y_pred = tree.evaluate(x_safe)
                    
                    # Handle NaNs in prediction
                    if np.any(np.isnan(y_pred)) or np.any(np.isinf(y_pred)):
                        print("[Phase 2] WARNING: CPU Eval produced NaN/Inf. GPU Result might be unstable.")
                        y_pred = np.nan_to_num(y_pred, nan=0.0, posinf=1e9, neginf=-1e9)
                        
                    mse = np.mean((y_values - y_pred)**2)
                    rmse = np.sqrt(mse)
                    
                    if np.isnan(rmse): rmse = 1e9 # Penalize but keep
                    
                    results.append({'formula': best_formula_str, 'rmse': rmse, 'status': 'success'})
                except Exception as e:
                    print(f"[Phase 2] Eval Error: {e}")
                    results.append({'formula': best_formula_str, 'rmse': 999.0, 'status': 'eval_error'})
            else:
                 print("[Phase 2] GPU Engine returned no valid formula.")
                 
        except Exception as e:
            print(f"[Phase 2] GPU Engine Failed: {e}")
            import traceback
            traceback.print_exc()

# ... (Worker fallback ignored) ...

    total_time = time.time() - start_time

    # Find best result across all workers
    best_result = None
    best_rmse = float('inf')
    
    for res in results:
        r = res['rmse']
        if np.isnan(r): r = 1e9
        
        if res['formula'] and r < best_rmse:
            best_rmse = r
            best_result = res['formula']
            
    if best_result:
        # print(f"--- Hybrid Search Completed in {total_time:.2f}s ---")
        # print(f"Best Formula (Parallel): {best_result} (RMSE: {best_rmse:.5f})")
        
        return {
            'formula': best_result,
            'rmse': best_rmse,
            'source': 'Alpha-GP Hybrid',
            'time': total_time,
            'seeds_tried': seeds if seeds else []
        }
    else:
        print(f"--- Hybrid Search Failed (All workers failed) ---")
        return {
            'formula': None,
            'rmse': 1e9,
            'source': 'Alpha-GP Hybrid',
            'time': total_time,
            'seeds_tried': seeds if seeds else []
        }

if __name__ == "__main__":
    # Test
    class MockModel(torch.nn.Module):
        def forward(self, x, y, seq):
            bs, seq_len = seq.shape
            vocab = 20
            return torch.randn(bs, seq_len, vocab), None

    print("Testing Parallel Hybrid Search...")
    x = np.linspace(-5, 5, 20)
    y = x**2 - 5
    try:
        # Important: must protect entry point for multiprocessing on Windows
        res = hybrid_solve(x, y, MockModel(), torch.device("cpu"), beam_width=5, max_workers=2)
        print(res)
    except Exception as e:
        print(f"Test failed: {e}")
