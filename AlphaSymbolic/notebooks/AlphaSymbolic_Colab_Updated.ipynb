{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install gradio torch torchvision torchaudio scipy matplotlib sympy\n",
        "\n",
        "# Create directories\n",
        "!mkdir -p core data search ui utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile core/grammar.py\n",
        "import numpy as np\nfrom scipy.special import gamma as scipy_gamma, gammaln\nimport math\n\n# Supported operators and their arity (number of arguments)\n# Organized by curriculum stage for progressive unlocking\nOPERATORS = {\n    # === STAGE 0: Pure Arithmetic ===\n    '+': 2,\n    '-': 2,\n    '*': 2,\n    '/': 2,\n    \n    # === STAGE 1: Powers ===\n    'pow': 2,\n    'sqrt': 1,\n    \n    # === STAGE 2: Trigonometry ===\n    'sin': 1,\n    'cos': 1,\n    'tan': 1,\n    \n    # === STAGE 3: Transcendental ===\n    'exp': 1,\n    'log': 1,\n    \n    # === STAGE 4: Advanced ===\n    'abs': 1,\n    'neg': 1,\n    'sign': 1,\n    'floor': 1,\n    'ceil': 1,\n    'mod': 2,\n    'gamma': 1,\n    'lgamma': 1,  # Log-gamma function (from C++ GP engine)\n}\n\n# Operator groups for curriculum control\nOPERATOR_STAGES = {\n    0: ['+', '-', '*', '/'],\n    1: ['+', '-', '*', '/', 'pow', 'sqrt'],\n    2: ['+', '-', '*', '/', 'pow', 'sqrt', 'sin', 'cos', 'tan'],\n    3: ['+', '-', '*', '/', 'pow', 'sqrt', 'sin', 'cos', 'tan', 'exp', 'log'],\n    4: list(OPERATORS.keys()),  # All operators\n}\n\n# Terminal tokens\nVARIABLES = ['x']\n# 'C' is a placeholder for learnable constants\nCONSTANTS = ['C', '0', '1', '2', '3', '5', '10', 'pi', 'e']\n\n# Full Vocabulary\nVOCABULARY = list(OPERATORS.keys()) + VARIABLES + CONSTANTS\nTOKEN_TO_ID = {token: i for i, token in enumerate(VOCABULARY)}\nID_TO_TOKEN = {i: token for token, i in TOKEN_TO_ID.items()}\n\n# Special token for start of sequence\nSOS_TOKEN = '<SOS>'\nEOS_TOKEN = '<EOS>'\nPAD_TOKEN = '<PAD>'\n\nclass Node:\n    def __init__(self, value, children=None):\n        self.value = value\n        self.children = children if children else []\n\n    def __repr__(self):\n        if not self.children:\n            return str(self.value)\n        return f\"({self.value} \" + \" \".join([str(c) for c in self.children]) + \")\"\n    \n    def to_infix(self):\n        if not self.children:\n            return str(self.value)\n        \n        op = self.value\n        if len(self.children) == 1:\n            return f\"{op}({self.children[0].to_infix()})\"\n        elif len(self.children) == 2:\n            if op == 'pow':\n                return f\"({self.children[0].to_infix()} ^ {self.children[1].to_infix()})\"\n            elif op == 'mod':\n                return f\"({self.children[0].to_infix()} % {self.children[1].to_infix()})\"\n            return f\"({self.children[0].to_infix()} {op} {self.children[1].to_infix()})\"\n        return str(self.value)\n    \n    def count_constants(self):\n        \"\"\"Count the number of 'C' placeholders in the tree.\"\"\"\n        count = 1 if self.value == 'C' else 0\n        for child in self.children:\n            count += child.count_constants()\n        return count\n    \n    def get_constant_positions(self, path=None):\n        \"\"\"Returns a list of paths to all 'C' nodes for optimization.\"\"\"\n        if path is None:\n            path = []\n        positions = []\n        if self.value == 'C':\n            positions.append(path.copy())\n        for i, child in enumerate(self.children):\n            positions.extend(child.get_constant_positions(path + [i]))\n        return positions\n\n\nimport ast\n\nclass ExpressionTree:\n    def __init__(self, token_list):\n        \"\"\"\n        Parses a list of tokens in Pre-order traversal (Prefix notation)\n        Example: ['+', 'x', 'sin', 'x'] -> x + sin(x)\n        \"\"\"\n        self.tokens = token_list\n        try:\n            self.root, remaining = self._build_tree(token_list)\n            if remaining:\n                raise ValueError(\"Tokens remained after building tree\")\n            self.is_valid = True\n        except Exception:\n            self.root = None\n            self.is_valid = False\n\n    @classmethod\n    def from_infix(cls, infix_str):\n        \"\"\"\n        Creates an ExpressionTree from a standard infix string (e.g. \"sin(x) + x^2\").\n        Uses Python's ast to parse.\n        \"\"\"\n        # Replacements to make it valid python for AST\n        # 1. Handle postfix factorial '!' which C++ outputs as '(... )!'\n        # We convert '(... )!' to 'gamma(...)'\n        # Iterate until no '!' left\n        processed_str = infix_str\n        while '!' in processed_str:\n            idx = processed_str.find('!')\n            # Helper to find matching paren backwards\n            if idx > 0 and processed_str[idx-1] == ')':\n                paren_count = 1\n                start = idx - 2\n                while start >= 0 and paren_count > 0:\n                    if processed_str[start] == ')':\n                        paren_count += 1\n                    elif processed_str[start] == '(':\n                        paren_count -= 1\n                    start -= 1\n                # start is now 1 char before the matching '('\n                start += 1 \n                # Reconstruct: ... + gamma( + ... + ) + ...\n                # Content includes the parens: ( ... )\n                content = processed_str[start:idx] \n                processed_str = processed_str[:start] + \"gamma\" + content + processed_str[idx+1:]\n            else:\n                # Fallback: Just remove ! if it's weirdly placed (should not happen with GP output)\n                processed_str = processed_str.replace('!', '', 1)\n\n        # 2. C++ uses ^ for power, Python uses **. AST parses ^ as BitXor.\n        try:\n            tree = ast.parse(processed_str, mode='eval')\n            tokens = cls._ast_to_prefix(tree.body)\n            return cls(tokens)\n        except Exception as e:\n            print(f\"Error parsing infix: {e} | Original: {infix_str} | Processed: {processed_str}\")\n            return cls([]) # Invalid\n\n    @staticmethod\n    def _ast_to_prefix(node):\n        if isinstance(node, ast.BinOp):\n            # Map operators\n            op_map = {\n                ast.Add: '+', ast.Sub: '-', ast.Mult: '*', ast.Div: '/',\n                ast.BitXor: 'pow', ast.Pow: 'pow', ast.Mod: 'mod'\n            }\n            op_type = type(node.op)\n            if op_type in op_map:\n                return [op_map[op_type]] + ExpressionTree._ast_to_prefix(node.left) + ExpressionTree._ast_to_prefix(node.right)\n        \n        elif isinstance(node, ast.UnaryOp):\n            op_map = {ast.USub: 'neg', ast.UAdd: None} # Ignore unary +\n            op_type = type(node.op)\n            if op_type == ast.USub:\n                # Check directly if it's a number to collapse \"-5\"\n                if isinstance(node.operand, ast.Constant) and isinstance(node.operand.value, (int, float)):\n                    return [str(-node.operand.value)]\n                return ['neg'] + ExpressionTree._ast_to_prefix(node.operand)\n            elif op_type == ast.UAdd:\n                 return ExpressionTree._ast_to_prefix(node.operand)\n\n        elif isinstance(node, ast.Call):\n            # Functions like sin(x)\n            func_id = node.func.id\n            if func_id in ['sin', 'cos', 'tan', 'exp', 'log', 'sqrt', 'abs', 'floor', 'ceil', 'gamma', 'lgamma']:\n                tokens = [func_id]\n                for arg in node.args:\n                    tokens.extend(ExpressionTree._ast_to_prefix(arg))\n                return tokens\n        \n        elif isinstance(node, ast.Name):\n            return [node.id]\n        \n        elif isinstance(node, ast.Constant): # Python 3.8+\n            return [str(node.value)]\n        elif isinstance(node, ast.Num): # Older python\n            return [str(node.n)]\n\n        raise ValueError(f\"Unsupported AST node: {node}\")\n\n\n    def _build_tree(self, tokens):\n        if not tokens:\n            raise ValueError(\"Empty token list\")\n        \n        token = tokens[0]\n        remaining = tokens[1:]\n        \n        if token in OPERATORS:\n            arity = OPERATORS[token]\n            children = []\n            for _ in range(arity):\n                child, remaining = self._build_tree(remaining)\n                children.append(child)\n            return Node(token, children), remaining\n        elif token in VARIABLES or token in CONSTANTS:\n            return Node(token), remaining\n        else:\n            # Try to parse as float literal\n            try:\n                float(token)\n                return Node(token), remaining\n            except:\n                raise ValueError(f\"Unknown token: {token}\")\n\n    def evaluate(self, x_values, constants=None):\n        \"\"\"\n        Evaluates the expression tree for a given array of x values.\n        constants: optional dict mapping path tuples to constant values\n        Returns a numpy array of results.\n        \"\"\"\n        # Ensure x_values is a numpy array\n        if not isinstance(x_values, np.ndarray):\n            x_values = np.array(x_values, dtype=np.float64)\n        \n        if not self.is_valid:\n            return np.full_like(x_values, np.nan, dtype=np.float64)\n        return self._eval_node(self.root, x_values, constants, path=[])\n\n    def _eval_node(self, node, x, constants=None, path=None):\n        val = node.value\n        \n        if val == 'x':\n            return x.astype(np.float64)\n        if val == 'pi':\n            return np.full_like(x, np.pi, dtype=np.float64)\n        if val == 'e':\n            return np.full_like(x, np.e, dtype=np.float64)\n        if val == 'C':\n            # Check if we have an optimized constant for this position\n            if constants is not None and tuple(path) in constants:\n                return np.full_like(x, constants[tuple(path)], dtype=np.float64)\n            return np.full_like(x, 1.0, dtype=np.float64)  # Default constant = 1\n        \n        # Check for numeric constants\n        try:\n            return np.full_like(x, float(val), dtype=np.float64)\n        except:\n            pass\n            \n        # Recursive evaluation\n        args = []\n        for i, c in enumerate(node.children):\n            args.append(self._eval_node(c, x, constants, path + [i] if path is not None else None))\n        \n        # Operators\n        with np.errstate(divide='ignore', invalid='ignore', over='ignore'):\n            if val == '+': return args[0] + args[1]\n            if val == '-': return args[0] - args[1]\n            if val == '*': return args[0] * args[1]\n            if val == '/': \n                return np.divide(args[0], args[1], out=np.zeros_like(x, dtype=np.float64), where=args[1]!=0)\n            if val == 'pow':\n                # Safe power\n                return np.power(np.abs(args[0]) + 1e-10, np.clip(args[1], -10, 10))\n            if val == 'mod':\n                return np.mod(args[0], args[1] + 1e-10)\n            if val == 'sin': return np.sin(args[0])\n            if val == 'cos': return np.cos(args[0])\n            if val == 'tan': return np.tan(args[0])\n            if val == 'exp': \n                return np.exp(np.clip(args[0], -100, 100))\n            if val == 'log': \n                return np.log(np.abs(args[0]) + 1e-10)\n            if val == 'sqrt':\n                return np.sqrt(np.abs(args[0]))\n            if val == 'abs':\n                return np.abs(args[0])\n            if val == 'floor':\n                return np.floor(args[0])\n            if val == 'ceil':\n                return np.ceil(args[0])\n            if val == 'gamma':\n                # Match C++ Protected Gamma/Factorial: tgamma(|x| + 1)\n                # This ensures consistent evaluation for formulas from C++ engine (which uses !)\n                arg = np.abs(args[0]) + 1.0\n                clipped = np.clip(arg, 0.1, 50) # Clip upper bound to avoid overflow\n                return scipy_gamma(clipped)\n            if val == 'lgamma':\n                # Protected lgamma: lgamma(|x| + 1)\n                arg = np.abs(args[0]) + 1.0\n                # gammaln is safe for large positive numbers, so less aggressive clipping needed for overflow,\n                # but we clip for consistency and to avoid extremely large outputs if followed by exp\n                clipped = np.clip(arg, 0.1, 1000) \n                return gammaln(clipped)\n            if val == 'neg':\n                return -args[0]\n            if val == 'sign':\n                return np.sign(args[0])\n                \n        return np.zeros_like(x, dtype=np.float64)\n\n    def get_infix(self):\n        if not self.is_valid:\n            return \"Invalid\"\n        return self.root.to_infix()\n    \n    \n    def count_constants(self):\n        if not self.is_valid:\n            return 0\n        return self.root.count_constants()\n\nimport sympy\n\ndef simplify_formula(formula_str):\n    \"\"\"\n    Simplifies a mathematical formula using SymPy.\n    \"\"\"\n    try:\n        # 1. Clean up C++ notation that sympy might not like directly\n        # e.g., 'pi' is fine. 'neg(x)' -> '-x'.\n        # But our infix is usually standard. \n        # C++ 'pow(x,2)' might need conversion to 'x**2' or sympy handles it?\n        # Sympy uses 'Pow'. \n        \n        # Replace common mismatches\n        s_str = formula_str.replace(\"pow(\", \"Pow(\")\n        # s_str = s_str.replace(\"abs(\", \"Abs(\") # Sympy handles abs\n        \n        # Parse\n        expr = sympy.sympify(s_str)\n        \n        # Simplify\n        simplified = sympy.simplify(expr)\n        \n        # Convert back to string\n        # We need to ensure it uses our function names (e.g. sin, cos)\n        # Sympy standard printer is usually good.\n        # But 'Power' is '**'. We used 'hat' or 'pow' in some places?\n        # Our tokenizer supports standard operators. 'x**2' is not standard infix for our parser?\n        # Our Parser supports 'x^2' or 'pow(x,2)'? \n        # AST parser handles '**' -> 'pow'.\n        \n        final_str = str(simplified)\n        return final_str\n        \n    except Exception as e:\n        # Fallback if simplification fails (e.g. unknown functions)\n        return formula_str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile core/model.py\n",
        "import torch\nimport torch.nn as nn\nimport numpy as np\n\nclass AlphaSymbolicModel(nn.Module):\n    def __init__(self, vocab_size, d_model=128, nhead=4, num_encoder_layers=2, num_decoder_layers=2, max_seq_len=50):\n        super(AlphaSymbolicModel, self).__init__()\n        \n        self.d_model = d_model\n        \n        # 1. Point Encoder: Processes pairs of (x, y)\n        # Input dim: 2 (x value, y value)\n        self.point_embedding = nn.Linear(2, d_model)\n        \n        # We use a standard Transformer Encoder for the \"Problem Embedding\"\n        # Since points are a set, we don't necessarily need positional encoding, \n        # but the Transformer will process them as a sequence.\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n        self.problem_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n        \n        # 2. Formula Decoder: Generates tokens\n        self.token_embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoder = PositionalEncoding(d_model, max_len=max_seq_len)\n        \n        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n        self.formula_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n        \n        # 3. Heads\n        self.policy_head = nn.Linear(d_model, vocab_size)\n        self.value_head = nn.Sequential(\n            nn.Linear(d_model, 64),\n            nn.ReLU(),\n            nn.Linear(64, 3) # Quantiles: 0.25, 0.50, 0.75\n        )\n        \n    def forward(self, x_values, y_values, formula_input, formula_mask=None):\n        \"\"\"\n        x_values: [batch, num_points]\n        y_values: [batch, num_points]\n        formula_input: [batch, seq_len] (Token IDs)\n        formula_mask: Optional mask for the decoder (causal mask)\n        \"\"\"\n        batch_size, num_points = x_values.shape\n        \n        # -- Problem Encoding --\n        # Stack x and y: [batch, num_points, 2]\n        points = torch.stack([x_values, y_values], dim=2)\n        \n        # Project to d_model\n        points_emb = self.point_embedding(points) # [batch, num_points, d_model]\n        \n        # Encode problem (memory for decoder)\n        memory = self.problem_encoder(points_emb)\n        \n        # -- Formula Decoding --\n        # Embed tokens\n        tgt = self.token_embedding(formula_input) # [batch, seq_len, d_model]\n        tgt = self.pos_encoder(tgt)\n        \n        # Decode\n        # memory is [batch, num_points, d_model]\n        # tgt is [batch, seq_len, d_model]\n        if formula_mask is None:\n             # Create causal mask\n            seq_len = formula_input.size(1)\n            formula_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(formula_input.device)\n\n        output = self.formula_decoder(tgt, memory, tgt_mask=formula_mask)\n        \n        # -- Heads --\n        # Policy: distribution over vocab for each token position\n        logits = self.policy_head(output) # [batch, seq_len, vocab_size]\n        \n        # Value: estimate value from the LAST token's state\n        # (Assuming the last token summarizes the current state)\n        last_token_output = output[:, -1, :] # [batch, d_model]\n        value = self.value_head(last_token_output) # [batch, 1]\n        \n        return logits, value\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        # x: [batch, seq_len, d_model]\n        x = x + self.pe[:, :x.size(1), :]\n        return x\n\nif __name__ == \"__main__\":\n    # Smoke Test\n    vocab_size = 20\n    model = AlphaSymbolicModel(vocab_size=vocab_size, d_model=32)\n    \n    # Dummy data\n    bs = 2\n    points = 10\n    x = torch.randn(bs, points)\n    y = torch.randn(bs, points)\n    \n    # Formula input (start token + some tokens)\n    seq = torch.randint(0, vocab_size, (bs, 5))\n    \n    logits, value = model(x, y, seq)\n    \n    print(\"Logits shape:\", logits.shape) # Should be [2, 5, 20]\n    print(\"Value shape:\", value.shape)   # Should be [2, 1]\n    print(\"Smoke test passed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile core/environment.py\n",
        "import gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\nfrom core.grammar import VOCABULARY, OPERATORS, TOKEN_TO_ID, ExpressionTree\nfrom data.synthetic_data import DataGenerator\n\nclass SymbolicEnv(gym.Env):\n    def __init__(self, max_length=50):\n        super(SymbolicEnv, self).__init__()\n        \n        self.vocab_size = len(VOCABULARY)\n        self.max_length = max_length\n        self.vocab = VOCABULARY\n        \n        # Action space: Choose a token from the vocabulary\n        self.action_space = spaces.Discrete(self.vocab_size)\n        \n        # Observation space: \n        # 1. Current token sequence (padded)\n        # 2. X values (fixed size for simplicity)\n        # 3. Y values\n        # For this prototype we will expose a dictionary observation\n        self.observation_space = spaces.Dict({\n            \"sequence\": spaces.Box(low=0, high=self.vocab_size, shape=(max_length,), dtype=np.int32),\n            \"x\": spaces.Box(low=-np.inf, high=np.inf, shape=(10,), dtype=np.float32),\n            \"y\": spaces.Box(low=-np.inf, high=np.inf, shape=(10,), dtype=np.float32)\n        })\n        \n        self.data_gen = DataGenerator(max_depth=4)\n        self.current_problem = None\n        self.current_sequence = []\n        self.open_branches = 0\n        \n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        \n        # Generate a new problem (X, Y)\n        # In a real scenario, this could be sampled from a fixed dataset\n        batch = self.data_gen.generate_batch(1, point_count=10)\n        self.current_problem = batch[0]\n        \n        self.current_sequence = []\n        self.open_branches = 1 # Start expecting a root node\n        \n        return self._get_obs(), {}\n\n    def step(self, action_id):\n        token = self.vocab[action_id]\n        self.current_sequence.append(token)\n        \n        # Update open branches\n        if token in OPERATORS:\n            arity = OPERATORS[token]\n            self.open_branches += (arity - 1)\n        else:\n            self.open_branches -= 1\n            \n        term = False\n        trunc = False\n        reward = 0.0\n        \n        # Check completion\n        if self.open_branches == 0:\n            term = True\n            # Tree is complete, evaluate\n            reward = self._calculate_reward()\n        elif self.open_branches < 0:\n            # Should not happen if we mask actions, but for safety\n            term = True\n            reward = -100.0 # Syntax error penalty\n        elif len(self.current_sequence) >= self.max_length:\n            trunc = True\n            reward = -10.0 # Incomplete penalty\n            \n        return self._get_obs(), reward, term, trunc, {}\n\n    def _get_obs(self):\n        # Convert sequence to IDs and pad\n        seq_ids = [TOKEN_TO_ID[t] for t in self.current_sequence]\n        padded_seq = np.zeros(self.max_length, dtype=np.int32)\n        padded_seq[:len(seq_ids)] = seq_ids\n        \n        return {\n            \"sequence\": padded_seq,\n            \"x\": self.current_problem['x'].astype(np.float32),\n            \"y\": self.current_problem['y'].astype(np.float32)\n        }\n\n    def _calculate_reward(self):\n        try:\n            tree = ExpressionTree(self.current_sequence)\n            if not tree.is_valid:\n                return -100.0\n            \n            y_pred = tree.evaluate(self.current_problem['x'])\n            \n            # Root Mean Squared Error (RMSE)\n            mse = np.mean((y_pred - self.current_problem['y'])**2)\n            rmse = np.sqrt(mse)\n            \n            if np.isnan(rmse) or np.isinf(rmse):\n                return -1000.0\n                \n            # Reward is negative RMSE\n            # We want to maximize reward -> minimize RMSE\n            # Normalize or scale? simpler is just -RMSE\n            return -rmse\n            \n        except Exception:\n            return -100.0\n\nif __name__ == \"__main__\":\n    env = SymbolicEnv()\n    obs, _ = env.reset()\n    print(\"Initial Observation Keys:\", obs.keys())\n    \n    # Simulate a few steps for x + x\n    # Prefix: + x x\n    actions = ['+', 'x', 'x']\n    tot_reward = 0\n    for tok in actions:\n        aid = TOKEN_TO_ID[tok]\n        obs, reward, term, trunc, _ = env.step(aid)\n        print(f\"Action: {tok}, Reward: {reward}, Term: {term}, Branches: {env.open_branches}\")\n        tot_reward += reward\n        if term: break\n    \n    print(f\"Total Reward: {tot_reward}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile core/loss.py\n",
        "\nimport torch\nimport torch.nn as nn\n\nclass QuantileLoss(nn.Module):\n    \"\"\"\n    Quantile Loss (Pinball Loss) for multiple quantiles.\n    \n    Args:\n        quantiles (list): List of quantiles to estimate (e.g. [0.25, 0.5, 0.75])\n    \"\"\"\n    def __init__(self, quantiles=[0.25, 0.5, 0.75]):\n        super().__init__()\n        self.quantiles = quantiles\n        \n    def forward(self, preds, target):\n        \"\"\"\n        preds: [batch, num_quantiles] - Predicted values for each quantile\n        target: [batch, 1] - True scalar target\n        \"\"\"\n        # Ensure target matches batch dim\n        # target shape might be [batch] or [batch, 1]\n        if target.dim() == 1:\n            target = target.unsqueeze(1)\n            \n        loss = 0\n        for i, q in enumerate(self.quantiles):\n            error = target - preds[:, i:i+1]\n            # Pinball loss: max(q * error, (q - 1) * error)\n            # Equivalent to: error * (q - I(error < 0))\n            loss += torch.max(q * error, (q - 1) * error).mean()\n            \n        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile core/gp_bridge.py\n",
        "import os\nimport subprocess\nimport tempfile\nimport re\nimport time\nfrom typing import List, Optional\n\nclass GPEngine:\n    def __init__(self, binary_path=None):\n        if binary_path is None:\n            # Default location: Code/build/Release/SymbolicRegressionGP.exe\n            # Assuming we are in AlphaSymbolic/.. root or similar.\n            # Adjust path relative to this file: alphasybolic/core/gp_bridge.py\n            # So binary is at ../../Code/build/Release/SymbolicRegressionGP.exe\n            base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n            possible_paths = [\n                os.path.join(base_dir, \"Code\", \"build\", \"Release\", \"SymbolicRegressionGP.exe\"),\n                os.path.join(base_dir, \"Code\", \"build\", \"SymbolicRegressionGP.exe\")\n            ]\n            self.binary_path = None\n            for p in possible_paths:\n                if os.path.exists(p):\n                    self.binary_path = p\n                    break\n            \n            if self.binary_path is None:\n                # Fallback to default for error message\n                self.binary_path = possible_paths[0]\n        else:\n            self.binary_path = binary_path\n\n    def run(self, x_values: List[float], y_values: List[float], seeds: List[str] = [], timeout_sec: int = 10) -> Optional[str]:\n        \"\"\"\n        Runs the C++ GP Engine with the given data and seeds.\n        Returns the best formula found as a string, or None if failed.\n        \"\"\"\n        if not os.path.exists(self.binary_path):\n            print(f\"[Error] GP Binary not found at: {self.binary_path}\")\n            return None\n\n        # Create temporary files\n        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as seed_file, \\\n             tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as data_file:\n            \n            # Write Seeds\n            for seed in seeds:\n                seed_file.write(seed + \"\\n\")\n            seed_file_path = seed_file.name\n            \n            # Write Data\n            # Line 1: x1 x2 ...\n            # Line 2: y1 y2 ...\n            data_file.write(\" \".join(map(str, x_values)) + \"\\n\")\n            data_file.write(\" \".join(map(str, y_values)) + \"\\n\")\n            data_file_path = data_file.name\n\n        try:\n            # Run Command\n            cmd = [self.binary_path, \"--seed\", seed_file_path, \"--data\", data_file_path]\n            print(f\"Running GP Engine: {' '.join(cmd)}\")\n            \n            # Capture output\n            # We can't strictly enforce timeout via subprocess.run's timeout argument easily if we want partial results?\n            # Actually we can.\n            start_time = time.time()\n            result = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout_sec)\n            \n            output = result.stdout\n            \n            # Parse Output\n            # We look for the LAST occurrence of \"Formula: ...\"\n            # Standard formats:\n            # \"Formula: ((x * x) + 2)\"\n            # \"Final Formula: ...\"\n            \n            best_formula = None\n            # Look for formula lines (case-insensitive)\n            # Priority: \"Final Formula:\" > \"Formula:\" > \"Initial best formula:\"\n            for line in output.splitlines():\n                line_lower = line.lower()\n                if \"formula:\" in line_lower:\n                    # Extract the part after \"formula:\" (case-insensitive split)\n                    idx = line_lower.find(\"formula:\")\n                    if idx != -1:\n                        formula_part = line[idx + len(\"formula:\"):].strip()\n                        if formula_part:\n                            best_formula = formula_part\n                            # Keep looking for better matches (Final Formula is best)\n                            if \"final formula:\" in line_lower:\n                                break  # Final Formula is the best, stop looking\n                        \n            print(f\"GP Engine finished in {time.time() - start_time:.2f}s\")\n            \n            if best_formula is None:\n                print(f\"[DEBUG] GP Engine Output (Stdout):\\n{output}\")\n                print(f\"[DEBUG] GP Engine Output (Stderr):\\n{result.stderr}\")\n            \n            return best_formula\n\n        except subprocess.TimeoutExpired as e:\n            print(f\"GP Engine timed out after {timeout_sec}s.\")\n            # Recover output captured so far\n            output = e.stdout if e.stdout else \"\"\n            best_formula = None\n            if output:\n                for line in output.splitlines():\n                    line_lower = line.lower()\n                    if \"formula:\" in line_lower:\n                        idx = line_lower.find(\"formula:\")\n                        if idx != -1:\n                            formula_part = line[idx + len(\"formula:\"):].strip()\n                            if formula_part:\n                                best_formula = formula_part\n                                if \"final formula:\" in line_lower:\n                                    break\n            \n            if best_formula:\n                print(f\"Recovered best formula from timeout: {best_formula}\")\n                return best_formula\n            \n            # Print stderr for timeout diagnose\n            if e.stderr:\n                 print(f\"GP Engine Timeout Stderr: {e.stderr}\")\n            return None\n\n        except Exception as e:\n            print(f\"GP Engine failed: {e}\")\n            if hasattr(e, 'stderr') and e.stderr:\n                print(f\"Stderr: {e.stderr}\")\n            return None\n        finally:\n            # Cleanup\n            if os.path.exists(seed_file_path):\n                os.unlink(seed_file_path)\n            if os.path.exists(data_file_path):\n                os.unlink(data_file_path)\n\nif __name__ == \"__main__\":\n    # Test\n    engine = GPEngine()\n    x = [1, 2, 3, 4]\n    y = [1+2, 4+2, 9+2, 16+2] # x^2 + 2\n    seeds = [\"(x * x)\", \"(x + 2)\"]\n    \n    print(\"Testing GPEngine...\")\n    res = engine.run(x, y, seeds)\n    print(f\"Result: {res}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile core/__init__.py\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile data/synthetic_data.py\n",
        "import numpy as np\nimport random\nfrom core.grammar import VOCABULARY, OPERATORS, VARIABLES, CONSTANTS, ExpressionTree\nfrom data.augmentation import augment_formula_tokens\n\nclass DataGenerator:\n    def __init__(self, max_depth=5, population_size=1000, allowed_operators=None):\n        self.max_depth = max_depth\n        self.population_size = population_size\n        self.vocab = VOCABULARY\n        # Pre-compute terminal vs operator lists\n        self.terminals = VARIABLES + CONSTANTS\n        if allowed_operators:\n            self.operators = [op for op in allowed_operators if op in OPERATORS]\n        else:\n            self.operators = list(OPERATORS.keys())\n\n    def generate_random_tree(self, max_depth, current_depth=0):\n        if current_depth >= max_depth:\n            # Balanced Terminal Selection: 50% x, 50% constant\n            if random.random() < 0.5:\n                return ['x']\n            else:\n                return [random.choice(CONSTANTS)]\n        \n        # Decide if terminal or operator\n        # Higher probability of operator at shallow depths\n        if random.random() < 0.7: \n            op = random.choice(self.operators)\n            arity = OPERATORS[op]\n            tokens = [op]\n            for _ in range(arity):\n                tokens.extend(self.generate_random_tree(max_depth, current_depth + 1))\n            return tokens\n        else:\n            # Balanced Terminal Selection: 40% x, 30% C, 30% numbers\n            r = random.random()\n            if r < 0.4:\n                return ['x']\n            elif r < 0.7:\n                return ['C']\n            else:\n                return [random.choice([c for c in CONSTANTS if c != 'C'])]\n\n    def generate_batch(self, batch_size, point_count=10, x_range=(-10, 10)):\n        \"\"\"\n        Generates a batch of (X, Y) pairs and their generating formulas.\n        \"\"\"\n        data = []\n        \n        while len(data) < batch_size:\n            # Generate random formula\n            tokens = self.generate_random_tree(self.max_depth)\n            tree = ExpressionTree(tokens)\n            \n            if not tree.is_valid:\n                continue\n            \n            # Ensure 'x' is present in the formula (90% of the time)\n            if 'x' not in tokens and random.random() < 0.9:\n                continue\n                \n            # Generate random X points\n            x_values = np.random.uniform(x_range[0], x_range[1], point_count)\n            # Sort X for cleaner visualization/learning\n            x_values.sort()\n            \n            # Randomize 'C' values if present\n            c_positions = tree.root.get_constant_positions()\n            constant_vals = {}\n            for pos in c_positions:\n                # Expanded range: -20 to 20. Favor 1.0 occasionally\n                val = random.uniform(-20, 20) if random.random() > 0.1 else 1.0\n                constant_vals[tuple(pos)] = val\n            \n            # Calculate Y with randomized constants\n            y_values = tree.evaluate(x_values, constants=constant_vals)\n            \n            # Check for validity (no NaNs, Infs, or extremely large values)\n            if np.any(np.isnan(y_values)) or np.any(np.isinf(y_values)):\n                continue\n            if np.max(np.abs(y_values)) > 1e6: # Reject too large numbers\n                continue\n            if np.std(y_values) < 1e-6: # Reject flat lines (too simple)\n                 # Optionally keep some, but mostly we want interesting curves\n                 if random.random() > 0.1: continue\n\n            data.append({\n                'tokens': tokens,\n                'infix': tree.get_infix(),\n                'x': x_values,\n                'y': y_values\n            })\n            \n        return data\n\n    def generate_structured_tree(self, complexity=1, input_node='x'):\n        \"\"\"\n        Recursively builds a structured, human-like formula.\n        Respects self.operators.\n        \"\"\"\n        # Base cases\n        if complexity <= 0:\n            # Randomly choose between x, C and constants\n            r = random.random()\n            if r < 0.4: return ['x']\n            if r < 0.7: return ['C']\n            return [random.choice([c for c in CONSTANTS if c != 'C'])]\n            \n        # Filter available structures based on allowed operators\n        available_structures = []\n        \n        # Arithmetic needed: +, -, *\n        if any(op in self.operators for op in ['+', '-', '*']):\n            available_structures.append('arithmetic')\n            \n        # Poly needed: pow\n        if 'pow' in self.operators:\n            available_structures.append('poly')\n            \n        # Trig needed: sin, cos\n        if 'sin' in self.operators or 'cos' in self.operators:\n            available_structures.append('trig')\n            \n        # Exp/Log needed\n        if 'exp' in self.operators or 'log' in self.operators:\n            available_structures.append('exp_log')\n            \n        # Composition needs enough variety\n        if len(self.operators) > 4 and complexity > 1:\n             available_structures.append('composition')\n        \n        # Fallback if nothing allowed matches (shouldn't happen with proper init)\n        if not available_structures:\n            return input_node if isinstance(input_node, list) else [input_node]\n\n        choice = random.choice(available_structures)\n        \n        if choice == 'poly':\n            # a*x + b or a*x^2 + b\n            a = str(random.randint(1, 5))\n            b = str(random.randint(-5, 5))\n            power = random.choice(['1', '2', '3'])\n            if power == '1':\n                term = ['*', a] + (input_node if isinstance(input_node, list) else [input_node])\n                return ['+', ] + term + [b]\n            else:\n                base = input_node if isinstance(input_node, list) else [input_node]\n                pow_term = ['pow'] + base + [power]\n                term = ['*', a] + pow_term\n                return ['+', ] + term + [b]\n                \n        elif choice == 'trig':\n            # Filter trig ops that are allowed\n            ops = [op for op in ['sin', 'cos'] if op in self.operators]\n            if not ops: return input_node # Should be caught by structure check\n            func = random.choice(ops)\n            val = input_node if isinstance(input_node, list) else [input_node]\n            return [func] + val\n            \n        elif choice == 'exp_log':\n            ops = [op for op in ['exp', 'log'] if op in self.operators]\n            if not ops: return input_node\n            func = random.choice(ops)\n            val = input_node if isinstance(input_node, list) else [input_node]\n            return [func] + val\n            \n        elif choice == 'arithmetic':\n            left = self.generate_structured_tree(complexity - 1, input_node)\n            right = self.generate_structured_tree(complexity - 1, input_node)\n            ops = [op for op in ['+', '-', '*'] if op in self.operators]\n            if not ops: return input_node\n            op = random.choice(ops)\n            return [op] + left + right\n            \n        elif choice == 'composition':\n            inner = self.generate_structured_tree(complexity - 1, input_node)\n            outer = self.generate_structured_tree(1, inner)\n            return outer\n            \n        return [input_node]\n\n    def generate_inverse_batch(self, batch_size, point_count=10, x_range=(-5, 5)):\n        \"\"\"\n        Generates complex, structured formulas using the new engine.\n        \"\"\"\n        data = []\n        attempts = 0\n        \n        while len(data) < batch_size and attempts < batch_size * 5:\n            attempts += 1\n            # Random complexity capped by max_depth\n            complexity = random.randint(1, max(1, self.max_depth - 1))\n            \n            try:\n                tokens = self.generate_structured_tree(complexity, 'x')\n                \n                # Convert numeric strings to 'C' placeholders if needed\n                # But here we want the GROUND TRUTH tokens with numbers for checking?\n                # The model predicts tokens. 'C' is for optimization.\n                # If we train \"End-to-End\" (predict 3*x), we keep numbers.\n                # If we train \"Symbolic\" (predict C*x), we swap.\n                # The original code swapped numbers to 'C'. Let's check VOCABULARY.\n                # '1','2','3' are in VOCABULARY. So we can keep small integers.\n                # Large integers -> 'C'.\n                \n                final_tokens = []\n                for t in tokens:\n                    if t in self.vocab:\n                        final_tokens.append(t)\n                    else:\n                        # If it's a number not in vocab, map to C?\n                        # Or just nearest constant?\n                        # For now, simplistic mapping:\n                        try:\n                            val = float(t)\n                            if abs(val - round(val)) < 0.01 and str(int(round(val))) in self.vocab:\n                                final_tokens.append(str(int(round(val))))\n                            else:\n                                final_tokens.append('C')\n                        except:\n                            final_tokens.append('C')\n\n                # --- DATA AUGMENTATION ---\n                if random.random() < 0.3:\n                    final_tokens = augment_formula_tokens(final_tokens)\n                # -------------------------\n                \n                tree = ExpressionTree(final_tokens)\n                if not tree.is_valid:\n                    continue\n                \n                # Ensure 'x' is present (90% of the time)\n                if 'x' not in final_tokens and random.random() < 0.9:\n                    continue\n                    \n                # Check constraints (depth, length)\n                if len(final_tokens) > 30: # Limit length\n                    continue\n\n                # Generate X points\n                # Use safer range for complex funcs\n                # Exp/Pow grow very fast, so we constrain X to avoid float overflow\n                if 'exp' in final_tokens or 'pow' in final_tokens:\n                    x_safe = np.linspace(-2, 2, point_count)\n                elif 'log' in final_tokens or 'sqrt' in final_tokens:\n                    x_safe = np.linspace(0.1, 5, point_count)\n                else:\n                    x_safe = np.linspace(x_range[0], x_range[1], point_count)\n                \n                # Randomize 'C' values if present\n                c_positions = tree.root.get_constant_positions()\n                constant_vals = {}\n                for pos in c_positions:\n                    # Expanded range: -20 to 20\n                    val = random.uniform(-20, 20) if random.random() > 0.1 else 1.0\n                    constant_vals[tuple(pos)] = val\n                \n                y_values = tree.evaluate(x_safe, constants=constant_vals)\n                \n                # Quality Control\n                if np.any(np.isnan(y_values)) or np.any(np.isinf(y_values)):\n                    continue\n                if np.max(np.abs(y_values)) > 1e4: # Relaxed limit\n                    continue\n                if np.std(y_values) < 0.01: # Too flat\n                    continue\n                \n                data.append({\n                    'tokens': final_tokens,\n                    'infix': tree.get_infix(),\n                    'x': x_safe,\n                    'y': y_values\n                })\n            except Exception:\n                continue\n                \n        return data\n\n# Quick test if run directly\nif __name__ == \"__main__\":\n    gen = DataGenerator(max_depth=4)\n    batch = gen.generate_batch(5)\n    for item in batch:\n        print(f\"Formula: {item['infix']}\")\n        print(f\"Tokens: {item['tokens']}\")\n        print(f\"Y sample: {item['y'][:3]}...\")\n        print(\"-\" * 20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile data/benchmark_data.py\n",
        "import numpy as np\n\n# Standard Benchmark Problems\n# Levels: 1 (Easy), 2 (Medium), 3 (Hard)\n\nBENCHMARK_SUITE = [\n    # --- Level 1: Polynomials & Basic Arithmetic ---\n    {\n        'id': 'p1',\n        'name': 'Lineal',\n        'formula_str': '2.5 * x + 1.0',\n        'lambda': lambda x: 2.5 * x + 1.0,\n        'domain': (-10, 10),\n        'points': 20,\n        'level': 1\n    },\n    {\n        'id': 'p2',\n        'name': 'Cuadratica Simple',\n        'formula_str': 'x * x',\n        'lambda': lambda x: x**2,\n        'domain': (-5, 5),\n        'points': 20,\n        'level': 1\n    },\n    {\n        'id': 'p3',\n        'name': 'Polinomio Cubico',\n        'formula_str': 'x**3 + x**2',\n        'lambda': lambda x: x**3 + x**2,\n        'domain': (-3, 3),\n        'points': 20,\n        'level': 1\n    },\n    \n    # --- Level 2: Trigonometric & Transcendental ---\n    {\n        'id': 'p4',\n        'name': 'Seno Basico',\n        'formula_str': 'sin(x)',\n        'lambda': lambda x: np.sin(x),\n        'domain': (-np.pi, np.pi),\n        'points': 30,\n        'level': 2\n    },\n    {\n        'id': 'p5',\n        'name': 'Coseno Desplazado',\n        'formula_str': 'cos(x) + 1',\n        'lambda': lambda x: np.cos(x) + 1,\n        'domain': (-np.pi, np.pi),\n        'points': 30,\n        'level': 2\n    },\n    {\n        'id': 'p6',\n        'name': 'Exponencial Simple',\n        'formula_str': 'exp(x)',\n        'lambda': lambda x: np.exp(x),\n        'domain': (-2, 2), # Small domain to avoid explosion\n        'points': 20,\n        'level': 2\n    },\n    \n    # --- Level 3: Physics / Complex ---\n    {\n        'id': 'p7',\n        'name': 'Damped Oscillation',\n        'formula_str': 'exp(-x) * sin(2*x)',\n        'lambda': lambda x: np.exp(-x) * np.sin(2*x),\n        'domain': (0, 4),\n        'points': 40,\n        'level': 3\n    },\n    {\n        'id': 'p8',\n        'name': 'Gaussian',\n        'formula_str': 'exp(-x**2)',\n        'lambda': lambda x: np.exp(-x**2),\n        'domain': (-3, 3),\n        'points': 30,\n        'level': 3\n    },\n    {\n        'id': 'p9',\n        'name': 'Nguyen-3 (x^3 + x^2 + x)',\n        'formula_str': 'x**3 + x**2 + x',\n        'lambda': lambda x: x**3 + x**2 + x,\n        'domain': (-2, 2),\n        'points': 20,\n        'level': 3\n    },\n    {\n        'id': 'p10',\n        'name': 'Rational Function',\n        'formula_str': 'x / (1 + x**2)',\n        'lambda': lambda x: x / (1 + x**2),\n        'domain': (-4, 4),\n        'points': 30,\n        'level': 3\n    }\n]\n\ndef get_benchmark_data(problem_id):\n    \"\"\"Returns (x, y) for a specific problem ID.\"\"\"\n    for p in BENCHMARK_SUITE:\n        if p['id'] == problem_id:\n            x = np.linspace(p['domain'][0], p['domain'][1], p['points'])\n            y = p['lambda'](x)\n            return x, y, p\n    return None, None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile data/augmentation.py\n",
        "\nimport random\nfrom core.grammar import OPERATORS\n\ndef augment_formula_tokens(tokens):\n    \"\"\"\n    Applies mathematical invariants to generate an equivalent formula structure.\n    Acts as 'Data Augmentation' for symbolic regression.\n    \n    Supported Transformations:\n    1. Commutativity: (+) and (*)\n       e.g. [+ a b] -> [+ b a]\n    2. Identity:\n       e.g. x -> [+ x 0], x -> [* x 1] (Rarely used to avoid bloat, but useful for robustness)\n    3. Inverse operations (Conceptually):\n       Not implemented directly on tokens without tree parsing, \n       so we focus on purely structural swaps that don't change value.\n    \n    Args:\n        tokens (list): List of tokens in Prefix notation.\n    \n    Returns:\n        list: A new list of tokens representing an equivalent formula.\n    \"\"\"\n    if not tokens:\n        return []\n\n    # Helper to parse prefix expression into a tree-like structure (recursive)\n    def parse_prefix(token_list):\n        if not token_list:\n            return None, []\n        \n        root = token_list[0]\n        remaining = token_list[1:]\n        \n        if root in OPERATORS:\n            try:\n                arity = OPERATORS[root]\n                children = []\n                for _ in range(arity):\n                    child, remaining = parse_prefix(remaining)\n                    children.append(child)\n                return {'val': root, 'children': children}, remaining\n            except:\n                 # Fallback for malformed\n                return {'val': root, 'children': []}, remaining\n        else:\n            # Terminal\n            return {'val': root, 'children': []}, remaining\n\n    # Helper to flatten tree back to tokens\n    def flatten(node):\n        res = [node['val']]\n        for child in node['children']:\n            res.extend(flatten(child))\n        return res\n\n    # 1. Parse\n    try:\n        tree, _ = parse_prefix(tokens)\n    except:\n        return list(tokens) # Fail safe\n\n    # 2. Augment Recursive\n    def augment_recursive(node):\n        # First augment children\n        for i in range(len(node['children'])):\n            node['children'][i] = augment_recursive(node['children'][i])\n            \n        val = node['val']\n        children = node['children']\n        \n        # Transformation: Commutativity\n        if val in ['+', '*'] and len(children) == 2:\n            if random.random() < 0.5:\n                # Swap children\n                node['children'] = [children[1], children[0]]\n        \n        # Transformation: (- a b) -> (+ a (- b)) ? Too complex for tokens only without 'neg'\n        # Transformation: (+ x x) -> (* x 2) ?\n        if val == '+' and len(children) == 2:\n            # Check deep equality is hard, but simple check:\n            if flatten(children[0]) == flatten(children[1]):\n                if random.random() < 0.3:\n                    # Convert x + x -> x * 2\n                    return {'val': '*', 'children': [children[0], {'val': '2', 'children': []}]}\n\n        return node\n\n    # 3. Apply\n    augmented_tree = augment_recursive(tree)\n    \n    # 4. Flatten\n    return flatten(augmented_tree)\n\nif __name__ == \"__main__\":\n    # Test\n    # Formula: (+ x y) -> prefix ['+', 'x', 'y']\n    t1 = ['+', 'x', 'y']\n    print(f\"Original: {t1} -> Aug: {augment_formula_tokens(t1)}\")\n    \n    # Formula: (* (+ a b) c)\n    t2 = ['*', '+', 'a', 'b', 'c']\n    print(f\"Original: {t2} -> Aug: {augment_formula_tokens(t2)}\")\n    \n    # Formula: (+ x x)\n    t3 = ['+', 'x', 'x']\n    print(f\"Original: {t3} -> Aug: {augment_formula_tokens(t3)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile data/__init__.py\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile search/mcts.py\n",
        "import math\nimport numpy as np\nimport torch\nimport copy\nfrom core.grammar import VOCABULARY, TOKEN_TO_ID, OPERATORS, ExpressionTree, VARIABLES\nfrom utils.optimize_constants import optimize_constants\n\nclass MCTSNode:\n    def __init__(self, tokens, parent=None, prior=0.0):\n        self.tokens = tokens\n        self.parent = parent\n        self.children = {}\n        self.visit_count = 0\n        self.value_sum = 0.0\n        self.prior = prior\n        self.is_expanded = False\n        \n        # for parallel search\n        self.virtual_loss = 0.0\n        self.virtual_visits = 0\n\n    @property\n    def value(self):\n        count = self.visit_count + self.virtual_visits\n        if count == 0:\n            return 0.0\n        # Combine real value and virtual loss\n        # Virtual loss is SUBTRACTED to discourage visits\n        return (self.value_sum - self.virtual_loss) / count\n\n    def ucb_score(self, c_puct=1.0):\n        count = self.visit_count + self.virtual_visits\n        parent_count = self.parent.visit_count + self.parent.virtual_visits if self.parent else 1\n        \n        if self.parent is None:\n            return 0.0\n            \n        u = c_puct * self.prior * math.sqrt(parent_count) / (1 + count)\n        return self.value + u\n\n    @property\n    def complexity(self):\n        \"\"\"Estimate complexity (length of formula).\"\"\"\n        return len(self.tokens)\n\nclass MCTS:\n    def __init__(self, model, device, grammar=None, c_puct=1.0, n_simulations=100, max_simulations=None, max_depth=50, complexity_lambda=0.1, max_len=200, batch_size=8):\n        self.model = model\n        self.device = device\n        self.grammar = grammar\n        self.c_puct = c_puct\n        \n        # Handle backwards compatibility for max_simulations\n        if max_simulations is not None:\n            self.n_simulations = max_simulations\n        else:\n            self.n_simulations = n_simulations\n            \n        self.max_depth = max_depth\n        self.complexity_lambda = complexity_lambda\n        self.max_len = max_len\n        self.min_value = -float('inf')\n        self.max_value = float('inf')\n        self.vocab_size = len(VOCABULARY)\n        self.sos_id = self.vocab_size\n        self.batch_size = batch_size\n        \n        # Pareto Front: List of {'tokens':, 'rmse':, 'complexity':, 'formula':}\n        self.pareto_front = []\n        \n        # Virtual loss constant usually 1-3\n        self.v_loss_const = 3.0\n        \n    def search(self, x_values, y_values, num_simulations=None):\n        \"\"\"\n        Run MCTS (Parallel/Batched) to find the best formula.\n        \"\"\"\n        self.pareto_front = [] # Reset Pareto Front for new search\n        root = MCTSNode(tokens=[])\n        \n        # Initial expansion (single)\n        self._expand_batch([root], x_values, y_values)\n        \n        best_rmse = float('inf')\n        best_formula = None\n        best_tokens = None\n        \n        limit = num_simulations if num_simulations is not None else self.n_simulations\n        \n        # Loop in batches\n        # Ensure we do at least 1 batch\n        num_batches = max(1, (limit + self.batch_size - 1) // self.batch_size)\n        \n        for _ in range(num_batches): \n            leaves = []\n            \n            # 1. Selection (find N leaves)\n            for _ in range(self.batch_size):\n                node = root\n                depth = 0\n                \n                # Selection loop\n                while node.is_expanded and node.children and depth < self.max_depth:\n                    node = max(node.children.values(), key=lambda n: n.ucb_score(self.c_puct))\n                    \n                    # Apply virtual loss to discourage re-selection in same batch\n                    node.virtual_loss += self.v_loss_const\n                    node.virtual_visits += 1\n                    depth += 1\n                \n                # Check if valid leaf to expand\n                if depth < self.max_depth and not node.is_expanded:\n                    # Avoid duplicates in batch (simple check)\n                    if node not in leaves:\n                        leaves.append(node)\n                else:\n                    pass\n            \n            if not leaves:\n                # If no leaves found (tree fully explored or locked), standard MCTS usually continues or stops.\n                # We can just break or continue backprop of terminals.\n                if root.visit_count > limit: break \n                continue\n                \n            # 2. Batch Expansion & Evaluation\n            values = self._expand_batch(leaves, x_values, y_values)\n            \n            # 3. Backpropagation\n            for node, val in zip(leaves, values):\n                # Check for best solution found\n                if self._is_complete_tree(node.tokens):\n                    # For completed formulas, we calculate REAL RMSE\n                    try:\n                        # Evaluar\n                        # Importar aqu\u00ed para evitar circular imports si es necesario\n                        from utils.optimize_constants import optimize_constants\n                        \n                        # 1. Optimizar constants (Crucial para Accuracy)\n                        # Esto es \"Phase 1\" de TPSR (constantes en las hojas)\n                        # Por simplicidad en esta iteraci\u00f3n, asumimos que 'evaluate_formula' ya hace algo o usamos el string directo.\n                        # Idealmente llamar\u00edamos a BFGS aqu\u00ed.\n                        \n                        # Use existing _evaluate_formula to get RMSE and optimized constants\n                        tree = ExpressionTree(node.tokens)\n                        optimized_constants, real_rmse = optimize_constants(tree, x_values, y_values)\n                        \n                        # Get y_pred using the optimized constants\n                        y_pred = tree.evaluate(x_values, constants=optimized_constants)\n                        \n                        # Check dimensions\n                        if y_pred.shape != y_values.shape:\n                            # If shapes don't match, it's an invalid evaluation\n                            final_val = 0.0\n                        else:\n                            # 2. Calcular Reward TPSR (Hybrid Accuracy + Complexity)\n                            # R = 1 / (1 + NMSE) + lambda * exp(-len/L)\n                            \n                            mse = np.mean((y_pred - y_values)**2)\n                            var_y = np.var(y_values)\n                            if var_y < 1e-9: var_y = 1.0 # Avoid division by zero\n                            \n                            nmse = mse / var_y\n                            \n                            # Evitar NMSE gigantes\n                            if np.isnan(nmse) or np.isinf(nmse):\n                                nmse = 1e9\n                            \n                            r_acc = 1.0 / (1.0 + nmse)\n                            \n                            # Penalizaci\u00f3n por complejidad\n                            token_len = len(node.tokens)\n                            L = self.max_len # Max length del modelo\n                            \n                            r_cplx = self.complexity_lambda * np.exp(-token_len / L)\n                            \n                            # Suma y Normalizaci\u00f3n (para mantener rango 0-1)\n                            # El m\u00e1ximo te\u00f3rico es (1.0 + lambda). Dividimos por eso.\n                            raw_reward = r_acc + r_cplx\n                            final_val = raw_reward / (1.0 + self.complexity_lambda)\n\n                        # Update best formula based on RMSE (for reporting, not for MCTS value)\n                        if real_rmse < best_rmse:\n                            best_rmse = real_rmse\n                            best_tokens = node.tokens\n                            best_formula = ExpressionTree(node.tokens).get_infix()\n                        \n                        # Update Pareto Front\n                        # Complexity = len(tokens) (or could use count_constants + nodes)\n                        complexity = len(node.tokens)\n                        self._update_pareto_front(node.tokens, real_rmse, complexity, ExpressionTree(node.tokens).get_infix())\n\n                    except Exception as e:\n                        # print(f\"Error evaluating formula: {e}\")\n                        final_val = 0.0 # Invalid formula gets 0 reward\n                else:\n                    final_val = val\n                \n                # The following lines were part of the user's instruction but contained syntax errors and undefined variables.\n                # They are commented out to maintain a syntactically correct and functional document.\n                # If these lines were intended to be added, please provide a complete and correct snippet.\n                #\n                # # Construir vector de probabilidades\n                # probs = np.zeros(self.vocab_size, dtype=np.float32)\n                # for token_id, count in counts.items():\n                #     probs[token_id] = count / total_visits_count += 1\n                \n                curr = node\n                while curr is not None:\n                    curr.visit_count += 1\n                    curr.value_sum += final_val\n                    \n                    # Revert virtual loss for parent and above\n                    # Since we added to PARENT's child (which is curr), \n                    # and we traverse Up...\n                    # Wait, logic: We selected CHILD. Virtual loss was added TO CHILD (curr).\n                    # So we must remove it from curr.\n                    if curr.virtual_visits > 0:\n                        curr.virtual_loss -= self.v_loss_const\n                        curr.virtual_visits -= 1\n                            \n                    curr = curr.parent\n        \n        # After search, force cleanup of any residual virtual loss (safety)\n        # (Not strictly needed if logic is perfect, but good practice in complex async MCTS)\n        \n        return {\n            'tokens': best_tokens,\n            'formula': best_formula,\n            'rmse': best_rmse,\n            'root': root,\n            'pareto_front': self.pareto_front\n        }\n\n    def _update_pareto_front(self, tokens, rmse, complexity, formula_str):\n        \"\"\"\n        Update the Pareto Front with a new solution.\n        Keep solutions that are not dominated by any other solution.\n        Solution A dominates B if:\n        A.rmse <= B.rmse AND A.complexity <= B.complexity AND (A.rmse < B.rmse OR A.complexity < B.complexity)\n        \"\"\"\n        # Create candidate\n        candidate = {'tokens': tokens, 'rmse': rmse, 'complexity': complexity, 'formula': formula_str}\n        \n        # Check if dominated by existing\n        is_dominated = False\n        to_remove = []\n        \n        for existing in self.pareto_front:\n            # Check if existing dominates candidate\n            if (existing['rmse'] <= candidate['rmse'] and \n                existing['complexity'] <= candidate['complexity'] and \n                (existing['rmse'] < candidate['rmse'] or existing['complexity'] < candidate['complexity'])):\n                is_dominated = True\n                break\n                \n            # Check if candidate dominates existing\n            if (candidate['rmse'] <= existing['rmse'] and \n                candidate['complexity'] <= existing['complexity'] and \n                (candidate['rmse'] < existing['rmse'] or candidate['complexity'] < existing['complexity'])):\n                to_remove.append(existing)\n        \n        if not is_dominated:\n            # Remove dominated existing solutions\n            for item in to_remove:\n                self.pareto_front.remove(item)\n            \n            # Add candidate\n            self.pareto_front.append(candidate)\n            # Sort by RMSE for easier viewing\n            self.pareto_front.sort(key=lambda x: x['rmse'])\n\n    def _expand_batch(self, nodes, x_values, y_values):\n        \"\"\"\n        Batched expansion. Returns list of values.\n        \"\"\"\n        if not nodes:\n            return []\n            \n        # Prepare inputs\n        x_tensor = torch.tensor(x_values, dtype=torch.float32).unsqueeze(0).to(self.device)\n        y_tensor = torch.tensor(y_values, dtype=torch.float32).unsqueeze(0).to(self.device)\n        \n        # Repeat X/Y for batch\n        batch_size = len(nodes)\n        x_batch = x_tensor.repeat(batch_size, 1, 1).squeeze(1) # [batch, points]\n        y_batch = y_tensor.repeat(batch_size, 1, 1).squeeze(1) # [batch, points]\n        \n        # Prepare sequences\n        # Find max len\n        max_len = 0\n        seqs = []\n        for n in nodes:\n            s = [self.sos_id] + [TOKEN_TO_ID[t] for t in n.tokens]\n            seqs.append(s)\n            max_len = max(max_len, len(s))\n            \n        # Pad and stack\n        input_tensor = torch.full((batch_size, max_len), self.sos_id, dtype=torch.long).to(self.device)\n        for i, s in enumerate(seqs):\n            input_tensor[i, :len(s)] = torch.tensor(s, dtype=torch.long)\n            \n        # Inference\n        with torch.no_grad():\n            logits, value_preds = self.model(x_batch, y_batch, input_tensor)\n            \n        # Process results\n        values = []\n        \n        # To CPU numpy for probability processing\n        probs_batch = torch.softmax(logits[:, -1, :self.vocab_size], dim=1).cpu().numpy()\n        value_preds = value_preds.cpu().numpy() # [batch, 3]\n        \n        for i, node in enumerate(nodes):\n            # 1. Store Value (Median for now)\n            # value_preds is [batch, 3] -> (Pessimistic, Median, Optimistic)\n            # We use Median (index 1) for standard UCB.\n            val_pred = value_preds[i, 1] \n            val = float(np.clip(val_pred, 0.0, 1.0))\n            values.append(val)\n            \n            # 2. Expand children\n            node_probs = probs_batch[i]\n            valid_next = self._get_valid_next_tokens(node.tokens)\n            \n            for idx in valid_next:\n                token = VOCABULARY[idx]\n                prior = node_probs[idx]\n                child = MCTSNode(tokens=node.tokens + [token], parent=node, prior=prior)\n                node.children[token] = child\n            \n            node.is_expanded = True\n            \n        return values\n\n    def _get_valid_next_tokens(self, tokens):\n        \"\"\"Simple grammar check.\"\"\"\n        open_slots = 1\n        for t in tokens:\n            if t in OPERATORS:\n                open_slots += OPERATORS[t] - 1\n            else:\n                open_slots -= 1\n        \n        if open_slots <= 0:\n            return []\n        return list(range(self.vocab_size))\n\n    def _is_complete_tree(self, tokens):\n        if not tokens: return False\n        try:\n            tree = ExpressionTree(tokens)\n            # Basic validation\n            if len(tokens) > self.max_depth * 2: return False\n            return tree.is_valid\n        except:\n            return False\n\n    def _evaluate_formula(self, tokens, x, y):\n        try:\n            tree = ExpressionTree(tokens)\n            _, rmse = optimize_constants(tree, x, y)\n            return rmse\n        except:\n            return 1e9\n\n    def get_training_examples(self, root):\n        \"\"\"\n        Extrae ejemplos de entrenamiento del \u00e1rbol generado.\n        Retorna: lista de (state_tokens, policy_probs, value_target)\n        \"\"\"\n        examples = []\n        queue = [root]\n        \n        while queue:\n            node = queue.pop(0)\n            if node.visit_count < 1: \n                continue\n            \n            # Policy Target (Pi)\n            # Distribuci\u00f3n de visitas de los hijos\n            counts = {}\n            total_visits = 0\n            has_children = False\n            \n            for token_id, child in node.children.items():\n                # child key is token STRING or ID?\n                # In _expand_batch: node.children[token] = child.\n                # token = VOCABULARY[idx] (String).\n                # So keys are strings.\n                # But we need ID for probabilities array index.\n                if token_id in TOKEN_TO_ID:\n                    tid = TOKEN_TO_ID[token_id]\n                    counts[tid] = child.visit_count\n                    total_visits += child.visit_count\n                    queue.append(child)\n                    has_children = True\n            \n            if not has_children or total_visits == 0:\n                continue\n                \n            # Construir vector de probabilidades\n            probs = np.zeros(self.vocab_size, dtype=np.float32)\n            for tid, count in counts.items():\n                probs[tid] = count / total_visits\n            \n            # Value Target (V)\n            # Usamos el Q-value (valor esperado) del nodo como target para el Value Head.\n            # Q = value_sum / visit_count\n            v = node.value_sum / node.visit_count\n            \n            # State: node.tokens (lista de ids?)\n            # node.tokens is list of strings (from VOCABULARY).\n            # self_play.py expects tokens as strings in ReplayBuffer.add.\n            examples.append((node.tokens, probs, v))\n            \n        return examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile search/beam_search.py\n",
        "\"\"\"\nBeam Search for AlphaSymbolic.\nExplores multiple formula candidates in parallel, keeping top-K at each step.\n\"\"\"\nimport torch\nimport numpy as np\nfrom core.grammar import VOCABULARY, OPERATORS, TOKEN_TO_ID, ExpressionTree, OPERATOR_STAGES\nfrom utils.optimize_constants import optimize_constants\n\nclass BeamSearch:\n    def __init__(self, model, device, beam_width=10, max_length=30, curriculum_stage=None):\n        self.model = model\n        self.device = device\n        self.beam_width = beam_width\n        self.max_length = max_length\n        self.vocab_size = len(VOCABULARY)\n        self.sos_id = self.vocab_size  # SOS token ID\n        \n        # Build token mask based on curriculum stage\n        self.token_mask = None\n        if curriculum_stage is not None:\n            allowed_ops = OPERATOR_STAGES.get(curriculum_stage, list(OPERATORS.keys()))\n            allowed_tokens = set(['x', 'C', '0', '1', '2', '3', '5', '10', 'pi', 'e'])\n            allowed_tokens.update(allowed_ops)\n            \n            # Create mask: 0 for allowed, -inf for disallowed\n            mask = torch.full((self.vocab_size,), float('-inf'), device=device)\n            for token in allowed_tokens:\n                if token in TOKEN_TO_ID:\n                    mask[TOKEN_TO_ID[token]] = 0.0\n            self.token_mask = mask\n        \n    def search(self, x_values, y_values, return_partial=False):\n        \"\"\"\n        Beam Search to find the best formula structure.\n        \"\"\"\n        # Prepare data once\n        x_tensor = torch.tensor(x_values, dtype=torch.float32).unsqueeze(0).to(self.device) # [1, points]\n        y_tensor = torch.tensor(y_values, dtype=torch.float32).unsqueeze(0).to(self.device) # [1, points]\n        \n        # Each element in beams is just the sequence of tokens (list of strings)\n        # We track scores and open branches in parallel lists or a list of dicts\n        beams = [{'seq': [], 'log_prob': 0.0, 'open': 1}]\n        \n        completed = []\n        \n        for step in range(self.max_length):\n            if not beams:\n                break\n                \n            # Filter valid beams just in case\n            active_beams = [b for b in beams if b['open'] > 0]\n            if not active_beams:\n                break\n                \n            # Prepare batch for model\n            # Batch size = number of active beams\n            batch_size = len(active_beams)\n            \n            # Expand X and Y to match batch size [batch, points]\n            x_batch = x_tensor.expand(batch_size, -1)\n            y_batch = y_tensor.expand(batch_size, -1)\n            \n            # Prepare input sequences [batch, current_seq_len]\n            # Must prepend SOS token\n            seqs = [[self.sos_id] + [TOKEN_TO_ID[t] for t in b['seq']] for b in active_beams]\n            input_tensor = torch.tensor(seqs, dtype=torch.long).to(self.device)\n            \n            # Single model call for all beams\n            with torch.no_grad():\n                logits, _ = self.model(x_batch, y_batch, input_tensor)\n            \n            # Logits shape: [batch, seq_len, vocab_size]\n            # We want the last token's probabilities\n            last_token_logits = logits[:, -1, :self.vocab_size]\n            \n            # Apply curriculum mask if set\n            if self.token_mask is not None:\n                last_token_logits = last_token_logits + self.token_mask\n            \n            log_probs = torch.log_softmax(last_token_logits, dim=-1) # [batch, vocab]\n            \n            # --- Repetition Penalty (Simple) ---\n            # If the same token was generated recently, penalize it slightly.\n            # This prevents 10 ////////// loops.\n            penalty_factor = 2.0  # Reduce log_prob (which is negative) by absolute amount or multiplier?\n            # Log probs are negative (e.g. -0.1). Making them MORE negative penalizes.\n            # If we multiply by 1.2, -0.1 becomes -0.12 (lower probability).\n            \n            for i, beam in enumerate(active_beams):\n                if beam['seq']:\n                     # Get last token ID\n                    last_token = beam['seq'][-1]\n                    if last_token in TOKEN_TO_ID:\n                        last_id = TOKEN_TO_ID[last_token]\n                        # Penalize current step logits for this token\n                        # If log_prob is close to 0 (high prob), e.g. -0.01 -> -0.012\n                        # If log_prob is -10 (low prob), -> -12\n                        # Check bounds to avoid NaN if -inf\n                        if log_probs[i, last_id] > -1e9:\n                             log_probs[i, last_id] *= 1.5 \n            # -----------------------------------\n            \n            # We need to find the top-K candidates ACROSS current beams?\n            # Standard beam search: expand all, then prune to K\n            \n            all_candidates = []\n            \n            # Get top-K for EACH beam to avoid explosion (e.g. top 2*width)\n            k_per_beam = min(self.beam_width, self.vocab_size)\n            beam_topk_scores, beam_topk_indices = torch.topk(log_probs, k_per_beam, dim=-1)\n            \n            # Move to CPU for processing logic\n            beam_topk_scores = beam_topk_scores.cpu().numpy()\n            beam_topk_indices = beam_topk_indices.cpu().numpy()\n            \n            for i, beam in enumerate(active_beams):\n                for score, idx in zip(beam_topk_scores[i], beam_topk_indices[i]):\n                    token = VOCABULARY[idx]\n                    new_seq = beam['seq'] + [token]\n                    \n                    # Calculate new open branches\n                    if token in OPERATORS:\n                        new_open = beam['open'] + OPERATORS[token] - 1\n                    else:\n                        new_open = beam['open'] - 1\n                    \n                    if new_open < 0:\n                        continue\n                        \n                    all_candidates.append({\n                        'seq': new_seq,\n                        'log_prob': beam['log_prob'] + score,\n                        'open': new_open\n                    })\n            \n            # Global prune: keep top beam_width\n            all_candidates.sort(key=lambda x: x['log_prob'], reverse=True)\n            beams = all_candidates[:self.beam_width]\n            \n            # Check for completions\n            still_active = []\n            for b in beams:\n                if b['open'] == 0:\n                    completed.append(b)\n                else:\n                    still_active.append(b)\n            \n            beams = still_active\n            # If we filled up on completions, we might still want to explore? \n            # Usually we keep exploring until all beams are done or max length\n            if len(completed) >= self.beam_width:\n                 # Optional: early exit if we found enough good candidates\n                 pass\n\n        # Evaluate results\n        scored_results = []\n        for beam in completed:\n            tree = ExpressionTree(beam['seq'])\n            if tree.is_valid:\n                constants, rmse = optimize_constants(tree, x_values, y_values)\n                scored_results.append({\n                    'tokens': beam['seq'],\n                    'log_prob': beam['log_prob'],\n                    'rmse': rmse,\n                    'constants': constants,\n                    'formula': tree.get_infix()\n                })\n        \n        scored_results.sort(key=lambda x: x['rmse'])\n        \n        # If no results and return_partial is requested, return the best incomplete beam\n        if not scored_results and return_partial and beams:\n            # Take the beam with highest probability\n            best_beam = beams[0] \n            # Construct a partial result\n            # We can't optimize constants or get a valid infix easily, but we can show tokens\n            scored_results.append({\n                'tokens': best_beam['seq'],\n                'log_prob': best_beam['log_prob'],\n                'rmse': float('inf'),\n                'constants': {},\n                'formula': \"Partial: \" + \" \".join(best_beam['seq']) + \"...\"\n            })\n            \n        return scored_results\n\n\ndef beam_solve(target_x, target_y, model, device, beam_width=20, max_length=25):\n    \"\"\"\n    Solve symbolic regression using beam search.\n    \"\"\"\n    searcher = BeamSearch(model, device, beam_width=beam_width, max_length=max_length)\n    results = searcher.search(target_x, target_y)\n    \n    if not results:\n        return None\n        \n    return results  # Return all results for Pareto analysis\n\n\nif __name__ == \"__main__\":\n    from core.model import AlphaSymbolicModel\n    \n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    VOCAB_SIZE = len(VOCABULARY)\n    \n    model = AlphaSymbolicModel(vocab_size=VOCAB_SIZE + 1, d_model=64).to(DEVICE)\n    try:\n        model.load_state_dict(torch.load(\"alpha_symbolic_model.pth\", map_location=DEVICE, weights_only=True))\n    except:\n        print(\"Model not found, using random weights\")\n    model.eval()\n    \n    # Test\n    x_test = np.linspace(-5, 5, 20).astype(np.float64)\n    y_test = 2 * x_test + 3\n    \n    print(\"Running Beam Search...\")\n    results = beam_solve(x_test, y_test, model, DEVICE, beam_width=10)\n    \n    print(f\"\\nFound {len(results)} valid formulas:\")\n    for i, r in enumerate(results[:5]):\n        print(f\"  {i+1}. {r['formula']} (RMSE: {r['rmse']:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile search/hybrid_search.py\n",
        "import time\nimport torch\nimport numpy as np\nfrom typing import List, Dict, Any, Optional\n\nfrom core.gp_bridge import GPEngine\nfrom search.beam_search import BeamSearch, beam_solve\n\ndef hybrid_solve(\n    x_values: np.ndarray,\n    y_values: np.ndarray,\n    model: torch.nn.Module,\n    device: torch.device,\n    beam_width: int = 50,\n    gp_timeout: int = 10,\n    gp_binary_path: Optional[str] = None\n) -> Dict[str, Any]:\n    \"\"\"\n    Solves Symbolic Regression using a Hybrid Neuro-Evolutionary approach.\n    \n    Phase 1: Neural Beam Search (The Brain)\n             - Rapidly scans the search space.\n             - Generates diverse, high-likelihood formula skeletons.\n             \n    Phase 2: Genetic Programming Refinement (The Muscle)\n             - Takes the best skeletons from Phase 1.\n             - Uses GPU-accelerated evolution to optimize constants and structure.\n             - Runs for `gp_timeout` seconds.\n             \n    Returns:\n        Best found formula result dict.\n    \"\"\"\n    \n    print(f\"--- Starting Alpha-GP Hybrid Search ---\")\n    start_time = time.time()\n    \n    # 1. Neural Beam Search (Phase 1)\n    print(f\"[Phase 1] Neural Beam Search (Width={beam_width})...\")\n    # We use a larger beam width to ensure diversity for the GP\n    # If the user requests beam_width=X, we might want to multiply it for the \"seeds\"\n    # But let's stick to what is passed.\n    \n    neural_results = beam_solve(x_values, y_values, model, device, beam_width=beam_width)\n    \n    seeds = []\n    if neural_results:\n        print(f\"[Phase 1] Found {len(neural_results)} candidates.\")\n        # Extract formulas tokens/string\n        # neural_results is a list of dicts with 'formula' key (infix string)\n        # GPEngine expects infix strings (e.g. \"((x*x)+2)\")\n        \n        # Filter for uniqueness and validity\n        seen_formulas = set()\n        for res in neural_results:\n            f_str = res['formula']\n            # Basic validation: must verify it's not a Partial result\n            if f_str.startswith(\"Partial\"): continue\n            \n            if f_str not in seen_formulas:\n                seeds.append(f_str)\n                seen_formulas.add(f_str)\n        \n        print(f\"[Phase 1] Generated {len(seeds)} unique seeds for GP.\")\n        if len(seeds) > 0:\n            print(f\"Top Seed: {seeds[0]}\")\n    else:\n        print(\"[Phase 1] No valid candidates found (Beam Search failed).\")\n        print(\"[Phase 1] Falling back to pure GP (Random Initialization).\")\n        seeds = []\n\n    # 2. GP Refinement (Phase 2)\n    print(f\"[Phase 2] GPU Genetic Refinement (Timeout={gp_timeout}s)...\")\n    gp_engine = GPEngine(binary_path=gp_binary_path)\n    \n    # Run GP\n    # We pass the seeds. GP engine handles the rest.\n    # Ensure x_values and y_values are lists for gp_engine\n    x_list = x_values.tolist() if hasattr(x_values, 'tolist') else list(x_values)\n    y_list = y_values.tolist() if hasattr(y_values, 'tolist') else list(y_values)\n    gp_result_str = gp_engine.run(x_list, y_list, seeds, timeout_sec=gp_timeout)\n    \n    total_time = time.time() - start_time\n    \n    if gp_result_str:\n        print(f\"--- Hybrid Search Completed in {total_time:.2f}s ---\")\n        print(f\"Best Formula: {gp_result_str}\")\n        \n        # Construct a result dict similar to Beam Search for consistency\n        # Ideally we would evaluate it here to get RMSE, but GP output doesn't give us RMSE directly in a structured way (only stdout).\n        # We can implement a quick evaluator if needed, or assume the user trusts the string.\n        # For UI display, we probably want RMSE.\n        \n        return {\n            'formula': gp_result_str,\n            'rmse': 0.0, # Placeholder, will be evaluated by UI if needed or we can do it here\n            'source': 'Alpha-GP Hybrid',\n            'time': total_time\n        }\n    else:\n        print(f\"--- Hybrid Search Failed (GP did not return valid result) ---\")\n        return None\n\nif __name__ == \"__main__\":\n    # Test\n    # Mock Model\n    class MockModel(torch.nn.Module):\n        def forward(self, x, y, seq):\n            # Return random logits\n            bs, seq_len = seq.shape\n            vocab = 20\n            return torch.randn(bs, seq_len, vocab), None\n\n    print(\"Testing Hybrid Search...\")\n    x = np.linspace(-5, 5, 10)\n    y = x**2\n    try:\n        res = hybrid_solve(x, y, MockModel(), torch.device(\"cpu\"), beam_width=5)\n        print(res)\n    except Exception as e:\n        print(f\"Test failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile search/pareto.py\n",
        "\"\"\"\nPareto Front Manager for AlphaSymbolic.\nMaintains a set of non-dominated solutions (accuracy vs complexity).\n\"\"\"\nimport numpy as np\nfrom core.grammar import ExpressionTree\n\nclass ParetoSolution:\n    def __init__(self, tokens, rmse, complexity, formula_str, constants=None):\n        self.tokens = tokens\n        self.rmse = rmse  # Lower is better\n        self.complexity = complexity  # Lower is better (number of nodes)\n        self.formula = formula_str\n        self.constants = constants or {}\n        \n    def dominates(self, other):\n        \"\"\"Returns True if self dominates other (better in all objectives).\"\"\"\n        # Self dominates other if:\n        # - Self is at least as good in all objectives\n        # - Self is strictly better in at least one objective\n        at_least_as_good = (self.rmse <= other.rmse) and (self.complexity <= other.complexity)\n        strictly_better = (self.rmse < other.rmse) or (self.complexity < other.complexity)\n        return at_least_as_good and strictly_better\n    \n    def __repr__(self):\n        return f\"ParetoSolution(rmse={self.rmse:.4f}, complexity={self.complexity}, formula='{self.formula}')\"\n\n\nclass ParetoFront:\n    def __init__(self, max_size=50):\n        self.solutions = []\n        self.max_size = max_size\n        \n    def add(self, solution):\n        \"\"\"\n        Attempts to add a solution to the Pareto front.\n        Returns True if added, False if dominated.\n        \"\"\"\n        # Check if new solution is dominated by any existing\n        for existing in self.solutions:\n            if existing.dominates(solution):\n                return False  # New solution is dominated\n        \n        # Remove any solutions dominated by the new one\n        self.solutions = [s for s in self.solutions if not solution.dominates(s)]\n        \n        # Add the new solution\n        self.solutions.append(solution)\n        \n        # Enforce max size by removing worst solutions\n        if len(self.solutions) > self.max_size:\n            # Sort by a combined score and keep top max_size\n            self.solutions.sort(key=lambda s: s.rmse + 0.01 * s.complexity)\n            self.solutions = self.solutions[:self.max_size]\n        \n        return True\n    \n    def add_from_results(self, results_list):\n        \"\"\"\n        Add multiple results from beam search or MCTS.\n        results_list: list of dicts with 'tokens', 'rmse', 'constants', 'formula'\n        \"\"\"\n        added = 0\n        for r in results_list:\n            tree = ExpressionTree(r['tokens'])\n            complexity = len(r['tokens'])  # Simple complexity = token count\n            \n            sol = ParetoSolution(\n                tokens=r['tokens'],\n                rmse=r['rmse'],\n                complexity=complexity,\n                formula_str=r['formula'],\n                constants=r.get('constants', {})\n            )\n            \n            if self.add(sol):\n                added += 1\n        \n        return added\n    \n    def get_best_by_rmse(self):\n        \"\"\"Returns the solution with lowest RMSE.\"\"\"\n        if not self.solutions:\n            return None\n        return min(self.solutions, key=lambda s: s.rmse)\n    \n    def get_simplest(self):\n        \"\"\"Returns the solution with lowest complexity.\"\"\"\n        if not self.solutions:\n            return None\n        return min(self.solutions, key=lambda s: s.complexity)\n    \n    def get_balanced(self, alpha=0.5):\n        \"\"\"\n        Returns a balanced solution.\n        alpha: weight for RMSE (1-alpha for complexity)\n        \"\"\"\n        if not self.solutions:\n            return None\n        \n        # Normalize scores\n        rmse_vals = [s.rmse for s in self.solutions]\n        comp_vals = [s.complexity for s in self.solutions]\n        \n        min_rmse, max_rmse = min(rmse_vals), max(rmse_vals) + 1e-10\n        min_comp, max_comp = min(comp_vals), max(comp_vals) + 1e-10\n        \n        def score(s):\n            norm_rmse = (s.rmse - min_rmse) / (max_rmse - min_rmse)\n            norm_comp = (s.complexity - min_comp) / (max_comp - min_comp)\n            return alpha * norm_rmse + (1 - alpha) * norm_comp\n        \n        return min(self.solutions, key=score)\n    \n    def summary(self):\n        \"\"\"Print a summary of the Pareto front.\"\"\"\n        print(f\"\\n=== Pareto Front ({len(self.solutions)} solutions) ===\")\n        for i, sol in enumerate(sorted(self.solutions, key=lambda s: s.rmse)[:10]):\n            print(f\"  {i+1}. RMSE={sol.rmse:.6f}, Nodes={sol.complexity}, Formula: {sol.formula}\")\n\n\n# Quick test\nif __name__ == \"__main__\":\n    front = ParetoFront()\n    \n    # Add some test solutions\n    solutions = [\n        ParetoSolution(['x'], 10.0, 1, \"x\"),\n        ParetoSolution(['+', 'x', '1'], 5.0, 3, \"(x + 1)\"),\n        ParetoSolution(['*', '2', 'x'], 3.0, 3, \"(2 * x)\"),\n        ParetoSolution(['+', '*', '2', 'x', '3'], 0.5, 5, \"((2 * x) + 3)\"),\n        ParetoSolution(['+', '*', '*', '2', 'x', 'x', '+', 'x', '1'], 0.1, 9, \"complicated\"),\n    ]\n    \n    for sol in solutions:\n        added = front.add(sol)\n        print(f\"Added {sol.formula}: {added}\")\n    \n    front.summary()\n    \n    print(f\"\\nBest by RMSE: {front.get_best_by_rmse()}\")\n    print(f\"Simplest: {front.get_simplest()}\")\n    print(f\"Balanced: {front.get_balanced()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile search/__init__.py\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile ui/app_core.py\n",
        "\"\"\"\nCore state and model management for AlphaSymbolic Gradio App.\n\"\"\"\nimport torch\nimport os\nfrom core.model import AlphaSymbolicModel\nfrom core.grammar import VOCABULARY\n\nfrom collections import deque\nimport time\n\n# Global state\nMODEL = None\nDEVICE = None\nTRAINING_STATUS = {\"running\": False, \"epoch\": 0, \"loss\": 0, \"message\": \"Listo\"}\nSTOP_TRAINING = False  # Flag to request training stop\n\ndef request_stop_training():\n    \"\"\"Request training to stop gracefully.\"\"\"\n    global STOP_TRAINING\n    STOP_TRAINING = True\n    return \"\u23f9\ufe0f Deteniendo entrenamiento...\"\n\ndef should_stop_training():\n    \"\"\"Check if training should stop.\"\"\"\n    return STOP_TRAINING\n\ndef reset_stop_flag():\n    \"\"\"Reset the stop flag (call at start of training).\"\"\"\n    global STOP_TRAINING\n    STOP_TRAINING = False\n\n# Hall of Shame: Rolling buffer of recent failures\n# Format: {'time': str, 'target': str, 'predicted': str, 'loss': float, 'stage': str}\nTRAINING_ERRORS = deque(maxlen=20)\n\ndef add_training_error(target, predicted, loss, stage):\n    \"\"\"Add an error to the Hall of Shame.\"\"\"\n    TRAINING_ERRORS.append({\n        'time': time.strftime(\"%H:%M:%S\"),\n        'target': target,\n        'predicted': predicted,\n        'loss': float(loss),\n        'stage': stage\n    })\n\ndef get_training_errors():\n    \"\"\"Get list of errors for the UI.\"\"\"\n    return list(TRAINING_ERRORS)\n\nMODEL_PRESETS = {\n    'lite': {'d_model': 128, 'nhead': 4, 'num_encoder_layers': 3, 'num_decoder_layers': 3},\n    'pro': {'d_model': 256, 'nhead': 8, 'num_encoder_layers': 6, 'num_decoder_layers': 6}\n}\nCURRENT_PRESET = 'lite'\n\ndef get_device(force_cpu=False):\n    \"\"\"Get the best available device (CUDA > MPS > CPU).\"\"\"\n    if force_cpu:\n        return torch.device(\"cpu\")\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    return torch.device(\"cpu\")\n\ndef set_device(use_gpu=True):\n    \"\"\"Set the device (GPU or CPU).\"\"\"\n    global DEVICE, MODEL\n    new_device = get_device(force_cpu=not use_gpu)\n    \n    if MODEL is not None and DEVICE != new_device:\n        MODEL = MODEL.to(new_device)\n    \n    DEVICE = new_device\n    return get_device_info()\n\ndef get_device_info():\n    \"\"\"Get device info string.\"\"\"\n    global DEVICE\n    if DEVICE is None:\n        DEVICE = get_device()\n    \n    if DEVICE.type == \"cuda\":\n        return f\"CUDA ({torch.cuda.get_device_name(0)})\"\n    elif DEVICE.type == \"mps\":\n        return \"MPS (Apple Silicon)\"\n    else:\n        return \"CPU\"\n\ndef load_model(force_reload=False, preset_name=None):\n    \"\"\"Load or reload the model.\"\"\"\n    global MODEL, DEVICE, CURRENT_PRESET\n    \n    if preset_name:\n        CURRENT_PRESET = preset_name\n    \n    if DEVICE is None:\n        DEVICE = get_device()\n    \n    VOCAB_SIZE = len(VOCABULARY)\n    config = MODEL_PRESETS[CURRENT_PRESET]\n    \n    print(f\"Loading Model [{CURRENT_PRESET.upper()}]...\")\n    MODEL = AlphaSymbolicModel(\n        vocab_size=VOCAB_SIZE + 1, \n        d_model=config['d_model'], \n        nhead=config['nhead'],\n        num_encoder_layers=config['num_encoder_layers'], \n        num_decoder_layers=config['num_decoder_layers']\n    ).to(DEVICE)\n    \n    filename = f\"alpha_symbolic_model_{CURRENT_PRESET}.pth\"\n    status = f\"Nuevo modelo ({CURRENT_PRESET})\" # Default status\n    \n    if os.path.exists(filename):\n        try:\n            state_dict = torch.load(filename, map_location=DEVICE, weights_only=True)\n            \n            # Check for NaNs\n            has_nans = False\n            for k, v in state_dict.items():\n                if torch.isnan(v).any() or torch.isinf(v).any():\n                    has_nans = True\n                    break\n            \n            if has_nans:\n                print(f\"\u26a0\ufe0f Modelo corrupto detectado (NaNs) en {filename}. Eliminando y esperando reinicio.\")\n                try:\n                    os.remove(filename)\n                    print(\"\u2705 Archivo corrupto eliminado.\")\n                except OSError as e:\n                    print(f\"Error al eliminar archivo: {e}\")\n                status = \"\u26a0\ufe0f Modelo corrupto eliminado y reiniciado\"\n            else:\n                MODEL.load_state_dict(state_dict)\n                MODEL.eval()\n                status = f\"Modelo cargado ({CURRENT_PRESET})\"\n                \n        except RuntimeError as e:\n            print(f\"\u26a0\ufe0f Error de compatibilidad ({e}). Iniciando modelo fresco.\")\n            status = f\"Nuevo modelo ({CURRENT_PRESET})\"\n        except Exception as e:\n            print(f\"Error cargando: {e}\")\n            status = \"Sin modelo pre-entrenado\"\n    \n    return status, get_device_info()\n\ndef get_model():\n    \"\"\"Get the current model, loading if needed.\"\"\"\n    global MODEL, DEVICE\n    if MODEL is None:\n        load_model()\n    return MODEL, DEVICE\n\ndef save_model():\n    \"\"\"Save the current model.\"\"\"\n    global MODEL, CURRENT_PRESET\n    if MODEL is not None:\n        filename = f\"alpha_symbolic_model_{CURRENT_PRESET}.pth\"\n        torch.save(MODEL.state_dict(), filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile ui/app_search.py\n",
        "\"\"\"\nSearch/Solve functions for AlphaSymbolic Gradio App.\nSupports both Beam Search and MCTS.\n\"\"\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport gradio as gr\n\nfrom core.grammar import ExpressionTree\nfrom search.beam_search import BeamSearch\nfrom search.mcts import MCTS\nfrom search.hybrid_search import hybrid_solve\nfrom utils.simplify import simplify_tree\nfrom search.pareto import ParetoFront\nfrom utils.detect_pattern import detect_pattern\nfrom utils.optimize_constants import optimize_constants, substitute_constants\nfrom ui.app_core import get_model\n\n\ndef parse_data(x_str, y_str):\n    \"\"\"Parse comma-separated input strings.\"\"\"\n    try:\n        x = np.array([float(v.strip()) for v in x_str.split(',')], dtype=np.float64)\n        y = np.array([float(v.strip()) for v in y_str.split(',')], dtype=np.float64)\n        if len(x) != len(y):\n            return None, None, \"Error: X e Y deben tener igual longitud\"\n        return x, y, None\n    except Exception as e:\n        return None, None, f\"Error: {str(e)}\"\n\n\ndef create_fit_plot(x, y, y_pred, formula):\n    \"\"\"Create a plot showing data vs prediction.\"\"\"\n    fig, ax = plt.subplots(figsize=(8, 5), facecolor='#1a1a2e')\n    ax.set_facecolor('#1a1a2e')\n    \n    ax.scatter(x, y, color='#00d4ff', s=100, label='Datos Reales', zorder=3, edgecolors='white', linewidth=1)\n    \n    sort_idx = np.argsort(x)\n    ax.plot(x[sort_idx], y_pred[sort_idx], color='#ff6b6b', linewidth=3, label='Prediccion', zorder=2)\n    \n    ax.set_xlabel('X', color='white', fontsize=12)\n    ax.set_ylabel('Y', color='white', fontsize=12)\n    ax.set_title('Ajuste de la Formula', color='white', fontsize=14, fontweight='bold')\n    ax.legend(facecolor='#16213e', edgecolor='#00d4ff', labelcolor='white')\n    ax.tick_params(colors='white')\n    ax.grid(True, alpha=0.2, color='white')\n    \n    for spine in ax.spines.values():\n        spine.set_color('#00d4ff')\n    \n    plt.tight_layout()\n    return fig\n\n\ndef solve_formula(x_str, y_str, beam_width, search_method, progress=gr.Progress()):\n    \"\"\"Main solving function with search method selection.\"\"\"\n    x, y, error = parse_data(x_str, y_str)\n    if error:\n        return error, None, \"\", \"\", \"\"\n    \n    MODEL, DEVICE = get_model()\n    \n    progress(0.1, desc=f\"Analizando patron... [{DEVICE.type.upper()}]\")\n    pattern = detect_pattern(x, y)\n    \n    progress(0.3, desc=f\"Buscando formulas ({search_method})... [{DEVICE.type.upper()}]\")\n    start_time = time.time()\n    \n    results = []\n    \n    if search_method == \"Alpha-GP Hybrid\":\n        # Using hybrid search\n        progress(0.4, desc=\"Fase 1: Neural Beam Search...\")\n        # Note: Hybrid search handles its own phases printing, but we want UI updates.\n        # We pass beam_width. gp_timeout is increased to 30s to allow convergence on complex problems.\n        hybrid_res = hybrid_solve(x, y, MODEL, DEVICE, beam_width=int(beam_width), gp_timeout=30)\n        \n        if hybrid_res:\n            progress(0.9, desc=\"Procesando resultados GP...\")\n            # Convert infix string back to tokens for consistency\n            tree = ExpressionTree.from_infix(hybrid_res['formula'])\n            if tree.is_valid:\n                 # Evaluate RMSE roughly (GP result should be good, but let's confirm)\n                 # Optimization is already done by GP, but we might want to fine-tune \n                 # or at least extract constants if they are numbers in the string.\n                 # The string from GP has numbers like 2.345 embedded.\n                 # optimize_constants expects a tree with 'C' placeholders if we want to re-optimize.\n                 # But GP output is fully instantiated.\n                 # So we just evaluate.\n                 \n                 y_pred_check = tree.evaluate(x)\n                 rmse_check = np.sqrt(np.mean((y_pred_check - y)**2))\n                 \n                 results = [{\n                     'tokens': tree.tokens,\n                     'formula': tree.get_infix(),\n                     'rmse': rmse_check,\n                     'constants': {} # Constants are baked into the formula string\n                 }]\n    \n    elif search_method == \"Beam Search\":\n        searcher = BeamSearch(MODEL, DEVICE, beam_width=int(beam_width), max_length=25)\n        results = searcher.search(x, y)\n    else:  # MCTS\n        mcts = MCTS(MODEL, DEVICE, max_simulations=int(beam_width) * 10)\n        result = mcts.search(x, y)\n        if result and result.get('tokens'):\n            tokens = result['tokens']\n            tree = ExpressionTree(tokens)\n            if tree.is_valid:\n                constants, rmse = optimize_constants(tree, x, y)\n                results = [{\n                    'tokens': tokens,\n                    'formula': tree.get_infix(),\n                    'rmse': rmse,\n                    'constants': constants\n                }]\n    \n    search_time = time.time() - start_time\n    \n    if not results:\n        return \"No se encontraron formulas validas\", None, \"\", \"\", \"\"\n    \n    progress(0.7, desc=\"Optimizando constantes...\")\n    pareto = ParetoFront()\n    pareto.add_from_results(results)\n    best = pareto.get_best_by_rmse()\n    \n    if not best:\n        return \"Error en optimizacion\", None, \"\", \"\", \"\"\n    \n    progress(0.9, desc=\"Simplificando...\")\n    tree = ExpressionTree(best.tokens)\n    simplified = simplify_tree(tree)\n    y_pred = tree.evaluate(x, constants=best.constants)\n    \n    # Substitute constants for display\n    substituted_formula = simplified\n    if best.constants:\n        try:\n            positions = tree.root.get_constant_positions()\n            # We use the raw infix for substitution to ensure matching C positions\n            raw_infix = tree.get_infix()\n            substituted_formula = substitute_constants(raw_infix, best.constants, positions)\n        except:\n            substituted_formula = simplified\n    \n    fig = create_fit_plot(x, y, y_pred, simplified)\n    \n    # Format results\n    result_html = f\"\"\"\n    <div style=\"background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); padding: 20px; border-radius: 15px; border: 2px solid #00d4ff;\">\n        <h2 style=\"color: #00d4ff; margin: 0; font-size: 24px;\">Formula Encontrada</h2>\n        <div style=\"background: #0f0f23; padding: 15px; border-radius: 10px; margin: 15px 0; border-left: 4px solid #ff6b6b;\">\n            <code style=\"color: #ff6b6b; font-size: 28px; font-weight: bold;\">{substituted_formula}</code>\n        </div>\n        <div style=\"display: grid; grid-template-columns: repeat(4, 1fr); gap: 10px;\">\n            <div style=\"background: #0f0f23; padding: 10px; border-radius: 8px; text-align: center;\">\n                <span style=\"color: #888;\">RMSE</span><br>\n                <span style=\"color: #00d4ff; font-size: 16px; font-weight: bold;\">{best.rmse:.6f}</span>\n            </div>\n            <div style=\"background: #0f0f23; padding: 10px; border-radius: 8px; text-align: center;\">\n                <span style=\"color: #888;\">Nodos</span><br>\n                <span style=\"color: #00d4ff; font-size: 16px; font-weight: bold;\">{best.complexity}</span>\n            </div>\n            <div style=\"background: #0f0f23; padding: 10px; border-radius: 8px; text-align: center;\">\n                <span style=\"color: #888;\">Tiempo</span><br>\n                <span style=\"color: #00d4ff; font-size: 16px; font-weight: bold;\">{search_time:.2f}s</span>\n            </div>\n            <div style=\"background: #0f0f23; padding: 10px; border-radius: 8px; text-align: center;\">\n                <span style=\"color: #888;\">Metodo</span><br>\n                <span style=\"color: #4ade80; font-size: 16px; font-weight: bold;\">{search_method}</span>\n            </div>\n        </div>\n        <div style=\"margin-top: 15px; padding: 10px; background: #0f0f23; border-radius: 8px;\">\n            <span style=\"color: #888;\">Patron:</span> \n            <span style=\"color: #ffd93d;\">{pattern['type']}</span> \n            <span style=\"color: #666;\">({pattern['confidence']:.0%})</span>\n            <span style=\"color: #888; margin-left: 20px;\">Device:</span>\n            <span style=\"color: #4ade80;\">{DEVICE.type.upper()}</span>\n        </div>\n    \"\"\"\n    \n    # Add constants if any\n    # Add constants if any\n    if best.constants:\n        # Sort and format cleanly\n        sorted_items = sorted(best.constants.items(), key=lambda x: str(x[0]))\n        clean_consts = []\n        for i, (k, v) in enumerate(sorted_items):\n            clean_consts.append(f\"C_{i+1}: {v:.4f}\")\n        const_str = \"  |  \".join(clean_consts)\n        \n        result_html += f\"\"\"\n        <div style=\"margin-top: 10px; padding: 10px; background: #0f0f23; border-radius: 8px; border-left: 3px solid #ffd93d;\">\n            <span style=\"color: #888;\">Constantes:</span>\n            <span style=\"color: #fff; font-family: monospace; margin-left: 10px;\">{const_str}</span>\n        </div>\n        \"\"\"\n        \n    result_html += \"</div>\"\n    \n    # Predictions table\n    pred_html = '<table style=\"width: 100%; border-collapse: collapse; background: #1a1a2e; border-radius: 10px; overflow: hidden;\">'\n    pred_html += '<tr style=\"background: #16213e;\"><th style=\"padding: 10px; color: #00d4ff;\">X</th><th style=\"color: #00d4ff;\">Pred</th><th style=\"color: #00d4ff;\">Real</th><th style=\"color: #00d4ff;\">Delta</th></tr>'\n    for i in range(min(50, len(x))):\n        delta = abs(y_pred[i] - y[i])\n        color = \"#4ade80\" if delta < 0.1 else \"#fbbf24\" if delta < 1 else \"#ef4444\"\n        pred_html += f'<tr style=\"border-bottom: 1px solid #333;\"><td style=\"padding: 8px; color: white; text-align: center;\">{x[i]:.2f}</td><td style=\"color: white; text-align: center;\">{y_pred[i]:.4f}</td><td style=\"color: white; text-align: center;\">{y[i]:.4f}</td><td style=\"color: {color}; text-align: center; font-weight: bold;\">{delta:.4f}</td></tr>'\n    pred_html += '</table>'\n    \n    # Alternatives\n    alt_html = '<div style=\"background: #1a1a2e; padding: 15px; border-radius: 10px;\">'\n    alt_html += '<h4 style=\"color: #00d4ff; margin-top: 0;\">Alternativas</h4>'\n    for i, sol in enumerate(pareto.solutions[:4]):\n        alt_html += f'<div style=\"padding: 5px 10px; margin: 5px 0; background: #0f0f23; border-radius: 5px; border-left: 3px solid {\"#00d4ff\" if i == 0 else \"#666\"};\"><code style=\"color: {\"#ff6b6b\" if i == 0 else \"#888\"};\">{sol.formula}</code> <span style=\"color: #666; font-size: 12px;\">RMSE: {sol.rmse:.4f}</span></div>'\n    alt_html += '</div>'\n    \n    return result_html, fig, pred_html, alt_html, simplified\n\n\ndef generate_example(tipo):\n    \"\"\"Generate example data.\"\"\"\n    if tipo == \"lineal\":\n        x = np.linspace(1, 10, 10)\n        y = 2 * x + 3\n    elif tipo == \"cuadratico\":\n        x = np.linspace(-5, 5, 11)\n        y = x**2 + 1\n    elif tipo == \"trig\":\n        x = np.linspace(0, 6.28, 20)\n        y = np.sin(x)\n    elif tipo == \"exp\":\n        x = np.linspace(0, 5, 15)\n        y = 2 * np.exp(0.5 * x)\n    else:\n        x = np.linspace(1, 10, 10)\n        y = 2 * x + 3\n    \n    return \", \".join([f\"{v:.2f}\" for v in x]), \", \".join([f\"{v:.4f}\" for v in y])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile ui/app_training.py\n",
        "\"\"\"\nTraining functions for AlphaSymbolic Gradio App.\nWith proper data normalization.\n\"\"\"\nimport os\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gradio as gr\nfrom collections import deque\nimport random\nimport time\n\nfrom core.grammar import VOCABULARY, TOKEN_TO_ID, OPERATORS, OPERATOR_STAGES\nfrom data.synthetic_data import DataGenerator\nfrom ui.app_core import get_model, save_model, TRAINING_STATUS, add_training_error, should_stop_training, reset_stop_flag\nfrom core.loss import QuantileLoss\nfrom search.hybrid_search import hybrid_solve\nfrom core.grammar import ExpressionTree, simplify_formula\n\n\ndef get_allowed_token_mask(stage, vocab_size, device):\n    \"\"\"\n    Creates a mask tensor for token logits.\n    Allowed tokens = 1.0, Disallowed = 0.0 (for multiplication mask)\n    Or returns indices of allowed tokens for -inf masking.\n    \"\"\"\n    allowed_ops = OPERATOR_STAGES.get(stage, list(OPERATORS.keys()))\n    \n    # All terminals are always allowed\n    allowed_tokens = set(['x', 'C', '0', '1', '2', '3', '5', '10', 'pi', 'e'])\n    allowed_tokens.update(allowed_ops)\n    \n    # Build mask\n    mask = torch.zeros(vocab_size + 1, device=device)  # +1 for SOS token\n    for token in allowed_tokens:\n        if token in TOKEN_TO_ID:\n            mask[TOKEN_TO_ID[token]] = 1.0\n    mask[vocab_size] = 1.0  # SOS always allowed\n    \n    return mask\n\n\ndef normalize_batch(x_list, y_list):\n    \"\"\"Normalize X and Y values to prevent numerical instability.\"\"\"\n    normalized_x = []\n    normalized_y = []\n    \n    for x, y in zip(x_list, y_list):\n        # Normalize X to [-1, 1]\n        x_min, x_max = x.min(), x.max()\n        if x_max - x_min > 1e-6:\n            x_norm = 2 * (x - x_min) / (x_max - x_min) - 1\n        else:\n            x_norm = np.zeros_like(x)\n        \n        # Normalize Y to [-1, 1] \n        y_min, y_max = y.min(), y.max()\n        if y_max - y_min > 1e-6:\n            y_norm = 2 * (y - y_min) / (y_max - y_min) - 1\n        else:\n            y_norm = np.zeros_like(y)\n        \n        normalized_x.append(x_norm)\n        normalized_y.append(y_norm)\n    \n    return normalized_x, normalized_y\n\n\ndef train_basic(epochs, batch_size, point_count=10, progress=gr.Progress()):\n    \"\"\"Basic training with synthetic data.\"\"\"\n    global TRAINING_STATUS\n    \n    if TRAINING_STATUS[\"running\"]:\n        return \"Entrenamiento ya en progreso\", None\n    \n    TRAINING_STATUS[\"running\"] = True\n    \n    try:\n        MODEL, DEVICE = get_model()\n        \n        MODEL.train()\n        optimizer = torch.optim.AdamW(MODEL.parameters(), lr=1e-4, weight_decay=0.01)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(epochs), eta_min=1e-6)\n        ce_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n        \n        VOCAB_SIZE = len(VOCABULARY)\n        SOS_ID = VOCAB_SIZE\n        \n        data_gen = DataGenerator(max_depth=4)\n        losses = []\n        \n        for epoch in range(int(epochs)):\n            progress((epoch + 1) / epochs, desc=f\"Epoca {epoch+1}/{int(epochs)} [{DEVICE.type.upper()}]\")\n            \n            # Mix of inverse (known formulas) + random data (AlphaTensor-style)\n            half_batch = int(batch_size) // 2\n            batch_inverse = data_gen.generate_inverse_batch(half_batch, point_count=int(point_count))\n            batch_random = data_gen.generate_batch(int(batch_size) - half_batch, point_count=int(point_count))\n            batch = batch_inverse + batch_random\n            if len(batch) < 2:\n                continue\n            \n            x_list = [d['x'] for d in batch]\n            y_list = [d['y'] for d in batch]\n            \n            # Normalize data\n            x_list, y_list = normalize_batch(x_list, y_list)\n            \n            token_lists = [[TOKEN_TO_ID[t] for t in d['tokens']] for d in batch]\n            \n            max_len = max(len(s) for s in token_lists)\n            decoder_input = torch.full((len(batch), max_len + 1), SOS_ID, dtype=torch.long)\n            targets = torch.full((len(batch), max_len + 1), -1, dtype=torch.long)\n            \n            for i, seq in enumerate(token_lists):\n                decoder_input[i, 1:len(seq)+1] = torch.tensor(seq, dtype=torch.long)\n                targets[i, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n            \n            x_tensor = torch.tensor(np.array(x_list), dtype=torch.float32).to(DEVICE)\n            y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n            decoder_input = decoder_input.to(DEVICE)\n            targets = targets.to(DEVICE)\n            \n            # Forward\n            optimizer.zero_grad()\n            logits, _ = MODEL(x_tensor, y_tensor, decoder_input)\n            loss = ce_loss(logits.view(-1, VOCAB_SIZE + 1), targets.view(-1))\n            \n            # Skip if loss is NaN\n            if torch.isnan(loss) or torch.isinf(loss):\n                continue\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(MODEL.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            \n            losses.append(loss.item())\n        \n        save_model()\n        MODEL.eval()\n        TRAINING_STATUS[\"running\"] = False\n        \n        if not losses:\n            return \"Error: No se pudo calcular loss (revisar datos)\", None\n        \n        fig = create_loss_plot(losses, \"Entrenamiento Basico\")\n        \n        result = f\"\"\"\n        <div style=\"background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); padding: 20px; border-radius: 15px; border: 2px solid #4ade80;\">\n            <h2 style=\"color: #4ade80; margin: 0;\">Entrenamiento Completado</h2>\n            <p style=\"color: white;\">Epocas: {int(epochs)} | Loss Final: {losses[-1]:.4f}</p>\n            <p style=\"color: #00d4ff;\">Dispositivo: {DEVICE.type.upper()}</p>\n        </div>\n        \"\"\"\n        return result, fig\n        \n    except Exception as e:\n        TRAINING_STATUS[\"running\"] = False\n        return f\"Error: {str(e)}\", None\n\n\ndef train_curriculum(epochs, batch_size, point_count=10, progress=gr.Progress()):\n    \"\"\"Curriculum Learning - starts simple, increases difficulty gradually.\"\"\"\n    global TRAINING_STATUS\n    \n    if TRAINING_STATUS[\"running\"]:\n        return \"Entrenamiento ya en progreso\", None\n    \n    TRAINING_STATUS[\"running\"] = True\n    \n    try:\n        MODEL, DEVICE = get_model()\n        \n        MODEL.train()\n        optimizer = torch.optim.AdamW(MODEL.parameters(), lr=5e-5, weight_decay=0.01)  # Lower LR\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2)\n        ce_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n        \n        VOCAB_SIZE = len(VOCABULARY)\n        SOS_ID = VOCAB_SIZE\n        losses = []\n        \n        for epoch in range(int(epochs)):\n            # Curriculum: slow progression\n            # Stage 1 (0-50%): depth 2-3, 80% inverse data\n            # Stage 2 (50-80%): depth 3-4, 50% inverse data  \n            # Stage 3 (80-100%): depth 4-5, 20% inverse data\n            progress_pct = epoch / epochs\n            \n            if progress_pct < 0.5:\n                current_depth = 2 + int(progress_pct * 2)  # 2-3\n                inverse_ratio = 0.8\n            elif progress_pct < 0.8:\n                current_depth = 3 + int((progress_pct - 0.5) * 3.3)  # 3-4\n                inverse_ratio = 0.5\n            else:\n                current_depth = 4 + int((progress_pct - 0.8) * 5)  # 4-5\n                inverse_ratio = 0.2\n            \n            progress((epoch + 1) / epochs, desc=f\"Epoca {epoch+1}/{int(epochs)} (prof: {current_depth}, inv: {inverse_ratio:.0%}) [{DEVICE.type.upper()}]\")\n            \n            data_gen = DataGenerator(max_depth=current_depth)\n            \n            # Mix inverse + random based on curriculum stage\n            n_inverse = int(batch_size * inverse_ratio)\n            n_random = int(batch_size) - n_inverse\n            \n            batch_inverse = data_gen.generate_inverse_batch(max(1, n_inverse), point_count=int(point_count)) if n_inverse > 0 else []\n            batch_random = data_gen.generate_batch(max(1, n_random), point_count=int(point_count)) if n_random > 0 else []\n            batch = batch_inverse + batch_random\n            if len(batch) < 2:\n                continue\n            \n            x_list = [d['x'] for d in batch]\n            y_list = [d['y'] for d in batch]\n            x_list, y_list = normalize_batch(x_list, y_list)\n            \n            token_lists = [[TOKEN_TO_ID[t] for t in d['tokens']] for d in batch]\n            \n            max_len = max(len(s) for s in token_lists)\n            decoder_input = torch.full((len(batch), max_len + 1), SOS_ID, dtype=torch.long)\n            targets = torch.full((len(batch), max_len + 1), -1, dtype=torch.long)\n            \n            for i, seq in enumerate(token_lists):\n                decoder_input[i, 1:len(seq)+1] = torch.tensor(seq, dtype=torch.long)\n                targets[i, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n            \n            x_tensor = torch.tensor(np.array(x_list), dtype=torch.float32).to(DEVICE)\n            y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n            decoder_input = decoder_input.to(DEVICE)\n            targets = targets.to(DEVICE)\n            \n            optimizer.zero_grad()\n            logits, value_pred = MODEL(x_tensor, y_tensor, decoder_input)\n            \n            # Policy Loss\n            loss_policy = ce_loss(logits.view(-1, VOCAB_SIZE + 1), targets.view(-1))\n            \n            # Value Loss\n            # For supervised learning, these are \"perfect\" solutions, so Value Target = 1.0\n            value_targets = torch.ones_like(value_pred)\n            loss_value = torch.nn.functional.mse_loss(value_pred, value_targets)\n            \n            # Combined Loss\n            loss = loss_policy + 0.5 * loss_value\n            \n            if torch.isnan(loss) or torch.isinf(loss):\n                continue\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(MODEL.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            \n            losses.append(loss.item())\n        \n        save_model()\n        MODEL.eval()\n        TRAINING_STATUS[\"running\"] = False\n        \n        if not losses:\n            return \"Error: No se pudo calcular loss\", None\n        \n        fig = create_loss_plot(losses, \"Curriculum Learning\")\n        \n        result = f\"\"\"\n        <div style=\"background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); padding: 20px; border-radius: 15px; border: 2px solid #00d4ff;\">\n            <h2 style=\"color: #00d4ff; margin: 0;\">Curriculum Learning Completado</h2>\n            <p style=\"color: white;\">Epocas: {int(epochs)} | Loss Final: {losses[-1]:.4f}</p>\n            <p style=\"color: #888;\">Profundidad maxima: 6 | Dispositivo: {DEVICE.type.upper()}</p>\n        </div>\n        \"\"\"\n        return result, fig\n        \n    except Exception as e:\n        TRAINING_STATUS[\"running\"] = False\n        return f\"Error: {str(e)}\", None\n\n\ndef train_self_play(iterations, problems_per_iter, point_count=10, progress=gr.Progress()):\n    \"\"\"AlphaZero Self-Play loop.\"\"\"\n    global TRAINING_STATUS\n    \n    if TRAINING_STATUS[\"running\"]:\n        return \"Entrenamiento ya en progreso\", None\n    \n    TRAINING_STATUS[\"running\"] = True\n    reset_stop_flag()  # Reset stop flag at start\n    \n    try:\n        MODEL, DEVICE = get_model()\n        \n        from search.mcts import MCTS\n        \n        optimizer = torch.optim.AdamW(MODEL.parameters(), lr=5e-5, weight_decay=0.01)\n        # Scheduler: Reduce LR when plateauing to help convergence\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15, min_lr=1e-6)\n        \n        # Losses for AlphaZero\n        # Policy: KLDiv (comparing distributions)\n        # Value: Quantile Loss (3 Quantiles)\n        kl_loss = torch.nn.KLDivLoss(reduction='batchmean')\n        quantile_loss_fn = QuantileLoss()\n        \n        VOCAB_SIZE = len(VOCABULARY)\n        SOS_ID = VOCAB_SIZE\n        \n        replay_buffer = deque(maxlen=20000)\n        \n        # Adaptive Curriculum State\n        current_depth = 2\n        data_gen = DataGenerator(max_depth=current_depth)\n        \n        # MCTS for A100: Increase batch size and simulations significantly\n        # Adjusted for RTX 3050/i5: Batch 64 is smoother (less CPU wait)\n        searcher = MCTS(MODEL, DEVICE, max_simulations=500, complexity_lambda=0.1, batch_size=64)\n        \n        rmses = []\n        losses = []\n        best_avg_rmse = float('inf')\n        \n        start_time = time.time()\n        \n        for iteration in range(int(iterations)):\n            # Check for stop request\n            if should_stop_training():\n                print(\"\u23f9\ufe0f Training stopped by user\")\n                break\n            # ETA Calculation\n            elapsed = time.time() - start_time\n            if iteration > 0:\n                avg_time_per_iter = elapsed / iteration\n                remaining_iters = int(iterations) - iteration\n                eta_seconds = remaining_iters * avg_time_per_iter\n                \n                # Format ETA\n                if eta_seconds > 3600:\n                    eta_str = f\"{eta_seconds/3600:.1f}h\"\n                elif eta_seconds > 60:\n                    eta_str = f\"{eta_seconds/60:.0f}m\"\n                else:\n                    eta_str = f\"{eta_seconds:.0f}s\"\n            else:\n                eta_str = \"Calculando...\"\n\n            # Adaptive Curriculum Check\n            # Stages: 0=Arithmetic, 1=Poly, 2=Trig, 3=Adv, 4=Complex\n            CURRICULUM_LEVELS = [\n                {'depth': 1, 'ops': ['+', '-', '*', '/']},\n                {'depth': 2, 'ops': ['+', '-', '*', '/']},\n                {'depth': 3, 'ops': ['+', '-', '*', '/', 'pow', 'sqrt']},\n                {'depth': 4, 'ops': ['+', '-', '*', '/', 'pow', 'sqrt', 'sin', 'cos']},\n                {'depth': 5, 'ops': None} # All\n            ]\n            \n            # Initialize state if not present\n            if 'curriculum_stage' not in locals():\n                curriculum_stage = 0\n            \n            recent_rmse = np.mean(rmses[-20:]) if len(rmses) >= 20 else 1.0\n            \n            # Graduation condition: RMSE < 0.1 stable\n            if len(rmses) > 20 and recent_rmse < 0.1 and curriculum_stage < len(CURRICULUM_LEVELS) - 1:\n                curriculum_stage += 1\n                stage_info = CURRICULUM_LEVELS[curriculum_stage]\n                data_gen = DataGenerator(max_depth=stage_info['depth'], allowed_operators=stage_info['ops'])\n                print(f\"*** Curriculum Level Up! Stage {curriculum_stage} ({stage_info['depth']}, {stage_info['ops']}) ***\")\n                # Clear buffer to avoid training on old easy data? Maybe keep some for replay.\n            \n            # Ensure data_gen is initialized at start\n            if iteration == 0:\n                 stage_info = CURRICULUM_LEVELS[0]\n                 data_gen = DataGenerator(max_depth=stage_info['depth'], allowed_operators=stage_info['ops'])\n\n            stage_name = [\"Arithmetic\", \"Polynomials\", \"Trigonometry\", \"Advanced\", \"Complex\"][curriculum_stage]\n            \n            # Safe access to current_lr\n            curr_lr_disp = optimizer.param_groups[0]['lr']\n            msg = f\"Iter {iteration+1}/{int(iterations)} [{stage_name}] RMSE:{recent_rmse:.3f} LR:{curr_lr_disp:.1e} | ETA: {eta_str}\"\n            progress((iteration + 1) / iterations, desc=msg)\n            \n            # Active Learning / Hard Mining Phase\n            MODEL.eval()\n            \n            # Generate a large pool of candidates candidates to find the \"hard\" ones\n            pool_size = problems_per_iter * 3  # Generate 3x more than we need\n            candidates = data_gen.generate_inverse_batch(pool_size, point_count=int(point_count))\n            \n            if not candidates:\n                continue\n                \n            # Quick forward pass to estimate difficulty (Loss)\n            # We want to train on problems where the model currently FAILS (High Loss)\n            hard_problems = []\n            \n            with torch.no_grad():\n                # Process in chunks to avoid OOM\n                chunk_size = 32\n                for i in range(0, len(candidates), chunk_size):\n                    chunk = candidates[i:i+chunk_size]\n                    \n                    x_list = [d['x'] for d in chunk]\n                    y_list = [d['y'] for d in chunk]\n                    x_list, y_list = normalize_batch(x_list, y_list)\n                    \n                    token_lists = [[TOKEN_TO_ID.get(t, TOKEN_TO_ID['C']) for t in d['tokens']] for d in chunk]\n                    max_len = max(len(s) for s in token_lists)\n                    \n                    # Prepare tensors\n                    dec_in = torch.full((len(chunk), max_len + 1), SOS_ID, dtype=torch.long).to(DEVICE)\n                    targets = torch.full((len(chunk), max_len + 1), -1, dtype=torch.long).to(DEVICE)\n                    \n                    for j, seq in enumerate(token_lists):\n                        dec_in[j, 1:len(seq)+1] = torch.tensor(seq, dtype=torch.long)\n                        targets[j, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n                        \n                    x_tensor = torch.tensor(np.array(x_list), dtype=torch.float32).to(DEVICE)\n                    y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n                    \n                    logits, _ = MODEL(x_tensor, y_tensor, dec_in)\n                    \n                    # Calculate loss per item\n                    # CrossEntropy usually aggregates, so we use reduction='none'\n                    loss_f = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='none')\n                    raw_losses = loss_f(logits.view(-1, VOCAB_SIZE + 1), targets.view(-1))\n                    \n                    # Reshape back to [Batch, Seq] to sum/mean per sample\n                    raw_losses = raw_losses.view(len(chunk), -1)\n                    # Average loss per non-padded token\n                    mask = (targets != -1)\n                    sample_losses = (raw_losses * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n                    \n                    for j, loss_val in enumerate(sample_losses):\n                        # Store (Loss, Problem)\n                        hard_problems.append((loss_val.item(), chunk[j]))\n            \n            # Sort by difficulty (Loss descending)\n            hard_problems.sort(key=lambda x: x[0], reverse=True)\n            \n            # Stabilization: Mix Hardest (70%) + Random Examples (30%)\n            # This prevents \"Catastrophic Forgetting\" of simpler patterns\n            n_hard = int(problems_per_iter * 0.7)\n            n_random = int(problems_per_iter) - n_hard\n            \n            # Top K hardest\n            selected_hard = [p[1] for p in hard_problems[:n_hard]]\n            \n            # Random selection from the rest of the pool (to keep variety)\n            remaining_pool = [p[1] for p in hard_problems[n_hard:]]\n            selected_random = random.sample(remaining_pool, min(n_random, len(remaining_pool))) if remaining_pool else []\n            \n            selected_problems = selected_hard + selected_random\n            \n            avg_pool_loss = np.mean([p[0] for p in hard_problems])\n            top_loss = np.mean([p[0] for p in hard_problems[:n_hard]]) if n_hard > 0 else 0\n            \n            print(f\"Active Learning: Pool Loss {avg_pool_loss:.3f} -> Selected Mix (Hard:{top_loss:.3f})\")\n\n            # --- HALL OF SHAME CAPTURE ---\n            # Capture what the model predicts for the top 3 hardest failures\n            try:\n                top_failures = hard_problems[:3]\n                x_fail = [p[1]['x'].astype(np.float64) for p in top_failures]\n                y_fail = [p[1]['y'].astype(np.float64) for p in top_failures]\n                target_formulas = [p[1]['infix'] for p in top_failures]\n                fail_losses = [p[0] for p in top_failures]\n                \n                # Simple Greedy Decode to see what it predicts\n                from search.beam_search import BeamSearch\n                # Use beam search with width 1 (Greedy) for speed, with curriculum mask\n                bs = BeamSearch(MODEL, DEVICE, beam_width=1, max_length=20, curriculum_stage=curriculum_stage)\n                \n                for i in range(len(top_failures)):\n                    try:\n                        # Decode\n                        # Enable return_partial to see what the model is thinking if it fails\n                        res = bs.search(x_fail[i], y_fail[i], return_partial=True)\n                        if not res:\n                            pred_formula = \"Search Empty (No Tokens)\"\n                        else:\n                            pred_formula = res[0]['formula']\n                            \n                        # Detect Looping (e.g. \"10 / / / / / /\")\n                        # Basic heuristic: check if last 10 chars contain > 80% same char or repeating pattern\n                        if len(pred_formula) > 20:\n                            # Check for repeating slashes or other single chars\n                            if pred_formula.count('/') > 10 and pred_formula.endswith('/ .'): \n                                 pred_formula = pred_formula[:20] + \" ... [Loop Detected]\"\n                            elif \" / / / \" in pred_formula:\n                                 pred_formula = pred_formula.split(\" / / / \")[0] + \" ... [Loop Detected]\"\n                        \n                        add_training_error(\n                            target=target_formulas[i],\n                            predicted=pred_formula,\n                            loss=fail_losses[i],\n                            stage=stage_name\n                        )\n                    except Exception as e:\n                        print(f\"HoS Inner Error: {e}\")\n                        add_training_error(\n                            target=target_formulas[i],\n                            predicted=f\"CRASH: {str(e)[:20]}\",\n                            loss=fail_losses[i],\n                            stage=stage_name\n                        )\n            except Exception as e:\n                import traceback\n                print(f\"HoS Outer Error: {e}\")\n                traceback.print_exc()\n\n            # --- MCTS SOLVE ---\n            for prob in selected_problems:\n                x_data = prob['x'].astype(np.float64)\n                y_data = prob['y'].astype(np.float64)\n                \n                try:\n                    # Use MCTS to find the solution (or improve upon it)\n                    # For inverse problems, we KNOW the solution, but MCTS helps explore variations\n                    # and generates the policy distribution we want to learn.\n                    result = searcher.search(x_data, y_data)\n                    \n                    # 1. Store Training Examples\n                    if 'root' in result:\n                        examples = searcher.get_training_examples(result['root'])\n                        for (tokens, policy, value) in examples:\n                            replay_buffer.append({\n                                'x': x_data, 'y': y_data,\n                                'tokens': tokens,\n                                'policy': policy,\n                                'value': value\n                            })\n                    \n                    # 2. Track Metrics\n                    if result.get('tokens'):\n                        rmses.append(result['rmse'])\n                        \n                except Exception as e:\n                    print(f\"Self-play error: {e}\")\n                    continue\n            \n            # Training phase\n            # To saturate GPU: Increase batch size and number of updates\n            if len(replay_buffer) >= 64:\n                MODEL.train()\n                \n                # Dynamic training steps: Train more if we have more data\n                # AlphaZero ratio usually high (e.g. 10 epochs on new data)\n                # Here we sample from buffer.\n                train_batch_size = 128\n                if len(replay_buffer) < train_batch_size:\n                    train_batch_size = 64\n                \n                # Steps: roughly cover 20% of buffer or at least 10 steps\n                steps = max(10, min(50, len(replay_buffer) // train_batch_size))\n                \n                for _ in range(steps):\n                    batch = random.sample(list(replay_buffer), min(train_batch_size, len(replay_buffer)))\n                    \n                    x_list = [exp['x'] for exp in batch]\n                    y_list = [exp['y'] for exp in batch]\n                    x_list, y_list = normalize_batch(x_list, y_list)\n                    \n                    token_lists = [[TOKEN_TO_ID[t] for t in exp['tokens']] for exp in batch]\n                    policy_targets = [exp['policy'] for exp in batch]\n                    value_targets_list = [exp['value'] for exp in batch]\n                    \n                    max_len = max(len(s) for s in token_lists)\n                    decoder_input = torch.full((len(batch), max_len + 1), SOS_ID, dtype=torch.long)\n                    \n                    # Policy targets (for KLDiv) and Value targets\n                    policy_target_tensor = torch.tensor(np.array(policy_targets), dtype=torch.float32).to(DEVICE)\n                    value_target_tensor = torch.tensor(np.array(value_targets_list), dtype=torch.float32).unsqueeze(1).to(DEVICE)\n                    \n                    for i, seq in enumerate(token_lists):\n                        l = len(seq)\n                        decoder_input[i, 1:l+1] = torch.tensor(seq, dtype=torch.long)\n                    \n                    x_tensor = torch.tensor(np.array(x_list), dtype=torch.float32).to(DEVICE)\n                    y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n                    decoder_input = decoder_input.to(DEVICE)\n                    \n                    optimizer.zero_grad()\n                    logits, value_pred = MODEL(x_tensor, y_tensor, decoder_input)\n                    \n                    # Policy Loss (KL Divergence)\n                    # Get logits for the last token position of each sequence\n                    last_logits = []\n                    for i, seq in enumerate(token_lists):\n                        idx = len(seq) # Post-padding index? No, index in padded tensor.\n                        # decoder_input: [SOS, T1, T2]\n                        # logits: [PredSOS, PredT1, PredT2]\n                        # We want prediction AFTER T2? No.\n                        # MCTS Example: State=[T1, T2]. Policy=Dist for T3.\n                        # Model Input: [SOS, T1, T2]. Output Last: Dist for T3.\n                        # Index is len(seq).\n                        last_logits.append(logits[i, idx, :VOCAB_SIZE])\n                    \n                    last_logits = torch.stack(last_logits)\n                    log_probs = torch.nn.functional.log_softmax(last_logits, dim=1)\n                    \n                    loss_policy = kl_loss(log_probs, policy_target_tensor)\n                    \n                    # Value Loss (Quantile)\n                    loss_value = quantile_loss_fn(value_pred, value_target_tensor)\n                    \n                    # Total Loss\n                    loss = loss_policy + loss_value \n                    \n                    if not (torch.isnan(loss) or torch.isinf(loss)):\n                        loss.backward()\n                        torch.nn.utils.clip_grad_norm_(MODEL.parameters(), 1.0)\n                        optimizer.step()\n                        losses.append(loss.item())\n            \n            # Step Scheduler based on recent Loss\n            if losses:\n                current_loss = np.mean(losses[-10:])\n                scheduler.step(current_loss)\n            \n            current_lr = optimizer.param_groups[0]['lr']\n            \n            # Periodic save\n            if (iteration + 1) % 10 == 0:\n                save_model()\n        \n        save_model()\n        MODEL.eval()\n        TRAINING_STATUS[\"running\"] = False\n        \n        fig = create_selfplay_plot(losses, rmses)\n        \n        avg_rmse = np.mean(rmses[-50:]) if rmses else 0\n        result = f\"\"\"\n        <div style=\"background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); padding: 20px; border-radius: 15px; border: 2px solid #ff6b6b;\">\n            <h2 style=\"color: #ff6b6b; margin: 0;\">Self-Play Completado</h2>\n            <p style=\"color: white;\">Iteraciones: {int(iterations)} | Problemas: {len(rmses)}</p>\n            <p style=\"color: #888;\">RMSE Promedio: {avg_rmse:.4f} | Dispositivo: {DEVICE.type.upper()}</p>\n        </div>\n        \"\"\"\n        return result, fig\n        \n    except Exception as e:\n        TRAINING_STATUS[\"running\"] = False\n        return f\"Error: {str(e)}\", None\n\n\ndef create_loss_plot(losses, title):\n    \"\"\"Create a loss plot with dark theme.\"\"\"\n    fig, ax = plt.subplots(figsize=(8, 4), facecolor='#1a1a2e')\n    ax.set_facecolor('#1a1a2e')\n    ax.plot(losses, color='#00d4ff', linewidth=2)\n    ax.set_xlabel('Epoca', color='white')\n    ax.set_ylabel('Loss', color='white')\n    ax.set_title(title, color='white', fontweight='bold')\n    ax.tick_params(colors='white')\n    ax.grid(True, alpha=0.2)\n    for spine in ax.spines.values():\n        spine.set_color('#00d4ff')\n    plt.tight_layout()\n    return fig\n\n\ndef create_selfplay_plot(losses, rmses):\n    \"\"\"Create dual plot for self-play results.\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4), facecolor='#1a1a2e')\n    \n    ax1.set_facecolor('#1a1a2e')\n    if losses:\n        ax1.plot(losses, color='#00d4ff', linewidth=2)\n    ax1.set_xlabel('Step', color='white')\n    ax1.set_ylabel('Loss', color='white')\n    ax1.set_title('Policy Loss', color='white', fontweight='bold')\n    ax1.tick_params(colors='white')\n    ax1.grid(True, alpha=0.2)\n    \n    ax2.set_facecolor('#1a1a2e')\n    if rmses:\n        ax2.plot(rmses, color='#ff6b6b', linewidth=1, alpha=0.5)\n        if len(rmses) > 10:\n            ma = np.convolve(rmses, np.ones(10)/10, mode='valid')\n            ax2.plot(range(9, len(rmses)), ma, color='#ff6b6b', linewidth=2)\n    ax2.set_xlabel('Problema', color='white')\n    ax2.set_ylabel('RMSE', color='white')\n    ax2.set_title('RMSE', color='white', fontweight='bold')\n    ax2.tick_params(colors='white')\n    ax2.grid(True, alpha=0.2)\n    \n    for ax in [ax1, ax2]:\n        for spine in ax.spines.values():\n            spine.set_color('#00d4ff')\n    \n    plt.tight_layout()\n    return fig\n\ndef train_supervised(iterations, batch_size=128, point_count=10, progress=gr.Progress()):\n    \"\"\"\n    Massive Supervised Pre-training (Warmup).\n    Focus: Syntax, Basic Arithmetic, Overcoming \"Collapse to Constant\".\n    Speed: High (No MCTS, just random generation + CrossEntropy).\n    \"\"\"\n    global TRAINING_STATUS\n    \n    if TRAINING_STATUS[\"running\"]:\n        return \"Entrenamiento ya en progreso\", None\n    \n    TRAINING_STATUS[\"running\"] = True\n    reset_stop_flag()  # Reset stop flag at start\n    \n    try:\n        MODEL, DEVICE = get_model()\n        \n        MODEL.train()\n        optimizer = torch.optim.AdamW(MODEL.parameters(), lr=1e-4, weight_decay=0.01)\n        # Slower decay: T_max = iterations * 2 keeps LR higher for longer\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(iterations*2), eta_min=1e-6)\n        ce_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n        \n        VOCAB_SIZE = len(VOCABULARY)\n        SOS_ID = VOCAB_SIZE\n        \n        # Start extremely simple (Depth 1: x+1, x*x, etc.)\n        allowed_ops = OPERATOR_STAGES[0]\n        data_gen = DataGenerator(max_depth=1, allowed_operators=allowed_ops) \n        allowed_mask = get_allowed_token_mask(0, VOCAB_SIZE, DEVICE) # Stage 0 mask\n        losses = []\n        \n        start_time = time.time()\n        \n        for i in range(int(iterations)):\n            # Check for stop request\n            if should_stop_training():\n                print(\"\u23f9\ufe0f Pre-training stopped by user\")\n                break\n            # ETA\n            elapsed = time.time() - start_time\n            if i > 0:\n                iter_per_sec = i / elapsed\n                remaining = int(iterations) - i\n                eta = remaining / iter_per_sec\n                eta_str = f\"{eta:.0f}s\"\n            else:\n                eta_str = \"...\"\n                \n            current_lr = optimizer.param_groups[0]['lr']\n            msg = f\"Iter {i+1}/{int(iterations)} Loss:{np.mean(losses[-50:]) if losses else 0:.3f} LR:{current_lr:.1e} ETA:{eta_str}\"\n            progress((i + 1) / iterations, desc=msg)\n            \n            # Generate Random Batch (High Speed)\n            batch = data_gen.generate_batch(int(batch_size), point_count=int(point_count))\n            \n            if not batch:\n                continue\n            \n            x_list = [d['x'] for d in batch]\n            y_list = [d['y'] for d in batch]\n            x_list, y_list = normalize_batch(x_list, y_list)\n            \n            token_lists = [[TOKEN_TO_ID.get(t, TOKEN_TO_ID['C']) for t in d['tokens']] for d in batch]\n            \n            max_len = max(len(s) for s in token_lists)\n            decoder_input = torch.full((len(batch), max_len + 1), SOS_ID, dtype=torch.long)\n            targets = torch.full((len(batch), max_len + 1), -1, dtype=torch.long)\n            \n            for j, seq in enumerate(token_lists):\n                decoder_input[j, 1:len(seq)+1] = torch.tensor(seq, dtype=torch.long)\n                targets[j, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n                \n            x_tensor = torch.tensor(np.array(x_list), dtype=torch.float32).to(DEVICE)\n            y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n            decoder_input = decoder_input.to(DEVICE)\n            targets = targets.to(DEVICE)\n            \n            optimizer.zero_grad()\n            logits, _ = MODEL(x_tensor, y_tensor, decoder_input)\n            \n            # Apply Stage 0 mask to bridge Pre-training with Curriculum\n            # Use a more stable value (-1e4 instead of -1e9) to avoid overflow\n            logits = logits + (1 - allowed_mask.view(1, 1, -1)) * -1e4\n            \n            loss = ce_loss(logits.view(-1, VOCAB_SIZE + 1), targets.view(-1))\n            \n            if not (torch.isnan(loss) or torch.isinf(loss)):\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(MODEL.parameters(), 1.0)\n                optimizer.step()\n                scheduler.step()\n                losses.append(loss.item())\n                \n            if (i+1) % 100 == 0:\n                save_model()\n                \n        save_model()\n        MODEL.eval()\n        TRAINING_STATUS[\"running\"] = False\n        \n        fig = create_loss_plot(losses, \"Pre-Entrenamiento Supervisado\")\n        \n        result = f\"\"\"\n        <div style=\"background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); padding: 20px; border-radius: 15px; border: 2px solid #ffd93d;\">\n            <h2 style=\"color: #ffd93d; margin: 0;\">Escuela Primaria (Warmup) Completada</h2>\n            <p style=\"color: white;\">Iteraciones: {int(iterations)} | Loss Final: {losses[-1]:.4f}</p>\n            <p style=\"color: #888;\">El modelo ha aprendido sintaxis basica.</p>\n        </div>\n        \"\"\"\n        return result, fig\n        \n    except Exception as e:\n        TRAINING_STATUS[\"running\"] = False\n        return f\"Error: {str(e)}\", None\n\n\ndef train_hybrid_feedback_loop(iterations, problems_per_iter=10, gp_timeout=10, progress=gr.Progress()):\n    \"\"\"\n    Teacher-Student Distillation Loop.\n    1. Find problems where model has high loss.\n    2. Use Hybrid Search (GP) to solve them.\n    3. Train model on GP solutions.\n    \"\"\"\n    global TRAINING_STATUS\n    \n    if TRAINING_STATUS[\"running\"]:\n        return \"Entrenamiento ya en progreso\", None\n    \n    TRAINING_STATUS[\"running\"] = True\n    reset_stop_flag()\n    \n    try:\n        MODEL, DEVICE = get_model()\n        \n        optimizer = torch.optim.AdamW(MODEL.parameters(), lr=5e-5, weight_decay=0.01)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n        \n        VOCAB_SIZE = len(VOCABULARY)\n        SOS_ID = VOCAB_SIZE\n        \n        # Replay buffer for \"Gold Standard\" examples found by GP\n        replay_buffer = deque(maxlen=5000)\n        \n        # Start with simple problems and grow\n        data_gen = DataGenerator(max_depth=3)\n        \n        losses = []\n        gp_successes = 0\n        gp_attempts = 0\n        \n        start_time = time.time()\n        \n        for iteration in range(int(iterations)):\n            if should_stop_training():\n                print(\"\u23f9\ufe0f Feedback Loop stopped\")\n                break\n                \n            elapsed = time.time() - start_time\n            # eta_str = f\"{(int(iterations)-iteration) * (elapsed/(iteration+1) if iteration>0 else 0):.0f}s\"\n            iter_dur = elapsed/(iteration+1) if iteration > 0 else 0\n            eta_seconds = (int(iterations)-iteration) * iter_dur\n            eta_str = f\"{eta_seconds:.0f}s\"\n\n            progress((iteration + 1) / iterations, \n                     desc=f\"Iter {iteration+1}/{int(iterations)} | GP Success: {gp_successes}/{gp_attempts} | Loss: {np.mean(losses[-10:]) if losses else 0:.3f}\")\n            \n            # --- PHASE 1: HARD MINING ---\n            MODEL.eval()\n            \n            # Generate candidates\n            pool_size = 50 \n            candidates = data_gen.generate_inverse_batch(pool_size, point_count=10)\n            \n            hard_problems = []\n            \n            with torch.no_grad():\n                # We want to find problems with HIGH LOSS (model failure)\n                # Quick batch forward\n                x_list = [d['x'] for d in candidates]\n                y_list = [d['y'] for d in candidates]\n                x_list, y_list = normalize_batch(x_list, y_list)\n                \n                token_lists = [[TOKEN_TO_ID.get(t, TOKEN_TO_ID['C']) for t in d['tokens']] for d in candidates]\n                max_len = max(len(s) for s in token_lists)\n                \n                dec_in = torch.full((pool_size, max_len + 1), SOS_ID, dtype=torch.long).to(DEVICE)\n                targets = torch.full((pool_size, max_len + 1), -1, dtype=torch.long).to(DEVICE)\n                \n                for j, seq in enumerate(token_lists):\n                    dec_in[j, 1:len(seq)+1] = torch.tensor(seq, dtype=torch.long)\n                    targets[j, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n                    \n                x_tensor = torch.tensor(np.array(x_list), dtype=torch.float32).to(DEVICE)\n                y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n                \n                logits, value_pred = MODEL(x_tensor, y_tensor, dec_in)\n                \n                loss_f = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='none')\n                raw_losses = loss_f(logits.view(-1, VOCAB_SIZE + 1), targets.view(-1))\n                raw_losses = raw_losses.view(pool_size, -1)\n                \n                mask = (targets != -1)\n                sample_losses = (raw_losses * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n                \n                # Filter: Keep if loss > 1.0 (arbitrary threshold for \"confused\")\n                for j, loss_val in enumerate(sample_losses):\n                    if loss_val.item() > 0.5: # Lower threshold to catch more\n                        hard_problems.append(candidates[j])\n            \n            # Take top K hardest\n            # Limit GP calls per iter to avoid slowness\n            problems_to_solve = hard_problems[:int(problems_per_iter)]\n            \n            if not problems_to_solve:\n                continue\n\n            # --- PHASE 2: TEACHER SOLVES (GP) ---\n            print(f\"Iter {iteration}: Asking Teacher to solve {len(problems_to_solve)} hard problems...\")\n            \n            for prob in problems_to_solve:\n                gp_attempts += 1\n                try:\n                    # Run Hybrid Search (Quick Mode)\n                    # We pass the model so beam search can seed the GP\n                    res = hybrid_solve(\n                        prob['x'], \n                        prob['y'], \n                        MODEL, \n                        DEVICE, \n                        beam_width=10,     # Faster beam\n                        gp_timeout=gp_timeout,\n                        gp_binary_path=None \n                    )\n                    \n                    if res and res.get('formula') and res.get('rmse', 1e6) < 0.01:\n                        # SUCCESS!\n                        gp_successes += 1\n                        \n                        # Parse formula to tokens\n                        try:\n                            # 1. Parse string to tree\n                            tree = ExpressionTree.from_infix(res['formula'])\n                            # 2. Get tokens\n                            tokens = tree.tokens\n                            \n                            replay_buffer.append({\n                                'x': prob['x'],\n                                'y': prob['y'],\n                                'tokens': tokens,\n                                'source': 'GP_Teacher'\n                            })\n                            \n                        except Exception as e:\n                            print(f\"Failed to tokenize GP result: {e}\")\n                            \n                except Exception as e:\n                    print(f\"GP Hybrid Error: {e}\")\n                    \n            # --- PHASE 3: STUDENT TRAINS (NN) ---\n            if len(replay_buffer) > 10:\n                MODEL.train()\n                # Train on batch from buffer\n                batch_size_train = min(len(replay_buffer), 64)\n                \n                # Multiple steps to enforce learning\n                steps = 5\n                \n                for _ in range(steps):\n                    batch = random.sample(list(replay_buffer), batch_size_train)\n                    \n                    x_list = [d['x'] for d in batch]\n                    y_list = [d['y'] for d in batch]\n                    x_list, y_list = normalize_batch(x_list, y_list)\n                    \n                    token_lists = [[TOKEN_TO_ID.get(t, TOKEN_TO_ID['C']) for t in d['tokens']] for d in batch]\n                    max_len = max(len(s) for s in token_lists)\n                    \n                    dec_in = torch.full((batch_size_train, max_len + 1), SOS_ID, dtype=torch.long).to(DEVICE)\n                    targets = torch.full((batch_size_train, max_len + 1), -1, dtype=torch.long).to(DEVICE)\n                    \n                    for j, seq in enumerate(token_lists):\n                        dec_in[j, 1:len(seq)+1] = torch.tensor(seq, dtype=torch.long)\n                        targets[j, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n                        \n                    x_t = torch.tensor(np.array(x_list), dtype=torch.float32).to(DEVICE)\n                    y_t = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n                    dec_in = dec_in.to(DEVICE)\n                    targets = targets.to(DEVICE)\n                    \n                    optimizer.zero_grad()\n                    logits, value_pred = MODEL(x_t, y_t, dec_in)\n                    \n                    # Policy Loss only (Standard Supervised)\n                    # We trust the GP solution is \"Correct\" (Value=1.0)\n                    loss_ce = torch.nn.CrossEntropyLoss(ignore_index=-1)(logits.view(-1, VOCAB_SIZE+1), targets.view(-1))\n                    \n                    # Value Loss\n                    value_targets = torch.ones_like(value_pred) # GP solutions are always valid\n                    loss_val = torch.nn.functional.mse_loss(value_pred, value_targets)\n                    \n                    loss = loss_ce + 0.1 * loss_val\n                    \n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(MODEL.parameters(), 1.0)\n                    optimizer.step()\n                    \n                    losses.append(loss.item())\n                    \n                scheduler.step(np.mean(losses[-10:]))\n                \n            if (iteration + 1) % 5 == 0:\n                save_model()\n                \n        save_model()\n        MODEL.eval()\n        TRAINING_STATUS[\"running\"] = False\n        \n        fig = create_loss_plot(losses, \"Feedback Loop Loss\")\n        \n        result_html = f\"\"\"\n        <div style=\"background: linear-gradient(135deg, #2c3e50 0%, #000000 100%); padding: 20px; border-radius: 15px; border: 2px solid #f1c40f;\">\n            <h2 style=\"color: #f1c40f; margin: 0;\">Feedback Loop Completado</h2>\n            <p style=\"color: white;\">Iteraciones: {iterations} | GP Success: {gp_successes}/{gp_attempts}</p>\n            <p style=\"color: #bbb;\">Nuevos Ejemplos Generados: {len(replay_buffer)}</p>\n        </div>\n        \"\"\"\n        return result_html, fig\n\n    except Exception as e:\n        TRAINING_STATUS[\"running\"] = False\n        import traceback\n        traceback.print_exc()\n        return f\"Error CRITICO: {str(e)}\", None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile ui/app_benchmark.py\n",
        "import gradio as gr\nfrom utils.benchmark_comparison import run_comparison_benchmark\nfrom ui.app_core import get_model, DEVICE\n\ndef get_benchmark_tab():\n    with gr.Tab(\"\ud83e\udd47 Benchmark (IQ Test)\"):\n        gr.Markdown(\"### Evaluar Inteligencia del Modelo (Comparativa)\")\n        gr.Markdown(\"Ejecuta una bater\u00eda de **10 problemas est\u00e1ndar** comparando diferentes m\u00e9todos de b\u00fasqueda.\")\n        \n        with gr.Row():\n            methods_chk = gr.CheckboxGroup(\n                choices=[\"beam\", \"mcts\", \"hybrid\"], \n                value=[\"hybrid\"], \n                label=\"M\u00e9todos a Evaluar\",\n                info=\"Selecciona uno o m\u00e1s m\u00e9todos para comparar.\"\n            )\n            timeout_slider = gr.Slider(\n                minimum=5, \n                maximum=60, \n                value=30, \n                step=5, \n                label=\"Timeout GP (s)\", \n                info=\"Tiempo m\u00e1ximo para Beta-GP por problema.\"\n            )\n        \n        run_btn = gr.Button(\"\ud83d\ude80 Iniciar Benchmark Comparativo\", variant=\"primary\")\n        \n        progress_bar = gr.HTML(\"\")\n        \n        # Area de resultados\n        summary_html = gr.HTML(\"Resultados aparecer\u00e1n aqu\u00ed...\")\n        \n        results_df = gr.Dataframe(\n            headers=[\"Problema\", \"Nivel\", \"M\u00e9todo\", \"Formula\", \"RMSE\", \"Tiempo\", \"Estado\"],\n            label=\"Resultados Detallados\",\n            interactive=False\n        )\n        \n        def run_bench(selected_methods, gp_timeout, progress=gr.Progress()):\n            model_obj, device_obj = get_model()\n            if not model_obj:\n                return \"<div>\u26a0\ufe0f Error: Modelo no cargado. Ve a la pesta\u00f1a 'Config' y carga un modelo.</div>\", None, []\n            \n            if not selected_methods:\n                return \"<div>\u26a0\ufe0f Error: Selecciona al menos un m\u00e9todo.</div>\", None, []\n                \n            progress(0, desc=\"Iniciando Benchmark...\")\n            \n            # Run comparison\n            try:\n                result_data = run_comparison_benchmark(\n                    model_obj, \n                    device_obj, \n                    methods=selected_methods,\n                    gp_timeout=gp_timeout,\n                    beam_width=50,\n                    progress_callback=lambda p, desc: progress(p, desc=desc)\n                )\n            except Exception as e:\n                import traceback\n                traceback.print_exc()\n                return f\"<div>\u274c Error en Benchmark: {e}</div>\", None, []\n            \n            results = result_data['results']\n            summary_dict = result_data['summary']\n            \n            # Format dataframe\n            rows = []\n            for r in results:\n                status_icon = \"\u2705\" if r['success'] else \"\u274c\"\n                rmse_val = f\"{r['rmse']:.5f}\" if r['rmse'] < 1e6 else \"> 10^6\"\n                rows.append([\n                    r['problem_name'],\n                    r['level'],\n                    r['method'].upper(),\n                    r['formula'],\n                    rmse_val,\n                    f\"{r['time']:.2f}s\",\n                    status_icon\n                ])\n            \n            # Generate HTML Summary\n            html_content = \"<div style='display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;'>\"\n            \n            # Determine winner if multiple methods\n            winner_method = None\n            if len(selected_methods) > 1:\n                winner_method = max(summary_dict.items(), key=lambda x: (x[1]['solved'], -x[1]['avg_rmse']))[0]\n            \n            for method, stats in summary_dict.items():\n                is_winner = (method == winner_method)\n                border_color = \"#4CAF50\" if is_winner else (\"#FF9800\" if stats['score'] > 50 else \"#F44336\")\n                bg_color = \"#1e1e2f\"\n                if is_winner:\n                    bg_color = \"#1b3a24\" # Dark green tint for winner\n                    \n                trophy = \"\ud83c\udfc6 GANADOR\" if is_winner else \"\"\n                \n                html_content += f\"\"\"\n                <div style=\"background: {bg_color}; padding: 15px; border-radius: 10px; border: 2px solid {border_color}; min-width: 200px; text-align: center;\">\n                    <h2 style=\"color: {border_color}; margin: 0 0 10px 0;\">{method.upper()} {trophy}</h2>\n                    <div style=\"font-size: 24px; font-weight: bold; margin-bottom: 5px;\">{stats['solved']} / {stats['total']}</div>\n                    <div style=\"color: #ccc; font-size: 14px;\">Resueltos</div>\n                    <hr style=\"border-color: #444; margin: 10px 0;\">\n                    <div style=\"font-size: 14px;\">Nota: <b>{stats['score']:.1f}%</b></div>\n                    <div style=\"font-size: 14px;\">Tiempo Avg: <b>{stats['avg_time']:.2f}s</b></div>\n                </div>\n                \"\"\"\n            html_content += \"</div>\"\n            \n            return html_content, rows\n            \n        run_btn.click(run_bench, inputs=[methods_chk, timeout_slider], outputs=[summary_html, results_df])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile ui/__init__.py\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile utils/optimize_constants.py\n",
        "\"\"\"\nConstant Optimization Module for AlphaSymbolic.\nUses scipy.optimize to find optimal values for 'C' placeholders.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom core.grammar import ExpressionTree\n\ndef optimize_constants(tree, x_data, y_data, method='L-BFGS-B'):\n    \"\"\"\n    Given an ExpressionTree with 'C' placeholders, find optimal constant values.\n    \n    Args:\n        tree: ExpressionTree object\n        x_data: numpy array of x values\n        y_data: numpy array of target y values\n        method: optimization method ('L-BFGS-B', 'SLSQP', 'Nelder-Mead')\n        \n    Returns:\n        dict: mapping of path tuples to optimized constant values\n        float: final RMSE\n    \"\"\"\n    if not tree.is_valid:\n        return {}, float('inf')\n    \n    # Get positions of all constants\n    positions = tree.root.get_constant_positions()\n    n_constants = len(positions)\n    \n    if n_constants == 0:\n        # No constants to optimize, just evaluate\n        y_pred = tree.evaluate(x_data)\n        mse = np.mean((y_pred - y_data)**2)\n        return {}, np.sqrt(mse)\n    \n    def objective(params):\n        \"\"\"Objective function: RMSE given constant values.\"\"\"\n        # Build constants dict\n        constants = {tuple(pos): params[i] for i, pos in enumerate(positions)}\n        \n        # Evaluate\n        y_pred = tree.evaluate(x_data, constants=constants)\n        \n        # Handle invalid predictions\n        if np.any(np.isnan(y_pred)) or np.any(np.isinf(y_pred)):\n            return 1e10\n        \n        if not np.all(np.isfinite(y_pred)):\n            return 1e9\n        \n        # Clip huge values to prevent overflow in MSE\n        y_pred = np.clip(y_pred, -1e9, 1e9)\n        \n        mse = np.mean((y_pred - y_data)**2)\n        return mse\n    \n    # Initial guess: all 1s\n    x0 = np.ones(n_constants)\n    \n    # Bounds: reasonable range for constants\n    bounds = [(-1000, 1000)] * n_constants\n    \n    try:\n        result = minimize(\n            objective,\n            x0,\n            method=method,\n            bounds=bounds if method in ['L-BFGS-B', 'SLSQP'] else None,\n            options={'maxiter': 1000, 'disp': False}\n        )\n        \n        # Build final constants dict\n        optimized_constants = {tuple(pos): result.x[i] for i, pos in enumerate(positions)}\n        final_rmse = np.sqrt(result.fun) if result.fun > 0 else 0.0\n        \n        return optimized_constants, final_rmse\n        \n    except Exception as e:\n        return {}, float('inf')\n\ndef substitute_constants(infix_str, constants_dict, positions):\n    \"\"\"\n    Replace 'C' in the infix string with optimized values.\n    Simple approach: replace each C with optimized value.\n    \"\"\"\n    # For proper substitution, we'd need to track positions properly\n    # This is a simplified version that replaces all C with the first constant\n    result = infix_str\n    for i, pos in enumerate(positions):\n        if tuple(pos) in constants_dict:\n            val = constants_dict[tuple(pos)]\n            # Format nicely\n            if abs(val - round(val)) < 1e-6:\n                val_str = str(int(round(val)))\n            else:\n                val_str = f\"{val:.4f}\"\n            # Replace first occurrence of C\n            result = result.replace('C', val_str, 1)\n    return result\n\n\n# Quick test\nif __name__ == \"__main__\":\n    # Test: C * x + C should be optimized to fit y = 2*x + 3\n    x_test = np.array([1, 2, 3, 4, 5], dtype=np.float64)\n    y_test = 2 * x_test + 3  # y = 2x + 3\n    \n    tokens = ['+', '*', 'C', 'x', 'C']  # C*x + C\n    tree = ExpressionTree(tokens)\n    \n    print(f\"Formula structure: {tree.get_infix()}\")\n    print(f\"Target: y = 2x + 3\")\n    \n    constants, rmse = optimize_constants(tree, x_test, y_test)\n    print(f\"Optimized constants: {constants}\")\n    print(f\"Final RMSE: {rmse:.6f}\")\n    \n    # Verify\n    y_pred = tree.evaluate(x_test, constants=constants)\n    print(f\"Predictions: {y_pred}\")\n    print(f\"Targets: {y_test}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile utils/detect_pattern.py\n",
        "\"\"\"\nTarget Pattern Detection for AlphaSymbolic.\nAnalyzes target Y values to detect patterns (polynomial, exponential, periodic, etc.)\nand suggests initial search biases.\n\"\"\"\nimport numpy as np\nfrom scipy import stats\nfrom scipy.fft import fft\nfrom core.grammar import ExpressionTree\n\ndef detect_pattern(x_values, y_values):\n    \"\"\"\n    Analyze (x, y) data to detect patterns.\n    Returns a dict with pattern type probabilities and suggested operators.\n    \"\"\"\n    x = np.array(x_values, dtype=np.float64)\n    y = np.array(y_values, dtype=np.float64)\n    \n    results = {\n        'type': 'unknown',\n        'confidence': 0.0,\n        'suggested_ops': [],\n        'details': {}\n    }\n    \n    if len(x) < 3:\n        return results\n    \n    scores = {}\n    \n    # 1. Check for linear pattern (y = ax + b)\n    if len(x) >= 2:\n        slope, intercept, r_value, _, _ = stats.linregress(x, y)\n        scores['linear'] = r_value ** 2\n        results['details']['linear'] = {\n            'slope': slope,\n            'intercept': intercept,\n            'r_squared': r_value ** 2\n        }\n    \n    # 2. Check for quadratic pattern (y = ax^2 + bx + c)\n    if len(x) >= 3:\n        try:\n            coeffs = np.polyfit(x, y, 2)\n            y_pred = np.polyval(coeffs, x)\n            ss_res = np.sum((y - y_pred) ** 2)\n            ss_tot = np.sum((y - np.mean(y)) ** 2)\n            r2 = 1 - ss_res / ss_tot if ss_tot > 0 else 0\n            scores['quadratic'] = r2\n            results['details']['quadratic'] = {\n                'coefficients': coeffs.tolist(),\n                'r_squared': r2\n            }\n        except:\n            pass\n    \n    # 3. Check for exponential pattern (y = a * e^(bx))\n    if np.all(y > 0):  # Exponential only for positive y\n        try:\n            log_y = np.log(y)\n            slope, intercept, r_value, _, _ = stats.linregress(x, log_y)\n            scores['exponential'] = r_value ** 2\n            results['details']['exponential'] = {\n                'a': np.exp(intercept),\n                'b': slope,\n                'r_squared': r_value ** 2\n            }\n        except:\n            pass\n    \n    # 4. Check for periodic/sinusoidal pattern\n    if len(y) >= 4:\n        try:\n            # Simple FFT analysis\n            y_centered = y - np.mean(y)\n            fft_vals = np.abs(fft(y_centered))\n            \n            # Check if there's a dominant frequency\n            if len(fft_vals) > 1:\n                max_idx = np.argmax(fft_vals[1:len(fft_vals)//2]) + 1\n                max_power = fft_vals[max_idx]\n                total_power = np.sum(fft_vals[1:len(fft_vals)//2])\n                \n                if total_power > 0:\n                    periodicity = max_power / total_power\n                    scores['periodic'] = periodicity\n                    results['details']['periodic'] = {\n                        'dominant_freq_idx': int(max_idx),\n                        'periodicity_score': periodicity\n                    }\n        except:\n            pass\n    \n    # 5. Check for power law (y = a * x^b)\n    if np.all(x > 0) and np.all(y > 0):\n        try:\n            log_x = np.log(x)\n            log_y = np.log(y)\n            slope, intercept, r_value, _, _ = stats.linregress(log_x, log_y)\n            scores['power'] = r_value ** 2\n            results['details']['power'] = {\n                'a': np.exp(intercept),\n                'b': slope,\n                'r_squared': r_value ** 2\n            }\n        except:\n            pass\n    \n    # 6. Check for factorial/gamma pattern (for integer-like x)\n    if np.all(x > 0) and np.all(x == np.floor(x)):\n        try:\n            from scipy.special import gamma\n            x_int = x.astype(int)\n            y_gamma = gamma(x_int + 1)  # gamma(n+1) = n!\n            \n            # Simple linear fit between y and gamma\n            if not np.any(np.isinf(y_gamma)):\n                slope, intercept, r_value, _, _ = stats.linregress(y_gamma, y)\n                scores['factorial'] = r_value ** 2\n                results['details']['factorial'] = {\n                    'r_squared': r_value ** 2\n                }\n        except:\n            pass\n    \n    # Determine best pattern\n    if scores:\n        best_pattern = max(scores.items(), key=lambda x: x[1])\n        results['type'] = best_pattern[0]\n        results['confidence'] = best_pattern[1]\n        \n        # Suggest operators based on pattern\n        op_suggestions = {\n            'linear': ['+', '-', '*', 'x', 'C'],\n            'quadratic': ['pow', '+', '*', 'x', 'C', '2'],\n            'exponential': ['exp', '*', '+', 'x', 'C'],\n            'periodic': ['sin', 'cos', '*', '+', 'x', 'C'],\n            'power': ['pow', '*', 'x', 'C'],\n            'factorial': ['gamma', '*', '+', 'x', 'C']\n        }\n        results['suggested_ops'] = op_suggestions.get(best_pattern[0], [])\n    \n    return results\n\n\ndef summarize_pattern(result):\n    \"\"\"Pretty-print pattern detection result.\"\"\"\n    print(f\"\\n=== Pattern Detection ===\")\n    print(f\"Detected Type: {result['type']} (confidence: {result['confidence']:.2%})\")\n    print(f\"Suggested Operators: {', '.join(result['suggested_ops'])}\")\n    \n    if result['type'] in result['details']:\n        print(f\"Details: {result['details'][result['type']]}\")\n\n\nif __name__ == \"__main__\":\n    # Test with different patterns\n    \n    # Linear: y = 2x + 3\n    print(\"\\n--- Test: Linear ---\")\n    x1 = np.linspace(0, 10, 20)\n    y1 = 2 * x1 + 3 + np.random.normal(0, 0.1, 20)\n    result1 = detect_pattern(x1, y1)\n    summarize_pattern(result1)\n    \n    # Quadratic: y = x^2 + 1\n    print(\"\\n--- Test: Quadratic ---\")\n    x2 = np.linspace(-5, 5, 20)\n    y2 = x2**2 + 1\n    result2 = detect_pattern(x2, y2)\n    summarize_pattern(result2)\n    \n    # Exponential: y = 2 * e^(0.5x)\n    print(\"\\n--- Test: Exponential ---\")\n    x3 = np.linspace(0, 5, 20)\n    y3 = 2 * np.exp(0.5 * x3)\n    result3 = detect_pattern(x3, y3)\n    summarize_pattern(result3)\n    \n    # Periodic: y = sin(x)\n    print(\"\\n--- Test: Periodic ---\")\n    x4 = np.linspace(0, 4*np.pi, 50)\n    y4 = np.sin(x4)\n    result4 = detect_pattern(x4, y4)\n    summarize_pattern(result4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile utils/benchmark_runner.py\n",
        "import torch\nimport numpy as np\nimport time\nimport traceback\nfrom search.mcts import MCTS\nfrom data.benchmark_data import BENCHMARK_SUITE, get_benchmark_data\nfrom utils.optimize_constants import optimize_constants\n\ndef run_benchmark_suite(model, device, progress_callback=None):\n    \"\"\"\n    Runs the full benchmark suite.\n    Args:\n        model: Loaded AlphaSymbolic model\n        device: Torch device\n        progress_callback: Function(float, string) to update UI\n        \n    Returns:\n        results: List of result dicts\n        summary: Dict with aggregated stats\n    \"\"\"\n    results = []\n    \n    # Configure MCTS for benchmark (balanced speed/accuracy)\n    # 500 simulations is decent for benchmarking\n    mcts = MCTS(model, device, max_simulations=500, batch_size=32)\n    \n    total = len(BENCHMARK_SUITE)\n    solved_count = 0\n    \n    for i, problem in enumerate(BENCHMARK_SUITE):\n        if progress_callback:\n            progress_callback(i / total, f\"Testing: {problem['name']}...\")\n            \n        x, y, _ = get_benchmark_data(problem['id'])\n        \n        start_time = time.time()\n        \n        # Run Search\n        try:\n            search_result = mcts.search(x, y)\n             # Determine success\n            # Success threshold: RMSE < 0.01 (or 1% relative error)\n            rmse = search_result['rmse']\n            is_solved = rmse < 0.05 # Looser threshold for general regression\n            \n            # Special check for exact integer symbolic match? No, RMSE is ground truth.\n            \n            elapsed = time.time() - start_time\n            \n            if is_solved:\n                solved_count += 1\n                status = \"\u2705 SOLVED\"\n            else:\n                status = \"\u274c FAILED\"\n                \n            results.append({\n                'id': problem['id'],\n                'name': problem['name'],\n                'level': problem['level'],\n                'rmse': rmse,\n                'time': elapsed,\n                'status': status,\n                'found_formula': search_result.get('formula', '???'),\n                'is_solved': is_solved\n            })\n            \n        except Exception as e:\n            print(f\"Error in benchmark {problem['name']}:\")\n            traceback.print_exc()\n            results.append({\n                'id': problem['id'],\n                'name': problem['name'],\n                'level': problem['level'],\n                'rmse': 1e9,\n                'time': 0,\n                'status': \"\u26a0\ufe0f ERROR\",\n                'found_formula': \"Error\",\n                'is_solved': False\n            })\n\n    # Summary\n    if progress_callback:\n        progress_callback(1.0, \"Done!\")\n        \n    score = (solved_count / total) * 100\n    summary = {\n        'total': total,\n        'solved': solved_count,\n        'score': score,\n        'avg_time': np.mean([r['time'] for r in results]) if results else 0\n    }\n    \n    return results, summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile utils/benchmark_comparison.py\n",
        "\"\"\"\nComparative Benchmark: Beam Search vs MCTS vs Alpha-GP Hybrid\nRuns all three search methods on the standard benchmark suite and compares performance.\n\"\"\"\nimport torch\nimport numpy as np\nimport time\nimport traceback\nfrom typing import List, Dict, Callable, Optional\n\nfrom search.mcts import MCTS\nfrom search.beam_search import BeamSearch\nfrom search.hybrid_search import hybrid_solve\nfrom data.benchmark_data import BENCHMARK_SUITE, get_benchmark_data\nfrom core.grammar import ExpressionTree\nfrom utils.optimize_constants import optimize_constants\n\n\ndef run_single_problem(\n    x: np.ndarray, \n    y: np.ndarray, \n    method: str, \n    model, \n    device,\n    timeout_sec: int = 30,\n    beam_width: int = 50\n) -> Dict:\n    \"\"\"\n    Runs a single search method on a single problem.\n    \n    Returns:\n        dict with keys: formula, rmse, time, success\n    \"\"\"\n    start_time = time.time()\n    \n    try:\n        if method == \"beam\":\n            searcher = BeamSearch(model, device, beam_width=beam_width)\n            # BeamSearch expects list-like input and returns a list of results sorted by RMSE\n            results_list = searcher.search(x.tolist(), y.tolist())\n            elapsed = time.time() - start_time\n            if results_list and len(results_list) > 0:\n                result = results_list[0]  # Best result (sorted by RMSE)\n                return {\n                    'formula': result.get('formula', 'N/A'),\n                    'rmse': result.get('rmse', 1e9),\n                    'time': elapsed,\n                    'success': result.get('rmse', 1e9) < 0.05\n                }\n            else:\n                return {'formula': 'No Result', 'rmse': 1e9, 'time': elapsed, 'success': False}\n            \n        elif method == \"mcts\":\n            mcts = MCTS(model, device, max_simulations=500, batch_size=32)\n            # MCTS expects list-like input \n            result = mcts.search(x.tolist(), y.tolist())\n            elapsed = time.time() - start_time\n            return {\n                'formula': result.get('formula', 'N/A'),\n                'rmse': result.get('rmse', 1e9),\n                'time': elapsed,\n                'success': result.get('rmse', 1e9) < 0.05\n            }\n            \n        elif method == \"hybrid\":\n            result = hybrid_solve(\n                model=model,\n                device=device,\n                x_values=x.tolist(),\n                y_values=y.tolist(),\n                beam_width=beam_width,\n                gp_timeout=timeout_sec\n            )\n            elapsed = time.time() - start_time\n            \n            if result['formula']:\n                # Evaluate RMSE for hybrid result\n                try:\n                    tree = ExpressionTree.from_infix(result['formula'])\n                    if tree.is_valid:\n                        preds = tree.evaluate(x)\n                        rmse = np.sqrt(np.mean((preds - y) ** 2))\n                    else:\n                        rmse = 1e9\n                except:\n                    rmse = 1e9\n            else:\n                rmse = 1e9\n                \n            return {\n                'formula': result.get('formula', 'N/A') or 'Failed',\n                'rmse': rmse,\n                'time': elapsed,\n                'success': rmse < 0.05\n            }\n        else:\n            return {'formula': 'Unknown Method', 'rmse': 1e9, 'time': 0, 'success': False}\n            \n    except Exception as e:\n        print(f\"[ERROR] Method {method} failed: {e}\")\n        traceback.print_exc()\n        return {'formula': 'Error', 'rmse': 1e9, 'time': time.time() - start_time, 'success': False}\n\n\ndef run_comparison_benchmark(\n    model, \n    device, \n    methods: List[str] = [\"beam\", \"mcts\", \"hybrid\"],\n    gp_timeout: int = 30,\n    beam_width: int = 50,\n    progress_callback: Optional[Callable] = None\n) -> Dict:\n    \"\"\"\n    Runs all methods on all benchmark problems.\n    \n    Returns:\n        Dict with 'results' (per-problem-per-method) and 'summary' (aggregated stats)\n    \"\"\"\n    results = []\n    method_stats = {m: {'solved': 0, 'total_time': 0, 'total_rmse': 0} for m in methods}\n    \n    total_steps = len(BENCHMARK_SUITE) * len(methods)\n    current_step = 0\n    \n    for problem in BENCHMARK_SUITE:\n        x, y, _ = get_benchmark_data(problem['id'])\n        \n        for method in methods:\n            current_step += 1\n            \n            if progress_callback:\n                progress_callback(\n                    current_step / total_steps, \n                    f\"[{method.upper()}] {problem['name']}...\"\n                )\n            \n            result = run_single_problem(x, y, method, model, device, gp_timeout, beam_width)\n            \n            results.append({\n                'problem_id': problem['id'],\n                'problem_name': problem['name'],\n                'level': problem['level'],\n                'method': method,\n                'formula': result['formula'],\n                'rmse': result['rmse'],\n                'time': result['time'],\n                'success': result['success']\n            })\n            \n            # Update stats\n            method_stats[method]['total_time'] += result['time']\n            method_stats[method]['total_rmse'] += result['rmse'] if result['rmse'] < 1e6 else 0\n            if result['success']:\n                method_stats[method]['solved'] += 1\n    \n    # Compute summary\n    num_problems = len(BENCHMARK_SUITE)\n    summary = {}\n    for method in methods:\n        stats = method_stats[method]\n        summary[method] = {\n            'solved': stats['solved'],\n            'total': num_problems,\n            'score': (stats['solved'] / num_problems) * 100,\n            'avg_time': stats['total_time'] / num_problems,\n            'avg_rmse': stats['total_rmse'] / num_problems\n        }\n    \n    if progress_callback:\n        progress_callback(1.0, \"Benchmark Complete!\")\n    \n    return {'results': results, 'summary': summary}\n\n\ndef format_comparison_table(results: List[Dict]) -> str:\n    \"\"\"\n    Formats the results as a human-readable table.\n    \"\"\"\n    # Group by problem\n    problems = {}\n    for r in results:\n        pid = r['problem_id']\n        if pid not in problems:\n            problems[pid] = {'name': r['problem_name'], 'level': r['level'], 'methods': {}}\n        problems[pid]['methods'][r['method']] = {\n            'rmse': r['rmse'],\n            'time': r['time'],\n            'success': r['success'],\n            'formula': r['formula']\n        }\n    \n    output = []\n    output.append(\"=\" * 100)\n    output.append(f\"{'Problem':<25} | {'Method':<8} | {'RMSE':<12} | {'Time':<8} | {'Status':<10} | Formula\")\n    output.append(\"=\" * 100)\n    \n    for pid, pdata in problems.items():\n        name = pdata['name'][:24]\n        for method, mdata in pdata['methods'].items():\n            rmse_str = f\"{mdata['rmse']:.6f}\" if mdata['rmse'] < 1e6 else \"FAILED\"\n            time_str = f\"{mdata['time']:.2f}s\"\n            status = \"[OK]\" if mdata['success'] else \"[FAIL]\"\n            formula = mdata['formula'][:40] if mdata['formula'] else \"N/A\"\n            output.append(f\"{name:<25} | {method:<8} | {rmse_str:<12} | {time_str:<8} | {status:<10} | {formula}\")\n        output.append(\"-\" * 100)\n    \n    return \"\\n\".join(output)\n\n\ndef print_summary(summary: Dict):\n    \"\"\"\n    Prints a formatted summary comparison.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"BENCHMARK SUMMARY - Method Comparison\")\n    print(\"=\" * 60)\n    print(f\"{'Method':<12} | {'Solved':<10} | {'Score':<10} | {'Avg Time':<10} | {'Avg RMSE':<12}\")\n    print(\"-\" * 60)\n    \n    for method, stats in summary.items():\n        solved_str = f\"{stats['solved']}/{stats['total']}\"\n        score_str = f\"{stats['score']:.1f}%\"\n        time_str = f\"{stats['avg_time']:.2f}s\"\n        rmse_str = f\"{stats['avg_rmse']:.6f}\"\n        print(f\"{method.upper():<12} | {solved_str:<10} | {score_str:<10} | {time_str:<10} | {rmse_str:<12}\")\n    \n    print(\"=\" * 60)\n    \n    # Determine winner\n    best_method = max(summary.items(), key=lambda x: (x[1]['solved'], -x[1]['avg_rmse']))\n    print(f\"\\n*** WINNER: {best_method[0].upper()} with {best_method[1]['solved']}/{best_method[1]['total']} problems solved! ***\")\n\n\nif __name__ == \"__main__\":\n    # Standalone test\n    import sys\n    sys.path.insert(0, '.')\n    \n    from ui.app_core import load_model, get_model\n    \n    print(\"Loading model...\")\n    load_model()\n    model, device = get_model()\n    \n    if model is None:\n        print(\"Error: No model loaded!\")\n        exit(1)\n    \n    print(\"Running comparison benchmark...\")\n    result = run_comparison_benchmark(\n        model, \n        device, \n        methods=[\"beam\", \"mcts\", \"hybrid\"],\n        gp_timeout=30,\n        beam_width=50\n    )\n    \n    print(format_comparison_table(result['results']))\n    print_summary(result['summary'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile utils/simplify.py\n",
        "\"\"\"\nAlgebraic Simplification Module for AlphaSymbolic.\nUses SymPy for symbolic math simplification.\n\"\"\"\nimport sympy as sp\nfrom core.grammar import Node, ExpressionTree, OPERATORS\n\n# SymPy symbol for x\nx_sym = sp.Symbol('x')\n\ndef tree_to_sympy(node):\n    \"\"\"Convert an ExpressionTree Node to a SymPy expression.\"\"\"\n    if node is None:\n        return sp.Integer(0)\n    \n    val = node.value\n    \n    # Terminals\n    if val == 'x':\n        return x_sym\n    if val == 'pi':\n        return sp.pi\n    if val == 'e':\n        return sp.E\n    if val == 'C':\n        # Keep C as symbol for now\n        return sp.Symbol('C')\n    \n    # Try numeric\n    try:\n        return sp.Float(float(val))\n    except:\n        pass\n    \n    # Operators\n    args = [tree_to_sympy(c) for c in node.children]\n    \n    if val == '+': return args[0] + args[1]\n    if val == '-': return args[0] - args[1]\n    if val == '*': return args[0] * args[1]\n    if val == '/': return args[0] / args[1]\n    if val == 'pow': return sp.Pow(args[0], args[1])\n    if val == 'mod': return sp.Mod(args[0], args[1])\n    if val == 'sin': return sp.sin(args[0])\n    if val == 'cos': return sp.cos(args[0])\n    if val == 'tan': return sp.tan(args[0])\n    if val == 'exp': return sp.exp(args[0])\n    if val == 'log': return sp.log(args[0])\n    if val == 'sqrt': return sp.sqrt(args[0])\n    if val == 'abs': return sp.Abs(args[0])\n    if val == 'floor': return sp.floor(args[0])\n    if val == 'ceil': return sp.ceiling(args[0])\n    if val == 'gamma': return sp.gamma(args[0])\n    if val == 'lgamma': return sp.loggamma(args[0])  # SymPy's log-gamma\n    if val == 'neg': return -args[0]\n    \n    return sp.Integer(0)\n\ndef sympy_to_infix(expr):\n    \"\"\"Convert SymPy expression back to a readable string.\"\"\"\n    return str(expr)\n\ndef simplify_tree(tree):\n    \"\"\"\n    Takes an ExpressionTree and returns a simplified infix string.\n    \"\"\"\n    if not tree.is_valid:\n        return \"Invalid\"\n    \n    original_infix = tree.get_infix()\n    \n    try:\n        sympy_expr = tree_to_sympy(tree.root)\n        simplified = sp.simplify(sympy_expr)\n        result_str = str(simplified)\n        \n        # Validate: reject results containing invalid SymPy artifacts\n        # zoo = complex infinity, nan, oo = infinity\n        invalid_terms = ['zoo', 'nan', 'I*']  # I* indicates complex numbers\n        for term in invalid_terms:\n            if term in result_str:\n                return original_infix  # Fall back to original\n        \n        return result_str\n    except Exception as e:\n        # If simplification fails, return original\n        return original_infix\n\ndef simplify_infix(infix_str):\n    \"\"\"\n    Takes an infix string and returns a simplified version.\n    \"\"\"\n    try:\n        expr = sp.sympify(infix_str)\n        simplified = sp.simplify(expr)\n        return str(simplified)\n    except:\n        return infix_str\n\n# Quick test\nif __name__ == \"__main__\":\n    from core.grammar import ExpressionTree\n    \n    # Test: x + 0 should simplify to x\n    tokens = ['+', 'x', '0']\n    tree = ExpressionTree(tokens)\n    print(f\"Original: {tree.get_infix()}\")\n    print(f\"Simplified: {simplify_tree(tree)}\")\n    \n    # Test: x * 1 should simplify to x\n    tokens2 = ['*', 'x', '1']\n    tree2 = ExpressionTree(tokens2)\n    print(f\"Original: {tree2.get_infix()}\")\n    print(f\"Simplified: {simplify_tree(tree2)}\")\n    \n    # Test: x - x should simplify to 0\n    tokens3 = ['-', 'x', 'x']\n    tree3 = ExpressionTree(tokens3)\n    print(f\"Original: {tree3.get_infix()}\")\n    print(f\"Simplified: {simplify_tree(tree3)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile utils/__init__.py\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\"\"\"\nAlphaSymbolic - Gradio Web Interface\nWith GPU/CPU toggle and search method selection.\n\"\"\"\nimport gradio as gr\nimport torch\n\nfrom ui.app_core import load_model, get_device, get_device_info, set_device, get_training_errors, request_stop_training\nfrom ui.app_training import train_basic, train_curriculum, train_self_play, train_supervised, train_hybrid_feedback_loop\nfrom ui.app_search import solve_formula, generate_example\nfrom ui.app_benchmark import get_benchmark_tab\n\n\ndef toggle_device(use_gpu):\n    \"\"\"Toggle between GPU and CPU.\"\"\"\n    device_info = set_device(use_gpu)\n    color = \"#4ade80\" if \"CUDA\" in device_info else \"#fbbf24\" if \"MPS\" in device_info else \"#888\"\n    return f'<div style=\"padding: 10px; background: #0f0f23; border-radius: 8px; border-left: 3px solid {color};\"><span style=\"color: {color}; font-weight: bold;\">{device_info}</span></div>'\n\n\ndef create_app():\n    \"\"\"Create the Gradio app.\"\"\"\n    \n    with gr.Blocks(title=\"AlphaSymbolic\") as demo:\n        \n        # Header\n        device_info = get_device_info()\n        device_color = \"#4ade80\" if \"CUDA\" in device_info else \"#fbbf24\" if \"MPS\" in device_info else \"#888\"\n        \n        gr.HTML(f\"\"\"\n        <div style=\"text-align: center; padding: 20px; background: linear-gradient(90deg, #00d4ff22, transparent, #ff6b6b22); border-radius: 15px; margin-bottom: 20px;\">\n            <h1 style=\"color: #00d4ff; font-size: 42px; margin: 0;\">AlphaSymbolic</h1>\n            <p style=\"color: #888; font-size: 18px; margin: 5px 0;\">Deep Reinforcement Learning para Regresion Simbolica</p>\n        </div>\n        \"\"\")\n        \n        # System Controls\n        with gr.Row():\n            with gr.Column(scale=1):\n                model_selector = gr.Dropdown(choices=[\"lite\", \"pro\"], value=\"lite\", label=\"Arquitectura (Cerebro)\", interactive=True)\n            with gr.Column(scale=3):\n                model_status = gr.Textbox(label=\"Estado del Modelo\", value=\"Lite (Laptop Optimized) - Vocabulario Extendido\", interactive=False)\n        \n        def on_model_change(preset):\n            status, _ = load_model(preset_name=preset)\n            return status\n\n        model_selector.change(on_model_change, model_selector, model_status)\n        \n        with gr.Tabs():\n            # TAB 1: Search\n            with gr.Tab(\"Buscar Formula\"):\n                with gr.Row():\n                    with gr.Column(scale=1):\n                        gr.HTML('<h3 style=\"color: #00d4ff;\">Datos de Entrada</h3>')\n                        x_input = gr.Textbox(label=\"Valores X\", placeholder=\"1, 2, 3, 4, 5...\", lines=2)\n                        y_input = gr.Textbox(label=\"Valores Y\", placeholder=\"5, 7, 9, 11, 13...\", lines=2)\n                        \n                        with gr.Row():\n                            search_method = gr.Radio(\n                                choices=[\"Beam Search\", \"MCTS\", \"Alpha-GP Hybrid\"],\n                                value=\"Alpha-GP Hybrid\",\n                                label=\"Metodo de Busqueda\"\n                            )\n                        \n                        beam_slider = gr.Slider(5, 500, value=50, step=5, label=\"Beam Width / Simulaciones\")\n                        \n                        solve_btn = gr.Button(\"Buscar Formula\", variant=\"primary\", size=\"lg\")\n                        \n                        with gr.Row():\n                            gr.Button(\"Lineal\", size=\"sm\").click(lambda: generate_example(\"lineal\"), outputs=[x_input, y_input])\n                            gr.Button(\"Cuadratico\", size=\"sm\").click(lambda: generate_example(\"cuadratico\"), outputs=[x_input, y_input])\n                            gr.Button(\"Seno\", size=\"sm\").click(lambda: generate_example(\"trig\"), outputs=[x_input, y_input])\n                            gr.Button(\"Exponencial\", size=\"sm\").click(lambda: generate_example(\"exp\"), outputs=[x_input, y_input])\n                    \n                    with gr.Column(scale=2):\n                        result_html = gr.HTML(label=\"Resultado\")\n                        plot_output = gr.Plot(label=\"Visualizacion\")\n                \n                with gr.Row():\n                    pred_html = gr.HTML(label=\"Predicciones\")\n                    alt_html = gr.HTML(label=\"Alternativas\")\n                \n                raw_formula = gr.Textbox(visible=False)\n                \n                solve_btn.click(solve_formula, [x_input, y_input, beam_slider, search_method], \n                               [result_html, plot_output, pred_html, alt_html, raw_formula])\n            \n            # TAB 2: Training\n            with gr.Tab(\"Entrenar Modelo\"):\n                with gr.Row():\n                    gr.HTML(\"\"\"\n                    <div style=\"background: #16213e; padding: 20px; border-radius: 10px; flex: 1;\">\n                        <h3 style=\"color: #ffd93d; margin: 0;\">Centro de Entrenamiento</h3>\n                    </div>\n                    \"\"\")\n                    with gr.Column():\n                        use_gpu = gr.Checkbox(label=\"Usar GPU\", value=torch.cuda.is_available())\n                        device_display = gr.HTML(value=f'<div style=\"padding: 10px; background: #0f0f23; border-radius: 8px; border-left: 3px solid {device_color};\"><span style=\"color: {device_color}; font-weight: bold;\">{device_info}</span></div>')\n                        use_gpu.change(toggle_device, [use_gpu], [device_display])\n                    with gr.Column():\n                        delete_model_btn = gr.Button(\"\ud83d\uddd1\ufe0f Borrar Modelo\", variant=\"secondary\", size=\"sm\")\n                        delete_status = gr.HTML()\n                        \n                        def delete_model_action():\n                            import os\n                            from ui.app_core import CURRENT_PRESET\n                            filename = f\"alpha_symbolic_model_{CURRENT_PRESET}.pth\"\n                            if os.path.exists(filename):\n                                os.remove(filename)\n                                return f'<div style=\"color: #4ade80; padding: 5px;\">\u2705 Modelo [{CURRENT_PRESET}] eliminado. Reinicia la app para usar pesos nuevos.</div>'\n                            return f'<div style=\"color: #888; padding: 5px;\">No hay modelo [{CURRENT_PRESET}] guardado.</div>'\n                        \n                        delete_model_btn.click(delete_model_action, outputs=[delete_status])\n                        \n                        stop_train_btn = gr.Button(\"\u23f9\ufe0f Detener Entrenamiento\", variant=\"stop\", size=\"sm\")\n                        stop_status = gr.HTML()\n                        stop_train_btn.click(request_stop_training, outputs=[stop_status])\n                \n                with gr.Tabs():\n                    # Basic\n                    with gr.Tab(\"Basico\"):\n                        gr.HTML('<p style=\"color: #888;\">Entrenamiento rapido con datos sinteticos</p>')\n                        with gr.Row():\n                            with gr.Column():\n                                epochs_basic = gr.Slider(10, 500, value=100, step=10, label=\"Epocas\")\n                                batch_basic = gr.Slider(16, 128, value=32, step=16, label=\"Batch Size\")\n                                points_basic = gr.Slider(10, 100, value=20, step=10, label=\"Puntos por Formula\")\n                                train_basic_btn = gr.Button(\"Entrenar Basico\", variant=\"primary\")\n                            with gr.Column():\n                                result_basic = gr.HTML()\n                                plot_basic = gr.Plot()\n                        train_basic_btn.click(train_basic, [epochs_basic, batch_basic, points_basic], [result_basic, plot_basic])\n                    \n                    # Curriculum\n                    with gr.Tab(\"Curriculum\"):\n                        gr.HTML('''\n                        <div style=\"background: #0f0f23; padding: 15px; border-radius: 8px; margin-bottom: 15px;\">\n                            <p style=\"color: #00d4ff; margin: 0;\"><strong>Curriculum Learning</strong></p>\n                            <p style=\"color: #888; margin: 5px 0 0 0;\">Empieza con formulas simples y aumenta la dificultad.</p>\n                        </div>\n                        ''')\n                        with gr.Row():\n                            with gr.Column():\n                                epochs_curriculum = gr.Slider(50, 2000, value=200, step=50, label=\"Epocas\")\n                                batch_curriculum = gr.Slider(16, 128, value=64, step=16, label=\"Batch Size\")\n                                points_curriculum = gr.Slider(10, 100, value=20, step=10, label=\"Puntos por Formula\")\n                                train_curriculum_btn = gr.Button(\"Entrenar Curriculum\", variant=\"primary\")\n                            with gr.Column():\n                                result_curriculum = gr.HTML()\n                                plot_curriculum = gr.Plot()\n                        train_curriculum_btn.click(train_curriculum, [epochs_curriculum, batch_curriculum, points_curriculum], [result_curriculum, plot_curriculum])\n                    \n                    # Self-Play\n                    with gr.Tab(\"Self-Play\"):\n                        gr.HTML('''\n                        <div style=\"background: #0f0f23; padding: 15px; border-radius: 8px; margin-bottom: 15px; border-left: 3px solid #ff6b6b;\">\n                            <p style=\"color: #ff6b6b; margin: 0;\"><strong>AlphaZero Self-Play</strong></p>\n                            <p style=\"color: #888; margin: 5px 0 0 0;\">El modelo resuelve problemas y aprende de sus exitos.</p>\n                        </div>\n                        ''')\n                        with gr.Row():\n                            with gr.Column():\n                                iterations_sp = gr.Slider(10, 1000, value=100, step=10, label=\"Iteraciones\")\n                                problems_sp = gr.Slider(5, 200, value=10, step=5, label=\"Problemas/Iter\")\n                                points_sp = gr.Slider(10, 100, value=20, step=10, label=\"Puntos por Formula\")\n                                train_sp_btn = gr.Button(\"Iniciar Self-Play\", variant=\"primary\")\n                            with gr.Column():\n                                result_sp = gr.HTML()\n                                plot_sp = gr.Plot()\n                        train_sp_btn.click(train_self_play, [iterations_sp, problems_sp, points_sp], [result_sp, plot_sp])\n                \n                    # Feedback Loop (Teacher-Student)\n                    with gr.Tab(\"Feedback Loop (Hybrid)\"):\n                        gr.HTML('''\n                        <div style=\"background: #0f0f23; padding: 15px; border-radius: 8px; margin-bottom: 15px; border-left: 3px solid #f1c40f;\">\n                            <p style=\"color: #f1c40f; margin: 0;\"><strong>Teacher-Student Feedback Loop</strong></p>\n                            <p style=\"color: #888; margin: 5px 0 0 0;\">El modelo (Estudiante) intenta resolver problemas. Si falla, el Alpha-GP (Maestro) interviene y a\u00f1ade la soluci\u00f3n al dataset.</p>\n                        </div>\n                        ''')\n                        with gr.Row():\n                            with gr.Column():\n                                iterations_fb = gr.Slider(5, 500, value=20, step=5, label=\"Ciclos\")\n                                problems_fb = gr.Slider(5, 50, value=10, step=5, label=\"Problemas Dif\u00edciles / Ciclo\")\n                                timeout_fb = gr.Slider(5, 30, value=10, step=5, label=\"Timeout Maestro (s)\")\n                                train_fb_btn = gr.Button(\"Iniciar Feedback Loop\", variant=\"primary\")\n                            with gr.Column():\n                                result_fb = gr.HTML()\n                                plot_fb = gr.Plot()\n                        train_fb_btn.click(train_hybrid_feedback_loop, [iterations_fb, problems_fb, timeout_fb], [result_fb, plot_fb])\n                \n                # --- PRE-TRAINING (Warmup) ---\n                with gr.Accordion(\"\ud83c\udf93 Escuela Primaria (Pre-Entrenamiento)\", open=False):\n                    gr.Markdown(\"Entrenamiento masivo supervisado de alta velocidad para aprender sintaxis basica. **Recomendado al inicio.**\")\n                    with gr.Row():\n                        with gr.Column():\n                            epochs_pre = gr.Slider(100, 10000, value=2000, step=100, label=\"Iteraciones R\u00e1pidas\")\n                            train_pre_btn = gr.Button(\"Iniciar Pre-Entrenamiento\", variant=\"primary\")\n                        with gr.Column():\n                            result_pre = gr.HTML()\n                            plot_pre = gr.Plot()\n                    train_pre_btn.click(train_supervised, [epochs_pre], [result_pre, plot_pre])\n\n                # --- HALL OF SHAME (Error Analysis) ---\n                with gr.Accordion(\"\ud83d\udd75\ufe0f\u200d\u2642\ufe0f Hall of Shame (Analisis de Errores)\", open=False):\n                    gr.Markdown(\"Aqu\u00ed se muestran los problemas donde el modelo fall\u00f3 dr\u00e1sticamente hoy.\")\n                    error_table = gr.DataFrame(\n                        headers=[\"Time\", \"Target Formula\", \"Predicted\", \"Loss\", \"Stage\"],\n                        datatype=[\"str\", \"str\", \"str\", \"number\", \"str\"],\n                        interactive=False\n                    )\n                    refresh_errors_btn = gr.Button(\"\ud83d\udd04 Actualizar Errores\", size=\"sm\")\n                    \n                    def update_errors():\n                        errors = get_training_errors()\n                        # Reverse to show newest first\n                        data = [[\n                            e['time'], e['target'], e['predicted'], round(e['loss'], 2), e['stage']\n                        ] for e in reversed(errors)]\n                        return data\n                    \n                    refresh_errors_btn.click(update_errors, outputs=[error_table])\n            \n            # TAB 4: Benchmark\n            get_benchmark_tab()\n\n            # TAB 5: Info\n            with gr.Tab(\"Informacion\"):\n                device_info_current = get_device_info()\n                device_color_current = \"#4ade80\" if \"CUDA\" in device_info_current else \"#fbbf24\" if \"MPS\" in device_info_current else \"#888\"\n                \n                gr.HTML(f\"\"\"\n                <div style=\"background: #1a1a2e; padding: 30px; border-radius: 15px;\">\n                    <h2 style=\"color: #00d4ff;\">Que es AlphaSymbolic?</h2>\n                    <p style=\"color: #ccc; line-height: 1.8;\">\n                        Sistema de <strong style=\"color: #ff6b6b;\">regresion simbolica</strong> \n                        basado en <strong style=\"color: #00d4ff;\">Deep Learning</strong> y \n                        <strong style=\"color: #ffd93d;\">Monte Carlo Tree Search</strong>.\n                    </p>\n                    \n                    <h3 style=\"color: #00d4ff; margin-top: 30px;\">Dispositivo Actual</h3>\n                    <p style=\"color: {device_color_current}; font-size: 20px;\">{device_info_current}</p>\n                    \n                    <h3 style=\"color: #00d4ff; margin-top: 30px;\">Metodos de Busqueda</h3>\n                    <ul style=\"color: #ccc;\">\n                        <li><strong>Beam Search:</strong> Explora multiples candidatos en paralelo (rapido)</li>\n                        <li><strong>MCTS:</strong> Monte Carlo Tree Search (mas preciso, lento)</li>\n                        <li><strong>Alpha-GP Hybrid:</strong> Fusiona Neural Search con Algoritmo Genetico GPU (Extremo)</li>\n                    </ul>\n                    \n                    <h3 style=\"color: #00d4ff; margin-top: 30px;\">Operadores</h3>\n                    <div style=\"display: flex; flex-wrap: wrap; gap: 10px; margin: 15px 0;\">\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #00d4ff;\">+</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #00d4ff;\">-</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #00d4ff;\">*</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #00d4ff;\">/</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #ff6b6b;\">sin</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #ff6b6b;\">cos</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #ffd93d;\">exp</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #ffd93d;\">log</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #4ade80;\">pow</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #4ade80;\">sqrt</span>\n                    </div>\n                </div>\n                \"\"\")\n        \n        gr.HTML(\"\"\"\n        <div style=\"text-align: center; padding: 20px; color: #666; margin-top: 30px;\">\n            <p>Powered by PyTorch - SymPy - Scipy - Gradio</p>\n        </div>\n        \"\"\")\n    \n    return demo\n\n\n\n# --- Global Initialization for Hot Reloading ---\nprint(\"Iniciando AlphaSymbolic (Global Init)...\")\n# Load model once at module level so 'gradio app.py' works\nstatus_init, device_info_init = load_model() \nprint(f\"   {status_init} | {device_info_init}\")\n\n# Create the app instance globally\ndemo = create_app()\n\nif __name__ == \"__main__\":\n    print(\"Abriendo navegador...\")\n    # Launch with auto-reload compatibility if run directly (though proper reload needs 'gradio app.py')\n    demo.launch(share=True, inbrowser=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the application\n",
        "!python app.py\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}