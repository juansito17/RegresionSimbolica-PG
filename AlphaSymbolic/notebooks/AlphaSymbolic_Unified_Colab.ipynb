{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AlphaSymbolic - Unified Hybrid System\n",
        "# -------------------------------------\n",
        "# Instructions:\n",
        "# 1. Runtime -> Change runtime type -> T4 GPU\n",
        "# 2. Mount Google Drive to PERSIST models\n",
        "# 3. Run All\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    os.makedirs('/content/drive/MyDrive/AlphaSymbolic_Models', exist_ok=True)\n",
        "    print(\"\u2705 Google Drive mounted correctly\")\n",
        "except Exception as e:\n",
        "    print(\"\u26a0\ufe0f Google Drive NOT mounted. Models will be LOST after session ends.\")\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "# Install dependencies\n",
        "!pip install gradio torch torchvision torchaudio scipy matplotlib sympy\n",
        "\n",
        "# Create Directory Structure\n",
        "import os\n",
        "os.makedirs('Code/src', exist_ok=True)\n",
        "os.makedirs('Code/build', exist_ok=True)\n",
        "os.makedirs('AlphaSymbolic', exist_ok=True)\n",
        "os.makedirs('AlphaSymbolic/data/benchmarks', exist_ok=True)\n",
        "directories = ['core', 'data', 'search', 'ui', 'utils', 'models', 'results', 'tools', 'logs', 'notebooks']\n",
        "for d in directories:\n",
        "    os.makedirs(os.path.join('AlphaSymbolic', d), exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/AdvancedFeatures.cpp\n",
        "#include \"AdvancedFeatures.h\"\n#include \"GradientOptimizer.h\"\n#include \"Globals.h\"\n#include \"GeneticOperators.h\"\n#include \"Fitness.h\"\n#include \"ExpressionTree.h\" // Necesario para tree_to_string en la simplificaci\u00f3n\n#include <cmath>\n#include <numeric>\n#include <algorithm>\n#include <iostream>\n#include <unordered_map>\n#include <vector>\n\n//---------------------------------\n// EvolutionParameters\n//---------------------------------\nEvolutionParameters EvolutionParameters::create_default() {\n    // Usa constantes globales para los valores por defecto\n    return {BASE_MUTATION_RATE, BASE_ELITE_PERCENTAGE, DEFAULT_TOURNAMENT_SIZE, DEFAULT_CROSSOVER_RATE};\n}\n\nvoid EvolutionParameters::mutate(int stagnation_counter) {\n    auto& rng = get_rng();\n    double aggression_factor = 1.0;\n    // Ajuste del factor de agresi\u00f3n basado en el estancamiento\n    if (stagnation_counter > STAGNATION_LIMIT_ISLAND / 2) {\n        // Aumenta la agresi\u00f3n si hay estancamiento significativo\n        aggression_factor = 1.0 + (static_cast<double>(stagnation_counter - STAGNATION_LIMIT_ISLAND / 2) / (STAGNATION_LIMIT_ISLAND / 2.0)) * 0.5; // Escala de 1.0 a 1.5\n        aggression_factor = std::min(aggression_factor, 2.0); // Limitar la agresi\u00f3n m\u00e1xima\n    } else if (stagnation_counter < STAGNATION_LIMIT_ISLAND / 4 && stagnation_counter > 0) {\n        // Reduce la agresi\u00f3n si no hay mucho estancamiento, pero no es 0\n        aggression_factor = 1.0 - (static_cast<double>(STAGNATION_LIMIT_ISLAND / 4 - stagnation_counter) / (STAGNATION_LIMIT_ISLAND / 4.0)) * 0.5; // Escala de 0.5 a 1.0\n        aggression_factor = std::max(aggression_factor, 0.5); // Limitar la agresi\u00f3n m\u00ednima\n    } else if (stagnation_counter == 0) {\n        // Muy poco estancamiento, cambios muy peque\u00f1os\n        aggression_factor = 0.2; // Cambios muy conservadores\n    }\n\n    std::uniform_real_distribution<double> base_rate_change(-0.05, 0.05);\n    std::uniform_int_distribution<int> base_tourney_change(-2, 2);\n\n    double rate_change_val = base_rate_change(rng) * aggression_factor;\n    int tourney_change_val = static_cast<int>(std::round(base_tourney_change(rng) * aggression_factor));\n    \n    // Asegurar que haya alg\u00fan cambio si la agresi\u00f3n es alta y el cambio base es 0\n    if (aggression_factor > 1.0 && tourney_change_val == 0 && base_tourney_change(rng) != 0) {\n         tourney_change_val = (base_tourney_change(rng) > 0) ? 1 : -1;\n    }\n\n    // Definir l\u00edmites din\u00e1micos para los par\u00e1metros\n    double min_mutation = 0.05;\n    double max_mutation_base = 0.5;\n    double max_mutation = min_mutation + (max_mutation_base - min_mutation) * (1.0 + aggression_factor / 2.0);\n\n    double min_elite = 0.02;\n    double max_elite_base = 0.25;\n    double max_elite = min_elite + (max_elite_base - min_elite) * (1.0 + aggression_factor / 2.0);\n\n    int min_tournament = 3;\n    int max_tournament_base = 30;\n    int max_tournament = min_tournament + static_cast<int>((max_tournament_base - min_tournament) * (1.0 + aggression_factor / 2.0));\n\n    // Aplicar los cambios y asegurar que est\u00e9n dentro de los l\u00edmites\n    mutation_rate = std::clamp(mutation_rate + rate_change_val, min_mutation, max_mutation);\n    elite_percentage = std::clamp(elite_percentage + rate_change_val, min_elite, max_elite);\n    tournament_size = std::clamp(tournament_size + tourney_change_val, min_tournament, max_tournament);\n    crossover_rate = std::clamp(crossover_rate + rate_change_val, 0.5, 0.95);\n}\n\n//---------------------------------\n// PatternMemory\n//---------------------------------\nvoid PatternMemory::record_success(const NodePtr& tree, double fitness) {\n    std::string pattern = extract_pattern(tree);\n    if (pattern.empty() || pattern.length() > 50 || pattern.length() < 3 || pattern == \"N\") return;\n    auto it = patterns.find(pattern);\n    if (it == patterns.end()) {\n        patterns[pattern] = {pattern, fitness, 1, (fitness < INF ? 1.0 : 0.0)};\n    } else {\n        auto& p = it->second;\n        p.uses++;\n        double improvement = (fitness < p.best_fitness && p.best_fitness < INF) ? 1.0 : 0.0;\n        p.success_rate = ((p.success_rate * (p.uses - 1)) + improvement) / p.uses;\n        p.best_fitness = std::min(p.best_fitness, fitness);\n    }\n}\n\nNodePtr PatternMemory::suggest_pattern_based_tree(int max_depth) {\n    if (patterns.empty()) return nullptr;\n    std::vector<std::pair<std::string, double>> candidates;\n    for (const auto& [pattern_str, info] : patterns) {\n        if (info.uses >= PATTERN_MEM_MIN_USES && (info.success_rate > 0.1 || info.best_fitness < PATTERN_RECORD_FITNESS_THRESHOLD)) {\n             double weight = info.success_rate + (1.0 / (1.0 + info.best_fitness));\n             candidates.emplace_back(pattern_str, weight);\n        }\n    }\n    if (candidates.empty()) return nullptr;\n    std::vector<double> weights;\n    std::transform(candidates.begin(), candidates.end(), std::back_inserter(weights), [](const auto& p){ return p.second; });\n    std::discrete_distribution<> dist(weights.begin(), weights.end());\n    auto& rng = get_rng();\n    int selected_idx = dist(rng);\n    return parse_pattern(candidates[selected_idx].first, max_depth);\n}\n\nstd::string PatternMemory::extract_pattern(const NodePtr& node) {\n    if (!node) return \"N\";\n    switch (node->type) {\n        case NodeType::Constant: return \"#\";\n        case NodeType::Variable: return \"x\";\n        case NodeType::Operator:\n            return \"(\" + extract_pattern(node->left) + node->op + extract_pattern(node->right) + \")\";\n        default: return \"?\";\n    }\n}\n\nNodePtr PatternMemory::parse_pattern(const std::string& pattern, int max_depth) {\n    // Placeholder implementation\n    if (pattern == \"#\") {\n        auto node = std::make_shared<Node>(NodeType::Constant);\n        if (FORCE_INTEGER_CONSTANTS) { std::uniform_int_distribution<int> cd(CONSTANT_INT_MIN_VALUE, CONSTANT_INT_MAX_VALUE); node->value = static_cast<double>(cd(get_rng())); }\n        else { std::uniform_real_distribution<double> cd(CONSTANT_MIN_VALUE, CONSTANT_MAX_VALUE); node->value = cd(get_rng()); }\n        if(std::fabs(node->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) node->value = 0.0;\n        return node;\n    }\n    if (pattern == \"x\") return std::make_shared<Node>(NodeType::Variable);\n    if (pattern == \"N\") return nullptr;\n    if (pattern.length() > 3 && pattern.front() == '(' && pattern.back() == ')') {\n          return generate_random_tree(max_depth); // Fallback\n     }\n    return generate_random_tree(max_depth); // Fallback\n}\n\n//---------------------------------\n// Pareto Optimizer\n//---------------------------------\nParetoSolution::ParetoSolution(NodePtr t, double acc, double complexity_val) : tree(std::move(t)), accuracy(acc), complexity(complexity_val), dominated(false) {}\n\nbool ParetoSolution::dominates(const ParetoSolution& other) const {\n    bool better_in_one = (accuracy < other.accuracy) || (complexity < other.complexity);\n    bool not_worse_in_any = (accuracy <= other.accuracy) && (complexity <= other.complexity);\n    return better_in_one && not_worse_in_any;\n}\n\nvoid ParetoOptimizer::update(const std::vector<Individual>& population, const std::vector<double>& targets, const std::vector<std::vector<double>>& x_values) {\n    std::vector<ParetoSolution> candidates = pareto_front;\n    for (const auto& ind : population) {\n        if (ind.tree && ind.fitness_valid && ind.fitness < INF) {\n            candidates.emplace_back(ind.tree, ind.fitness, static_cast<double>(tree_size(ind.tree)));\n        }\n    }\n    for (auto& sol1 : candidates) {\n        sol1.dominated = false;\n        for (const auto& sol2 : candidates) {\n            if (&sol1 == &sol2) continue;\n            if (sol2.dominates(sol1)) { sol1.dominated = true; break; }\n        }\n    }\n    pareto_front.clear();\n    std::copy_if(candidates.begin(), candidates.end(), std::back_inserter(pareto_front),\n                 [](const auto& sol) { return !sol.dominated; });\n    if (pareto_front.size() > PARETO_MAX_FRONT_SIZE) {\n        std::sort(pareto_front.begin(), pareto_front.end(), [](const auto& a, const auto& b){ return a.accuracy < b.accuracy; });\n        pareto_front.resize(PARETO_MAX_FRONT_SIZE);\n    }\n}\n\nstd::vector<NodePtr> ParetoOptimizer::get_pareto_solutions() {\n    std::vector<NodePtr> result;\n    result.reserve(pareto_front.size());\n    std::transform(pareto_front.begin(), pareto_front.end(), std::back_inserter(result),\n                   [](const auto& sol) { return sol.tree; });\n    return result;\n}\n\n//---------------------------------\n// Domain Constraints\n//---------------------------------\nbool DomainConstraints::is_valid_recursive(const NodePtr& node) {\n     if (!node) return true;\n     if (node->type == NodeType::Operator) {\n         if (node->op == '/' && node->right && node->right->type == NodeType::Constant && std::fabs(node->right->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) return false;\n         if (node->op == '^') { // Solo chequear 0^negativo/0\n              if (node->left && node->left->type == NodeType::Constant && std::fabs(node->left->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE &&\n                  node->right && node->right->type == NodeType::Constant && node->right->value <= SIMPLIFY_NEAR_ZERO_TOLERANCE) {\n                      return false;\n              }\n         }\n         if ((node->op == '*' || node->op == '/') && node->right && node->right->type == NodeType::Constant && std::fabs(node->right->value - 1.0) < SIMPLIFY_NEAR_ONE_TOLERANCE) return false;\n         if ((node->op == '+' || node->op == '-') && node->right && node->right->type == NodeType::Constant && std::fabs(node->right->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) return false;\n         if (node->op == '*' && node->left && node->left->type == NodeType::Constant && std::fabs(node->left->value - 1.0) < SIMPLIFY_NEAR_ONE_TOLERANCE) return false;\n         if (node->op == '+' && node->left && node->left->type == NodeType::Constant && std::fabs(node->left->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) return false;\n         if (!is_valid_recursive(node->left) || !is_valid_recursive(node->right)) return false;\n     }\n     return true;\n }\n\nbool DomainConstraints::is_valid(const NodePtr& tree) {\n    return is_valid_recursive(tree);\n}\n\nNodePtr DomainConstraints::simplify_recursive(NodePtr node) {\n    if (!node || node->type != NodeType::Operator) return node;\n    node->left = simplify_recursive(node->left);\n    node->right = simplify_recursive(node->right);\n\n    // Manejo de hijos nulos\n    bool is_unary = (node->op == 's' || node->op == 'c' || node->op == 'l' || node->op == 'e' || node->op == '!' || node->op == '_' || node->op == 'g');\n\n    // Constant Folding (First priority)\n    bool left_is_const = (node->left && node->left->type == NodeType::Constant);\n    bool right_is_const = (node->right && node->right->type == NodeType::Constant);\n    \n    // Fold if binary op with 2 constants OR unary op with 1 constant\n    if ((left_is_const && right_is_const) || (is_unary && left_is_const)) {\n        try {\n            double result = evaluate_tree(node, std::vector<double>{0.0}); \n            if (!std::isnan(result) && !std::isinf(result)) {\n                auto cn = std::make_shared<Node>(NodeType::Constant);\n                if (FORCE_INTEGER_CONSTANTS) cn->value = std::round(result); else cn->value = result;\n                if (std::fabs(cn->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) cn->value = 0.0; return cn;\n            }\n        } catch (const std::exception&) {}\n    }\n\n    if (node->left && !node->right) {\n        if (is_unary) return node; // Correct state for unary ops (Constant folding didn't trigger, so var inside)\n        return node->left; // Simplify \"A op null\" -> A (for binary ops? dangerous but existing logic)\n    }\n    if (!node->left && node->right) return node->right;\n    if (!node->left && !node->right) { auto cn = std::make_shared<Node>(NodeType::Constant); cn->value = 1.0; return cn; }\n\n    // Identity Simplifications & Fixes\n     if ((node->op == '+' || node->op == '-') && node->right && node->right->type == NodeType::Constant && std::fabs(node->right->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) return node->left;\n     if (node->op == '+' && node->left && node->left->type == NodeType::Constant && std::fabs(node->left->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) return node->right;\n     if ((node->op == '*' || node->op == '/') && node->right && node->right->type == NodeType::Constant && std::fabs(node->right->value - 1.0) < SIMPLIFY_NEAR_ONE_TOLERANCE) return node->left;\n     if (node->op == '*' && node->left && node->left->type == NodeType::Constant && std::fabs(node->left->value - 1.0) < SIMPLIFY_NEAR_ONE_TOLERANCE) return node->right;\n     if (node->op == '*' && ((node->left && node->left->type == NodeType::Constant && std::fabs(node->left->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) || (node->right && node->right->type == NodeType::Constant && std::fabs(node->right->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE))) { auto z = std::make_shared<Node>(NodeType::Constant); z->value = 0.0; return z; }\n     if (node->op == '^' && node->right && node->right->type == NodeType::Constant && std::fabs(node->right->value - 1.0) < SIMPLIFY_NEAR_ONE_TOLERANCE) return node->left; // A^1 -> A\n     if (node->op == '^' && node->right && node->right->type == NodeType::Constant && std::fabs(node->right->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) { auto o = std::make_shared<Node>(NodeType::Constant); o->value = 1.0; return o; } // A^0 -> 1\n    // Fix div by zero (constante)\n    if (node->op == '/' && node->right && node->right->type == NodeType::Constant && std::fabs(node->right->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) node->right->value = 1.0;\n\n    // --- NUEVAS REGLAS DE SIMPLIFICACI\u00d3N ---\n    // X / X = 1 (si X no es cero)\n    if (node->op == '/' && node->left && node->right) {\n        if (tree_to_string(node->left) == tree_to_string(node->right)) {\n            // Verificar que el divisor no sea cero para evitar 0/0\n            if (node->right->type != NodeType::Constant || std::fabs(node->right->value) >= SIMPLIFY_NEAR_ZERO_TOLERANCE) {\n                auto one = std::make_shared<Node>(NodeType::Constant);\n                one->value = 1.0;\n                return one;\n            }\n        }\n    }\n\n    // X - X = 0\n    if (node->op == '-' && node->left && node->right) {\n        if (tree_to_string(node->left) == tree_to_string(node->right)) {\n            auto zero = std::make_shared<Node>(NodeType::Constant);\n            zero->value = 0.0;\n            return zero;\n        }\n    }\n    // Ya no se hace clamp de exponente constante aqu\u00ed, se quit\u00f3 la restricci\u00f3n\n\n    return node;\n}\n\nNodePtr DomainConstraints::fix_or_simplify(NodePtr tree) {\n    if (!tree) return nullptr;\n    NodePtr cloned_tree = clone_tree(tree);\n    NodePtr simplified_tree = simplify_recursive(cloned_tree);\n    return simplified_tree;\n}\n\n//---------------------------------\n// Local Improvement\n//---------------------------------\n//---------------------------------\n// Local Improvement\n//---------------------------------\nvoid optimize_constants(NodePtr& tree, const std::vector<double>& targets, const std::vector<std::vector<double>>& x_values, double* d_targets, double* d_x_values) {\n    if (!tree) return;\n    \n    // 1. Collect constant nodes\n    std::vector<Node*> constants;\n    std::vector<Node*> stack;\n    stack.push_back(tree.get());\n    while(!stack.empty()){\n        Node* n = stack.back(); stack.pop_back();\n        if(!n) continue;\n        if(n->type == NodeType::Constant) constants.push_back(n);\n        else if(n->type == NodeType::Operator){\n            stack.push_back(n->right.get());\n            stack.push_back(n->left.get());\n        }\n    }\n    \n    if (constants.empty()) return;\n\n    // 2. Hill Climbing (Numeric Optimization)\n    int max_iter = 20; // Fast local search\n    auto& rng = get_rng();\n    \n    // Evaluate initial fitness\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n    double current_fitness = evaluate_fitness(tree, targets, x_values, d_targets, d_x_values);\n#else\n    double current_fitness = evaluate_fitness(tree, targets, x_values);\n#endif\n\n    std::normal_distribution<double> perturbation(0.0, 0.5); // Perturb standard deviation 0.5\n\n    for(int i=0; i<max_iter; ++i) {\n        // Select a random constant\n        int idx = std::uniform_int_distribution<int>(0, constants.size()-1)(rng);\n        double old_val = constants[idx]->value;\n        \n        // Perturb\n        double delta = perturbation(rng);\n        constants[idx]->value += delta;\n        if (FORCE_INTEGER_CONSTANTS) constants[idx]->value = std::round(constants[idx]->value);\n\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n        double new_fitness = evaluate_fitness(tree, targets, x_values, d_targets, d_x_values);\n#else\n        double new_fitness = evaluate_fitness(tree, targets, x_values);\n#endif\n\n        if (new_fitness < current_fitness) {\n            current_fitness = new_fitness; // Accept\n            // Adapt perturbation? Maybe reduce sigma?\n        } else {\n            constants[idx]->value = old_val; // Revert\n        }\n        \n        if (current_fitness < EXACT_SOLUTION_THRESHOLD) break;\n    }\n}\n\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\nstd::pair<NodePtr, double> try_local_improvement(const NodePtr& tree, double current_fitness, const std::vector<double>& targets, const std::vector<std::vector<double>>& x_values, int attempts, double* d_targets, double* d_x_values) {\n    // 1. First, try to optimize constants of the CURRENT tree using GRADIENT DESCENT\n    NodePtr optimized_tree = clone_tree(tree);\n    // Use Gradient Optimization (Adam) - much more precise than Hill Climbing\n    optimize_constants_gradient(optimized_tree, targets, x_values, 0.05, 30);\n    \n    #ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n        double optimized_fitness = evaluate_fitness(optimized_tree, targets, x_values, d_targets, d_x_values);\n    #else\n        double optimized_fitness = evaluate_fitness(optimized_tree, targets, x_values);\n    #endif\n    \n    NodePtr best_neighbor = (optimized_fitness < current_fitness) ? optimized_tree : tree;\n    double best_neighbor_fitness = (optimized_fitness < current_fitness) ? optimized_fitness : current_fitness;\n\n    if (best_neighbor_fitness >= INF) return {best_neighbor, best_neighbor_fitness};\n\n    // 2. Structural Search (as before)\n    for (int i = 0; i < attempts; ++i) {\n        NodePtr neighbor = mutate_tree(best_neighbor, 1.0, 2); // Mutate the BEST so far\n        neighbor = DomainConstraints::fix_or_simplify(neighbor);\n        if (!neighbor) continue;\n        \n        // Also optimize constants of structural neighbor?\n        // Maybe too expensive. Let's do a quick random constant tweak.\n        // optimize_constants(neighbor, targets, x_values, d_targets, d_x_values); \n        \n        double neighbor_fitness = evaluate_fitness(neighbor, targets, x_values, d_targets, d_x_values);\n        if (neighbor_fitness < best_neighbor_fitness) {\n            best_neighbor = neighbor;\n            best_neighbor_fitness = neighbor_fitness;\n        }\n    }\n    return {best_neighbor, best_neighbor_fitness};\n}\n#else\nstd::pair<NodePtr, double> try_local_improvement(const NodePtr& tree, double current_fitness, const std::vector<double>& targets, const std::vector<std::vector<double>>& x_values, int attempts) {\n    // 1. First, try to optimize constants of the CURRENT tree using GRADIENT DESCENT\n    NodePtr optimized_tree = clone_tree(tree);\n    optimize_constants_gradient(optimized_tree, targets, x_values, 0.05, 30);\n    double optimized_fitness = evaluate_fitness(optimized_tree, targets, x_values);\n    \n    NodePtr best_neighbor = (optimized_fitness < current_fitness) ? optimized_tree : tree;\n    double best_neighbor_fitness = (optimized_fitness < current_fitness) ? optimized_fitness : current_fitness;\n\n    if (best_neighbor_fitness >= INF) return {best_neighbor, best_neighbor_fitness};\n\n    for (int i = 0; i < attempts; ++i) {\n        NodePtr neighbor = mutate_tree(best_neighbor, 1.0, 2);\n        neighbor = DomainConstraints::fix_or_simplify(neighbor);\n        if (!neighbor) continue;\n        double neighbor_fitness = evaluate_fitness(neighbor, targets, x_values);\n        if (neighbor_fitness < best_neighbor_fitness) {\n            best_neighbor = neighbor;\n            best_neighbor_fitness = neighbor_fitness;\n        }\n    }\n    return {best_neighbor, best_neighbor_fitness};\n}\n#endif\n\n//---------------------------------\n// Target Pattern Detection\n//---------------------------------\nstd::pair<std::string, double> detect_target_pattern(const std::vector<double>& targets) {\n    if (targets.size() < 3) return {\"none\", 0.0};\n    bool is_arithmetic = true; double diff = targets[1] - targets[0];\n    for (size_t i = 2; i < targets.size(); ++i) if (std::fabs((targets[i] - targets[i-1]) - diff) > 1e-6) { is_arithmetic = false; break; }\n    if (is_arithmetic) return {\"arithmetic\", diff};\n    bool is_geometric = true;\n    if (std::fabs(targets[0]) < 1e-9) {\n        bool all_zero = true; for(double t : targets) if (std::fabs(t) > 1e-9) { all_zero = false; break; }\n        if(all_zero) return {\"constant_zero\", 0.0}; else is_geometric = false;\n    }\n    if (is_geometric && std::fabs(targets[0]) >= 1e-9) {\n        double ratio = targets[1] / targets[0];\n        for (size_t i = 2; i < targets.size(); ++i) {\n             if (std::fabs(targets[i-1]) < 1e-9) { if (std::fabs(targets[i]) > 1e-9) { is_geometric = false; break; } }\n             else { if (std::fabs((targets[i] / targets[i-1]) - ratio) > 1e-6) { is_geometric = false; break; } }\n        }\n        if (is_geometric) return {\"geometric\", ratio};\n    }\n    return {\"none\", 0.0};\n}\n\n//---------------------------------\n// Generate Pattern Based Tree\n//---------------------------------\nNodePtr generate_pattern_based_tree(const std::string& pattern_type, double pattern_value) {\n    if (X_VALUES.empty() || RAW_TARGETS.empty()) return nullptr;\n    double a = RAW_TARGETS[0]; \n    double x0 = (!X_VALUES[0].empty()) ? X_VALUES[0][0] : 0.0;\n    if (pattern_type == \"arithmetic\") {\n        double d = pattern_value; auto root = std::make_shared<Node>(NodeType::Operator); root->op = '+';\n        auto cp = std::make_shared<Node>(NodeType::Constant); double cv = a - d * x0; if (FORCE_INTEGER_CONSTANTS) cv = std::round(cv); cp->value = (std::fabs(cv) < SIMPLIFY_NEAR_ZERO_TOLERANCE) ? 0.0 : cv; // Use RAW_TARGETS to avoid \"TARGETS\" not found\n\n        auto vp = std::make_shared<Node>(NodeType::Operator); vp->op = '*';\n        auto dc = std::make_shared<Node>(NodeType::Constant); double dv = d; if (FORCE_INTEGER_CONSTANTS) dv = std::round(dv); dc->value = (std::fabs(dv) < SIMPLIFY_NEAR_ZERO_TOLERANCE) ? 0.0 : dv;\n        auto xv = std::make_shared<Node>(NodeType::Variable); vp->left = dc; vp->right = xv;\n        root->left = cp; root->right = vp; return DomainConstraints::fix_or_simplify(root);\n    } else if (pattern_type == \"geometric\") {\n        double r = pattern_value; if (std::fabs(r) < SIMPLIFY_NEAR_ZERO_TOLERANCE) return nullptr;\n        auto root = std::make_shared<Node>(NodeType::Operator); root->op = '*';\n        auto cp = std::make_shared<Node>(NodeType::Constant); double rpx0 = std::pow(r, x0); if (std::fabs(rpx0) < 1e-100) return nullptr;\n        double cv = a / rpx0; if (FORCE_INTEGER_CONSTANTS) cv = std::round(cv); cp->value = (std::fabs(cv) < SIMPLIFY_NEAR_ZERO_TOLERANCE) ? 0.0 : cv;\n        auto vp = std::make_shared<Node>(NodeType::Operator); vp->op = '^';\n        auto rc = std::make_shared<Node>(NodeType::Constant); double rv = r; if (FORCE_INTEGER_CONSTANTS) rv = std::round(rv); rc->value = (std::fabs(rv) < SIMPLIFY_NEAR_ZERO_TOLERANCE) ? 0.0 : rv;\n        auto xv = std::make_shared<Node>(NodeType::Variable); vp->left = rc; vp->right = xv;\n        root->left = cp; root->right = vp; return DomainConstraints::fix_or_simplify(root);\n    } else if (pattern_type == \"constant_zero\") {\n         auto node = std::make_shared<Node>(NodeType::Constant); node->value = 0.0; return node;\n     }\n    return nullptr; // No pattern tree generated\n}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/AdvancedFeatures.h\n",
        "#ifndef ADVANCEDFEATURES_H\n#define ADVANCEDFEATURES_H\n\n#include \"ExpressionTree.h\"\n#include \"Globals.h\" // Incluir Globals.h para INF\n#include <vector>\n#include <string>\n#include <map>\n#include <set>\n#include <utility> // Para std::pair\n#include <unordered_map>\n\n// Meta-evoluci\u00f3n: Par\u00e1metros que pueden adaptarse durante la ejecuci\u00f3n.\nstruct EvolutionParameters {\n    double mutation_rate;    // Tasa de mutaci\u00f3n actual\n    double elite_percentage; // Porcentaje de \u00e9lite actual\n    int tournament_size;     // Tama\u00f1o del torneo actual\n    double crossover_rate;   // Tasa de cruce actual\n\n    // Crea un conjunto de par\u00e1metros con valores por defecto (iniciales).\n    static EvolutionParameters create_default();\n\n    // Adapta (muta) los par\u00e1metros ligeramente.\n    // AHORA RECIBE el contador de estancamiento para ajustar la intensidad.\n    void mutate(int stagnation_counter);\n};\n\n// Memoria de patrones: Almacena sub-estructuras exitosas (Reinforcement Learning).\nclass PatternMemory {\n    struct PatternInfo {\n        std::string pattern_str; // Representaci\u00f3n del patr\u00f3n\n        double best_fitness = INF; // Mejor fitness visto para este patr\u00f3n\n        int uses = 0;             // N\u00famero de veces usado/visto\n        double success_rate = 0.0; // Tasa de \u00e9xito estimada\n    };\n    std::unordered_map<std::string, PatternInfo> patterns; // Mapa para almacenar patrones\n    int min_uses_for_suggestion = 3; // M\u00ednimo de usos para considerar sugerir un patr\u00f3n\n\npublic:\n    // Registra el \u00e9xito de un \u00e1rbol (y su patr\u00f3n) basado en su fitness.\n    void record_success(const NodePtr& tree, double fitness);\n    // Sugiere un \u00e1rbol basado en los patrones exitosos almacenados.\n    NodePtr suggest_pattern_based_tree(int max_depth);\n\nprivate:\n    // Extrae la representaci\u00f3n estructural (string) de un \u00e1rbol.\n    std::string extract_pattern(const NodePtr& tree);\n    // Intenta construir un \u00e1rbol a partir de un patr\u00f3n (string) - funci\u00f3n simplificada.\n    NodePtr parse_pattern(const std::string& pattern, int max_depth);\n};\n\n\n// Optimizaci\u00f3n Pareto: Mantiene un frente de soluciones no dominadas (compromiso precisi\u00f3n/complejidad).\nstruct ParetoSolution {\n    NodePtr tree = nullptr;   // \u00c1rbol de la soluci\u00f3n\n    double accuracy = INF;    // Objetivo 1: Precisi\u00f3n (fitness)\n    double complexity = INF;  // Objetivo 2: Complejidad (tama\u00f1o)\n    bool dominated = false;   // Bandera: \u00bfest\u00e1 dominada por otra soluci\u00f3n?\n\n    // Constructor por defecto (necesario si se usa en contenedores)\n    ParetoSolution() = default;\n    // Constructor principal\n    ParetoSolution(NodePtr t, double acc, double complexity_val);\n\n    // Comprueba si esta soluci\u00f3n domina a otra.\n    bool dominates(const ParetoSolution& other) const;\n};\n\nclass ParetoOptimizer {\n    std::vector<ParetoSolution> pareto_front; // Almacena las soluciones del frente\n    size_t max_front_size = 50; // L\u00edmite opcional para el tama\u00f1o del frente\n\npublic:\n    // Actualiza el frente de Pareto con individuos de la poblaci\u00f3n actual.\n    void update(const std::vector<struct Individual>& population, // Usa Individual struct\n                const std::vector<double>& targets,\n                const std::vector<std::vector<double>>& x_values);\n\n    // Obtiene los \u00e1rboles (NodePtr) de las soluciones en el frente actual.\n    std::vector<NodePtr> get_pareto_solutions();\n\n    // Obtiene una referencia constante al frente de Pareto completo.\n    const std::vector<ParetoSolution>& get_pareto_front() const { return pareto_front; }\n};\n\n\n// Restricciones de Dominio: Verifica y corrige/simplifica \u00e1rboles problem\u00e1ticos.\nclass DomainConstraints {\npublic:\n    // Comprueba si un \u00e1rbol cumple reglas b\u00e1sicas de validez est\u00e1tica.\n    static bool is_valid(const NodePtr& tree);\n\n    // Intenta simplificar/corregir un \u00e1rbol (devuelve una copia modificada).\n    static NodePtr fix_or_simplify(NodePtr tree);\n\nprivate:\n     // Ayudante recursivo para la simplificaci\u00f3n.\n    static NodePtr simplify_recursive(NodePtr node);\n    // Ayudante recursivo para la validaci\u00f3n est\u00e1tica.\n    static bool is_valid_recursive(const NodePtr& node);\n};\n\n// B\u00fasqueda Local: Intenta mejorar una soluci\u00f3n dada explorando vecinos cercanos.\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\nstd::pair<NodePtr, double> try_local_improvement(const NodePtr& tree,\n                                                  double current_fitness,\n                                                  const std::vector<double>& targets,\n                                                  const std::vector<std::vector<double>>& x_values,\n                                                  int attempts,\n                                                  double* d_targets, double* d_x_values);\n#else\nstd::pair<NodePtr, double> try_local_improvement(const NodePtr& tree,\n                                                  double current_fitness,\n                                                  const std::vector<double>& targets,\n                                                  const std::vector<std::vector<double>>& x_values,\n                                                  int attempts = 10);\n#endif\n\n\n// Detecci\u00f3n de Patrones en los Datos Objetivo.\nstd::pair<std::string, double> detect_target_pattern(const std::vector<double>& targets);\nNodePtr generate_pattern_based_tree(const std::string& pattern_type, double pattern_value);\n\n\n#endif // ADVANCEDFEATURES_H\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/ExpressionTree.cpp\n",
        "#include \"ExpressionTree.h\"\n#include \"Globals.h\"\n#include <cmath>\n#include <limits>\n#include <stdexcept>\n#include <vector>\n#include <iostream>\n#include <iomanip>\n#include <string>\n#include <sstream>\n#include <stack>\n#include <unordered_map>\n#include <algorithm> // Para std::remove_if\n#include <cctype>    // Para isdigit, isspace\n#include <thread>    // Para thread_local RNG\n\n// --- Funci\u00f3n auxiliar para formatear constantes ---\n// --- Funci\u00f3n auxiliar para formatear constantes ---\nstd::string format_constant(double val) {\n    // Si es un entero o muy cercano a un entero, formatarlo como tal.\n    if (std::fabs(val - std::round(val)) < SIMPLIFY_NEAR_ZERO_TOLERANCE) {\n        return std::to_string(static_cast<long long>(std::round(val)));\n    } else {\n        std::ostringstream oss;\n        // Usar notaci\u00f3n cient\u00edfica para valores muy grandes o muy peque\u00f1os,\n        // o notaci\u00f3n fija para el resto, con precisi\u00f3n adecuada.\n        // Esto evita cadenas muy largas o p\u00e9rdida de informaci\u00f3n.\n        if (std::fabs(val) >= 1e6 || std::fabs(val) <= 1e-6) { // Umbrales ajustables\n            oss << std::scientific << std::setprecision(8) << val;\n        } else {\n            oss << std::fixed << std::setprecision(8) << val;\n        }\n        \n        std::string s = oss.str();\n        // Eliminar ceros finales y el punto decimal si no hay parte fraccionaria\n        // Esto puede ser delicado con std::scientific, as\u00ed que hay que ser cuidadosos.\n        // Para std::fixed:\n        if (s.find('.') != std::string::npos) {\n            s.erase(s.find_last_not_of('0') + 1, std::string::npos);\n            if (!s.empty() && s.back() == '.') s.pop_back();\n        }\n        return s.empty() ? \"0\" : s;\n    }\n}\n\n// --- evaluate_tree ---\ndouble evaluate_tree(const NodePtr& node, const std::vector<double>& vars) {\n    if (!node) return std::nan(\"\");\n    switch (node->type) {\n        case NodeType::Constant: return node->value;\n        case NodeType::Variable: \n            if (node->var_index >= 0 && node->var_index < vars.size()) {\n                return vars[node->var_index];\n            }\n            return std::nan(\"\"); // Index out of bounds\n        case NodeType::Operator: {\n            // Determine arity\n            bool is_unary = (node->op == 's' || node->op == 'c' || node->op == 'l' || node->op == 'e' || node->op == '!' || node->op == '_' || node->op == 'g' || node->op == 't' || node->op == 'q' || node->op == 'a' || node->op == 'n' || node->op == 'u' || node->op == 'S' || node->op == 'C' || node->op == 'T');\n\n            double leftVal = evaluate_tree(node->left, vars);\n            double rightVal = 0.0;\n            if (!is_unary) {\n                rightVal = evaluate_tree(node->right, vars);\n            }\n\n            if (std::isnan(leftVal)) return std::nan(\"\");\n            if (!is_unary && std::isnan(rightVal)) return std::nan(\"\");\n\n            double result = std::nan(\"\");\n            try {\n                switch (node->op) {\n                    case '+': result = leftVal + rightVal; break;\n                    case '-': result = leftVal - rightVal; break;\n                    case '*': result = leftVal * rightVal; break;\n                    case '/':\n                        if (std::fabs(rightVal) < SIMPLIFY_NEAR_ZERO_TOLERANCE) return INF;\n                        result = leftVal / rightVal;\n                        break;\n                    case '^':\n                        if (leftVal == 0.0 && rightVal == 0.0) result = 1.0;\n                        else if (leftVal == 0.0 && rightVal < 0.0) return INF;\n                        else if (leftVal < 0.0 && std::fabs(rightVal - std::round(rightVal)) > SIMPLIFY_NEAR_ZERO_TOLERANCE) return INF;\n                        else result = std::pow(leftVal, rightVal);\n                        break;\n                    case '%':\n                        if (std::fabs(rightVal) < SIMPLIFY_NEAR_ZERO_TOLERANCE) return INF;\n                        result = std::fmod(leftVal, rightVal);\n                        break;\n                    case 's': result = std::sin(leftVal); break;\n                    case 'c': result = std::cos(leftVal); break;\n                    case 't': result = std::tan(leftVal); break;\n                    case 'q': \n                        // Protected Sqrt: sqrt(|x|)\n                        result = std::sqrt(std::abs(leftVal)); \n                        break;\n                    case 'a': result = std::abs(leftVal); break;\n                    case 'n': result = (leftVal > 0) ? 1.0 : ((leftVal < 0) ? -1.0 : 0.0); break;\n                    case 'l': \n                        // Protected Log: log(|x|)\n                        if (std::abs(leftVal) <= 1e-9) return INF; \n                        result = std::log(std::abs(leftVal)); \n                        break;\n                    case 'e': \n                        if (leftVal > 700.0) return INF; // Overflow check\n                        result = std::exp(leftVal); \n                        break;\n                    case '!': \n                        // Protected Factorial/Gamma: tgamma(|x|+1)\n                        if (std::abs(leftVal) > 170.0) return INF; \n                        result = std::tgamma(std::abs(leftVal) + 1.0); \n                        break;\n                    case '_': result = std::floor(leftVal); break;\n                    case 'u': result = std::ceil(leftVal); break; // 'u' for ceil (up)\n                    case 'g':\n                        result = std::lgamma(std::abs(leftVal) + 1.0); \n                        break;\n                    case 'S': \n                        // Protected Asin: asin(clip(x, -1, 1))\n                        result = std::asin(std::max(-1.0, std::min(1.0, leftVal))); \n                        break;\n                    case 'C': \n                        // Protected Acos: acos(clip(x, -1, 1))\n                        result = std::acos(std::max(-1.0, std::min(1.0, leftVal))); \n                        break;\n                    case 'T': result = std::atan(leftVal); break;\n                    default: return std::nan(\"\");\n                }\n            } catch (const std::exception& e) { return INF; }\n            if (std::isinf(result)) return INF;\n            if (std::isnan(result)) return std::nan(\"\");\n            return result;\n        }\n        default: return std::nan(\"\");\n    }\n}\n\n// Convenience overload for single variable case\ndouble evaluate_tree(const NodePtr& node, double val) {\n    return evaluate_tree(node, std::vector<double>{val});\n}\n\n// --- tree_to_string ---\nstd::string tree_to_string(const NodePtr& node) {\n     if (!node) return \"NULL\";\n     switch (node->type) {\n        case NodeType::Constant: return format_constant(node->value);\n        case NodeType::Variable: return \"x\" + std::to_string(node->var_index);\n        case NodeType::Operator: {\n            NodePtr left_node = node->left;\n            std::string left_str = tree_to_string(left_node);\n            \n            // Check arity\n            bool is_unary = (node->op == 's' || node->op == 'c' || node->op == 'l' || node->op == 'e' || node->op == '!' || node->op == '_' || node->op == 'g' || node->op == 't' || node->op == 'q' || node->op == 'a' || node->op == 'n' || node->op == 'u' || node->op == 'S' || node->op == 'C' || node->op == 'T');\n\n            if (is_unary) {\n                switch(node->op) {\n                    case 's': return \"sin(\" + left_str + \")\";\n                    case 'c': return \"cos(\" + left_str + \")\";\n                    case 't': return \"tan(\" + left_str + \")\";\n                    case 'q': return \"sqrt(\" + left_str + \")\";\n                    case 'a': return \"abs(\" + left_str + \")\";\n                    case 'n': return \"sign(\" + left_str + \")\";\n                    case 'l': return \"log(\" + left_str + \")\";\n                    case 'e': return \"exp(\" + left_str + \")\";\n                    case '!': return \"(\" + left_str + \")!\"; // Postfix for factorial\n                    case '_': return \"floor(\" + left_str + \")\";\n                    case 'u': return \"ceil(\" + left_str + \")\";\n                    case 'g': return \"lgamma(\" + left_str + \")\";\n                    case 'S': return \"asin(\" + left_str + \")\";\n                    case 'C': return \"acos(\" + left_str + \")\";\n                    case 'T': return \"atan(\" + left_str + \")\";\n                    default: return \"op(\" + left_str + \")\";\n                }\n            }\n\n            NodePtr right_node = node->right;\n            std::string right_str = tree_to_string(right_node);\n            char current_op = node->op;\n            bool right_is_neg_const = (right_node && right_node->type == NodeType::Constant && right_node->value < 0.0);\n            if (right_is_neg_const) {\n                double abs_right_val = std::fabs(right_node->value);\n                std::string abs_right_str = format_constant(abs_right_val);\n                if (node->op == '+') { current_op = '-'; right_str = abs_right_str; }\n                else if (node->op == '-') { current_op = '+'; right_str = abs_right_str; }\n            }\n            // Simplificar impresi\u00f3n de (0-A) a (-A)\n            if (left_node && left_node->type == NodeType::Constant && left_node->value == 0.0 && current_op == '-') {\n                 return \"(-\" + right_str + \")\";\n            }\n            return \"(\" + left_str + current_op + right_str + \")\";\n        }\n        default: return \"?\";\n    }\n}\n\n// --- tree_size ---\nint tree_size(const NodePtr& node) {\n    if (!node) return 0;\n    if (node->type == NodeType::Constant || node->type == NodeType::Variable) return 1;\n    if (node->type == NodeType::Operator) {\n        return 1 + tree_size(node->left) + tree_size(node->right);\n    }\n    return 0;\n}\n\n// --- clone_tree ---\nNodePtr clone_tree(const NodePtr& node) {\n    if (!node) return nullptr;\n    auto newNode = std::make_shared<Node>(node->type);\n    newNode->value = node->value;\n    newNode->var_index = node->var_index;\n    newNode->op = node->op;\n    newNode->left = clone_tree(node->left);\n    newNode->right = clone_tree(node->right);\n    return newNode;\n}\n\n// --- collect_node_ptrs ---\nvoid collect_node_ptrs(NodePtr& node, std::vector<NodePtr*>& vec) {\n    if (!node) return;\n    vec.push_back(&node);\n    if (node->type == NodeType::Operator) {\n        collect_node_ptrs(node->left, vec);\n        collect_node_ptrs(node->right, vec);\n    }\n}\n\n// --- get_rng ---\n// === OPTIMIZACI\u00d3N: RNG thread-local para evitar contenci\u00f3n en OpenMP ===\nstd::mt19937& get_rng() {\n    thread_local std::mt19937 local_rng(\n        std::random_device{}() ^ \n        static_cast<unsigned>(std::hash<std::thread::id>{}(std::this_thread::get_id()))\n    );\n    return local_rng;\n}\n\n// --- get_tree_depth ---\nint get_tree_depth(const NodePtr& node) {\n    if (!node) return 0;\n    if (node->type != NodeType::Operator) return 1;\n    return 1 + std::max(get_tree_depth(node->left), get_tree_depth(node->right));\n}\n\n// --- trim_tree ---\nvoid trim_tree(NodePtr& node, int max_depth) {\n    if (!node) return;\n    if (max_depth <= 1) {\n        // Force terminal if we reached depth limit\n        if (node->type == NodeType::Operator) {\n            // Replace with minimal terminal (Variable 'x' or Constant 1.0)\n            // Using 'x' is generally safer for retaining some logic, but 1.0 is neutral for *\n            // Let's pick a random terminal to avoid bias? \n            // For now, let's just make it a variable 'x' as it's often more useful than a constant 0 or 1.\n             node->type = NodeType::Variable;\n             node->op = 0;\n             node->left = nullptr;\n             node->right = nullptr;\n             // value ignored for variable\n        }\n        return;\n    }\n    \n    if (node->type == NodeType::Operator) {\n        trim_tree(node->left, max_depth - 1);\n        trim_tree(node->right, max_depth - 1);\n    }\n}\n\n\n// ============================================================\n// --- Parser de F\u00f3rmulas desde String (v4 - Parser Corregido) ---\n// ============================================================\n\n// Helper para obtener precedencia de operadores\nint get_precedence(char op) {\n    switch (op) {\n        case '+': case '-': return 1;\n        case '*': case '/': case '%': return 2;\n        case '^': return 3;\n        default: return 0;\n    }\n}\n\n// Helper para aplicar un operador binario\nNodePtr apply_binary_operation(NodePtr right, NodePtr left, char op) {\n    if (!left || !right) {\n        throw std::runtime_error(\"Error al aplicar operaci\u00f3n binaria '\" + std::string(1, op) + \"': operandos insuficientes.\");\n    }\n    auto node = std::make_shared<Node>(NodeType::Operator);\n    node->op = op;\n    node->left = left;\n    node->right = right;\n    return node;\n}\n\n// Funci\u00f3n principal para parsear la f\u00f3rmula\nNodePtr parse_formula_string(const std::string& formula_raw) {\n    std::string formula = formula_raw;\n    formula.erase(std::remove_if(formula.begin(), formula.end(), ::isspace), formula.end());\n    if (formula.empty()) throw std::runtime_error(\"La f\u00f3rmula est\u00e1 vac\u00eda.\");\n\n    std::stack<NodePtr> operand_stack;\n    std::stack<char> operator_stack;\n\n    // Funci\u00f3n interna para procesar operadores seg\u00fan precedencia y asociatividad\n    auto process_operators_by_precedence = [&](int current_precedence, char current_op_char = 0) {\n        // La asociatividad derecha para '^' significa que se procesa si el operador en la pila\n        // tiene MAYOR precedencia, no MAYOR O IGUAL.\n        bool is_right_associative = (current_op_char == '^');\n\n        while (!operator_stack.empty() && operator_stack.top() != '(') {\n            char top_op = operator_stack.top();\n            int top_precedence = get_precedence(top_op);\n\n            if (is_right_associative ? (top_precedence > current_precedence) : (top_precedence >= current_precedence)) {\n                operator_stack.pop(); // Sacar operador de la pila\n                if (operand_stack.size() < 2) throw std::runtime_error(\"Operandos insuficientes para operador '\" + std::string(1, top_op) + \"'.\");\n                NodePtr right = operand_stack.top(); operand_stack.pop();\n                NodePtr left = operand_stack.top(); operand_stack.pop();\n                operand_stack.push(apply_binary_operation(right, left, top_op));\n            } else {\n                break; // Parar si la precedencia es menor o si es asociativo a la derecha y es igual\n            }\n        }\n    };\n\n    bool last_token_was_operand = false;\n\n    for (int i = 0; i < formula.length(); /* Incremento manual */ ) {\n        char token = formula[i];\n\n        // --- A. Parsear N\u00fameros ---\n        bool starts_number = isdigit(token) || (token == '.' && i + 1 < formula.length() && isdigit(formula[i+1]));\n        if (starts_number) {\n             if (last_token_was_operand) { // Implicit multiplication\n                 process_operators_by_precedence(get_precedence('*'));\n                 operator_stack.push('*');\n                 last_token_was_operand = false;\n             }\n            std::string num_str;\n            if (token == '.') num_str += '0';\n            num_str += token;\n            i++;\n            while (i < formula.length() && (isdigit(formula[i]) || (formula[i] == '.' && num_str.find('.') == std::string::npos))) {\n                num_str += formula[i];\n                i++;\n            }\n            try {\n                double value = std::stod(num_str);\n                auto node = std::make_shared<Node>(NodeType::Constant); node->value = value;\n                operand_stack.push(node);\n                last_token_was_operand = true;\n            } catch (const std::invalid_argument& e) {\n                throw std::runtime_error(\"N\u00famero inv\u00e1lido (formato): '\" + num_str + \"' - \" + e.what());\n            } catch (const std::out_of_range& e) {\n                throw std::runtime_error(\"N\u00famero inv\u00e1lido (rango): '\" + num_str + \"' - \" + e.what());\n            }\n            continue;\n        }\n\n        // --- B. Parsear Funciones Unarias y Constantes ---\n        std::unordered_map<std::string, char> func_map = {\n            {\"sin\", 's'}, {\"cos\", 'c'}, {\"tan\", 't'}, \n            {\"asin\", 'S'}, {\"acos\", 'C'}, {\"atan\", 'T'},\n            {\"log\", 'l'}, {\"exp\", 'e'}, {\"sqrt\", 'q'},\n            {\"floor\", '_'}, {\"ceil\", 'u'}, {\"abs\", 'a'}, {\"sign\", 'n'},\n            {\"gamma\", '!'}, {\"lgamma\", 'g'}, {\"g\", 'g'}\n        };\n\n        // Special handling for Constants (pi, e, C)\n        if (token == 'C') {\n             if (last_token_was_operand) { // Implicit multiplication\n                 process_operators_by_precedence(get_precedence('*'));\n                 operator_stack.push('*');\n             }\n             auto node = std::make_shared<Node>(NodeType::Constant); \n             // Default constant value (will be optimized later)\n             node->value = 1.0; \n             // Mark it specifically as an optimizable constant in a way that clone/optimize respects?\n             // Actually, for C++ GP, usually constants are just numbers. \n             // But if we want to preserve 'C' semantics:\n             // Let's treat 'C' as a special Variable? No, Variable is x.\n             // Let's just treat it as 1.0 for now, or use a special Op 'C'?\n             // The system typically optimizes *numeric* constants attached to nodes.\n             // If we parse 'C', we should probably parse it as a random constant?\n             // Or better, a Constant node with a placeholder value.\n             // Re-reading ExpressionTree.h might help, but let's stick to 1.0 for now.\n             operand_stack.push(node);\n             last_token_was_operand = true;\n             i++;\n             continue;\n        }\n        if (i + 1 < formula.length() && formula.substr(i, 2) == \"pi\") {\n             if (last_token_was_operand) {\n                 process_operators_by_precedence(get_precedence('*'));\n                 operator_stack.push('*');\n             }\n             auto node = std::make_shared<Node>(NodeType::Constant); node->value = 3.14159265359;\n             operand_stack.push(node);\n             last_token_was_operand = true;\n             i += 2;\n             continue;\n        }\n        if (token == 'e' && (i+1 >= formula.length() || formula[i+1] != 'x')) { // Check it's not 'exp'\n             // Handle 'e' constant\n             if (last_token_was_operand) {\n                 process_operators_by_precedence(get_precedence('*'));\n                 operator_stack.push('*');\n             }\n             auto node = std::make_shared<Node>(NodeType::Constant); node->value = 2.71828182846;\n             operand_stack.push(node);\n             last_token_was_operand = true;\n             i++;\n             continue;\n        }\n        \n        // Try to match function names (check longer names first)\n        bool matched_func = false;\n        for (const auto& [func_name, func_op] : func_map) {\n            if (i + func_name.length() <= formula.length() && \n                formula.substr(i, func_name.length()) == func_name &&\n                (i + func_name.length() >= formula.length() || formula[i + func_name.length()] == '(')) {\n                \n                // Check if this is actually a function call (followed by '(')\n                size_t after_name = i + func_name.length();\n                if (after_name < formula.length() && formula[after_name] == '(') {\n                    if (last_token_was_operand) { // Implicit multiplication\n                        process_operators_by_precedence(get_precedence('*'));\n                        operator_stack.push('*');\n                        last_token_was_operand = false;\n                    }\n                    \n                    // Find the matching closing parenthesis\n                    int paren_count = 1;\n                    size_t arg_start = after_name + 1;\n                    size_t j = arg_start;\n                    while (j < formula.length() && paren_count > 0) {\n                        if (formula[j] == '(') paren_count++;\n                        else if (formula[j] == ')') paren_count--;\n                        j++;\n                    }\n                    if (paren_count != 0) {\n                        throw std::runtime_error(\"Par\u00e9ntesis sin cerrar en funci\u00f3n '\" + func_name + \"'.\");\n                    }\n                    size_t arg_end = j - 1; // Position of closing ')'\n                    \n                    // Extract and recursively parse the argument\n                    std::string arg_str = formula.substr(arg_start, arg_end - arg_start);\n                    NodePtr arg_tree = parse_formula_string(arg_str);\n                    \n                    // Create unary operator node\n                    auto func_node = std::make_shared<Node>(NodeType::Operator);\n                    func_node->op = func_op;\n                    func_node->left = arg_tree;\n                    func_node->right = nullptr;\n                    \n                    operand_stack.push(func_node);\n                    last_token_was_operand = true;\n                    i = j; // Skip past the closing ')'\n                    matched_func = true;\n                    break;\n                }\n            }\n        }\n        if (matched_func) continue;\n\n        // --- C. Parsear Variable 'x' ---\n        if (token == 'x') {\n            if (last_token_was_operand) { // Implicit multiplication\n                 process_operators_by_precedence(get_precedence('*'));\n                 operator_stack.push('*');\n                 last_token_was_operand = false;\n            }\n            auto node = std::make_shared<Node>(NodeType::Variable);\n            \n            // Check for digits after 'x' (for x0, x1, x10...)\n            i++;\n            if (i < formula.length() && isdigit(formula[i])) {\n                 std::string idx_str;\n                 while(i < formula.length() && isdigit(formula[i])) {\n                     idx_str += formula[i];\n                     i++;\n                 }\n                 try {\n                     node->var_index = std::stoi(idx_str);\n                 } catch (...) {\n                     node->var_index = 0; // Fallback\n                 }\n            } else {\n                 node->var_index = 0; // Default x -> x0\n            }\n\n            operand_stack.push(node);\n            last_token_was_operand = true;\n            continue;\n        }\n\n        // --- D. Parsear Par\u00e9ntesis de Apertura '(' ---\n        if (token == '(') {\n            if (last_token_was_operand) { // Implicit multiplication\n                 process_operators_by_precedence(get_precedence('*'));\n                 operator_stack.push('*');\n                 last_token_was_operand = false;\n            }\n            operator_stack.push('(');\n            last_token_was_operand = false;\n            i++;\n            continue;\n        }\n\n        // --- E. Parsear Par\u00e9ntesis de Cierre ')' ---\n        if (token == ')') {\n             if (!last_token_was_operand) {\n                  if (!operator_stack.empty() && operator_stack.top() == '(') throw std::runtime_error(\"Par\u00e9ntesis vac\u00edos '()' encontrados.\");\n                  else throw std::runtime_error(\"Se esperaba un operando antes de ')'.\");\n             }\n            while (!operator_stack.empty() && operator_stack.top() != '(') {\n                process_operators_by_precedence(0);\n            }\n            if (operator_stack.empty()) throw std::runtime_error(\"Par\u00e9ntesis ')' sin correspondiente '('.\");\n            operator_stack.pop(); // Sacar '('\n            last_token_was_operand = true;\n            i++;\n            continue;\n        }\n\n        // --- F. Parsear Operadores (+ - * / ^ %) ---\n        if (std::string(\"+-*/^%\").find(token) != std::string::npos) {\n            // Manejar '-' unario vs binario\n            if (token == '-' && !last_token_was_operand) {\n                // Es un '-' unario. Insertar un 0 como operando izquierdo impl\u00edcito.\n                // Esto permite tratar el '-' como un operador binario normal.\n                auto zero_node = std::make_shared<Node>(NodeType::Constant); zero_node->value = 0.0;\n                operand_stack.push(zero_node);\n                // No cambiar last_token_was_operand a true, ya que el 0 impl\u00edcito\n                // es solo para el operador unario y no un operando \"real\" previo.\n                // Si hubiera una multiplicaci\u00f3n impl\u00edcita (ej. \"2-x\"), ya se habr\u00eda manejado.\n            }\n            // Ignorar '+' unario (no afecta el valor, no necesita un 0 impl\u00edcito)\n            else if (token == '+' && !last_token_was_operand) {\n                // No hacer nada, simplemente avanzar al siguiente token\n                i++;\n                continue;\n            }\n            \n            // Operador binario normal\n            if (!last_token_was_operand && (token == '*' || token == '/' || token == '^' || token == '%')) {\n                throw std::runtime_error(\"Operador binario '\" + std::string(1, token) + \"' inesperado. Se esperaba operando.\");\n            }\n\n            // Procesar operadores en la pila con mayor o igual precedencia (o solo mayor para asociativos a derecha)\n            process_operators_by_precedence(get_precedence(token), token);\n            operator_stack.push(token);\n            last_token_was_operand = false; // Despu\u00e9s de un operador, se espera un operando\n            i++;\n            continue;\n        }\n\n        // --- G. Token Desconocido ---\n        throw std::runtime_error(\"Token desconocido en la f\u00f3rmula: '\" + std::string(1, token) + \"'\");\n\n    } // Fin del bucle for\n\n    // --- H. Procesamiento Final despu\u00e9s del bucle ---\n    while (!operator_stack.empty()) {\n        if (operator_stack.top() == '(') throw std::runtime_error(\"Par\u00e9ntesis '(' sin cerrar al final.\");\n        // Procesar todos los operadores restantes en la pila\n        process_operators_by_precedence(0); // 0 como precedencia m\u00ednima para forzar el procesamiento\n    }\n\n    // Verificaci\u00f3n final de la pila de operandos\n    if (operand_stack.size() != 1) {\n         if (operand_stack.empty() && formula.length() > 0) throw std::runtime_error(\"Error: No se gener\u00f3 ning\u00fan resultado del parseo. F\u00f3rmula inv\u00e1lida?\");\n         else if (operand_stack.size() > 1) throw std::runtime_error(\"Error en la estructura final (operandos restantes: \" + std::to_string(operand_stack.size()) + \"). Verifique operadores.\");\n         else throw std::runtime_error(\"Error desconocido al finalizar el parseo.\");\n    }\n\n    return operand_stack.top();\n}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/ExpressionTree.h\n",
        "#ifndef EXPRESSIONTREE_H\n#define EXPRESSIONTREE_H\n\n#include <memory>\n#include <string>\n#include <vector>\n#include <stdexcept> // Para std::runtime_error\n\n// Forward declaration\nstruct Node;\n\n// Use shared_ptr for automatic memory management\nusing NodePtr = std::shared_ptr<Node>;\n\nenum class NodeType { Constant, Variable, Operator };\n\nstruct Node {\n    NodeType type;\n    double value = 0.0;             // If type == Constant\n    int var_index = 0;              // If type == Variable: index of the variable (0 for x0, 1 for x1...)\n    char op = 0;                    // If type == Operator: '+', '-', '*', '/', '^'\n    NodePtr left = nullptr;         // Children (for Operators)\n    NodePtr right = nullptr;\n\n    // Constructor for convenience\n    Node(NodeType t = NodeType::Constant) : type(t) {}\n};\n\n// Core Tree Functions\n// MODIFIED: Takes a vector of variables instead of a single double\ndouble evaluate_tree(const NodePtr& node, const std::vector<double>& vars);\n\n// Convenience overload for single variable case\ndouble evaluate_tree(const NodePtr& node, double val);\nstd::string tree_to_string(const NodePtr& node);\nint tree_size(const NodePtr& node);\nNodePtr clone_tree(const NodePtr& node);\nint get_tree_depth(const NodePtr& node);\nvoid trim_tree(NodePtr& node, int max_depth);\n\n// Helper for mutation/crossover\nvoid collect_node_ptrs(NodePtr& node, std::vector<NodePtr*>& vec);\n\n// --- NUEVO: Funci\u00f3n para parsear una f\u00f3rmula desde string ---\n// Parsea una f\u00f3rmula en notaci\u00f3n infija simple (con par\u00e9ntesis).\n// Lanza std::runtime_error si hay error de sintaxis.\nNodePtr parse_formula_string(const std::string& formula);\n\n\n#endif // EXPRESSIONTREE_H\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/Fitness.cpp\n",
        "#include \"Fitness.h\"\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n#include \"FitnessGPU.cuh\" // Include for GPU fitness evaluation\n#endif\n#include \"Globals.h\" // Necesario para constantes globales e INF\n#include \"ExpressionTree.h\" // Necesario para tree_to_string\n#include <cmath>\n#include <limits>\n#include <vector>\n#include <numeric>\n#include <iostream> // Para std::cerr en caso de error futuro\n#include <iomanip>  // Para std::fixed/scientific si se necesita en errores\n\n// Calculates the raw fitness using global parameters.\n// This function will now dispatch to GPU if USE_GPU_ACCELERATION is enabled.\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\ndouble calculate_raw_fitness(const NodePtr& tree,\n                             const std::vector<double>& targets,\n                             const std::vector<std::vector<double>>& x_values,\n                             double* d_targets, double* d_x_values) {\n    // If GPU pointers are null (FORCE_CPU_MODE), use CPU evaluation\n    if (d_targets == nullptr || d_x_values == nullptr) {\n        // CPU fallback implementation\n        if (x_values.size() != targets.size() || x_values.empty()) return INF;\n\n        double sum_sq_error = 0.0;\n        double total_weight = 0.0;\n        bool all_precise = true;\n        size_t num_points = x_values.size();\n        bool calculation_failed = false;\n\n        for (size_t i = 0; i < num_points; ++i) {\n            double predicted_val = evaluate_tree(tree, x_values[i]);\n\n            if (std::isnan(predicted_val) || std::isinf(predicted_val)) {\n                calculation_failed = true;\n                break;\n            }\n\n            double target_val = targets[i];\n            double diff = predicted_val - target_val;\n            double abs_diff = std::fabs(diff);\n\n            if (abs_diff >= FITNESS_PRECISION_THRESHOLD) all_precise = false;\n\n            double weight = 1.0;\n            if (USE_WEIGHTED_FITNESS) {\n                weight = std::exp(static_cast<double>(i) * WEIGHTED_FITNESS_EXPONENT);\n            }\n            total_weight += weight;\n\n            double sq_error = diff * diff;\n            sum_sq_error += sq_error * weight;\n        }\n\n        if (calculation_failed) return INF;\n\n        // Normalize weighted error\n        double raw_error;\n        if (USE_WEIGHTED_FITNESS && total_weight > 0.0) {\n            sum_sq_error = sum_sq_error / total_weight * num_points;\n        }\n\n        if (USE_RMSE_FITNESS && num_points > 0) {\n            double mse = sum_sq_error / static_cast<double>(num_points);\n            raw_error = std::sqrt(mse);\n        } else {\n            raw_error = sum_sq_error;\n        }\n\n        if (std::isnan(raw_error) || std::isinf(raw_error) || raw_error < 0) {\n            return INF;\n        }\n\n        if (all_precise) {\n            raw_error *= FITNESS_PRECISION_BONUS;\n        }\n\n        return raw_error;\n    }\n    \n    // Use GPU evaluation\n    return evaluate_fitness_gpu(tree, targets, x_values, d_targets, d_x_values);\n}\n#else\ndouble calculate_raw_fitness(const NodePtr& tree,\n                             const std::vector<double>& targets,\n                             const std::vector<std::vector<double>>& x_values) {\n    if (x_values.size() != targets.size() || x_values.empty()) return INF;\n\n    double error_sum_pow13 = 0.0; // Solo si USE_RMSE_FITNESS = false\n    double sum_sq_error = 0.0;\n    double total_weight = 0.0; // Para normalizar el fitness ponderado\n    bool all_precise = true;\n    size_t num_points = x_values.size();\n    bool calculation_failed = false; // Flag para detectar INF/NaN\n\n    for (size_t i = 0; i < num_points; ++i) {\n        double predicted_val = evaluate_tree(tree, x_values[i]);\n\n        // Comprobar si la evaluaci\u00f3n fall\u00f3 (INF o NaN)\n        if (std::isnan(predicted_val) || std::isinf(predicted_val)) {\n            calculation_failed = true;\n            break; // Salir del bucle si la evaluaci\u00f3n falla para un punto\n        }\n\n        double target_val = targets[i];\n        double diff = predicted_val - target_val;\n        double abs_diff = std::fabs(diff);\n\n        if (abs_diff >= FITNESS_PRECISION_THRESHOLD) all_precise = false;\n\n        // --- PESO PARA FITNESS PONDERADO ---\n        // Hace que los \u00faltimos puntos (N altos) valgan much\u00edsimo m\u00e1s.\n        // Esto destruye a los polinomios porque fallan al final.\n        double weight = 1.0;\n        if (USE_WEIGHTED_FITNESS) {\n            // Peso exponencial: m\u00e1s agresivo para penalizar errores en N altos\n            weight = std::exp(static_cast<double>(i) * WEIGHTED_FITNESS_EXPONENT);\n        }\n        total_weight += weight;\n\n        // Acumular error para ambas m\u00e9tricas (si aplica)\n        if (!USE_RMSE_FITNESS) {\n             error_sum_pow13 += std::pow(abs_diff, FITNESS_ORIGINAL_POWER) * weight;\n        }\n\n        // Calcular y acumular error cuadr\u00e1tico PONDERADO\n        double sq_diff = diff * diff;\n        sum_sq_error += sq_diff * weight;\n\n        // Control de desbordamiento/Infinito en la suma\n        if (std::isinf(sum_sq_error) || (error_sum_pow13 >= INF / 10.0 && !USE_RMSE_FITNESS)) {\n            calculation_failed = true;\n            break;\n        }\n    } // Fin bucle for puntos\n\n    // Si la evaluaci\u00f3n o suma fall\u00f3 en alg\u00fan punto, devolver INF\n    if (calculation_failed) {\n        return INF;\n    }\n\n    // Seleccionar m\u00e9trica de error crudo\n    double raw_error;\n    if (USE_RMSE_FITNESS) {\n        if (num_points == 0 || total_weight == 0.0) return INF;\n        // MSE ponderado: normalizar por suma de pesos, no por num_points\n        double mse = sum_sq_error / total_weight;\n        if (std::isinf(mse) || std::isnan(mse) || mse < 0) {\n             raw_error = INF;\n        } else {\n             raw_error = std::sqrt(mse); // Calcular RMSE ponderado\n        }\n    } else {\n        raw_error = error_sum_pow13;\n    }\n\n    // Comprobar si el error crudo es inv\u00e1lido\n    if (std::isnan(raw_error) || std::isinf(raw_error) || raw_error < 0) {\n         return INF;\n    }\n\n    // Aplicar bonus de precisi\u00f3n si todos los puntos estaban dentro del umbral\n    if (all_precise) {\n         raw_error *= FITNESS_PRECISION_BONUS;\n    }\n\n    return raw_error; // Devolver el error crudo (sin penalizaci\u00f3n por complejidad a\u00fan)\n}\n#endif // USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n\n// Calcula el fitness final usando par\u00e1metros globales.\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\ndouble evaluate_fitness(const NodePtr& tree,\n                        const std::vector<double>& targets,\n                        const std::vector<std::vector<double>>& x_values,\n                        double* d_targets, double* d_x_values) {\n    double raw_fitness = calculate_raw_fitness(tree, targets, x_values, d_targets, d_x_values);\n#else\ndouble evaluate_fitness(const NodePtr& tree,\n                        const std::vector<double>& targets,\n                        const std::vector<std::vector<double>>& x_values) {\n    double raw_fitness = calculate_raw_fitness(tree, targets, x_values);\n#endif\n\n    if (raw_fitness >= INF / 10.0) {\n         return INF; // Si el error crudo es infinito, el fitness final es infinito\n    }\n\n    // Penalizaci\u00f3n por complejidad\n    double complexity = static_cast<double>(tree_size(tree));\n    double penalty = complexity * COMPLEXITY_PENALTY_FACTOR; // Usa constante global\n\n    // Aplicar penalizaci\u00f3n multiplicativa\n    double final_fitness = raw_fitness * (1.0 + penalty);\n\n    // Comprobaciones finales\n    if (std::isnan(final_fitness) || std::isinf(final_fitness) || final_fitness < 0) {\n         return INF;\n    }\n\n    return final_fitness;\n}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/Fitness.h\n",
        "#ifndef FITNESS_H\n#define FITNESS_H\n\n#include \"ExpressionTree.h\"\n#include <vector>\n\n// Calculates raw fitness based on target matching\n// Lower is better. Returns INF if evaluation results in NaN/Inf.\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\ndouble calculate_raw_fitness(const NodePtr& tree,\n                             const std::vector<double>& targets,\n                             const std::vector<std::vector<double>>& x_values,\n                             double* d_targets, double* d_x_values);\n#else\ndouble calculate_raw_fitness(const NodePtr& tree,\n                             const std::vector<double>& targets,\n                             const std::vector<std::vector<double>>& x_values);\n#endif\n\n// Calculates final fitness including complexity penalty\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\ndouble evaluate_fitness(const NodePtr& tree,\n                        const std::vector<double>& targets,\n                        const std::vector<std::vector<double>>& x_values,\n                        double* d_targets, double* d_x_values);\n#else\ndouble evaluate_fitness(const NodePtr& tree,\n                        const std::vector<double>& targets,\n                        const std::vector<std::vector<double>>& x_values);\n#endif\n\n#endif // FITNESS_H\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/FitnessGPU.cu\n",
        "#include \"FitnessGPU.cuh\"\n#include \"Globals.h\"\n#include <cuda_runtime.h>\n#include <math.h>\n#include <vector>\n#include <iostream>\n\n// Helper function to linearize the tree into a post-order array\nvoid linearize_tree(const NodePtr& node, std::vector<LinearGpuNode>& linear_tree) {\n    if (!node) {\n        return;\n    }\n    linearize_tree(node->left, linear_tree);\n    linearize_tree(node->right, linear_tree);\n    // Include var_index in the struct initialization\n    linear_tree.push_back({node->type, node->value, node->var_index, node->op});\n}\n\n#if USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n\n// Constant for large finite value\n#define GPU_MAX_DOUBLE 1e308\n\n// --- WEIGHTED FITNESS: Constantes para CUDA ---\n// Estas deben coincidir con los valores en Globals.h\n// CUDA device code no puede acceder a const C++, as\u00ed que usamos #define\n#define GPU_USE_WEIGHTED_FITNESS true\n#define GPU_WEIGHTED_FITNESS_EXPONENT 0.25\n\n// Single Tree Evaluation Kernel (Updated for Multivariable)\n__global__ void calculate_raw_fitness_kernel(const LinearGpuNode* d_linear_tree,\n                                             int tree_size,\n                                             const double* d_targets,\n                                             const double* d_x_values, // Flattened [num_points * num_vars]\n                                             size_t num_points,\n                                             int num_vars,\n                                             double* d_raw_fitness_results) {\n    // Shared memory optimization: Load tree into shared memory\n    extern __shared__ LinearGpuNode s_linear_tree[];\n\n    // Cooperative load\n    for (int i = threadIdx.x; i < tree_size; i += blockDim.x) {\n        s_linear_tree[i] = d_linear_tree[i];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx < num_points) {\n        double stack[64]; // Max tree depth\n        int stack_top = -1;\n\n        for (int i = 0; i < tree_size; ++i) {\n            LinearGpuNode node = s_linear_tree[i]; // Access from shared memory\n            if (node.type == NodeType::Constant) {\n                stack[++stack_top] = node.value;\n            } else if (node.type == NodeType::Variable) {\n                // Access correct variable for this sample\n                int var_idx = node.var_index;\n                if (var_idx >= num_vars) var_idx = 0; // Safety fallback\n                stack[++stack_top] = d_x_values[idx * num_vars + var_idx];\n            } else if (node.type == NodeType::Operator) {\n                bool is_unary = (node.op == 's' || node.op == 'c' || node.op == 'l' || node.op == 'e' || node.op == '!' || node.op == '_' || node.op == 'g');\n                double result = 0.0;\n                \n                if (is_unary) {\n                     if (stack_top < 0) {\n                         result = GPU_MAX_DOUBLE;\n                     } else {\n                         double val = stack[stack_top--];\n                         switch (node.op) {\n                            case 's': result = sin(val); break;\n                            case 'c': result = cos(val); break;\n                            case 'l': result = (val <= 1e-9) ? GPU_MAX_DOUBLE : log(val); break;\n                            case 'e': result = (val > 700.0) ? GPU_MAX_DOUBLE : exp(val); break;\n                            case '!': result = (val < 0 || val > 170.0) ? GPU_MAX_DOUBLE : tgamma(val + 1.0); break;\n                            case '_': result = floor(val); break;\n                            case 'g': result = (val <= -1.0) ? GPU_MAX_DOUBLE : lgamma(val + 1.0); break;\n                            default: result = NAN; break;\n                         }\n                     }\n                     stack[++stack_top] = result;\n                } else {\n                    if (stack_top < 1) { \n                        result = GPU_MAX_DOUBLE;\n                        stack[++stack_top] = result; // Push error\n                    } else {\n                        double right = stack[stack_top--];\n                        double left = stack[stack_top--];\n                        switch (node.op) {\n                            case '+': result = left + right; break;\n                            case '-': result = left - right; break;\n                            case '*': result = left * right; break;\n                            case '/':\n                                if (fabs(right) < 1e-9) { // Avoid division by zero\n                                    result = GPU_MAX_DOUBLE; \n                                } else {\n                                    result = left / right;\n                                }\n                                break;\n                            case '^': result = pow(left, right); break;\n                            case '%':\n                                if (fabs(right) < 1e-9) result = GPU_MAX_DOUBLE;\n                                else result = fmod(left, right);\n                                break;\n                            default: result = NAN; break;\n                        }\n                        stack[++stack_top] = result;\n                    }\n                }\n            }\n        }\n\n        double predicted_val = (stack_top == 0) ? stack[0] : NAN;\n\n        if (isnan(predicted_val) || isinf(predicted_val)) {\n            d_raw_fitness_results[idx] = GPU_MAX_DOUBLE; \n        } else {\n            double diff = predicted_val - d_targets[idx];\n            double sq_error = diff * diff;\n            // --- WEIGHTED FITNESS: Apply exponential weight ---\n            if (GPU_USE_WEIGHTED_FITNESS) {\n                double weight = exp((double)idx * GPU_WEIGHTED_FITNESS_EXPONENT);\n                sq_error *= weight;\n            }\n            d_raw_fitness_results[idx] = sq_error;\n        }\n    }\n}\n\n// CUDA kernel for parallel reduction (summation)\n__global__ void reduce_sum_kernel(double* d_data, int N) {\n    extern __shared__ double sdata[]; // Shared memory for reduction\n\n    unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    sdata[tid] = (i < N) ? d_data[i] : 0.0; // Load data into shared memory\n\n    __syncthreads(); // Synchronize threads in block\n\n    // Perform reduction in shared memory\n    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) { // Write result back to global memory (first element of block)\n        d_data[blockIdx.x] = sdata[0];\n    }\n}\n\n\n// --- New Batch Kernel (Updated for Multivariable) ---\n__global__ void evaluate_population_kernel(const LinearGpuNode* d_all_nodes,\n                                           const int* d_offsets,\n                                           const int* d_sizes,\n                                           int pop_size,\n                                           const double* d_targets,\n                                           const double* d_x_values,\n                                           int num_points,\n                                           int num_vars,\n                                           double* d_results) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx < pop_size) {\n        int offset = d_offsets[idx];\n        int size = d_sizes[idx];\n        double sum_sq_error = 0.0;\n        double total_weight = 0.0; // Para normalizar fitness ponderado\n        bool valid = true;\n\n        for (int p = 0; p < num_points; ++p) {\n            double stack[64]; \n            int stack_top = -1;\n\n            // Simple interpreter\n            for (int i = 0; i < size; ++i) {\n                LinearGpuNode node = d_all_nodes[offset + i];\n                if (node.type == NodeType::Constant) {\n                    stack[++stack_top] = node.value;\n                } else if (node.type == NodeType::Variable) {\n                    int var_idx = node.var_index;\n                    if (var_idx >= num_vars) var_idx = 0;\n                    stack[++stack_top] = d_x_values[p * num_vars + var_idx];\n                } else if (node.type == NodeType::Operator) {\n                    bool is_unary = (node.op == 's' || node.op == 'c' || node.op == 'l' || node.op == 'e' || node.op == '!' || node.op == '_' || node.op == 'g');\n                    \n                    if (is_unary) {\n                        if (stack_top < 0) { valid = false; break; }\n                        double val = stack[stack_top--];\n                        double result = 0.0;\n                         switch (node.op) {\n                            case 's': result = sin(val); break;\n                            case 'c': result = cos(val); break;\n                            case 'l': result = (val <= 1e-9) ? GPU_MAX_DOUBLE : log(val); break;\n                            case 'e': result = (val > 700.0) ? GPU_MAX_DOUBLE : exp(val); break;\n                            case '!': result = (val < 0 || val > 170.0) ? GPU_MAX_DOUBLE : tgamma(val + 1.0); break;\n                            case '_': result = floor(val); break;\n                            case 'g': result = (val <= -1.0) ? GPU_MAX_DOUBLE : lgamma(val + 1.0); break;\n                             default: result = NAN; break;\n                        }\n                        stack[++stack_top] = result;\n                    } else {\n                        // Safety check index\n                        if (stack_top < 1) { valid = false; break; }\n\n                        double right = stack[stack_top--];\n                        double left = stack[stack_top--];\n                        double result;\n                        switch (node.op) {\n                            case '+': result = left + right; break;\n                            case '-': result = left - right; break;\n                            case '*': result = left * right; break;\n                            case '/':\n                                if (fabs(right) < 1e-9) { \n                                    result = GPU_MAX_DOUBLE; \n                                } else {\n                                    result = left / right;\n                                }\n                                break;\n                            case '^': result = pow(left, right); break;\n                            case '%':\n                                if (fabs(right) < 1e-9) result = GPU_MAX_DOUBLE;\n                                else result = fmod(left, right);\n                                break;\n                            default: result = NAN; break;\n                        }\n                        stack[++stack_top] = result;\n                    }\n                }\n            }\n\n            if (!valid || stack_top != 0) {\n                sum_sq_error = GPU_MAX_DOUBLE;\n                break;\n            }\n\n            double predicted_val = stack[0];\n            if (isnan(predicted_val) || isinf(predicted_val)) {\n                sum_sq_error = GPU_MAX_DOUBLE;\n                break;\n            }\n\n            double diff = predicted_val - d_targets[p];\n            double sq_error = diff * diff;\n            \n            // --- WEIGHTED FITNESS: Peso exponencial ---\n            double weight = 1.0;\n            if (GPU_USE_WEIGHTED_FITNESS) {\n                weight = exp((double)p * GPU_WEIGHTED_FITNESS_EXPONENT);\n            }\n            total_weight += weight;\n            sum_sq_error += sq_error * weight;\n        }\n\n        // Normalizar por suma de pesos para obtener MSE ponderado\n        if (GPU_USE_WEIGHTED_FITNESS && total_weight > 0.0) {\n            sum_sq_error = sum_sq_error / total_weight * num_points; // Escalar de vuelta\n        }\n        d_results[idx] = sum_sq_error;\n    }\n}\n\n\n// Host-side wrapper function to launch the CUDA kernel\ndouble evaluate_fitness_gpu(NodePtr tree,\n                            const std::vector<double>& targets,\n                            const std::vector<std::vector<double>>& x_values,\n                            double* d_targets, double* d_x_values) {\n    if (x_values.size() != targets.size() || x_values.empty()) return INF;\n\n    // Linearize the tree\n    std::vector<LinearGpuNode> h_linear_tree;\n    linearize_tree(tree, h_linear_tree);\n    int tree_size = h_linear_tree.size();\n\n    if (tree_size == 0) {\n        return INF;\n    }\n\n    size_t num_points = x_values.size();\n    int num_vars = (num_points > 0) ? x_values[0].size() : 0;\n    \n    LinearGpuNode* d_linear_tree;\n    double* d_raw_fitness_results; // This will hold individual errors and then the final sum\n\n    cudaMalloc((void**)&d_linear_tree, tree_size * sizeof(LinearGpuNode));\n    cudaMalloc((void**)&d_raw_fitness_results, num_points * sizeof(double));\n\n    cudaMemcpy(d_linear_tree, h_linear_tree.data(), tree_size * sizeof(LinearGpuNode), cudaMemcpyHostToDevice);\n\n    int threadsPerBlock = 256;\n    int blocksPerGrid = (num_points + threadsPerBlock - 1) / threadsPerBlock;\n\n    // Launch kernel to calculate individual squared errors\n    size_t shared_mem_size = tree_size * sizeof(LinearGpuNode);\n    calculate_raw_fitness_kernel<<<blocksPerGrid, threadsPerBlock, shared_mem_size>>>(\n        d_linear_tree, tree_size, d_targets, d_x_values, num_points, num_vars, d_raw_fitness_results\n    );\n    cudaDeviceSynchronize(); // Ensure kernel completes before reduction\n\n    // --- Perform reduction on the GPU ---\n    int current_size = num_points;\n    while (current_size > 1) {\n        int next_blocks_per_grid = (current_size + threadsPerBlock - 1) / threadsPerBlock;\n        // Use shared memory for reduction, size is threadsPerBlock * sizeof(double)\n        reduce_sum_kernel<<<next_blocks_per_grid, threadsPerBlock, threadsPerBlock * sizeof(double)>>>(\n            d_raw_fitness_results, current_size\n        );\n        cudaDeviceSynchronize(); // Ensure reduction step completes\n        current_size = next_blocks_per_grid; // The result is in the first `next_blocks_per_grid` elements\n    }\n\n    double sum_sq_error_gpu = 0.0;\n    cudaMemcpy(&sum_sq_error_gpu, d_raw_fitness_results, sizeof(double), cudaMemcpyDeviceToHost);\n\n    cudaFree(d_linear_tree);\n    cudaFree(d_raw_fitness_results);\n\n    // Check for invalid results (propagated from kernel)\n    if (isinf(sum_sq_error_gpu) || isnan(sum_sq_error_gpu)) {\n        return INF;\n    }\n\n    double raw_fitness;\n    if (USE_RMSE_FITNESS) {\n        if (num_points == 0) return INF;\n        double mse = sum_sq_error_gpu / num_points;\n        raw_fitness = sqrt(mse);\n    } else {\n        raw_fitness = sum_sq_error_gpu;\n    }\n\n    double complexity = static_cast<double>(::tree_size(tree));\n    double penalty = complexity * COMPLEXITY_PENALTY_FACTOR;\n    double final_fitness = raw_fitness * (1.0 + penalty);\n\n    if (isnan(final_fitness) || isinf(final_fitness) || final_fitness < 0) {\n        return INF;\n    }\n\n    return final_fitness;\n}\n\nvoid evaluate_population_gpu(const std::vector<LinearGpuNode>& all_nodes,\n                             const std::vector<int>& tree_offsets,\n                             const std::vector<int>& tree_sizes,\n                             const std::vector<double>& targets,\n                             const std::vector<std::vector<double>>& x_values,\n                             std::vector<double>& results,\n                             double* d_targets, double* d_x_values,\n                             void*& d_nodes_ptr, size_t& d_nodes_cap,\n                             void*& d_offsets_ptr, void*& d_sizes_ptr, void*& d_results_ptr, size_t& d_pop_cap) {\n    \n    int pop_size = tree_offsets.size();\n    if (pop_size == 0) return;\n\n    size_t total_nodes = all_nodes.size();\n    int num_points = x_values.size();\n    int num_vars = (num_points > 0) ? x_values[0].size() : 0;\n\n    // Buffer Management for Nodes\n    if (total_nodes > d_nodes_cap) {\n        if (d_nodes_ptr) cudaFree(d_nodes_ptr);\n        size_t new_cap = total_nodes * 1.5; // Growth factor\n        cudaMalloc(&d_nodes_ptr, new_cap * sizeof(LinearGpuNode));\n        d_nodes_cap = new_cap;\n    }\n\n    // Buffer Management for Population Arrays\n    if (pop_size > d_pop_cap) {\n        if (d_offsets_ptr) cudaFree(d_offsets_ptr);\n        if (d_sizes_ptr) cudaFree(d_sizes_ptr);\n        if (d_results_ptr) cudaFree(d_results_ptr);\n        \n        size_t new_cap = pop_size * 1.5;\n        cudaMalloc(&d_offsets_ptr, new_cap * sizeof(int));\n        cudaMalloc(&d_sizes_ptr, new_cap * sizeof(int));\n        cudaMalloc(&d_results_ptr, new_cap * sizeof(double));\n        d_pop_cap = new_cap;\n    }\n\n    LinearGpuNode* d_all_nodes = (LinearGpuNode*)d_nodes_ptr;\n    int* d_offsets = (int*)d_offsets_ptr;\n    int* d_sizes = (int*)d_sizes_ptr;\n    double* d_results = (double*)d_results_ptr;\n\n    cudaMemcpy(d_all_nodes, all_nodes.data(), total_nodes * sizeof(LinearGpuNode), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_offsets, tree_offsets.data(), pop_size * sizeof(int), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_sizes, tree_sizes.data(), pop_size * sizeof(int), cudaMemcpyHostToDevice);\n\n    int threadsPerBlock = 256;\n    int blocksPerGrid = (pop_size + threadsPerBlock - 1) / threadsPerBlock;\n\n    evaluate_population_kernel<<<blocksPerGrid, threadsPerBlock>>>(\n        d_all_nodes, d_offsets, d_sizes, pop_size, d_targets, d_x_values, num_points, num_vars, d_results\n    );\n\n    // Synchronize and copy back\n    cudaDeviceSynchronize();\n    \n    cudaMemcpy(results.data(), d_results, pop_size * sizeof(double), cudaMemcpyDeviceToHost);\n}\n\n// ============================================================\n// GLOBAL BATCH EVALUATION - Maximum GPU Utilization\n// ============================================================\n\nvoid init_global_gpu_buffers(GlobalGpuBuffers& buffers) {\n    // Create CUDA stream for async operations\n    cudaStream_t stream;\n    cudaStreamCreate(&stream);\n    buffers.cuda_stream = (void*)stream;\n    \n    // Pre-allocate initial buffers (will grow as needed)\n    // Initial capacity for 50,000 trees with ~30 nodes each\n    buffers.d_nodes_capacity = 1500000;\n    buffers.d_pop_capacity = 60000;\n    \n    cudaMalloc(&buffers.d_nodes, buffers.d_nodes_capacity * sizeof(LinearGpuNode));\n    cudaMalloc(&buffers.d_offsets, buffers.d_pop_capacity * sizeof(int));\n    cudaMalloc(&buffers.d_sizes, buffers.d_pop_capacity * sizeof(int));\n    cudaMalloc(&buffers.d_results, buffers.d_pop_capacity * sizeof(double));\n}\n\nvoid cleanup_global_gpu_buffers(GlobalGpuBuffers& buffers) {\n    if (buffers.cuda_stream) {\n        cudaStreamDestroy((cudaStream_t)buffers.cuda_stream);\n        buffers.cuda_stream = nullptr;\n    }\n    if (buffers.d_nodes) { cudaFree(buffers.d_nodes); buffers.d_nodes = nullptr; }\n    if (buffers.d_offsets) { cudaFree(buffers.d_offsets); buffers.d_offsets = nullptr; }\n    if (buffers.d_sizes) { cudaFree(buffers.d_sizes); buffers.d_sizes = nullptr; }\n    if (buffers.d_results) { cudaFree(buffers.d_results); buffers.d_results = nullptr; }\n    buffers.d_nodes_capacity = 0;\n    buffers.d_pop_capacity = 0;\n}\n\n// Optimized kernel: Process one tree per thread\n// REMOVED SHARED MEMORY FOR DATA to support arbitrary dataset sizes and multivariable\n__global__ void evaluate_all_populations_kernel(\n    const LinearGpuNode* __restrict__ d_all_nodes,\n    const int* __restrict__ d_offsets,\n    const int* __restrict__ d_sizes,\n    int total_trees,\n    const double* __restrict__ d_targets,\n    const double* __restrict__ d_x_values,\n    int num_points,\n    int num_vars,\n    double* __restrict__ d_results,\n    double complexity_penalty_factor,\n    bool use_rmse) \n{\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx < total_trees) {\n        int offset = d_offsets[idx];\n        int size = d_sizes[idx];\n        double sum_sq_error = 0.0;\n        double total_weight = 0.0;\n        bool valid = true;\n\n        // Evaluate tree on all data points\n        for (int p = 0; p < num_points && valid; ++p) {\n            double stack[64]; \n            int stack_top = -1;\n\n            // Interpret the linearized tree\n            for (int i = 0; i < size && valid; ++i) {\n                LinearGpuNode node = d_all_nodes[offset + i];\n                \n                if (node.type == NodeType::Constant) {\n                    stack[++stack_top] = node.value;\n                } else if (node.type == NodeType::Variable) {\n                    // Access global memory directly\n                    int var_idx = node.var_index;\n                    if (var_idx >= num_vars) var_idx = 0;\n                    stack[++stack_top] = d_x_values[p * num_vars + var_idx];\n                } else if (node.type == NodeType::Operator) {\n                    bool is_unary = (node.op == 's' || node.op == 'c' || node.op == 'l' || \n                                    node.op == 'e' || node.op == '!' || node.op == '_' || node.op == 'g');\n                    \n                    if (is_unary) {\n                        if (stack_top < 0) { valid = false; break; }\n                        double val = stack[stack_top--];\n                        double result = 0.0;\n                        switch (node.op) {\n                            case 's': result = sin(val); break;\n                            case 'c': result = cos(val); break;\n                            case 'l': result = (val <= 1e-9) ? GPU_MAX_DOUBLE : log(val); break;\n                            case 'e': result = (val > 700.0) ? GPU_MAX_DOUBLE : exp(val); break;\n                            case '!': result = (val < 0 || val > 170.0) ? GPU_MAX_DOUBLE : tgamma(val + 1.0); break;\n                            case '_': result = floor(val); break;\n                            case 'g': result = (val <= -1.0) ? GPU_MAX_DOUBLE : lgamma(val + 1.0); break;\n                            default: result = NAN; break;\n                        }\n                        stack[++stack_top] = result;\n                    } else {\n                        if (stack_top < 1) { valid = false; break; }\n                        double right = stack[stack_top--];\n                        double left = stack[stack_top--];\n                        double result;\n                        switch (node.op) {\n                            case '+': result = left + right; break;\n                            case '-': result = left - right; break;\n                            case '*': result = left * right; break;\n                            case '/': result = (fabs(right) < 1e-9) ? GPU_MAX_DOUBLE : left / right; break;\n                            case '^': result = pow(left, right); break;\n                            case '%': result = (fabs(right) < 1e-9) ? GPU_MAX_DOUBLE : fmod(left, right); break;\n                            default: result = NAN; break;\n                        }\n                        stack[++stack_top] = result;\n                    }\n                }\n            }\n\n            if (!valid || stack_top != 0) {\n                sum_sq_error = GPU_MAX_DOUBLE;\n                valid = false;\n                break;\n            }\n\n            double predicted_val = stack[0];\n            if (isnan(predicted_val) || isinf(predicted_val)) {\n                sum_sq_error = GPU_MAX_DOUBLE;\n                valid = false;\n                break;\n            }\n\n            double diff = predicted_val - d_targets[p];\n            double sq_error = diff * diff;\n            \n            // Weighted fitness\n            double weight = 1.0;\n            if (GPU_USE_WEIGHTED_FITNESS) {\n                weight = exp((double)p * GPU_WEIGHTED_FITNESS_EXPONENT);\n            }\n            total_weight += weight;\n            sum_sq_error += sq_error * weight;\n        }\n\n        // Calculate final fitness with complexity penalty ON GPU\n        double raw_fitness = GPU_MAX_DOUBLE;\n        if (valid && sum_sq_error < 1e300) {\n            if (GPU_USE_WEIGHTED_FITNESS && total_weight > 0.0) {\n                sum_sq_error = sum_sq_error / total_weight * num_points;\n            }\n            \n            if (use_rmse && num_points > 0) {\n                double mse = sum_sq_error / num_points;\n                raw_fitness = sqrt(mse);\n            } else {\n                raw_fitness = sum_sq_error;\n            }\n            \n            // Apply complexity penalty (size is same as tree size in linearized form)\n            double complexity = (double)size;\n            double penalty = complexity * complexity_penalty_factor;\n            raw_fitness = raw_fitness * (1.0 + penalty);\n        }\n        \n        d_results[idx] = raw_fitness;\n    }\n}\n\nvoid evaluate_all_populations_gpu(\n    const std::vector<LinearGpuNode>& all_nodes,\n    const std::vector<int>& tree_offsets,\n    const std::vector<int>& tree_sizes,\n    const std::vector<int>& tree_complexities,\n    int total_trees,\n    const std::vector<double>& targets,\n    const std::vector<std::vector<double>>& x_values,\n    std::vector<double>& results,\n    double* d_targets, double* d_x_values,\n    GlobalGpuBuffers& buffers)\n{\n    if (total_trees == 0) return;\n    \n    cudaStream_t stream = (cudaStream_t)buffers.cuda_stream;\n    size_t total_nodes = all_nodes.size();\n    int num_points = x_values.size();\n    int num_vars = (num_points > 0) ? x_values[0].size() : 0;\n\n    // Dynamic buffer resizing with growth factor\n    if (total_nodes > buffers.d_nodes_capacity) {\n        if (buffers.d_nodes) cudaFree(buffers.d_nodes);\n        size_t new_cap = total_nodes * 1.5;\n        cudaMalloc(&buffers.d_nodes, new_cap * sizeof(LinearGpuNode));\n        buffers.d_nodes_capacity = new_cap;\n    }\n\n    if ((size_t)total_trees > buffers.d_pop_capacity) {\n        if (buffers.d_offsets) cudaFree(buffers.d_offsets);\n        if (buffers.d_sizes) cudaFree(buffers.d_sizes);\n        if (buffers.d_results) cudaFree(buffers.d_results);\n        \n        size_t new_cap = total_trees * 1.5;\n        cudaMalloc(&buffers.d_offsets, new_cap * sizeof(int));\n        cudaMalloc(&buffers.d_sizes, new_cap * sizeof(int));\n        cudaMalloc(&buffers.d_results, new_cap * sizeof(double));\n        buffers.d_pop_capacity = new_cap;\n    }\n\n    LinearGpuNode* d_all_nodes = (LinearGpuNode*)buffers.d_nodes;\n    int* d_offsets = (int*)buffers.d_offsets;\n    int* d_sizes = (int*)buffers.d_sizes;\n    double* d_results = (double*)buffers.d_results;\n\n    // Async memory transfers using CUDA stream\n    cudaMemcpyAsync(d_all_nodes, all_nodes.data(), total_nodes * sizeof(LinearGpuNode), \n                    cudaMemcpyHostToDevice, stream);\n    cudaMemcpyAsync(d_offsets, tree_offsets.data(), total_trees * sizeof(int), \n                    cudaMemcpyHostToDevice, stream);\n    cudaMemcpyAsync(d_sizes, tree_sizes.data(), total_trees * sizeof(int), \n                    cudaMemcpyHostToDevice, stream);\n\n    // Optimized kernel launch configuration for RTX 3050\n    // RTX 3050 has 20 SMs, each can handle 2048 threads max\n    // For 50k trees, we want maximum occupancy\n    int threadsPerBlock = 256;\n    int blocksPerGrid = (total_trees + threadsPerBlock - 1) / threadsPerBlock;\n\n    // Launch kernel on stream\n    evaluate_all_populations_kernel<<<blocksPerGrid, threadsPerBlock, 0, stream>>>(\n        d_all_nodes, d_offsets, d_sizes, total_trees,\n        d_targets, d_x_values, num_points, num_vars, d_results,\n        COMPLEXITY_PENALTY_FACTOR, USE_RMSE_FITNESS\n    );\n\n    // Async copy results back\n    cudaMemcpyAsync(results.data(), d_results, total_trees * sizeof(double), \n                    cudaMemcpyDeviceToHost, stream);\n    \n    // Synchronize stream\n    cudaStreamSynchronize(stream);\n}\n\n// ============================================================\n// DOUBLE-BUFFERED GPU IMPLEMENTATION\n// ============================================================\n\nvoid init_double_buffered_gpu(DoubleBufferedGpu& db) {\n    // Create two streams for overlapped execution\n    cudaStreamCreate((cudaStream_t*)&db.streams[0]);\n    cudaStreamCreate((cudaStream_t*)&db.streams[1]);\n    \n    // Pre-allocate buffers for both ping and pong\n    size_t initial_nodes_cap = 1500000;  // 50k trees * 30 nodes avg\n    size_t initial_pop_cap = 60000;      // Slightly more than 50k\n    \n    for (int i = 0; i < 2; ++i) {\n        db.d_nodes_capacity[i] = initial_nodes_cap;\n        db.d_pop_capacity[i] = initial_pop_cap;\n        \n        cudaMalloc(&db.d_nodes[i], initial_nodes_cap * sizeof(LinearGpuNode));\n        cudaMalloc(&db.d_offsets[i], initial_pop_cap * sizeof(int));\n        cudaMalloc(&db.d_sizes[i], initial_pop_cap * sizeof(int));\n        cudaMalloc(&db.d_results[i], initial_pop_cap * sizeof(double));\n    }\n    \n    // Allocate pinned host memory for faster H2D/D2H transfers\n    db.h_pinned_capacity = initial_pop_cap;\n    cudaMallocHost(&db.h_pinned_results, initial_pop_cap * sizeof(double));\n    \n    db.current_buffer = 0;\n}\n\nvoid cleanup_double_buffered_gpu(DoubleBufferedGpu& db) {\n    for (int i = 0; i < 2; ++i) {\n        if (db.streams[i]) {\n            cudaStreamDestroy((cudaStream_t)db.streams[i]);\n            db.streams[i] = nullptr;\n        }\n        if (db.d_nodes[i]) { cudaFree(db.d_nodes[i]); db.d_nodes[i] = nullptr; }\n        if (db.d_offsets[i]) { cudaFree(db.d_offsets[i]); db.d_offsets[i] = nullptr; }\n        if (db.d_sizes[i]) { cudaFree(db.d_sizes[i]); db.d_sizes[i] = nullptr; }\n        if (db.d_results[i]) { cudaFree(db.d_results[i]); db.d_results[i] = nullptr; }\n    }\n    \n    if (db.h_pinned_results) {\n        cudaFreeHost(db.h_pinned_results);\n        db.h_pinned_results = nullptr;\n    }\n}\n\nvoid launch_evaluation_async(\n    const std::vector<LinearGpuNode>& all_nodes,\n    const std::vector<int>& tree_offsets,\n    const std::vector<int>& tree_sizes,\n    int total_trees,\n    double* d_targets, double* d_x_values,\n    int num_points,\n    int num_vars,\n    DoubleBufferedGpu& db)\n{\n    if (total_trees == 0) return;\n    \n    int buf = db.current_buffer;\n    cudaStream_t stream = (cudaStream_t)db.streams[buf];\n    size_t total_nodes = all_nodes.size();\n    \n    // Ensure buffers are large enough\n    if (total_nodes > db.d_nodes_capacity[buf]) {\n        cudaFree(db.d_nodes[buf]);\n        size_t new_cap = total_nodes * 1.5;\n        cudaMalloc(&db.d_nodes[buf], new_cap * sizeof(LinearGpuNode));\n        db.d_nodes_capacity[buf] = new_cap;\n    }\n    \n    if ((size_t)total_trees > db.d_pop_capacity[buf]) {\n        cudaFree(db.d_offsets[buf]);\n        cudaFree(db.d_sizes[buf]);\n        cudaFree(db.d_results[buf]);\n        \n        size_t new_cap = total_trees * 1.5;\n        cudaMalloc(&db.d_offsets[buf], new_cap * sizeof(int));\n        cudaMalloc(&db.d_sizes[buf], new_cap * sizeof(int));\n        cudaMalloc(&db.d_results[buf], new_cap * sizeof(double));\n        db.d_pop_capacity[buf] = new_cap;\n    }\n    \n    // Ensure pinned results buffer is large enough\n    if ((size_t)total_trees > db.h_pinned_capacity) {\n        cudaFreeHost(db.h_pinned_results);\n        db.h_pinned_capacity = total_trees * 1.5;\n        cudaMallocHost(&db.h_pinned_results, db.h_pinned_capacity * sizeof(double));\n    }\n    \n    LinearGpuNode* d_nodes = (LinearGpuNode*)db.d_nodes[buf];\n    int* d_offsets = (int*)db.d_offsets[buf];\n    int* d_sizes = (int*)db.d_sizes[buf];\n    double* d_results = (double*)db.d_results[buf];\n    \n    // Async transfers\n    cudaMemcpyAsync(d_nodes, all_nodes.data(), total_nodes * sizeof(LinearGpuNode), \n                    cudaMemcpyHostToDevice, stream);\n    cudaMemcpyAsync(d_offsets, tree_offsets.data(), total_trees * sizeof(int), \n                    cudaMemcpyHostToDevice, stream);\n    cudaMemcpyAsync(d_sizes, tree_sizes.data(), total_trees * sizeof(int), \n                    cudaMemcpyHostToDevice, stream);\n    \n    // Launch kernel\n    int threadsPerBlock = 256;\n    int blocksPerGrid = (total_trees + threadsPerBlock - 1) / threadsPerBlock;\n    \n    evaluate_all_populations_kernel<<<blocksPerGrid, threadsPerBlock, 0, stream>>>(\n        d_nodes, d_offsets, d_sizes, total_trees,\n        d_targets, d_x_values, num_points, num_vars, d_results,\n        COMPLEXITY_PENALTY_FACTOR, USE_RMSE_FITNESS\n    );\n    \n    // Async copy results to pinned memory\n    cudaMemcpyAsync(db.h_pinned_results, d_results, total_trees * sizeof(double), \n                    cudaMemcpyDeviceToHost, stream);\n    \n    // DO NOT SYNC HERE - let CPU do other work\n}\n\nvoid retrieve_results_sync(\n    std::vector<double>& results,\n    int total_trees,\n    DoubleBufferedGpu& db)\n{\n    int buf = db.current_buffer;\n    cudaStream_t stream = (cudaStream_t)db.streams[buf];\n    \n    // Wait for this stream to complete\n    cudaStreamSynchronize(stream);\n    \n    // Copy from pinned memory to results vector (this is very fast - memory to memory)\n    results.resize(total_trees);\n    memcpy(results.data(), db.h_pinned_results, total_trees * sizeof(double));\n    \n    // Switch to other buffer for next generation\n    db.current_buffer = 1 - buf;\n}\n\n#endif // USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/FitnessGPU.cuh\n",
        "#ifndef FITNESS_GPU_CUH\n#define FITNESS_GPU_CUH\n\n#include <vector>\n#include <memory> // For NodePtr in the host-side wrapper\n#include \"ExpressionTree.h\" // For NodeType enum and original Node structure (host-side)\n#include \"Globals.h\" // For INF, USE_RMSE_FITNESS, COMPLEXITY_PENALTY_FACTOR etc.\n\n// Forward declaration for host-side NodePtr\nstruct Node;\nusing NodePtr = std::shared_ptr<Node>;\n\n// A simplified node structure for the linearized tree on the GPU\nstruct LinearGpuNode {\n    NodeType type;\n    double value;\n    int var_index;\n    char op;\n};\n\n// Helper function to linearize the tree into a post-order array\nvoid linearize_tree(const NodePtr& node, std::vector<LinearGpuNode>& linear_tree);\n\n// Host-side wrapper for launching CUDA kernel\n#if USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\ndouble evaluate_fitness_gpu(NodePtr tree,\n                            const std::vector<double>& targets,\n                            const std::vector<std::vector<double>>& x_values,\n                            double* d_targets, double* d_x_values);\n\n// Batch evaluation function with persistent buffers\nvoid evaluate_population_gpu(const std::vector<LinearGpuNode>& all_nodes,\n                             const std::vector<int>& tree_offsets,\n                             const std::vector<int>& tree_sizes,\n                             const std::vector<double>& targets,\n                             const std::vector<std::vector<double>>& x_values,\n                             std::vector<double>& results,\n                             double* d_targets, double* d_x_values,\n                             void*& d_nodes_ptr, size_t& d_nodes_cap,\n                             void*& d_offsets_ptr, void*& d_sizes_ptr, void*& d_results_ptr, size_t& d_pop_cap);\n\n// ============================================================\n// GLOBAL BATCH EVALUATION - Evaluates ALL islands in ONE kernel call\n// ============================================================\n// Persistent GPU buffers for global batch (managed by GeneticAlgorithm)\nstruct GlobalGpuBuffers {\n    void* d_nodes = nullptr;\n    void* d_offsets = nullptr;\n    void* d_sizes = nullptr;\n    void* d_results = nullptr;\n    size_t d_nodes_capacity = 0;\n    size_t d_pop_capacity = 0;\n    void* cuda_stream = nullptr; // cudaStream_t\n};\n\n// ============================================================\n// DOUBLE-BUFFERED GPU EVALUATION - Maximum overlap of CPU/GPU work\n// ============================================================\nstruct DoubleBufferedGpu {\n    // Two sets of device buffers for ping-pong operation\n    void* d_nodes[2] = {nullptr, nullptr};\n    void* d_offsets[2] = {nullptr, nullptr};\n    void* d_sizes[2] = {nullptr, nullptr};\n    void* d_results[2] = {nullptr, nullptr};\n    size_t d_nodes_capacity[2] = {0, 0};\n    size_t d_pop_capacity[2] = {0, 0};\n    \n    // Two streams for overlapped execution\n    void* streams[2] = {nullptr, nullptr};\n    \n    // Current buffer index (0 or 1)\n    int current_buffer = 0;\n    \n    // Host-side pinned memory for faster transfers\n    void* h_pinned_results = nullptr;\n    size_t h_pinned_capacity = 0;\n};\n\n// Initialize double-buffered GPU resources\nvoid init_double_buffered_gpu(DoubleBufferedGpu& db);\n\n// Cleanup double-buffered GPU resources\nvoid cleanup_double_buffered_gpu(DoubleBufferedGpu& db);\n\n// Async launch - starts GPU work without waiting (CPU can do other work)\nvoid launch_evaluation_async(\n    const std::vector<LinearGpuNode>& all_nodes,\n    const std::vector<int>& tree_offsets,\n    const std::vector<int>& tree_sizes,\n    int total_trees,\n    double* d_targets, double* d_x_values,\n    int num_points,\n    int num_vars,\n    DoubleBufferedGpu& db);\n\n// Wait for GPU work to complete and retrieve results\nvoid retrieve_results_sync(\n    std::vector<double>& results,\n    int total_trees,\n    DoubleBufferedGpu& db);\n\n// Initialize global GPU buffers and CUDA stream\nvoid init_global_gpu_buffers(GlobalGpuBuffers& buffers);\n\n// Cleanup global GPU buffers\nvoid cleanup_global_gpu_buffers(GlobalGpuBuffers& buffers);\n\n// Evaluate ALL trees from ALL islands in a single GPU batch call (maximum GPU utilization)\nvoid evaluate_all_populations_gpu(\n    const std::vector<LinearGpuNode>& all_nodes,\n    const std::vector<int>& tree_offsets,\n    const std::vector<int>& tree_sizes,\n    const std::vector<int>& tree_complexities, // For complexity penalty\n    int total_trees,\n    const std::vector<double>& targets,\n    const std::vector<std::vector<double>>& x_values,\n    std::vector<double>& results,\n    double* d_targets, double* d_x_values,\n    GlobalGpuBuffers& buffers);\n#endif\n\n#endif // FITNESS_GPU_CUH\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/GeneticAlgorithm.cpp\n",
        "#include \"GeneticAlgorithm.h\"\n#include \"Globals.h\"\n#include \"Fitness.h\"\n#include \"AdvancedFeatures.h\" // Incluir este para DomainConstraints::\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n#include \"FitnessGPU.cuh\"     // Para funciones de GPU\n#include <cuda_runtime.h>     // Para CUDA runtime\n#endif\n#include <iostream>\n#include <algorithm>\n#include <vector>\n#include <cmath>\n#include <omp.h>\n#include <iomanip>\n#include <iterator>\n#include <chrono>\n#include <unordered_set>\n\n// --- Constructor (Modificado para que evaluate_population procese todo) ---\n// --- Constructor (Modificado para que evaluate_population procese todo) ---\nGeneticAlgorithm::GeneticAlgorithm(const std::vector<double>& targets_ref,\n                                     const std::vector<std::vector<double>>& x_values_ref,\n                                     int total_pop,\n                                     int gens,\n                                     const std::vector<std::string>& seeds,\n                                     int n_islands)\n    : targets(targets_ref),\n      x_values(x_values_ref),\n      total_population_size(total_pop),\n      generations(gens),\n      num_islands(n_islands),\n      overall_best_fitness(INF),\n      last_overall_best_fitness(INF),\n      generation_last_improvement(0)\n{\n    // Validar y ajustar n\u00famero de islas y poblaci\u00f3n por isla\n    if (this->num_islands <= 0) this->num_islands = 1;\n    pop_per_island = this->total_population_size / this->num_islands;\n    if (pop_per_island < MIN_POP_PER_ISLAND) {\n        pop_per_island = MIN_POP_PER_ISLAND;\n        this->num_islands = this->total_population_size / pop_per_island;\n        if (this->num_islands == 0) this->num_islands = 1;\n        std::cerr << \"Warning: Adjusted number of islands to \" << this->num_islands\n                  << \" for minimum population size per island (\" << pop_per_island <<\").\" << std::endl;\n    }\n    this->total_population_size = this->num_islands * pop_per_island;\n    std::cout << \"Info: Running with \" << this->num_islands << \" islands, \"\n              << pop_per_island << \" individuals per island.\" << std::endl;\n\n    // Crear las islas\n    islands.reserve(this->num_islands);\n    for (int i = 0; i < this->num_islands; ++i) {\n        try {\n            islands.push_back(std::make_unique<Island>(i, pop_per_island));\n        }\n        catch (const std::exception& e) { std::cerr << \"[ERROR] Creating Island \" << i << \": \" << e.what() << std::endl; throw; }\n        catch (...) { std::cerr << \"[ERROR] Unknown exception creating island \" << i << std::endl; throw; }\n    }\n\n    // --- INJECT SEEDS ---\n    if (!seeds.empty()) {\n        std::cout << \"Info: Injecting \" << seeds.size() << \" seed formulas into population...\" << std::endl;\n        int seed_idx = 0;\n        \n        // Distribute seeds cyclically across islands to promote diversity\n        for (int i = 0; i < this->num_islands && seed_idx < seeds.size(); ++i) {\n            for(size_t j = 0; j < islands[i]->population.size(); ++j) {\n                if (seed_idx >= seeds.size()) break;\n\n                try {\n                    NodePtr parsed_tree = parse_formula_string(seeds[seed_idx]);\n                    if (parsed_tree) {\n                        islands[i]->population[j].tree = std::move(parsed_tree);\n                    }\n                    seed_idx++; \n                } catch (const std::exception& e) {\n                    std::cerr << \"[Warning] Failed to parse seed formula: \" << seeds[seed_idx] << \" | Error: \" << e.what() << std::endl;\n                    seed_idx++; // Skip this seed\n                }\n            }\n        }\n    }\n\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n    bool gpu_init_failed = false;\n    if (!FORCE_CPU_MODE) {\n        // Asignar memoria en la GPU y copiar datos\n        size_t targets_size = targets.size() * sizeof(double);\n        // Multivariable: flatten X values [NUM_SAMPLES * NUM_FEATURES]\n        size_t n_samples = x_values.size();\n        size_t n_features = (n_samples > 0) ? x_values[0].size() : 0;\n        size_t x_values_size = n_samples * n_features * sizeof(double);\n        \n        // Linearize\n        std::vector<double> flattened_x;\n        flattened_x.reserve(n_samples * n_features);\n        for(const auto& row : x_values) {\n            flattened_x.insert(flattened_x.end(), row.begin(), row.end());\n        }\n\n        cudaError_t err_t = cudaMalloc(&d_targets, targets_size);\n        cudaError_t err_x = cudaMalloc(&d_x_values, x_values_size);\n\n        if (err_t != cudaSuccess || err_x != cudaSuccess) {\n            std::cerr << \"[WARNING] CUDA memory allocation failed: \"\n                      << cudaGetErrorString(err_t) << \" | \" << cudaGetErrorString(err_x) << std::endl;\n            std::cerr << \"[INFO] Falling back to CPU mode.\" << std::endl;\n            gpu_init_failed = true;\n            // Clean up any partial allocation\n            if (d_targets) { cudaFree(d_targets); d_targets = nullptr; }\n            if (d_x_values) { cudaFree(d_x_values); d_x_values = nullptr; }\n        } else {\n            cudaMemcpy(d_targets, targets.data(), targets_size, cudaMemcpyHostToDevice);\n            cudaMemcpy(d_x_values, flattened_x.data(), x_values_size, cudaMemcpyHostToDevice);\n            \n            // Initialize global GPU buffers for batch evaluation of ALL islands\n            init_global_gpu_buffers(global_gpu_buffers);\n            \n            // Initialize double-buffered GPU for async pipelining\n            init_double_buffered_gpu(double_buffer_gpu);\n            \n            std::cout << \"GPU buffers initialized for global batch evaluation (max \" \n                      << total_population_size << \" trees in single kernel call)\" << std::endl;\n            std::cout << \"Double-buffered GPU enabled for async CPU/GPU overlap\" << std::endl;\n        }\n    }\n    \n    if (FORCE_CPU_MODE || gpu_init_failed) {\n        std::cout << \"Using CPU for all evaluations\" << std::endl;\n    }\n#endif\n\n     // Evaluaci\u00f3n inicial de TODA la poblaci\u00f3n (incluyendo la inyectada)\n     // La funci\u00f3n evaluate_population ahora simplificar\u00e1 y evaluar\u00e1 a todos.\n     std::cout << \"Evaluating initial population (simplifying all)...\" << std::endl;\n     evaluate_all_islands(); // Use new global batch evaluation\n\n     // Actualizar el mejor global inicial (en serie)\n     overall_best_fitness = INF;\n     overall_best_tree = nullptr;\n     int initial_best_island = -1;\n     int initial_best_idx = -1;\n\n     for (int i = 0; i < islands.size(); ++i) {\n        for(int j=0; j < islands[i]->population.size(); ++j) {\n            const auto& ind = islands[i]->population[j];\n            if (ind.tree && ind.fitness_valid && ind.fitness < overall_best_fitness) {\n                overall_best_fitness = ind.fitness;\n                initial_best_island = i;\n                initial_best_idx = j;\n            }\n        }\n     }\n     if(initial_best_island != -1 && initial_best_idx != -1) {\n         overall_best_tree = clone_tree(islands[initial_best_island]->population[initial_best_idx].tree);\n     }\n\n     last_overall_best_fitness = overall_best_fitness;\n     generation_last_improvement = 0;\n     std::cout << \"Initial best fitness: \" << std::scientific << overall_best_fitness << std::fixed << std::endl;\n     if (overall_best_tree) {\n          std::cout << \"Initial best formula size: \" << tree_size(overall_best_tree) << std::endl;\n          std::cout << \"Initial best formula: \" << tree_to_string(overall_best_tree) << std::endl;\n          // Nota para saber si el mejor inicial fue la f\u00f3rmula inyectada (ahora simplificada)\n          if (USE_INITIAL_FORMULA && initial_best_island != -1 && initial_best_idx == 0) {\n               std::cout << \"   (Note: Initial best is the (simplified) injected formula from Island \" << initial_best_island << \")\" << std::endl;\n          } else if (initial_best_island != -1) {\n               std::cout << \"   (Note: Initial best found in Island \" << initial_best_island << \", Index \" << initial_best_idx << \")\" << std::endl;\n          }\n      } else { std::cout << \"No valid initial solution found (all fitness INF?).\" << std::endl; }\n     std::cout << \"----------------------------------------\" << std::endl;\n}\n\nGeneticAlgorithm::~GeneticAlgorithm() {\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n    if (!FORCE_CPU_MODE) {\n        // Cleanup double-buffered GPU\n        cleanup_double_buffered_gpu(double_buffer_gpu);\n        \n        // Cleanup global GPU buffers\n        cleanup_global_gpu_buffers(global_gpu_buffers);\n        \n        if (d_targets) {\n            cudaFree(d_targets);\n            d_targets = nullptr;\n        }\n        if (d_x_values) {\n            cudaFree(d_x_values);\n            d_x_values = nullptr;\n        }\n    }\n#endif\n    // El destructor de std::unique_ptr en 'islands' se encarga de liberar la memoria de las islas.\n    // 'overall_best_tree' es un NodePtr. Si es un smart pointer (como std::unique_ptr<Node>),\n    // su memoria se liberar\u00e1 autom\u00e1ticamente. Si es un puntero crudo, necesitar\u00eda una funci\u00f3n delete_tree.\n    // Asumiendo que NodePtr es un smart pointer o que la liberaci\u00f3n se maneja en otro lugar,\n    // o que un \u00e1rbol nulo al final no causa fugas si no fue asignado con 'new'.\n    // Si NodePtr es un puntero crudo y se asigna con 'new' en clone_tree, entonces\n    // delete_tree(overall_best_tree) ser\u00eda necesario aqu\u00ed.\n    // Por ahora, se deja vac\u00edo, asumiendo manejo autom\u00e1tico o externo.\n}\n\nvoid GeneticAlgorithm::evaluate_population(Island& island) {\n    int pop_size = island.population.size();\n    if (pop_size == 0) return;\n\n    // 1. Simplify trees (CPU Parallel)\n    // We do this first so we only send simplified trees to GPU\n    #pragma omp parallel for schedule(dynamic)\n    for (int i = 0; i < pop_size; ++i) {\n        Individual& ind = island.population[i];\n        if (ind.tree) {\n            ind.tree = DomainConstraints::fix_or_simplify(ind.tree);\n        }\n    }\n\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n    // 2. Prepare for Batch GPU Evaluation\n    std::vector<LinearGpuNode> all_nodes;\n    std::vector<int> tree_offsets;\n    std::vector<int> tree_sizes;\n    \n    // Reserve memory to avoid reallocations (Optimization)\n    // Assuming average tree size is around 20-30 nodes. \n    // This dramatically reduces CPU overhead during linearization.\n    all_nodes.reserve(pop_size * 30); \n    tree_offsets.reserve(pop_size);\n    tree_sizes.reserve(pop_size);\n    \n    // We need map back to original index because some trees might be null\n    std::vector<int> valid_indices; \n    valid_indices.reserve(pop_size);\n\n    for (int i = 0; i < pop_size; ++i) {\n        if (island.population[i].tree) {\n            int start_offset = all_nodes.size();\n            linearize_tree(island.population[i].tree, all_nodes);\n            int size = all_nodes.size() - start_offset;\n            \n            if (size > 0) {\n                tree_offsets.push_back(start_offset);\n                tree_sizes.push_back(size);\n                valid_indices.push_back(i);\n            } else {\n                 island.population[i].fitness = INF;\n                 island.population[i].fitness_valid = true;\n            }\n        } else {\n             island.population[i].fitness = INF;\n             island.population[i].fitness_valid = true;\n        }\n    }\n\n    if (valid_indices.empty()) return;\n\n    // 3. call GPU Batch (d_targets and d_x_values already exist)\n    std::vector<double> raw_results(valid_indices.size());\n    evaluate_population_gpu(all_nodes, tree_offsets, tree_sizes, targets, x_values, raw_results, d_targets, d_x_values,\n                            island.d_nodes, island.d_nodes_capacity,\n                            island.d_offsets, island.d_sizes, island.d_results, island.d_pop_capacity);\n\n    // 4. Process results\n    for (size_t k = 0; k < valid_indices.size(); ++k) {\n        int idx = valid_indices[k];\n        double sum_sq_error = raw_results[k];\n        double raw_fitness = INF;\n\n        // Check for validity\n        if (!std::isnan(sum_sq_error) && !std::isinf(sum_sq_error) && sum_sq_error < 1e300) { // 1e300 as safety threshold\n             if (USE_RMSE_FITNESS) {\n                 if (x_values.size() > 0) {\n                     double mse = sum_sq_error / x_values.size();\n                     raw_fitness = sqrt(mse);\n                 }\n             } else {\n                 raw_fitness = sum_sq_error;\n             }\n        }\n\n        if (raw_fitness >= INF/2) {\n             island.population[idx].fitness = INF;\n        } else {\n             // Complexity Penalty\n             double complexity = static_cast<double>(tree_sizes[k]); // We already have the linear size\n             double penalty = complexity * COMPLEXITY_PENALTY_FACTOR;\n             island.population[idx].fitness = raw_fitness * (1.0 + penalty);\n        }\n        island.population[idx].fitness_valid = true;\n    }\n\n#else\n    // CPU Fallback (Parallel)\n    #pragma omp parallel for schedule(dynamic)\n    for (int i = 0; i < pop_size; ++i) {\n        Individual& ind = island.population[i];\n        if (ind.tree) {\n             ind.fitness = evaluate_fitness(ind.tree, targets, x_values);\n             ind.fitness_valid = true;\n        } else {\n             ind.fitness = INF;\n             ind.fitness_valid = true;\n        }\n    }\n#endif\n}\n\n\n// ============================================================\n// GLOBAL BATCH EVALUATION - Evaluates ALL islands in ONE GPU kernel call\n// ============================================================\nvoid GeneticAlgorithm::evaluate_all_islands() {\n    int total_trees = 0;\n    for (const auto& island : islands) {\n        total_trees += island->population.size();\n    }\n    if (total_trees == 0) return;\n\n    // Step 1: Simplify ALL trees in parallel (CPU)\n    // Note: collapse(2) not supported by MSVC OpenMP 2.0, using nested parallel for\n    #pragma omp parallel for schedule(dynamic)\n    for (int i = 0; i < static_cast<int>(islands.size()); ++i) {\n        for (int j = 0; j < static_cast<int>(islands[i]->population.size()); ++j) {\n            Individual& ind = islands[i]->population[j];\n            if (ind.tree) {\n                ind.tree = DomainConstraints::fix_or_simplify(ind.tree);\n            }\n        }\n    }\n\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n    // Runtime check: if FORCE_CPU_MODE is true or GPU init failed (d_targets == nullptr), use CPU\n    if (!FORCE_CPU_MODE && d_targets != nullptr) {\n    // Step 2: Linearize ALL trees from ALL islands into single buffer\n    // OPTIMIZATION: Parallel linearization using OpenMP\n    \n    // First pass: count valid trees and compute per-tree sizes in parallel\n    std::vector<int> tree_sizes_temp(total_trees, 0);\n    std::vector<std::pair<int, int>> index_mapping(total_trees); // (island, individual)\n    std::vector<bool> tree_valid(total_trees, false);\n    \n    int tree_idx = 0;\n    for (int i = 0; i < static_cast<int>(islands.size()); ++i) {\n        for (int j = 0; j < static_cast<int>(islands[i]->population.size()); ++j) {\n            index_mapping[tree_idx] = {i, j};\n            tree_idx++;\n        }\n    }\n    \n    // Parallel linearization into per-thread buffers\n    int num_threads = omp_get_max_threads();\n    std::vector<std::vector<LinearGpuNode>> thread_nodes(num_threads);\n    std::vector<std::vector<int>> thread_offsets(num_threads);\n    std::vector<std::vector<int>> thread_sizes(num_threads);\n    std::vector<std::vector<std::pair<int, int>>> thread_mappings(num_threads);\n    \n    // Pre-allocate per-thread buffers\n    int trees_per_thread = (total_trees + num_threads - 1) / num_threads;\n    for (int t = 0; t < num_threads; ++t) {\n        thread_nodes[t].reserve(trees_per_thread * 30);\n        thread_offsets[t].reserve(trees_per_thread);\n        thread_sizes[t].reserve(trees_per_thread);\n        thread_mappings[t].reserve(trees_per_thread);\n    }\n    \n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        auto& local_nodes = thread_nodes[tid];\n        auto& local_offsets = thread_offsets[tid];\n        auto& local_sizes = thread_sizes[tid];\n        auto& local_mappings = thread_mappings[tid];\n        \n        #pragma omp for schedule(static)\n        for (int t = 0; t < total_trees; ++t) {\n            int i = index_mapping[t].first;\n            int j = index_mapping[t].second;\n            Individual& ind = islands[i]->population[j];\n            \n            if (ind.tree) {\n                int start_offset = local_nodes.size();\n                linearize_tree(ind.tree, local_nodes);\n                int size = local_nodes.size() - start_offset;\n                \n                if (size > 0) {\n                    local_offsets.push_back(start_offset);\n                    local_sizes.push_back(size);\n                    local_mappings.push_back({i, j});\n                } else {\n                    ind.fitness = INF;\n                    ind.fitness_valid = true;\n                }\n            } else {\n                ind.fitness = INF;\n                ind.fitness_valid = true;\n            }\n        }\n    }\n    \n    // Merge thread-local buffers into global buffers\n    std::vector<LinearGpuNode> all_nodes;\n    std::vector<int> tree_offsets;\n    std::vector<int> tree_sizes;\n    std::vector<std::pair<int, int>> result_mapping;\n    \n    size_t total_node_count = 0;\n    size_t total_valid_trees = 0;\n    for (int t = 0; t < num_threads; ++t) {\n        total_node_count += thread_nodes[t].size();\n        total_valid_trees += thread_mappings[t].size();\n    }\n    \n    all_nodes.reserve(total_node_count);\n    tree_offsets.reserve(total_valid_trees);\n    tree_sizes.reserve(total_valid_trees);\n    result_mapping.reserve(total_valid_trees);\n    \n    for (int t = 0; t < num_threads; ++t) {\n        int offset_adjustment = all_nodes.size();\n        \n        // Copy nodes\n        all_nodes.insert(all_nodes.end(), thread_nodes[t].begin(), thread_nodes[t].end());\n        \n        // Adjust offsets and copy\n        for (size_t k = 0; k < thread_offsets[t].size(); ++k) {\n            tree_offsets.push_back(thread_offsets[t][k] + offset_adjustment);\n            tree_sizes.push_back(thread_sizes[t][k]);\n            result_mapping.push_back(thread_mappings[t][k]);\n        }\n    }\n    \n    std::vector<int> tree_complexities = tree_sizes; // Same as sizes for now\n\n    if (result_mapping.empty()) return;\n\n    int valid_trees = result_mapping.size();\n    int num_points = x_values.size();\n    int num_vars = (num_points > 0) ? x_values[0].size() : 0;\n    \n    // Step 3: Launch GPU evaluation ASYNC (no blocking!)\n    // GPU will work while CPU continues with other tasks\n    launch_evaluation_async(\n        all_nodes, tree_offsets, tree_sizes,\n        valid_trees, d_targets, d_x_values, num_points,\n        num_vars,\n        double_buffer_gpu\n    );\n    \n    // Step 4: Wait for GPU results (this is where we sync)\n    std::vector<double> results;\n    retrieve_results_sync(results, valid_trees, double_buffer_gpu);\n\n    // Step 5: Distribute results back to islands\n    for (size_t k = 0; k < static_cast<size_t>(valid_trees); ++k) {\n        int island_idx = result_mapping[k].first;\n        int ind_idx = result_mapping[k].second;\n        double fitness = results[k];\n        \n        // Validate result\n        if (std::isnan(fitness) || std::isinf(fitness) || fitness >= 1e300) {\n            fitness = INF;\n        }\n        \n        islands[island_idx]->population[ind_idx].fitness = fitness;\n        islands[island_idx]->population[ind_idx].fitness_valid = true;\n    }\n    \n    } else {\n        // FORCE_CPU_MODE is true OR GPU init failed: Use CPU\n        #pragma omp parallel for schedule(dynamic)\n        for (int i = 0; i < static_cast<int>(islands.size()); ++i) {\n            for (int j = 0; j < static_cast<int>(islands[i]->population.size()); ++j) {\n                Individual& ind = islands[i]->population[j];\n                if (ind.tree) {\n                    // Pass GPU pointers (even if null) to match signature\n                    ind.fitness = evaluate_fitness(ind.tree, targets, x_values, d_targets, d_x_values);\n                    ind.fitness_valid = true;\n                } else {\n                    ind.fitness = INF;\n                    ind.fitness_valid = true;\n                }\n            }\n        }\n    }\n\n#else\n    // CPU Fallback: CUDA not available, use parallel CPU evaluation\n    #pragma omp parallel for schedule(dynamic)\n    for (int i = 0; i < static_cast<int>(islands.size()); ++i) {\n        for (int j = 0; j < static_cast<int>(islands[i]->population.size()); ++j) {\n            Individual& ind = islands[i]->population[j];\n            if (ind.tree) {\n                ind.fitness = evaluate_fitness(ind.tree, targets, x_values);\n                ind.fitness_valid = true;\n            } else {\n                ind.fitness = INF;\n                ind.fitness_valid = true;\n            }\n        }\n    }\n#endif\n}\n\n\n// --- evolve_island ---\n// (Sin cambios)\nvoid GeneticAlgorithm::evolve_island(Island& island, int current_generation) {\n    int current_pop_size = island.population.size(); if (current_pop_size == 0) return;\n    auto best_it = std::min_element(island.population.begin(), island.population.end(),\n        [](const Individual& a, const Individual& b) {\n            if (!a.tree || !a.fitness_valid) return false;\n            if (!b.tree || !b.fitness_valid) return true;\n            return a.fitness < b.fitness;\n        });\n    double current_best_fitness = INF;\n    int best_idx = -1;\n    if (best_it != island.population.end() && best_it->tree && best_it->fitness_valid) {\n        best_idx = std::distance(island.population.begin(), best_it);\n        current_best_fitness = best_it->fitness;\n    }\n    island.fitness_history.push_back(current_best_fitness);\n    if (best_idx != -1 && current_best_fitness < INF) {\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n         auto local_search_result = try_local_improvement(island.population[best_idx].tree, island.population[best_idx].fitness, targets, x_values, LOCAL_SEARCH_ATTEMPTS, d_targets, d_x_values);\n#else\n         auto local_search_result = try_local_improvement(island.population[best_idx].tree, island.population[best_idx].fitness, targets, x_values, LOCAL_SEARCH_ATTEMPTS);\n#endif\n         if (local_search_result.first && local_search_result.second < island.population[best_idx].fitness) {\n             island.population[best_idx].tree = local_search_result.first;\n             island.population[best_idx].fitness = local_search_result.second;\n             island.population[best_idx].fitness_valid = true;\n             current_best_fitness = local_search_result.second;\n         }\n    }\n    if (current_best_fitness < island.best_fitness - FITNESS_EQUALITY_TOLERANCE) {\n        island.best_fitness = current_best_fitness;\n        island.stagnation_counter = 0;\n    } else if (current_best_fitness < INF) {\n        island.stagnation_counter++;\n    }\n    island.pareto_optimizer.update(island.population, targets, x_values);\n    for(const auto& ind : island.population) {\n        if(ind.tree && ind.fitness_valid && ind.fitness < PATTERN_RECORD_FITNESS_THRESHOLD) {\n            island.pattern_memory.record_success(ind.tree, ind.fitness);\n        }\n    }\n    std::vector<Individual> next_generation;\n    next_generation.reserve(current_pop_size);\n    int elite_count = std::max(1, static_cast<int>(current_pop_size * island.params.elite_percentage));\n    if (elite_count > 0 && elite_count <= current_pop_size) {\n        std::partial_sort(island.population.begin(), island.population.begin() + elite_count, island.population.end());\n        int added_elites = 0;\n        for (int i = 0; i < elite_count && i < island.population.size(); ++i) {\n             if (island.population[i].tree && island.population[i].fitness_valid) {\n                 next_generation.emplace_back(clone_tree(island.population[i].tree));\n                 next_generation.back().fitness = island.population[i].fitness;\n                 next_generation.back().fitness_valid = true;\n                 added_elites++;\n             }\n        }\n        elite_count = added_elites;\n    } else { elite_count = 0; }\n    int random_injection_count = 0;\n    if (island.stagnation_counter > STAGNATION_LIMIT_ISLAND / 2) {\n        random_injection_count = static_cast<int>(current_pop_size * STAGNATION_RANDOM_INJECT_PERCENT);\n        for(int i = 0; i < random_injection_count && next_generation.size() < current_pop_size; ++i) {\n             NodePtr random_tree = generate_random_tree(MAX_TREE_DEPTH_INITIAL);\n             if (random_tree) next_generation.emplace_back(std::move(random_tree));\n        }\n    }\n    int pattern_injection_count = 0;\n    \n    // --- ISLAND CATACLYSM ---\n    // If enabled, triggers a hard reset if stagnation persists.\n    if (USE_ISLAND_CATACLYSM && island.stagnation_counter >= STAGNATION_LIMIT_ISLAND) {\n        // Keep only top 1 elite (already in next_generation[0] if elite_count > 0)\n        // Or if we need to enforce better elitism during cataclysm:\n        \n        int survivors = 1; // Only the absolute best one survives\n        // Resize to survivors\n        if (next_generation.size() > survivors) next_generation.resize(survivors);\n        \n        // Fill the rest with completely random trees\n        int to_fill = current_pop_size - next_generation.size();\n        for(int i=0; i<to_fill; ++i) {\n             NodePtr random_tree = generate_random_tree(MAX_TREE_DEPTH_INITIAL);\n             if (random_tree) next_generation.emplace_back(std::move(random_tree));\n        }\n        \n        island.stagnation_counter = 0; // Reset counter\n        // Optional: Pattern injection could also happen here, but random is better for total diversity.\n    }\n    // Only do standard injections if we didn't just nuke everything\n    else {\n        if (random_injection_count == 0 && current_generation % PATTERN_INJECT_INTERVAL == 0) {\n            pattern_injection_count = static_cast<int>(current_pop_size * PATTERN_INJECT_PERCENT);\n            for (int i = 0; i < pattern_injection_count && next_generation.size() < current_pop_size; ++i) {\n                NodePtr pt = island.pattern_memory.suggest_pattern_based_tree(MAX_TREE_DEPTH_INITIAL);\n                if (pt) { next_generation.emplace_back(std::move(pt)); }\n                else {\n                     NodePtr random_tree = generate_random_tree(MAX_TREE_DEPTH_INITIAL);\n                     if (random_tree) next_generation.emplace_back(std::move(random_tree));\n                }\n            }\n        }\n    }\n    auto& rng = get_rng();\n    std::uniform_real_distribution<double> prob_dist(0.0, 1.0);\n    // >>> Parallel Parent Selection Loop with Uniqueness Check <<<\n    \n    // 1. Initialize uniqueness set with survivors (elites/injected)\n    std::unordered_set<std::string> unique_signatures;\n    if (PREVENT_DUPLICATES) {\n        for (const auto& ind : next_generation) {\n            if (ind.tree) {\n                unique_signatures.insert(tree_to_string(ind.tree));\n            }\n        }\n    }\n\n    // 2. Fill the rest of the population\n    int fail_safe_counter = 0;\n    while (next_generation.size() < current_pop_size) {\n        int needed = current_pop_size - next_generation.size();\n        \n        // Generate candidates in parallel\n        std::vector<Individual> candidates(needed);\n        \n        #pragma omp parallel for schedule(dynamic)\n        for (int i = 0; i < needed; ++i) {\n            // Thread-local RNG\n            auto& rng = get_rng(); \n            \n            Individual offspring;\n            // Use distribution defined outside or create new one? \n            // Better create local to avoid shared state issues if not const\n            std::uniform_real_distribution<double> local_prob_dist(0.0, 1.0);\n\n            if (local_prob_dist(rng) < island.params.crossover_rate) {\n                Individual p1, p2;\n                if (USE_LEXICASE_SELECTION) {\n                    p1 = lexicase_selection(island.population, targets, x_values);\n                    p2 = lexicase_selection(island.population, targets, x_values);\n                } else {\n                    p1 = tournament_selection(island.population, island.params.tournament_size);\n                    p2 = tournament_selection(island.population, island.params.tournament_size);\n                }\n                // Use Semantic Crossover for better diversity\n                offspring = semantic_crossover(p1, p2, x_values);\n            } else {\n                Individual p1;\n                if (USE_LEXICASE_SELECTION) {\n                    p1 = lexicase_selection(island.population, targets, x_values);\n                } else {\n                    p1 = tournament_selection(island.population, island.params.tournament_size);\n                }\n                if (p1.tree) p1.tree = clone_tree(p1.tree); \n                mutate(p1, island.params.mutation_rate);\n                offspring = std::move(p1);\n            }\n            \n            candidates[i] = std::move(offspring);\n        }\n        \n        // Filter and add unique candidates (Serial)\n        int added_this_round = 0;\n        for (auto& cand : candidates) {\n            if (next_generation.size() >= current_pop_size) break;\n            \n            bool is_valid_to_add = true;\n            if (PREVENT_DUPLICATES && cand.tree) {\n                std::string sig = tree_to_string(cand.tree);\n                if (unique_signatures.find(sig) != unique_signatures.end()) {\n                    is_valid_to_add = false; \n                } else {\n                    unique_signatures.insert(sig);\n                }\n            }\n            \n            if (is_valid_to_add) {\n                next_generation.emplace_back(std::move(cand));\n                added_this_round++;\n            }\n        }\n        \n        // Deadlock prevention\n        if (added_this_round == 0) {\n            fail_safe_counter++;\n            if (fail_safe_counter > DUPLICATE_RETRIES) {\n                // Fill remaining with random trees\n                int remaining = current_pop_size - next_generation.size();\n                for (int k = 0; k < remaining; ++k) {\n                    NodePtr random_tree = generate_random_tree(MAX_TREE_DEPTH_INITIAL);\n                    if (random_tree) next_generation.emplace_back(std::move(random_tree));\n                }\n                break; // Exit loop\n            }\n        } else {\n            fail_safe_counter = 0; // Reset if we made progress\n        }\n    }\n     if (next_generation.size() > current_pop_size) next_generation.resize(current_pop_size);\n    island.population = std::move(next_generation);\n    if (current_generation > 0 && current_generation % PARAM_MUTATE_INTERVAL == 0) island.params.mutate(island.stagnation_counter);\n}\n\n// --- migrate ---\n// (Sin cambios)\nvoid GeneticAlgorithm::migrate() {\n    if (num_islands <= 1) return;\n    int current_pop_per_island = islands.empty() ? 0 : islands[0]->population.size();\n    if (current_pop_per_island == 0) return;\n    int num_migrants = std::min(MIGRATION_SIZE, current_pop_per_island / 5);\n    if (num_migrants <= 0) return;\n    std::vector<std::vector<Individual>> outgoing_migrants(num_islands);\n    #pragma omp parallel for\n    for (int i = 0; i < num_islands; ++i) {\n        Island& src = *islands[i];\n        if (src.population.size() < num_migrants) continue;\n        std::partial_sort(src.population.begin(), src.population.begin() + num_migrants, src.population.end());\n        outgoing_migrants[i].reserve(num_migrants);\n        int migrants_selected = 0;\n        for (int j = 0; j < src.population.size() && migrants_selected < num_migrants; ++j) {\n             if (src.population[j].tree && src.population[j].fitness_valid) {\n                 Individual migrant_copy;\n                 migrant_copy.tree = clone_tree(src.population[j].tree);\n                 migrant_copy.fitness = src.population[j].fitness;\n                 migrant_copy.fitness_valid = true;\n                 outgoing_migrants[i].push_back(std::move(migrant_copy));\n                 migrants_selected++;\n             }\n        }\n    }\n    for (int dest_idx = 0; dest_idx < num_islands; ++dest_idx) {\n        int src_idx = (dest_idx + num_islands - 1) % num_islands;\n        Island& dest = *islands[dest_idx];\n        const auto& migrants_to_receive = outgoing_migrants[src_idx];\n        if (migrants_to_receive.empty() || dest.population.empty()) continue;\n        int replace_count = std::min((int)migrants_to_receive.size(), (int)dest.population.size());\n        if (replace_count <= 0) continue;\n        std::partial_sort(dest.population.begin(), dest.population.end() - replace_count, dest.population.end());\n        int migrant_idx = 0;\n        for (int i = 0; i < replace_count; ++i) {\n            int replace_idx = dest.population.size() - 1 - i;\n            if (migrant_idx < migrants_to_receive.size()) {\n                 dest.population[replace_idx] = std::move(migrants_to_receive[migrant_idx++]);\n                 dest.population[replace_idx].fitness_valid = false; // Marcar para reevaluar\n            }\n        }\n    }\n}\n\n\n// --- run ---\n// (Sin cambios)\nNodePtr GeneticAlgorithm::run() {\n    std::cout << \"Starting Genetic Algorithm...\" << std::endl;\n    auto start_time = std::chrono::high_resolution_clock::now();\n\n    for (int gen = 0; gen < generations; ++gen) {\n        // [DEBUG] Trace execution\n        if (gen == 0) { std::cout << \"[DEBUG] Entering main loop, gen=0\" << std::endl; std::cout.flush(); }\n        \n        // 1. Evaluate ALL islands in ONE GPU kernel call (maximum GPU utilization)\n        evaluate_all_islands();\n\n        // 2. Evolve Islands (Parallel Island Loop)\n        // Genetic operators (crossover, mutation) are CPU-bound and independent per island.\n        #pragma omp parallel for\n        for (int i = 0; i < islands.size(); ++i) {\n             evolve_island(*islands[i], gen);\n        }\n\n        double current_gen_best_fitness = INF;\n        int best_island_idx = -1;\n        int best_ind_idx = -1;\n        for (int i = 0; i < islands.size(); ++i) {\n             for (int j = 0; j < islands[i]->population.size(); ++j) {\n                 const auto& ind = islands[i]->population[j];\n                 if (ind.tree && ind.fitness_valid && ind.fitness < current_gen_best_fitness) {\n                     current_gen_best_fitness = ind.fitness;\n                     best_island_idx = i; best_ind_idx = j;\n                 }\n             }\n        }\n\n        if (best_island_idx != -1 && current_gen_best_fitness < overall_best_fitness) {\n             if (current_gen_best_fitness < overall_best_fitness) {\n                  overall_best_fitness = current_gen_best_fitness;\n                  overall_best_tree = clone_tree(islands[best_island_idx]->population[best_ind_idx].tree);\n                  std::cout << \"\\n========================================\" << std::endl;\n                  std::cout << \"New Global Best Found (Gen \" << gen + 1 << \", Island \" << best_island_idx << \")\" << std::endl;\n                  std::cout << \"Fitness: \" << std::fixed << std::setprecision(8) << overall_best_fitness << std::endl;\n                  std::cout << \"Size: \" << tree_size(overall_best_tree) << std::endl;\n                  std::cout << \"Formula: \" << tree_to_string(overall_best_tree) << std::endl;\n                  std::cout.flush(); // Ensure Formula: line is captured\n                  std::cout << \"Predictions vs Targets:\" << std::endl;\n                  std::cout << std::fixed << std::setprecision(4);\n                  if (overall_best_tree && !x_values.empty()) {\n                      for (size_t j = 0; j < x_values.size(); ++j) {\n                          double val = evaluate_tree(overall_best_tree, x_values[j]);\n                          double target_val = (j < targets.size()) ? targets[j] : std::nan(\"\");\n                          double diff = (!std::isnan(val) && !std::isnan(target_val)) ? std::fabs(val - target_val) : std::nan(\"\");\n                          std::cout << \"  x=(\";\n                          for(size_t v=0; v<x_values[j].size(); ++v) std::cout << (v>0?\",\":\"\") << x_values[j][v];\n                          std::cout << \"): Pred=\" << std::setw(12) << val\n                                    << \", Target=\" << std::setw(12) << target_val\n                                    << \", Diff=\" << std::setw(12) << diff << std::endl;\n                      }\n                  } else { std::cout << \"  (No data or no valid tree to show predictions)\" << std::endl; }\n                  std::cout << \"========================================\" << std::endl;\n                  last_overall_best_fitness = overall_best_fitness;\n                  generation_last_improvement = gen;\n              }\n        } else {\n             if (overall_best_fitness < INF && (gen - generation_last_improvement) >= GLOBAL_STAGNATION_LIMIT) {\n                  std::cout << \"\\n========================================\" << std::endl;\n                  std::cout << \"TERMINATION: Global best fitness hasn't improved for \" << GLOBAL_STAGNATION_LIMIT << \" generations.\" << std::endl;\n                  std::cout << \"Stopping at Generation \" << gen + 1 << \".\" << std::endl;\n                  std::cout << \"========================================\" << std::endl;\n                  break;\n             }\n        }\n\n        if ((gen + 1) % MIGRATION_INTERVAL == 0 && num_islands > 1) {\n             migrate();\n             // Re-evaluate after migration using global batch\n             evaluate_all_islands();\n        }\n\n        if (overall_best_fitness < EXACT_SOLUTION_THRESHOLD) {\n            std::cout << \"\\n========================================\" << std::endl;\n            std::cout << \"Solution found meeting criteria at Generation \" << gen + 1 << \"!\" << std::endl;\n            std::cout << \"Final Fitness: \" << std::fixed << std::setprecision(8) << overall_best_fitness << std::endl;\n            if(overall_best_tree) {\n                 std::cout << \"Final Formula Size: \" << tree_size(overall_best_tree) << std::endl;\n                 std::cout << \"Final Formula: \" << tree_to_string(overall_best_tree) << std::endl;\n                 std::cout.flush(); // Ensure Final Formula: line is captured\n            }\n            std::cout << \"========================================\" << std::endl;\n            std::cout.flush(); // Ensure flush\n            break;\n        }\n\n        if ((gen + 1) % PROGRESS_REPORT_INTERVAL == 0 || gen == generations - 1) {\n             auto current_time = std::chrono::high_resolution_clock::now();\n             std::chrono::duration<double> elapsed = current_time - start_time;\n             std::cout << \"\\n--- Generation \" << gen + 1 << \"/\" << generations\n                       << \" (Elapsed: \" << std::fixed << std::setprecision(2) << elapsed.count() << \"s) ---\" << std::endl;\n             std::cout << \"Overall Best Fitness: \" << std::scientific << overall_best_fitness << std::fixed << std::endl;\n              if(overall_best_tree) { std::cout << \"Best Formula Size: \" << tree_size(overall_best_tree) << std::endl; }\n              else { std::cout << \"Best Formula Size: N/A\" << std::endl; }\n              std::cout << \"(Last improvement at gen: \" << generation_last_improvement + 1 << \")\" << std::endl;\n        }\n    }\n\n    auto end_time = std::chrono::high_resolution_clock::now();\n    std::chrono::duration<double> total_elapsed = end_time - start_time;\n    std::cout << \"\\n========================================\" << std::endl;\n    std::cout << \"Evolution Finished!\" << std::endl;\n    std::cout << \"Total Time: \" << std::fixed << std::setprecision(2) << total_elapsed.count() << \" seconds\" << std::endl;\n    std::cout << \"Final Best Fitness: \" << std::fixed << std::setprecision(8) << overall_best_fitness << std::endl;\n     if (overall_best_tree) {\n         std::cout << \"Final Best Formula Size: \" << tree_size(overall_best_tree) << std::endl;\n         std::cout << \"Final Formula: \" << tree_to_string(overall_best_tree) << std::endl;\n         std::cout.flush(); // Ensure Final Formula: line is captured\n          std::cout << \"--- Final Verification ---\" << std::endl;\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n          double final_check_fitness = evaluate_fitness(overall_best_tree, targets, x_values, d_targets, d_x_values);\n#else\n          double final_check_fitness = evaluate_fitness(overall_best_tree, targets, x_values);\n#endif\n          std::cout << \"Recalculated Fitness: \" << std::fixed << std::setprecision(8) << final_check_fitness << std::endl;\n          std::cout << std::fixed << std::setprecision(4);\n          for (size_t j = 0; j < x_values.size(); ++j) {\n                double val = evaluate_tree(overall_best_tree, x_values[j]);\n                 double target_val = (j < targets.size()) ? targets[j] : std::nan(\"\");\n                 double diff = (!std::isnan(val) && !std::isnan(target_val)) ? std::fabs(val - target_val) : std::nan(\"\");\n                 std::cout << \"  x=(\";\n                 for(size_t v=0; v<x_values[j].size(); ++v) std::cout << (v>0?\",\":\"\") << x_values[j][v];\n                 std::cout << \"): Pred=\" << std::setw(12) << val\n                          << \", Target=\" << std::setw(12) << target_val\n                          << \", Diff=\" << std::setw(12) << diff << std::endl;\n           }\n     } else { std::cout << \"No valid solution found.\" << std::endl; }\n      std::cout << \"========================================\" << std::endl;\n    return overall_best_tree;\n}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/GeneticAlgorithm.h\n",
        "// ============================================================\n// Archivo: src/GeneticAlgorithm.h\n// ============================================================\n#ifndef GENETICALGORITHM_H\n#define GENETICALGORITHM_H\n\n#include \"ExpressionTree.h\"\n#include \"GeneticOperators.h\"\n#include \"AdvancedFeatures.h\"\n#include \"Globals.h\" // Incluir Globals.h para INF, NUM_ISLANDS, etc.\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n#include \"FitnessGPU.cuh\" // For GlobalGpuBuffers definition\n#endif\n#include <vector>\n#include <string>\n#include <memory> // Para std::unique_ptr\n\nclass GeneticAlgorithm {\n    // Estructura interna para representar una isla\n    struct Island {\n        std::vector<Individual> population; // Poblaci\u00f3n de la isla\n        EvolutionParameters params;         // Par\u00e1metros evolutivos propios de la isla\n        PatternMemory pattern_memory;       // Memoria de patrones de la isla\n        ParetoOptimizer pareto_optimizer;   // Optimizador Pareto de la isla\n        int stagnation_counter = 0;         // Contador de estancamiento local de la isla\n        double best_fitness = INF;          // Mejor fitness hist\u00f3rico de la isla\n        std::vector<double> fitness_history;// Historial de fitness (opcional)\n        int id;                             // Identificador de la isla\n\n        // Constructor de la isla\n        explicit Island(int island_id, int pop_size) : id(island_id) {\n             population = create_initial_population(pop_size); // Crear poblaci\u00f3n inicial\n             params = EvolutionParameters::create_default();   // Usar par\u00e1metros por defecto\n        }\n\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n        // Persistent GPU buffers\n        void* d_nodes = nullptr;\n        void* d_offsets = nullptr;\n        void* d_sizes = nullptr;\n        void* d_results = nullptr;\n        size_t d_nodes_capacity = 0;\n        size_t d_pop_capacity = 0;\n\n        ~Island() {\n            // We cannot easily call cudaFree here because this header might be included\n            // where cuda_runtime.h is not. However, we can trust the OS/driver to clean up\n            // or we should add a cleanup function.\n            // For now, we will rely on GeneticAlgorithm destructor or explict cleanup if possible.\n            // But since Island is unique_ptr, we can't easily add a destructor that calls cudaFree \n            // without including cuda_runtime.\n            // Optimization: Let's rely on OS cleanup at exit, OR add a cleanup method called by GA.\n        }\n#endif\n    };\n\n    // Miembros principales de la clase GeneticAlgorithm\n    std::vector<std::unique_ptr<Island>> islands; // Vector de punteros \u00fanicos a las islas\n    const std::vector<double>& targets;           // Referencia a los datos objetivo\n    const std::vector<std::vector<double>>& x_values;          // Referencia a los valores de x [samples][features]\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n    double* d_targets = nullptr;                  // Puntero a los datos objetivo en la GPU\n    double* d_x_values = nullptr;                 // Puntero a los valores de x en la GPU\n    GlobalGpuBuffers global_gpu_buffers;          // Global buffers for batch evaluation of ALL islands\n    DoubleBufferedGpu double_buffer_gpu;          // Double-buffered GPU for async overlap\n#endif\n    int total_population_size;                    // Tama\u00f1o total de la poblaci\u00f3n\n    int generations;                              // N\u00famero m\u00e1ximo de generaciones\n    int num_islands;                              // N\u00famero de islas\n\n    // Seguimiento del mejor global\n    NodePtr overall_best_tree = nullptr;          // Mejor \u00e1rbol encontrado globalmente\n    double overall_best_fitness = INF;            // Mejor fitness encontrado globalmente\n\n    // --- NUEVO: Seguimiento de Estancamiento Global ---\n    int generation_last_improvement = 0;          // Generaci\u00f3n en la que mejor\u00f3 el overall_best_fitness\n    double last_overall_best_fitness = INF;       // Valor del overall_best_fitness en la \u00faltima mejora\n    // -------------------------------------------------\n\n    int pop_per_island;                           // Poblaci\u00f3n calculada por isla\n\npublic:\n    // Constructor\n    GeneticAlgorithm(const std::vector<double>& targets_ref,\n                       const std::vector<std::vector<double>>& x_values_ref,\n                       int total_pop,\n                       int gens,\n                       const std::vector<std::string>& seeds = {}, // Optional: Initial population seeds\n                       int n_islands = NUM_ISLANDS); // Usar valor de Globals.h por defecto\n    ~GeneticAlgorithm(); // Destructor para liberar memoria de la GPU\n\n    // Ejecuta el algoritmo gen\u00e9tico\n    NodePtr run();\n\nprivate:\n    // Funciones auxiliares internas\n    void evaluate_population(Island& island); // Eval\u00faa fitness de una isla (legacy)\n    void evaluate_all_islands(); // Eval\u00faa ALL islands in ONE GPU batch call (optimized)\n    void evolve_island(Island& island, int current_generation); // Evoluciona una isla por una generaci\u00f3n\n    void migrate(); // Realiza la migraci\u00f3n entre islas\n    void update_overall_best(const Island& island); // Actualiza el mejor global\n};\n\n\n#endif // GENETICALGORITHM_H\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/GeneticOperators.cpp\n",
        "#include \"GeneticOperators.h\"\n#include \"Globals.h\"\n#include \"Fitness.h\"\n#include \"AdvancedFeatures.h\"\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <numeric>  // For std::iota\n#include <map>\n#include <stdexcept>\n#include <set>\n#include <iostream> // Para mensajes de error/info\n\n// Genera un \u00e1rbol aleatorio (CON TODOS LOS OPERADORES, EXPONENTES SIN RESTRICCI\u00d3N)\nNodePtr generate_random_tree(int max_depth, int current_depth) {\n    std::uniform_real_distribution<double> prob_dist(0.0, 1.0);\n    auto& rng = get_rng();\n    double terminal_prob = 0.2 + 0.8 * (static_cast<double>(current_depth) / max_depth);\n\n    if (current_depth >= max_depth || prob_dist(rng) < terminal_prob) {\n        // Crear terminal\n        if (prob_dist(rng) < TERMINAL_VS_VARIABLE_PROB) { \n             auto var_node = std::make_shared<Node>(NodeType::Variable);\n             std::uniform_int_distribution<int> var_dist(0, NUM_VARIABLES - 1);\n             var_node->var_index = var_dist(rng);\n             return var_node;\n        }\n        else {\n            auto node = std::make_shared<Node>(NodeType::Constant);\n            if (FORCE_INTEGER_CONSTANTS) { std::uniform_int_distribution<int> cd(CONSTANT_INT_MIN_VALUE, CONSTANT_INT_MAX_VALUE); node->value = static_cast<double>(cd(rng)); }\n            else { std::uniform_real_distribution<double> cd(CONSTANT_MIN_VALUE, CONSTANT_MAX_VALUE); node->value = cd(rng); }\n            if (std::fabs(node->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) node->value = 0.0;\n            return node;\n        }\n    } else {\n\n        // Crear operador\n        auto node = std::make_shared<Node>(NodeType::Operator);\n        // Match the weights in Globals.h: +, -, *, /, ^, %, s, c, l, e, !, _, g\n        const std::vector<char> ops = {'+', '-', '*', '/', '^', '%', 's', 'c', 'l', 'e', '!', '_', 'g', 'S', 'C', 'T'};\n        std::discrete_distribution<int> op_dist(OPERATOR_WEIGHTS.begin(), OPERATOR_WEIGHTS.end());\n        node->op = ops[op_dist(rng)];\n\n        bool is_unary = (node->op == 's' || node->op == 'c' || node->op == 'l' || node->op == 'e' || node->op == '!' || node->op == '_' || node->op == 'g' || node->op == 'S' || node->op == 'C' || node->op == 'T');\n\n        // Generar hijos recursivamente\n        node->left = generate_random_tree(max_depth, current_depth + 1);\n        if (!is_unary) {\n            node->right = generate_random_tree(max_depth, current_depth + 1);\n        } else {\n            node->right = nullptr;\n        }\n\n        // Fallback para hijos nulos\n        auto generate_random_terminal = [&]() -> NodePtr {\n            if (prob_dist(rng) < TERMINAL_VS_VARIABLE_PROB) { \n                auto var_node = std::make_shared<Node>(NodeType::Variable);\n                std::uniform_int_distribution<int> var_dist(0, NUM_VARIABLES - 1);\n                var_node->var_index = var_dist(rng);\n                return var_node;\n            }\n            else {\n                auto const_node = std::make_shared<Node>(NodeType::Constant);\n                if (FORCE_INTEGER_CONSTANTS) { std::uniform_int_distribution<int> cd(CONSTANT_INT_MIN_VALUE, CONSTANT_INT_MAX_VALUE); const_node->value = static_cast<double>(cd(rng)); }\n                else { std::uniform_real_distribution<double> cd(CONSTANT_MIN_VALUE, CONSTANT_MAX_VALUE); const_node->value = cd(rng); }\n                if (std::fabs(const_node->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) const_node->value = 0.0;\n                return const_node;\n            }\n        };\n\n        if (!node->left) node->left = generate_random_terminal();\n        if (!is_unary && !node->right) node->right = generate_random_terminal();\n\n        // --- Manejo especial para el operador de potencia '^' ---\n        if (node->op == '^') {\n            // Regla 1: Evitar 0^0 o 0^negativo\n            if (node->left->type == NodeType::Constant && std::fabs(node->left->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) {\n                if (node->right->type == NodeType::Constant && node->right->value <= SIMPLIFY_NEAR_ZERO_TOLERANCE) {\n                    const std::vector<char> safe_ops = {'+', '-', '*'};\n                    std::uniform_int_distribution<int> safe_op_dist(0, safe_ops.size() - 1);\n                    node->op = safe_ops[safe_op_dist(rng)];\n                }\n            }\n            // Regla 2: Evitar base negativa con exponente no entero\n            else if (node->left->type == NodeType::Constant && node->left->value < 0.0) {\n                if (node->right->type == NodeType::Constant && std::fabs(node->right->value - std::round(node->right->value)) > SIMPLIFY_NEAR_ZERO_TOLERANCE) {\n                     // Change exponent to int\n                     std::uniform_int_distribution<int> int_exp_dist(-3, 3);\n                     node->right = std::make_shared<Node>(NodeType::Constant);\n                     node->right->value = static_cast<double>(int_exp_dist(rng));\n                }\n            }\n        }\n        return node;\n    }\n}\n\n// --- Crea la poblaci\u00f3n inicial (MODIFICADO para inyectar f\u00f3rmula) ---\n// === OPTIMIZACI\u00d3N: Paralelizado con OpenMP ===\nstd::vector<Individual> create_initial_population(int population_size) {\n    std::vector<Individual> population;\n    population.resize(population_size); // Pre-allocate all slots for parallel access\n\n    // --- NUEVO: Inyecci\u00f3n de F\u00f3rmula Inicial ---\n    int start_index = 0;\n    if (USE_INITIAL_FORMULA && !INITIAL_FORMULA_STRING.empty() && population_size > 0) {\n        try {\n            NodePtr initial_tree = parse_formula_string(INITIAL_FORMULA_STRING);\n            if (initial_tree) {\n                population[0] = Individual(std::move(initial_tree));\n                start_index = 1;\n                std::cout << \"[INFO] Injected initial formula: \" << INITIAL_FORMULA_STRING << std::endl;\n            } else {\n                 std::cerr << \"[WARNING] Parsing initial formula returned null. Skipping injection.\" << std::endl;\n            }\n        } catch (const std::exception& e) {\n            std::cerr << \"[ERROR] Failed to parse initial formula '\" << INITIAL_FORMULA_STRING\n                      << \"': \" << e.what() << \". Skipping injection.\" << std::endl;\n        }\n    }\n    // -----------------------------------------\n\n    // === OPTIMIZACI\u00d3N: Loop paralelo para generar \u00e1rboles ===\n    #pragma omp parallel for schedule(dynamic, 100)\n    for (int i = start_index; i < population_size; ++i) {\n        // Cada hilo tiene su propio RNG (thread_local en get_rng)\n        auto& rng = get_rng();\n        std::uniform_int_distribution<int> depth_dist(3, MAX_TREE_DEPTH_INITIAL);\n        \n        NodePtr random_tree = nullptr;\n        int attempts = 0;\n        const int max_attempts = 10;\n        while (!random_tree && attempts < max_attempts) {\n            random_tree = generate_random_tree(depth_dist(rng));\n            attempts++;\n        }\n        if (random_tree) {\n            population[i] = Individual(std::move(random_tree));\n        } else {\n            // Fallback: crear constante simple\n            auto fallback_node = std::make_shared<Node>(NodeType::Constant);\n            fallback_node->value = 0.0;\n            population[i] = Individual(std::move(fallback_node));\n        }\n    }\n    return population;\n}\n\n// --- Selecci\u00f3n por torneo con parsimonia ---\nIndividual tournament_selection(const std::vector<Individual>& population, int tournament_size) {\n    if (population.empty()) throw std::runtime_error(\"Cannot perform tournament selection on empty population.\");\n    if (tournament_size <= 0) tournament_size = 1;\n    tournament_size = std::min(tournament_size, (int)population.size());\n\n    std::uniform_int_distribution<int> dist(0, population.size() - 1);\n    auto& rng = get_rng();\n    const Individual* best_in_tournament = nullptr;\n\n    int attempts = 0; const int max_attempts = std::min((int)population.size() * 2, 100);\n    do {\n        best_in_tournament = &population[dist(rng)];\n        attempts++;\n    } while ((!best_in_tournament || !best_in_tournament->tree || !best_in_tournament->fitness_valid) && attempts < max_attempts);\n\n    if (!best_in_tournament || !best_in_tournament->tree || !best_in_tournament->fitness_valid) {\n         if (!population.empty()) return population[0];\n         else throw std::runtime_error(\"Tournament selection couldn't find any valid individual in a non-empty population.\");\n    }\n\n    for (int i = 1; i < tournament_size; ++i) {\n        const Individual& contender = population[dist(rng)];\n        if (!contender.tree || !contender.fitness_valid) continue;\n\n        if (contender.fitness < best_in_tournament->fitness) {\n            best_in_tournament = &contender;\n        }\n        else if (std::fabs(contender.fitness - best_in_tournament->fitness) < FITNESS_EQUALITY_TOLERANCE) {\n            int contender_size = tree_size(contender.tree);\n            int best_size = tree_size(best_in_tournament->tree);\n            if (contender_size < best_size) best_in_tournament = &contender;\n        }\n    }\n    return *best_in_tournament;\n}\n\n// --- Epsilon-Lexicase Selection Implementation ---\n// Calculates residuals on demand if not present (Lazy Eval)\nvoid ensure_errors_computed(Individual& ind, const std::vector<double>& targets, const std::vector<std::vector<double>>& x_values) {\n    if (!ind.errors.empty()) return; // Already computed\n    if (!ind.tree) return;\n    \n    ind.errors.reserve(targets.size());\n    for (size_t i = 0; i < targets.size(); ++i) {\n        double val = evaluate_tree(ind.tree, x_values[i]);\n        if (std::isnan(val) || std::isinf(val)) {\n            ind.errors.push_back(INF);\n        } else {\n            // Use ABSOLUTE error for lexicase\n            ind.errors.push_back(std::fabs(val - targets[i]));\n        }\n    }\n}\n\nIndividual lexicase_selection(std::vector<Individual>& population, const std::vector<double>& targets, const std::vector<std::vector<double>>& x_values) {\n    auto& rng = get_rng();\n    \n    // 1. Initial Candidates: Random subset (Tournament Size * 2) or Full Population?\n    // Efficiency: Using a subset is \"Tournament Lexicase\". Using Full is \"Standard Lexicase\".\n    // For 50k pop, full lexicase is slow. Let's use a large pool (e.g. 50-100).\n    int pool_size = 100; \n    std::vector<Individual*> candidates;\n    candidates.reserve(pool_size);\n    \n    std::uniform_int_distribution<int> dist(0, population.size() - 1);\n    for(int i=0; i<pool_size; ++i) {\n        Individual& ind = population[dist(rng)];\n        if(ind.tree && ind.fitness_valid) candidates.push_back(&ind);\n    }\n    \n    if (candidates.empty()) return population[0]; // Should not happen\n    \n    // 2. Shuffle test cases\n    std::vector<int> cases(targets.size());\n    std::iota(cases.begin(), cases.end(), 0);\n    std::shuffle(cases.begin(), cases.end(), rng);\n    \n    // 3. Filter loop\n    for (int case_idx : cases) {\n        // Compute errors for this case for all candidates (Lazy)\n        double min_error = INF;\n        for (Individual* cand : candidates) {\n            ensure_errors_computed(*cand, targets, x_values);\n            if (case_idx < cand->errors.size()) {\n                 if (cand->errors[case_idx] < min_error) min_error = cand->errors[case_idx];\n            }\n        }\n        \n        // Define epsilon (MAD or simple threshold)\n        // Here we use a simple dynamic epsilon based on min_error\n        double epsilon = std::max(min_error * 0.1, 1e-5); \n        // Or if min_error is 0, epsilon is 1e-5. Epsilon-Lexicase implies \"close enough\".\n        \n        // Filter\n        std::vector<Individual*> next_candidates;\n        next_candidates.reserve(candidates.size());\n        for (Individual* cand : candidates) {\n             if (case_idx < cand->errors.size()) {\n                 if (cand->errors[case_idx] <= min_error + epsilon) {\n                     next_candidates.push_back(cand);\n                 }\n             }\n        }\n        \n        candidates = std::move(next_candidates);\n        if (candidates.empty()) break; // Should not happen given min_error logic\n        if (candidates.size() == 1) return *candidates[0];\n    }\n    \n    // If multiple remain, pick random\n    if (candidates.empty()) return population[dist(rng)];\n    std::uniform_int_distribution<int> pick(0, candidates.size() - 1);\n    return *candidates[pick(rng)];\n}\n\n// Mutata un \u00e1rbol (EXPONENTES SIN RESTRICCI\u00d3N en OperatorChange)\nNodePtr mutate_tree(const NodePtr& tree, double mutation_rate, int max_depth) {\n    auto& rng = get_rng();\n    std::uniform_real_distribution<double> prob(0.0, 1.0);\n    auto new_tree = clone_tree(tree); // Siempre clonar primero\n    if (!new_tree) return nullptr; // Si el \u00e1rbol original era nulo, el clon tambi\u00e9n\n\n    if (prob(rng) >= mutation_rate) return new_tree; // No mutar\n\n    std::vector<NodePtr*> nodes; collect_node_ptrs(new_tree, nodes);\n    if (nodes.empty()) return new_tree; // No hay nodos para mutar (\u00e1rbol vac\u00edo?)\n\n    std::uniform_int_distribution<int> node_dist(0, nodes.size() - 1);\n    int node_idx = node_dist(rng);\n    NodePtr* node_to_mutate_ptr = nodes[node_idx];\n    if (!node_to_mutate_ptr || !(*node_to_mutate_ptr)) return new_tree; // Puntero o nodo nulo inesperado\n\n    const std::vector<MutationType> mutation_types = {\n        MutationType::ConstantChange, MutationType::OperatorChange,\n        MutationType::SubtreeReplace, MutationType::NodeInsertion,\n        MutationType::NodeDeletion\n    };\n    std::uniform_int_distribution<int> type_dist(0, mutation_types.size() - 1);\n    MutationType mut_type = mutation_types[type_dist(rng)];\n\n    NodePtr& current_node_ptr_ref = *node_to_mutate_ptr;\n    Node& current_node = *current_node_ptr_ref;\n\n    // Generar reemplazo aleatorio (usado en varios casos)\n    auto generate_replacement = [&](int depth) -> NodePtr {\n        NodePtr replacement = generate_random_tree(depth);\n        if (!replacement) { // Fallback si la generaci\u00f3n falla\n            replacement = std::make_shared<Node>(NodeType::Constant);\n            replacement->value = 1.0; // Usar 1.0 como fallback simple\n        }\n        return replacement;\n    };\n\n\n    switch (mut_type) {\n        case MutationType::ConstantChange:\n             if (current_node.type == NodeType::Constant) {\n                 // Cambiar valor de la constante\n                 double change_factor = std::uniform_real_distribution<double>(0.8, 1.2)(rng);\n                 double add_factor = std::uniform_real_distribution<double>(-1.0, 1.0)(rng);\n                 current_node.value = current_node.value * change_factor + add_factor;\n                 if (FORCE_INTEGER_CONSTANTS) current_node.value = std::round(current_node.value);\n                 if (std::fabs(current_node.value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) current_node.value = 0.0;\n             } else {\n                 // Si no es constante, reemplazar por un sub\u00e1rbol aleatorio peque\u00f1o\n                 *node_to_mutate_ptr = generate_replacement(1); // Profundidad 1 (terminal)\n             }\n            break;\n        case MutationType::OperatorChange:\n             if (current_node.type == NodeType::Operator) {\n                 // Usar la distribuci\u00f3n ponderada global para elegir el nuevo operador.\n                 // Esto asegura que si 'g' tiene peso alto, sea elegido frecuentemente.\n                 // Asumimos que OPERATOR_WEIGHTS tiene 0.0 para operadores deshabilitados.\n                 \n                 const std::vector<char> all_ops = {'+', '-', '*', '/', '^', '%', 's', 'c', 'l', 'e', '!', '_', 'g', 'S', 'C', 'T'};\n                 std::discrete_distribution<int> op_dist(OPERATOR_WEIGHTS.begin(), OPERATOR_WEIGHTS.end());\n                 \n                 // Verificar si hay al menos 2 operadores habilitados para evitar bucle infinito\n                 int enabled_count = 0;\n                 for (double w : OPERATOR_WEIGHTS) if (w > 0.0) enabled_count++;\n                 \n                 if (enabled_count > 1) {\n                     int attempts = 0;\n                     int new_op_idx = -1;\n                     do {\n                         new_op_idx = op_dist(rng);\n                         attempts++;\n                     } while (all_ops[new_op_idx] == current_node.op && attempts < 20); // Intentar cambiar\n                     \n                     if (attempts < 20) { // Si logramos encontrar uno diferente\n                         char old_op = current_node.op;\n                         char new_op = all_ops[new_op_idx];\n                         \n                         bool was_unary = (old_op == 's' || old_op == 'c' || old_op == 'l' || old_op == 'e' || old_op == '!' || old_op == '_' || old_op == 'g');\n                         bool is_unary = (new_op == 's' || new_op == 'c' || new_op == 'l' || new_op == 'e' || new_op == '!' || new_op == '_' || new_op == 'g' || new_op == 'S' || new_op == 'C' || new_op == 'T');\n\n                         if (was_unary && !is_unary) {\n                             current_node.right = generate_replacement(1);\n                         } else if (!was_unary && is_unary) {\n                             current_node.right = nullptr;\n                         }\n                         current_node.op = new_op;\n                     }\n                 }\n             } else {\n                 // Si no es operador, reemplazar por un sub\u00e1rbol aleatorio\n                  *node_to_mutate_ptr = generate_replacement(max_depth);\n             }\n            break;\n        case MutationType::SubtreeReplace:\n            *node_to_mutate_ptr = generate_replacement(max_depth);\n            break;\n        case MutationType::NodeInsertion:\n            {\n                auto new_op_node = std::make_shared<Node>(NodeType::Operator);\n                // Usar distribuci\u00f3n ponderada\n                const std::vector<char> all_ops = {'+', '-', '*', '/', '^', '%', 's', 'c', 'l', 'e', '!', '_', 'g', 'S', 'C', 'T'};\n                std::discrete_distribution<int> op_dist(OPERATOR_WEIGHTS.begin(), OPERATOR_WEIGHTS.end());\n                \n                int new_op_idx = op_dist(rng); // Siempre elegir\u00e1 uno habilitado\n                new_op_node->op = all_ops[new_op_idx];\n\n                bool is_unary = (new_op_node->op == 's' || new_op_node->op == 'c' || new_op_node->op == 'l' || new_op_node->op == 'e' || new_op_node->op == '!' || new_op_node->op == '_' || new_op_node->op == 'g' || new_op_node->op == 'S' || new_op_node->op == 'C' || new_op_node->op == 'T');\n\n                new_op_node->left = current_node_ptr_ref;\n\n                if (!is_unary) {\n                     if (prob(rng) < MUTATE_INSERT_CONST_PROB) {\n                         auto right_child = std::make_shared<Node>(NodeType::Constant);\n                         if (FORCE_INTEGER_CONSTANTS) { std::uniform_int_distribution<int> cv(MUTATE_INSERT_CONST_INT_MIN, MUTATE_INSERT_CONST_INT_MAX); right_child->value = static_cast<double>(cv(rng)); }\n                         else { std::uniform_real_distribution<double> cv(MUTATE_INSERT_CONST_FLOAT_MIN, MUTATE_INSERT_CONST_FLOAT_MAX); right_child->value = cv(rng); }\n                         if (std::fabs(right_child->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) right_child->value = 0.0;\n                         new_op_node->right = right_child;\n                     } else {\n                         auto var_node = std::make_shared<Node>(NodeType::Variable);\n                         std::uniform_int_distribution<int> var_dist(0, NUM_VARIABLES - 1);\n                         var_node->var_index = var_dist(rng);\n                         new_op_node->right = var_node;\n                     }\n                } else {\n                    new_op_node->right = nullptr;\n                }\n\n                *node_to_mutate_ptr = new_op_node;\n            }\n            break;\n        case MutationType::NodeDeletion:\n            {\n                // No eliminar la ra\u00edz directamente si es la \u00fanica opci\u00f3n\n                if (node_to_mutate_ptr == &new_tree && nodes.size() == 1) return new_tree;\n\n                if (current_node.type == NodeType::Operator) {\n                    // Si es operador, reemplazarlo por uno de sus hijos (aleatorio)\n                    NodePtr replacement = nullptr;\n                    bool has_left = (current_node.left != nullptr);\n                    bool has_right = (current_node.right != nullptr);\n\n                    if (has_left && has_right) {\n                        replacement = (prob(rng) < 0.5) ? current_node.left : current_node.right;\n                    } else if (has_left) {\n                        replacement = current_node.left;\n                    } else if (has_right) {\n                        replacement = current_node.right;\n                    }\n                    // Si no tiene hijos v\u00e1lidos (\u00bfc\u00f3mo?), reemplazar por terminal\n                    if (!replacement) replacement = generate_replacement(0); // Profundidad 0 (terminal)\n\n                    *node_to_mutate_ptr = replacement;\n                } else {\n                    // Si es terminal, reemplazar por otro terminal aleatorio\n                    // (Evitar eliminar si es la ra\u00edz y no hay m\u00e1s nodos)\n                     if (node_to_mutate_ptr != &new_tree || nodes.size() > 1) {\n                          *node_to_mutate_ptr = generate_replacement(0); // Profundidad 0 (terminal)\n                     }\n                }\n            }\n            break;\n         default: // Caso inesperado, reemplazar por seguridad\n             *node_to_mutate_ptr = generate_replacement(max_depth);\n            break;\n    }\n    if (USE_HARD_DEPTH_LIMIT) trim_tree(new_tree, MAX_TREE_DEPTH_HARD_LIMIT); // Enforce hard limit after mutation\n    return new_tree;\n}\n\n// Implementaci\u00f3n de crossover\nIndividual crossover(const Individual& parent1, const Individual& parent2) {\n    NodePtr tree1_clone = clone_tree(parent1.tree);\n    NodePtr tree2_clone = clone_tree(parent2.tree);\n    crossover_trees(tree1_clone, tree2_clone);\n    if (USE_HARD_DEPTH_LIMIT) trim_tree(tree1_clone, MAX_TREE_DEPTH_HARD_LIMIT); // Enforce hard limit\n    return Individual(tree1_clone); // Devolver uno de los hijos, el otro se descarta\n}\n\n// Implementaci\u00f3n de Semantic Crossover\nIndividual semantic_crossover(const Individual& p1, const Individual& p2, const std::vector<std::vector<double>>& x_values, int attempts) {\n    if (x_values.empty()) return crossover(p1, p2); // Fallback\n\n    auto& rng = get_rng();\n    \n    // 1. Select Semantic Sample (subset of data)\n    int sample_size = std::min((int)x_values.size(), 10);\n    std::vector<int> indices(sample_size);\n    std::uniform_int_distribution<int> idx_dist(0, x_values.size() - 1);\n    for(int i=0; i<sample_size; ++i) indices[i] = idx_dist(rng);\n    \n    // 2. Compute Parent Semantics\n    std::vector<double> sem_p1(sample_size);\n    std::vector<double> sem_p2(sample_size);\n    bool p1_valid = true, p2_valid = true;\n    \n    for(int i=0; i<sample_size; ++i) {\n        sem_p1[i] = evaluate_tree(p1.tree, x_values[indices[i]]);\n        sem_p2[i] = evaluate_tree(p2.tree, x_values[indices[i]]);\n        if (std::isnan(sem_p1[i]) || std::isinf(sem_p1[i])) p1_valid = false;\n        if (std::isnan(sem_p2[i]) || std::isinf(sem_p2[i])) p2_valid = false;\n    }\n    \n    Individual best_child;\n    double max_diversity = -1.0;\n    \n    for(int k=0; k<attempts; ++k) {\n        // Generate child\n        Individual child = crossover(p1, p2);\n        \n        // Compute Child Semantics\n        std::vector<double> sem_c(sample_size);\n        bool c_valid = true;\n        \n        double diff_p1 = 0.0;\n        double diff_p2 = 0.0;\n        \n        for(int i=0; i<sample_size; ++i) {\n            sem_c[i] = evaluate_tree(child.tree, x_values[indices[i]]);\n            if (std::isnan(sem_c[i]) || std::isinf(sem_c[i])) {\n                c_valid = false;\n                break;\n            }\n            if (p1_valid) diff_p1 += std::abs(sem_c[i] - sem_p1[i]);\n            if (p2_valid) diff_p2 += std::abs(sem_c[i] - sem_p2[i]);\n        }\n        \n        if (!c_valid) continue; // Skip bad children\n        \n        // Check Semantic Difference (Sensitivity)\n        double sensitivity = 1e-4;\n        if (diff_p1 > sensitivity && diff_p2 > sensitivity) {\n            return child; // Found a semantically unique child!\n        }\n        \n        // Keep the \"most different\" valid child found so far as fallback\n        double diversity = std::min(diff_p1, diff_p2);\n        if (diversity > max_diversity) {\n            max_diversity = diversity;\n            best_child = std::move(child);\n        }\n    }\n    \n    // Return best found or just a random crossover if all failed semantic check\n    if (best_child.tree) return best_child;\n    return crossover(p1, p2);\n}\n\n// Implementaci\u00f3n de mutate\nvoid mutate(Individual& individual, double mutation_rate) {\n    individual.tree = mutate_tree(individual.tree, mutation_rate, MAX_TREE_DEPTH_MUTATION);\n    individual.fitness_valid = false; // El fitness se invalida al mutar el \u00e1rbol\n}\n\n// Cruce\nvoid crossover_trees(NodePtr& tree1, NodePtr& tree2) {\n    if (!tree1 || !tree2) return; // No cruzar si alguno es nulo\n\n    std::vector<NodePtr*> nodes1, nodes2;\n    collect_node_ptrs(tree1, nodes1);\n    collect_node_ptrs(tree2, nodes2);\n\n    // No cruzar si alguno no tiene nodos (\u00e1rbol vac\u00edo o solo ra\u00edz nula?)\n    if (nodes1.empty() || nodes2.empty()) return;\n\n    auto& rng = get_rng();\n    std::uniform_int_distribution<int> d1(0, nodes1.size()-1);\n    std::uniform_int_distribution<int> d2(0, nodes2.size()-1);\n\n    // Seleccionar puntos de cruce\n    NodePtr* crossover_point1 = nodes1[d1(rng)];\n    NodePtr* crossover_point2 = nodes2[d2(rng)];\n\n    // Intercambiar los sub\u00e1rboles (los NodePtr)\n    std::swap(*crossover_point1, *crossover_point2);\n}\n\n// Implementaci\u00f3n de simplify_tree\nvoid simplify_tree(NodePtr& tree) {\n    if (USE_SIMPLIFICATION) {\n        tree = DomainConstraints::fix_or_simplify(tree);\n    }\n}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/GeneticOperators.h\n",
        "// ============================================================\n// Archivo: src/GeneticOperators.h\n// ============================================================\n#ifndef GENETICOPERATORS_H\n#define GENETICOPERATORS_H\n\n#include \"ExpressionTree.h\"\n#include \"Globals.h\" // Incluir Globals.h para INF\n#include <vector>\n#include <memory> // Para std::move\n\n// Estructura para representar un individuo en la poblaci\u00f3n.\n// Contiene el \u00e1rbol de expresi\u00f3n y su fitness cacheado.\nstruct Individual {\n    NodePtr tree; // Puntero inteligente al \u00e1rbol de expresi\u00f3n\n    double fitness = INF; // Fitness cacheado (menor es mejor), inicializado a infinito\n    std::vector<double> errors; // Cache of per-case errors for Lexicase Selection\n    bool fitness_valid = false; // Indica si el fitness cacheado es v\u00e1lido\n\n    // Constructor por defecto\n    Individual() = default;\n    // Constructor a partir de un \u00e1rbol (mueve el puntero)\n    explicit Individual(NodePtr t) : tree(std::move(t)) {}\n\n    // Operador de comparaci\u00f3n para ordenar individuos (menor fitness primero)\n    bool operator<(const Individual& other) const {\n        // Manejar casos donde uno o ambos fitness no son v\u00e1lidos\n        if (!fitness_valid && !other.fitness_valid) return false; // Iguales si ambos inv\u00e1lidos\n        if (!fitness_valid) return false; // Inv\u00e1lido es \"peor\" que v\u00e1lido (va despu\u00e9s)\n        if (!other.fitness_valid) return true; // V\u00e1lido es \"mejor\" que inv\u00e1lido (va antes)\n        // Comparar por fitness si ambos son v\u00e1lidos\n        return fitness < other.fitness;\n    }\n};\n\n\n// --- Funciones de Operadores Gen\u00e9ticos ---\n\n// Genera un \u00e1rbol de expresi\u00f3n aleatorio hasta una profundidad m\u00e1xima.\nNodePtr generate_random_tree(int max_depth, int current_depth = 0);\n\n// Crea la poblaci\u00f3n inicial de individuos.\nstd::vector<Individual> create_initial_population(int population_size);\n\n// Selecciona un individuo usando selecci\u00f3n por torneo con presi\u00f3n de parsimonia.\nIndividual tournament_selection(const std::vector<Individual>& population, int tournament_size);\n\n// Selecciona un individuo usando Epsilon-Lexicase Selection (m\u00e1s inteligente)\nIndividual lexicase_selection(std::vector<Individual>& population, const std::vector<double>& targets, const std::vector<std::vector<double>>& x_values);\n\n// Realiza el cruce (crossover) entre dos individuos y devuelve un nuevo individuo.\n// Realiza el cruce (crossover) entre dos individuos y devuelve un nuevo individuo.\nIndividual crossover(const Individual& parent1, const Individual& parent2);\n\n// Realiza cruce sem\u00e1ntico: intenta generar un hijo que sea funcionalmente distinto a los padres.\nIndividual semantic_crossover(const Individual& p1, const Individual& p2, const std::vector<std::vector<double>>& x_values, int attempts = 5);\n\n// Mutata un individuo in-place.\nvoid mutate(Individual& individual, double mutation_rate);\n\n// Simplifica un \u00e1rbol in-place.\nvoid simplify_tree(NodePtr& tree);\n\n// Tipos de mutaci\u00f3n posibles.\nenum class MutationType {\n    ConstantChange,\n    OperatorChange,\n    SubtreeReplace,\n    NodeInsertion,\n    NodeDeletion // <-- A\u00d1ADIDO: Tipo para eliminar un nodo\n    // Simplification (manejado por DomainConstraints)\n};\n\n// Mutata un \u00e1rbol aplicando uno de los tipos de mutaci\u00f3n con cierta probabilidad.\n// Devuelve un nuevo \u00e1rbol (clonado y potencialmente mutado).\nNodePtr mutate_tree(const NodePtr& tree, double mutation_rate, int max_depth);\n\n// Realiza el cruce (crossover) entre dos \u00e1rboles padres, modific\u00e1ndolos in-place.\nvoid crossover_trees(NodePtr& tree1, NodePtr& tree2);\n\n\n#endif // GENETICOPERATORS_H\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/main.cpp\n",
        "#include \"Globals.h\" // Necesario para las constantes globales\n#include \"GeneticAlgorithm.h\"\n#include \"Fitness.h\" // Para evaluate_fitness\n#include \"ExpressionTree.h\" // Para tree_to_string si se necesita aqu\u00ed\n#include <iostream>\n#include <vector>\n#include <memory> // Para shared_ptr\n#include <iomanip> // Para std::setprecision\n#include <omp.h>   // Para configuraci\u00f3n de OpenMP\n\n#include <fstream> // Para leer archivo\n#include <string>\n#include <sstream>\n\n// Definici\u00f3n de variable global externa\nint NUM_VARIABLES = 1;\n\nint main(int argc, char* argv[]) {\n    // === OPTIMIZACI\u00d3N: Configuraci\u00f3n expl\u00edcita de hilos OpenMP ===\n    int num_threads = omp_get_max_threads();\n    omp_set_num_threads(num_threads);\n    std::cout << \"[OpenMP] Using \" << num_threads << \" threads\" << std::endl;\n    \n    // Configurar precisi\u00f3n de salida para n\u00fameros flotantes\n    // Force immediate flush for each output (important for subprocess capture)\n    std::cout << std::unitbuf << std::fixed << std::setprecision(6);\n    \n    std::vector<std::string> seed_formulas;\n    std::string seed_file_path = \"\";\n    std::string data_file_path = \"\";\n    \n    // Parse arguments\n    for (int i = 1; i < argc; ++i) {\n        std::string arg = argv[i];\n        if ((arg == \"--seed\" || arg == \"-s\") && i + 1 < argc) {\n             seed_file_path = argv[i + 1];\n             i++; // Skip next arg\n        } else if ((arg == \"--data\" || arg == \"-d\") && i + 1 < argc) {\n             data_file_path = argv[i + 1];\n             i++;\n        }\n    }\n    \n    if (!seed_file_path.empty()) {\n        std::cout << \"Loading seeds from: \" << seed_file_path << std::endl;\n        std::ifstream file(seed_file_path);\n        if (file.is_open()) {\n            std::string line;\n            while (std::getline(file, line)) {\n                if (!line.empty()) {\n                    seed_formulas.push_back(line);\n                }\n            }\n            file.close();\n            std::cout << \"Loaded \" << seed_formulas.size() << \" formulas.\" << std::endl;\n        } else {\n            std::cerr << \"[Error] Could not open seed file: \" << seed_file_path << std::endl;\n        }\n    }\n\n    std::vector<double> targets;\n    // MODIFIED: final_x_values is now vector<vector<double>>\n    std::vector<std::vector<double>> final_x_values;\n\n    if (!data_file_path.empty()) {\n         std::cout << \"Loading data from: \" << data_file_path << std::endl;\n         std::ifstream dfile(data_file_path);\n         if (dfile.is_open()) {\n             // Format:\n             // Line 1: x1 x2 x3 ... (Assumed univariable if using this legacy format)\n             // Line 2: y1 y2 y3 ...\n             // Values separated by space or comma\n             \n             // Helper lambda to parse line\n             auto parse_line = [](const std::string& line) {\n                 std::vector<double> vals;\n                 std::stringstream ss(line);\n                 double val;\n                 while (ss >> val) {\n                     vals.push_back(val);\n                     if (ss.peek() == ',' || ss.peek() == ' ') ss.ignore();\n                 }\n                 return vals;\n             };\n             \n             std::string line;\n             std::vector<std::vector<double>> all_lines;\n             while (std::getline(dfile, line)) {\n                 if(!line.empty()) {\n                     all_lines.push_back(parse_line(line));\n                 }\n             }\n             dfile.close();\n             \n             if (all_lines.size() < 2) {\n                 std::cerr << \"[Error] Insufficient data in file (Need at least 1 feature line and 1 target line).\" << std::endl;\n                 return 1;\n             }\n             \n             targets = all_lines.back();\n             all_lines.pop_back(); // Now all_lines contains only features as rows\n             \n             size_t n_samples = targets.size();\n             size_t n_vars = all_lines.size();\n             NUM_VARIABLES = (int)n_vars;\n             \n             // Transpose: from [n_vars][n_samples] to [n_samples][n_vars]\n             final_x_values.clear();\n             final_x_values.reserve(n_samples);\n             \n             for (size_t s = 0; s < n_samples; ++s) {\n                 std::vector<double> sample_vars;\n                 sample_vars.reserve(n_vars);\n                 for (size_t v = 0; v < n_vars; ++v) {\n                     if (s < all_lines[v].size()) {\n                         sample_vars.push_back(all_lines[v][s]);\n                     } else {\n                         sample_vars.push_back(0.0); // Fallback for mismatched lines\n                     }\n                 }\n                 final_x_values.push_back(sample_vars);\n             }\n\n             std::cout << \"Loaded \" << final_x_values.size() << \" data points with \" << NUM_VARIABLES << \" variables from file.\" << std::endl;\n         } else {\n             std::cerr << \"[Error] Could not open data file: \" << data_file_path << std::endl;\n             return 1;\n         }\n    } else {\n        // Fallback to Globals.h\n        if (USE_LOG_TRANSFORMATION) {\n             std::cout << \"Info: Log Transformation is ON (Target = ln(Q(N))).\" << std::endl;\n             for (size_t i = 0; i < RAW_TARGETS.size(); ++i) {\n                 if (RAW_TARGETS[i] > 0) {\n                     targets.push_back(std::log(RAW_TARGETS[i]));\n                     final_x_values.push_back(X_VALUES[i]);\n                 }\n             }\n        } else {\n             std::cout << \"Info: Log Transformation is OFF.\" << std::endl;\n             targets = RAW_TARGETS;\n             final_x_values = X_VALUES;\n        }\n        \n        // Update NUM_VARIABLES based on data\n        if (!final_x_values.empty()) {\n            NUM_VARIABLES = final_x_values[0].size();\n        } else {\n            NUM_VARIABLES = 1; // Default\n        }\n    }\n\n    std::cout << \"Target Function Points (Effective):\" << std::endl;\n    std::cout << \"NUM_VARIABLES set to: \" << NUM_VARIABLES << std::endl;\n    // Imprimir los puntos objetivo\n    for (size_t i = 0; i < targets.size(); ++i) {\n        std::cout << \"  f(\";\n        for(size_t v=0; v<final_x_values[i].size(); ++v) {\n            std::cout << final_x_values[i][v];\n            if(v < final_x_values[i].size()-1) std::cout << \", \";\n        }\n        std::cout << \") = \" << targets[i] << std::endl;\n    }\n    std::cout << \"----------------------------------------\" << std::endl;\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n    std::cout << \"Info: Running with GPU acceleration.\" << std::endl;\n#else\n    std::cout << \"Info: Running with CPU acceleration.\" << std::endl;\n#endif\n    std::cout << \"----------------------------------------\" << std::endl;\n    std::cout << \"Parameters:\" << std::endl;\n    // Imprimir los par\u00e1metros globales definidos en Globals.h\n    std::cout << \"  Total Population: \" << TOTAL_POPULATION_SIZE << std::endl;\n    std::cout << \"  Generations: \" << GENERATIONS << std::endl;\n    std::cout << \"  Islands: \" << NUM_ISLANDS << std::endl;\n    std::cout << \"  Migration Interval: \" << MIGRATION_INTERVAL << std::endl;\n    std::cout << \"  Migration Size: \" << MIGRATION_SIZE << std::endl;\n    // --- NOMBRES CORREGIDOS ---\n    std::cout << \"  Mutation Rate (Initial): \" << BASE_MUTATION_RATE << std::endl; // <-- Nombre corregido\n    std::cout << \"  Elite Percentage (Initial): \" << BASE_ELITE_PERCENTAGE << std::endl; // <-- Nombre corregido\n    // --------------------------\n    std::cout << \"----------------------------------------\" << std::endl;\n\n\n    try {\n        // Crear la instancia del Algoritmo Gen\u00e9tico\n        // Pasa las referencias a los vectores de datos y los par\u00e1metros principales\n        GeneticAlgorithm ga(targets, final_x_values, TOTAL_POPULATION_SIZE, GENERATIONS, seed_formulas);\n\n        // Ejecutar el algoritmo\n        // La funci\u00f3n run() contiene el bucle principal de generaciones y devuelve el mejor \u00e1rbol encontrado\n        NodePtr best_solution_tree = ga.run();\n\n        // La funci\u00f3n run() ya imprime el resumen final y la verificaci\u00f3n.\n        // Comprobar si se encontr\u00f3 alguna soluci\u00f3n v\u00e1lida al final\n        if (!best_solution_tree) {\n            std::cerr << \"\\nFailed to find any valid solution.\" << std::endl;\n            return 1; // Salir con c\u00f3digo de error si no se encontr\u00f3 soluci\u00f3n\n        }\n    } catch (const std::exception& e) {\n        std::cerr << \"[CRITICAL ERROR] Exception caught in main: \" << e.what() << std::endl;\n        return 2;\n    } catch (...) {\n        std::cerr << \"[CRITICAL ERROR] Unknown exception caught in main.\" << std::endl;\n        return 3;\n    }\n\n    return 0; // Salir con \u00e9xito\n}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/TestOperators.cpp\n",
        "\n#include <iostream>\n#include <vector>\n#include <cmath>\n#include <iomanip>\n#include <string>\n#include <cassert>\n#include \"ExpressionTree.h\"\n#include \"AdvancedFeatures.h\"\n#include \"Fitness.h\"\n#include \"Globals.h\"\n\n// Define global variable for tests\nint NUM_VARIABLES = 1;\n\n// --- Helper Macros for Testing ---\n#define ASSERT_NEAR(val1, val2, tol) \\\n    if (std::fabs((val1) - (val2)) > (tol)) { \\\n        std::cerr << \"[FAIL] Line \" << __LINE__ << \": Expected \" << (val2) << \", got \" << (val1) << \" (diff: \" << std::fabs((val1)-(val2)) << \")\" << std::endl; \\\n        return false; \\\n    }\n\n#define ASSERT_TRUE(cond) \\\n    if (!(cond)) { \\\n        std::cerr << \"[FAIL] Line \" << __LINE__ << \": Condition failed: \" #cond << std::endl; \\\n        return false; \\\n    }\n\n#define ASSERT_INF(val) \\\n    if ((val) != INF && !std::isinf(val)) { \\\n        std::cerr << \"[FAIL] Line \" << __LINE__ << \": Expected INF, got \" << (val) << std::endl; \\\n        return false; \\\n    }\n\n// =============================\n// BINARY OPERATORS\n// =============================\nbool test_binary_operators() {\n    std::cout << \"Testing Binary Operators...\" << std::endl;\n    \n    NodePtr root; double val;\n\n    // Addition\n    root = parse_formula_string(\"2+3\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_NEAR(val, 5.0, 1e-9);\n\n    // Subtraction\n    root = parse_formula_string(\"10-4\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_NEAR(val, 6.0, 1e-9);\n\n    // Multiplication\n    root = parse_formula_string(\"3*4\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_NEAR(val, 12.0, 1e-9);\n\n    // Division\n    root = parse_formula_string(\"10/2\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_NEAR(val, 5.0, 1e-9);\n\n    // Power\n    root = parse_formula_string(\"2^3\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_NEAR(val, 8.0, 1e-9);\n\n    // Modulo\n    root = parse_formula_string(\"10%3\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_NEAR(val, 1.0, 1e-9);\n\n    // Negative numbers\n    root = parse_formula_string(\"-5+3\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_NEAR(val, -2.0, 1e-9);\n\n    // Operator precedence: 2+3*4 = 14\n    root = parse_formula_string(\"2+3*4\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_NEAR(val, 14.0, 1e-9);\n\n    // Parentheses override: (2+3)*4 = 20\n    root = parse_formula_string(\"(2+3)*4\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_NEAR(val, 20.0, 1e-9);\n\n    std::cout << \"  -> Binary Operators Passed\" << std::endl;\n    return true;\n}\n\n// =============================\n// UNARY OPERATORS\n// =============================\nbool test_unary_operators() {\n    std::cout << \"Testing Unary Operators...\" << std::endl;\n    NodePtr root; double val;\n\n    // sin\n    root = parse_formula_string(\"sin(0)\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_NEAR(val, 0.0, 1e-9);\n    \n    root = parse_formula_string(\"sin(1.5708)\"); // pi/2\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_NEAR(val, 1.0, 1e-4);\n\n    // cos\n    root = parse_formula_string(\"cos(0)\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_NEAR(val, 1.0, 1e-9);\n\n    // log (protected: log(|x|))\n    root = parse_formula_string(\"log(2.7182818)\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_NEAR(val, 1.0, 1e-5);\n\n    // exp\n    root = parse_formula_string(\"exp(1)\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_NEAR(val, 2.7182818, 1e-5);\n\n    // floor\n    root = parse_formula_string(\"floor(2.9)\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_NEAR(val, 2.0, 1e-9);\n    \n    root = parse_formula_string(\"floor(-2.1)\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_NEAR(val, -3.0, 1e-9);\n\n    // lgamma: Implementation is lgamma(|x|+1) => ln(|x|!)\n    // lgamma(3) -> lgamma(4) = ln(3!) = ln(6) = 1.791759\n    root = parse_formula_string(\"lgamma(3)\"); \n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_NEAR(val, 1.791759, 1e-4);\n\n    // g(x) alias\n    root = parse_formula_string(\"g(3)\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_NEAR(val, 1.791759, 1e-4);\n\n    // Factorial (!): Implementation is tgamma(|x|+1) = |x|!\n    // gamma(4) = 3! = 6\n    root = parse_formula_string(\"gamma(3)\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_NEAR(val, 6.0, 1e-4);\n\n    std::cout << \"  -> Unary Operators Passed\" << std::endl;\n    return true;\n}\n\n// =============================\n// EDGE CASES (Protection)\n// =============================\nbool test_edge_cases() {\n    std::cout << \"Testing Edge Cases...\" << std::endl;\n    NodePtr root; double val;\n\n    // Division by zero -> INF\n    root = parse_formula_string(\"1/0\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_INF(val);\n\n    // Modulo by zero -> INF\n    root = parse_formula_string(\"5%0\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_INF(val);\n\n    // log(0) -> INF (protected)\n    root = parse_formula_string(\"log(0)\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_INF(val);\n\n    // exp(800) -> INF (overflow)\n    root = parse_formula_string(\"exp(800)\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_INF(val);\n\n    // 0^(-1) -> INF\n    root = parse_formula_string(\"0^(-1)\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_INF(val);\n\n    // Factorial of large number -> INF\n    root = parse_formula_string(\"gamma(200)\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_INF(val);\n\n    // Negative base with non-integer exp -> INF (complex result)\n    root = parse_formula_string(\"(-2)^0.5\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_INF(val);\n\n    std::cout << \"  -> Edge Cases Passed\" << std::endl;\n    return true;\n}\n\n// =============================\n// SIMPLIFICATION RULES\n// =============================\nbool test_simplification() {\n    std::cout << \"Testing Simplification...\" << std::endl;\n    NodePtr root, simplified; double val; std::string str;\n\n    // x - x -> 0\n    root = parse_formula_string(\"x-x\");\n    simplified = DomainConstraints::fix_or_simplify(root);\n    val = evaluate_tree(simplified, std::vector<double>{5.0});\n    ASSERT_NEAR(val, 0.0, 1e-9);\n\n    // x / x -> 1\n    root = parse_formula_string(\"x/x\");\n    simplified = DomainConstraints::fix_or_simplify(root);\n    val = evaluate_tree(simplified, std::vector<double>{5.0});\n    ASSERT_NEAR(val, 1.0, 1e-9);\n\n    // Constant Folding: 2+3 -> 5\n    root = parse_formula_string(\"2+3\");\n    simplified = DomainConstraints::fix_or_simplify(root);\n    val = evaluate_tree(simplified, 0.0);\n    ASSERT_NEAR(val, 5.0, 1e-9);\n    ASSERT_TRUE(tree_size(simplified) == 1);\n\n    // x + 0 -> x\n    root = parse_formula_string(\"x+0\");\n    simplified = DomainConstraints::fix_or_simplify(root);\n    str = tree_to_string(simplified);\n    ASSERT_TRUE(str == \"x0\");\n\n    // x * 1 -> x\n    root = parse_formula_string(\"x*1\");\n    simplified = DomainConstraints::fix_or_simplify(root);\n    str = tree_to_string(simplified);\n    ASSERT_TRUE(str == \"x0\");\n\n    // x * 0 -> 0\n    root = parse_formula_string(\"x*0\");\n    simplified = DomainConstraints::fix_or_simplify(root);\n    val = evaluate_tree(simplified, std::vector<double>{100.0});\n    ASSERT_NEAR(val, 0.0, 1e-9);\n\n    // x^1 -> x\n    root = parse_formula_string(\"x^1\");\n    simplified = DomainConstraints::fix_or_simplify(root);\n    str = tree_to_string(simplified);\n    ASSERT_TRUE(str == \"x0\");\n\n    // x^0 -> 1\n    root = parse_formula_string(\"x^0\");\n    simplified = DomainConstraints::fix_or_simplify(root);\n    val = evaluate_tree(simplified, std::vector<double>{100.0});\n    ASSERT_NEAR(val, 1.0, 1e-9);\n\n    // Unary Operator Preservation: lgamma(x) should NOT simplify to x\n    root = parse_formula_string(\"lgamma(x)\");\n    simplified = DomainConstraints::fix_or_simplify(root);\n    str = tree_to_string(simplified);\n    ASSERT_TRUE(str.find(\"lgamma\") != std::string::npos || str.find(\"g(\") != std::string::npos);\n\n    // Unary Constant Folding: lgamma(3) -> constant\n    root = parse_formula_string(\"lgamma(3)\");\n    simplified = DomainConstraints::fix_or_simplify(root);\n    val = evaluate_tree(simplified, std::vector<double>{0.0});\n    ASSERT_NEAR(val, 1.791759, 1e-4);\n    ASSERT_TRUE(tree_size(simplified) == 1);\n\n    // sin(0) -> 0 (constant folding)\n    root = parse_formula_string(\"sin(0)\");\n    simplified = DomainConstraints::fix_or_simplify(root);\n    val = evaluate_tree(simplified, std::vector<double>{0.0});\n    ASSERT_NEAR(val, 0.0, 1e-9);\n    ASSERT_TRUE(tree_size(simplified) == 1);\n\n    std::cout << \"  -> Simplification Passed\" << std::endl;\n    return true;\n}\n\n// =============================\n// COMPLEX PARSING\n// =============================\nbool test_complex_parsing() {\n    std::cout << \"Testing Complex Parsing...\" << std::endl;\n    NodePtr root; double val;\n\n    // Nested functions: sin(cos(0)) = sin(1) \u2248 0.8415\n    root = parse_formula_string(\"sin(cos(0))\");\n    val = evaluate_tree(root, std::vector<double>{0.0});\n    ASSERT_NEAR(val, std::sin(1.0), 1e-4);\n\n    // Mixed: lgamma(x+1) at x=3 -> lgamma(4) = lgamma(5) = ln(4!) = ln(24) \u2248 3.178\n    root = parse_formula_string(\"lgamma(x+1)\");\n    val = evaluate_tree(root, std::vector<double>{3.0});\n    ASSERT_NEAR(val, std::lgamma(5.0), 1e-4);\n\n    // Formula from project: (g(x)-((x*909613)/1000000))+0.24423 at x=4\n    root = parse_formula_string(\"(g(x)-((x*909613)/1000000))+0.24423\");\n    val = evaluate_tree(root, std::vector<double>{4.0});\n    double expected = std::lgamma(5.0) - (4.0 * 909613.0 / 1000000.0) + 0.24423;\n    ASSERT_NEAR(val, expected, 1e-4);\n\n    // Implicit multiplication: 2x at x=3 -> 6\n    root = parse_formula_string(\"2x\");\n    val = evaluate_tree(root, std::vector<double>{3.0});\n    ASSERT_NEAR(val, 6.0, 1e-9);\n\n    // Deep nesting: exp(log(x)) at x=5 -> 5\n    root = parse_formula_string(\"exp(log(x))\");\n    val = evaluate_tree(root, std::vector<double>{5.0});\n    ASSERT_NEAR(val, 5.0, 1e-4);\n\n    // Chained operations: x^2+2*x+1 at x=3 -> 16\n    root = parse_formula_string(\"x^2+2*x+1\");\n    val = evaluate_tree(root, std::vector<double>{3.0});\n    ASSERT_NEAR(val, 16.0, 1e-9);\n\n    std::cout << \"  -> Complex Parsing Passed\" << std::endl;\n    return true;\n}\n\n// =============================\n// FITNESS CALCULATION\n// =============================\nbool test_fitness_calc() {\n    std::cout << \"Testing Fitness Calculation...\" << std::endl;\n\n    std::vector<double> targets = {1.0, 2.0, 3.0};\n    std::vector<std::vector<double>> x_values = {{1.0}, {2.0}, {3.0}};\n\n    // Perfect solution: x\n    NodePtr solution = parse_formula_string(\"x0\");\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n    double fitness = evaluate_fitness(solution, targets, x_values, (double*)nullptr, (double*)nullptr);\n#else\n    double fitness = evaluate_fitness(solution, targets, x_values);\n#endif\n    ASSERT_NEAR(fitness, 0.0, 1e-9);\n\n    // Imperfect solution: x+1 (errors: 1, 1, 1)\n    NodePtr imperfect = parse_formula_string(\"x+1\");\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n    fitness = evaluate_fitness(imperfect, targets, x_values, (double*)nullptr, (double*)nullptr);\n#else\n    fitness = evaluate_fitness(imperfect, targets, x_values);\n#endif\n    ASSERT_TRUE(fitness > 0.001);\n\n    // Very bad solution: constant 100\n    NodePtr bad = parse_formula_string(\"100\");\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n    double bad_fitness = evaluate_fitness(bad, targets, x_values, (double*)nullptr, (double*)nullptr);\n#else\n    double bad_fitness = evaluate_fitness(bad, targets, x_values);\n#endif\n    ASSERT_TRUE(bad_fitness > fitness); // Worse than x+1\n\n    std::cout << \"  -> Fitness Calculation Passed\" << std::endl;\n    return true;\n}\n\n// =============================\n// TREE UTILITIES\n// =============================\nbool test_tree_utilities() {\n    std::cout << \"Testing Tree Utilities...\" << std::endl;\n    \n    // tree_size\n    NodePtr root = parse_formula_string(\"x+1\");\n    ASSERT_TRUE(tree_size(root) == 3); // +, x, 1\n\n    root = parse_formula_string(\"lgamma(x)\");\n    ASSERT_TRUE(tree_size(root) == 2); // lgamma, x\n\n    // tree_to_string roundtrip\n    root = parse_formula_string(\"(x+1)*2\");\n    std::string str = tree_to_string(root);\n    ASSERT_TRUE(str.find(\"x\") != std::string::npos);\n    ASSERT_TRUE(str.find(\"1\") != std::string::npos);\n    ASSERT_TRUE(str.find(\"2\") != std::string::npos);\n\n    // clone_tree\n    NodePtr cloned = clone_tree(root);\n    ASSERT_TRUE(tree_to_string(cloned) == tree_to_string(root));\n    ASSERT_TRUE(cloned.get() != root.get()); // Different pointers\n\n    std::cout << \"  -> Tree Utilities Passed\" << std::endl;\n    return true;\n}\n\n// =============================\n// MAIN\n// =============================\nint main() {\n    std::cout << \"=======================================\" << std::endl;\n    std::cout << \" Running Comprehensive Operator Tests \" << std::endl;\n    std::cout << \"=======================================\" << std::endl;\n\n    bool all_passed = true;\n    all_passed &= test_binary_operators();\n    all_passed &= test_unary_operators();\n    all_passed &= test_edge_cases();\n    all_passed &= test_simplification();\n    all_passed &= test_complex_parsing();\n    all_passed &= test_fitness_calc();\n    all_passed &= test_tree_utilities();\n\n    std::cout << \"=======================================\" << std::endl;\n    if (all_passed) {\n        std::cout << \"ALL TESTS PASSED (\" << 7 << \" test suites)\" << std::endl;\n        return 0;\n    } else {\n        std::cerr << \"SOME TESTS FAILED\" << std::endl;\n        return 1;\n    }\n}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/Globals.h\n",
        "#ifndef GLOBALS_H\n#define GLOBALS_H\n\n#include <vector>\n#include <random>\n#include <string>\n#include <limits>\n#include <cmath>\n\n// ============================================================\n//                  PAR\u00c1METROS GLOBALES\n// ============================================================\n\n// ----------------------------------------\n// Datos del Problema (Regresi\u00f3n Simb\u00f3lica)\n// ----------------------------------------\n// MODIFICADO: Usamos log(TARGETS) para aplanar el crecimiento exponencial.\n// X representa N. TARGETS_LOG es ln(Q(N)).\n// Se han filtrado valores N<4 donde Q(N) es 0 o peque\u00f1o irrelevante.\n\n// MODIFICADO: RAW_TARGETS contiene los datos crudos. TARGETS se generar\u00e1 en runtime.\nconst std::vector<double> RAW_TARGETS = {2, 10, 4, 40, 92, 352, 724, 2680, 14200, 73712, 365596, 2279184, 14772512, 95815104, 666090624, 4968057848, 39029188884};\n// MODIFICADO: X_VALUES ahora es vector<vector<double>> para soporte multivariable.\n// Inicializador por defecto para problema univariable.\nconst std::vector<std::vector<double>> X_VALUES = {\n    {1, 1, 1},   // 1\n    {2, 2, 0},   // 2\n    {3, 3, 1},   // 3\n    {4, 4, 0},   // 4\n    {5, 5, 1},   // 5\n    {6, 0, 0},   // 6\n    {7, 1, 1},   // 7\n    {8, 2, 0},   // 8\n    {9, 3, 1},   // 9\n    {10, 4, 0},  // 10\n    {11, 5, 1},  // 11\n    {12, 0, 0},  // 12\n    {13, 1, 1},  // 13\n    {14, 2, 0},  // 14\n    {15, 3, 1},  // 15\n    {16, 4, 0},  // 16\n    {17, 5, 1},  // 17\n    {18, 0, 0},  // 18\n    {19, 1, 1},  // 19\n    {20, 2, 0},  // 20\n    {21, 3, 1},  // 21\n    {22, 4, 0},  // 22\n    {23, 5, 1},  // 23\n    {24, 0, 0},  // 24\n    {25, 1, 1},  // 25\n    {26, 2, 0}   // 26\n};extern int NUM_VARIABLES; // Definido en Globals.cpp o main.cpp\n\n// Flag para activar la transformaci\u00f3n logar\u00edtmica autom\u00e1tica\nconst bool USE_LOG_TRANSFORMATION = false;\n\n// ----------------------------------------\n// Configuraci\u00f3n General del Algoritmo Gen\u00e9tico\n// ----------------------------------------\n// Controla si se utiliza la aceleraci\u00f3n por GPU.\n// FORCE_CPU_MODE: Si es true, usa CPU aunque CUDA est\u00e9 disponible (\u00fatil para comparar rendimiento)\nconst bool FORCE_CPU_MODE = true;  // Cambiar a 'true' para forzar modo CPU\n\n// USE_GPU_ACCELERATION se define autom\u00e1ticamente por CMake si CUDA est\u00e1 disponible\n// Pero si FORCE_CPU_MODE es true, se ignora y usa CPU\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\nconst bool USE_GPU_ACCELERATION = !FORCE_CPU_MODE;\n#else\nconst bool USE_GPU_ACCELERATION = false;\n#endif\n// Aumentamos el tama\u00f1o de la poblaci\u00f3n y el n\u00famero de generaciones para maximizar la utilizaci\u00f3n de la GPU,\n// ya que la GPU puede procesar un gran n\u00famero de individuos en paralelo.\n// Ajustamos el tama\u00f1o de la poblaci\u00f3n para una GPU con 4GB de VRAM (RTX 3050),\n// buscando un equilibrio entre el aprovechamiento de la GPU y el uso de memoria.\n// Para hacer un uso a\u00fan m\u00e1s intensivo de la GPU y acelerar el algoritmo,\n// OPTIMIZADO para Hybrid Search: Poblaci\u00f3n m\u00e1s peque\u00f1a = convergencia m\u00e1s r\u00e1pida en timeouts cortos\nconst int TOTAL_POPULATION_SIZE = 5000; // Reducido de 50000 para convergencia r\u00e1pida\nconst int GENERATIONS = 50000;           // Reducido (timeout domina de todas formas)\nconst int NUM_ISLANDS = 5;               // Menos islas = m\u00e1s foco por isla\nconst int MIN_POP_PER_ISLAND = 10;        \n\n// --- F\u00f3rmula Inicial ---\nconst bool USE_INITIAL_FORMULA = false; // Poner en 'true' para inyectar la f\u00f3rmula\nconst std::string INITIAL_FORMULA_STRING = \"log((x1+exp((((((1.28237193+((x0+2.59195138)+8.54688985))*x0)+(log((((x2/-0.99681346)-(x0-8.00219939))/(0.35461932-x2)))+(x0+(88.95319019/((x0+x0)+x0)))))-x1)/((exp(exp(((exp(x2)*(1.39925709/x0))^exp(x0))))+0.76703064)*6.05423753)))))\";\n\n// ----------------------------------------\n// Par\u00e1metros del Modelo de Islas\n// ----------------------------------------\n// Aumentamos el intervalo y tama\u00f1o de migraci\u00f3n para permitir que las islas realicen m\u00e1s trabajo en paralelo\n// antes de intercambiar individuos, reduciendo la sobrecarga de comunicaci\u00f3n y maximizando el procesamiento GPU.\nconst int MIGRATION_INTERVAL = 100; // Incrementado para permitir m\u00e1s trabajo por isla entre migraciones\nconst int MIGRATION_SIZE = 50;      // Incrementado para una migraci\u00f3n m\u00e1s sustancial\n\n// ----------------------------------------\n// Par\u00e1metros de Generaci\u00f3n Inicial de \u00c1rboles\n// ----------------------------------------\nconst int MAX_TREE_DEPTH_INITIAL = 8; // Reducido para f\u00f3rmulas iniciales m\u00e1s simples y r\u00e1pidas\nconst double TERMINAL_VS_VARIABLE_PROB = 0.75;\nconst double CONSTANT_MIN_VALUE = -10.0;\nconst double CONSTANT_MAX_VALUE = 10.0;\nconst int CONSTANT_INT_MIN_VALUE = -10;\nconst int CONSTANT_INT_MAX_VALUE = 10;\nconst bool USE_HARD_DEPTH_LIMIT = true; // Toggle for hard depth limit\nconst int MAX_TREE_DEPTH_HARD_LIMIT = 12; // Hard limit to prevent bloat\n// Order: +, -, *, /, ^, %, s, c, l, e, !, _, g\n// ----------------------------------------\n// Par\u00e1metros de Operadores Gen\u00e9ticos (Configuraci\u00f3n de Operadores)\n// ----------------------------------------\nconst bool USE_OP_PLUS     = true; // +\nconst bool USE_OP_MINUS    = true; // -\nconst bool USE_OP_MULT     = true; // *\nconst bool USE_OP_DIV      = true; // /\nconst bool USE_OP_POW      = true; // ^\nconst bool USE_OP_MOD      = false; // % (DISABLED)\nconst bool USE_OP_SIN      = false; // s (DISABLED)\nconst bool USE_OP_COS      = false; // c (DISABLED)\nconst bool USE_OP_LOG      = true; // l\nconst bool USE_OP_EXP      = true; // e\nconst bool USE_OP_FACT     = false; // ! (DISABLED - using lgamma instead)\nconst bool USE_OP_FLOOR    = false; // _ (DISABLED)\nconst bool USE_OP_GAMMA    = true; // g\nconst bool USE_OP_ASIN     = false; // S (DISABLED)\nconst bool USE_OP_ACOS     = false; // C (DISABLED)\nconst bool USE_OP_ATAN     = false; // T (DISABLED)\n\n// Order: +, -, *, /, ^, %, s, c, l, e, !, _, g\n// Los pesos se multiplican por el flag (0 o 1) para habilitar/deshabilitar.\nconst std::vector<double> OPERATOR_WEIGHTS = {\n    0.20 * (USE_OP_PLUS  ? 1.0 : 0.0), // +\n    0.20 * (USE_OP_MINUS ? 1.0 : 0.0), // -\n    0.20 * (USE_OP_MULT  ? 1.0 : 0.0), // *\n    0.15 * (USE_OP_DIV   ? 1.0 : 0.0), // /\n    0.10 * (USE_OP_POW   ? 1.0 : 0.0), // ^\n    0.02 * (USE_OP_MOD   ? 1.0 : 0.0), // %\n    0.10 * (USE_OP_SIN   ? 1.0 : 0.0), // s\n    0.10 * (USE_OP_COS   ? 1.0 : 0.0), // c\n    0.05 * (USE_OP_LOG   ? 1.0 : 0.0), // l\n    0.05 * (USE_OP_EXP   ? 1.0 : 0.0), // e\n    0.01 * (USE_OP_FACT  ? 1.0 : 0.0), // !\n    0.01 * (USE_OP_FLOOR ? 1.0 : 0.0), // _\n    0.01 * (USE_OP_GAMMA ? 1.0 : 0.0), // g\n    0.01 * (USE_OP_ASIN  ? 1.0 : 0.0), // S\n    0.01 * (USE_OP_ACOS  ? 1.0 : 0.0), // C\n    0.01 * (USE_OP_ATAN  ? 1.0 : 0.0)  // T\n};\n\n// ----------------------------------------\n// Par\u00e1metros de Operadores Gen\u00e9ticos (Mutaci\u00f3n, Cruce, Selecci\u00f3n)\n// ----------------------------------------\nconst double BASE_MUTATION_RATE = 0.30;\nconst double BASE_ELITE_PERCENTAGE = 0.15;\nconst double DEFAULT_CROSSOVER_RATE = 0.85;\nconst int DEFAULT_TOURNAMENT_SIZE = 30;\nconst int MAX_TREE_DEPTH_MUTATION = 8; // Slight increase to allow complexity\nconst double MUTATE_INSERT_CONST_PROB = 0.6;\nconst int MUTATE_INSERT_CONST_INT_MIN = 1;\nconst int MUTATE_INSERT_CONST_INT_MAX = 5;\nconst double MUTATE_INSERT_CONST_FLOAT_MIN = 0.5;\nconst double MUTATE_INSERT_CONST_FLOAT_MAX = 5.0;\n\n// ----------------------------------------\n// Par\u00e1metros de Fitness y Evaluaci\u00f3n\n// ----------------------------------------\n// Reducimos ligeramente la penalizaci\u00f3n por complejidad para permitir que f\u00f3rmulas m\u00e1s complejas\n// (y computacionalmente m\u00e1s intensivas para la GPU) sean favorecidas por el algoritmo.\n// MODIFICADO: Ajustado para ser menos agresivo y permitir multivariable.\nconst double COMPLEXITY_PENALTY_FACTOR = 0.01; // Was 0.05. Reduced to 0.01.\nconst bool USE_RMSE_FITNESS = true;\nconst double FITNESS_ORIGINAL_POWER = 1.3;\nconst double FITNESS_PRECISION_THRESHOLD = 0.001;\nconst double FITNESS_PRECISION_BONUS = 0.0001;\nconst double FITNESS_EQUALITY_TOLERANCE = 1e-9;\nconst double EXACT_SOLUTION_THRESHOLD = 1e-8;\n\n// ----------------------------------------\n// Fitness Ponderado (Weighted Fitness)\n// ----------------------------------------\n// Activa el fitness ponderado para penalizar fuertemente errores en valores altos de N.\n// Esto destruye a las par\u00e1bolas que fallan en N=20 pero dan buen promedio general.\nconst bool USE_WEIGHTED_FITNESS = false;\n// Tipo de peso: \"quadratic\" usa i*i, \"exponential\" usa exp(i*WEIGHTED_FITNESS_EXPONENT)\n// Exponente para peso exponencial (m\u00e1s agresivo). Usar 0.2-0.3 para datasets peque\u00f1os.\nconst double WEIGHTED_FITNESS_EXPONENT = 0.25;\n\n// ----------------------------------------\n// Par\u00e1metros de Caracter\u00edsticas Avanzadas\n// ----------------------------------------\nconst int STAGNATION_LIMIT_ISLAND = 50;\n// Lowered from 5000 to allow faster early termination in Hybrid Search mode.\n// If best fitness doesn't improve for N generations, terminate early.\nconst int GLOBAL_STAGNATION_LIMIT = 100; // Reducido para terminar m\u00e1s r\u00e1pido si no mejora\nconst double STAGNATION_RANDOM_INJECT_PERCENT = 0.1;\nconst int PARAM_MUTATE_INTERVAL = 50;\nconst double PATTERN_RECORD_FITNESS_THRESHOLD = 10.0;\nconst int PATTERN_MEM_MIN_USES = 3;\nconst int PATTERN_INJECT_INTERVAL = 10;\nconst double PATTERN_INJECT_PERCENT = 0.05;\nconst size_t PARETO_MAX_FRONT_SIZE = 50;\nconst double SIMPLIFY_NEAR_ZERO_TOLERANCE = 1e-9;\nconst double SIMPLIFY_NEAR_ONE_TOLERANCE = 1e-9;\nconst int LOCAL_SEARCH_ATTEMPTS = 30;\n// Simplification Toggle\nconst bool USE_SIMPLIFICATION = true;\n// Anti-Stagnation: Island Cataclysm (Hard Reset)\nconst bool USE_ISLAND_CATACLYSM = true;\n// Selection Strategy: Epsilon-Lexicase Selection (Replaces Tournament)\nconst bool USE_LEXICASE_SELECTION = true;\n\n// ----------------------------------------\n// Otros Par\u00e1metros\n// ----------------------------------------\nconst int PROGRESS_REPORT_INTERVAL = 100;\n// Optimizaciones adicionales:\n// Deshabilitamos las constantes enteras forzadas para permitir una mayor flexibilidad\n// en las constantes generadas y mutadas, lo que podr\u00eda conducir a mejores soluciones\n// y mantener la GPU ocupada con un rango m\u00e1s amplio de valores.\nconst bool FORCE_INTEGER_CONSTANTS = false; // Mantenemos false para mayor flexibilidad\n\n// ----------------------------------------\n// Control de Duplicados\n// ----------------------------------------\nconst bool PREVENT_DUPLICATES = true; // Activa la verificaci\u00f3n de unicidad\nconst int DUPLICATE_RETRIES = 10;     // Intentos para generar un individuo \u00fanico antes de rendirse\n\n\n// ============================================================\n//                  UTILIDADES GLOBALES\n// ============================================================\nstd::mt19937& get_rng();\nconst double INF = std::numeric_limits<double>::infinity();\n\n#endif // GLOBALS_H\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile Code/CMakeLists.txt\n",
        "cmake_minimum_required(VERSION 3.18)\n\n# Initially only enable CXX. We will enable CUDA if found.\nproject(SymbolicRegressionGP LANGUAGES CXX)\n\n# Fix for \"PTX was compiled with an unsupported toolchain\" on Colab T4\n# This forces generation of SASS (binary) for the current GPU, avoiding JIT issues.\nset(CMAKE_CUDA_ARCHITECTURES native)\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED True)\n\n# Find OpenMP (Required for CPU parallelism)\nfind_package(OpenMP REQUIRED COMPONENTS CXX)\n\n# --- CUDA Detection ---\ninclude(CheckLanguage)\ncheck_language(CUDA)\n\nset(SOURCES\n    src/main.cpp\n    src/ExpressionTree.cpp\n    src/Fitness.cpp\n    src/GeneticOperators.cpp\n    src/AdvancedFeatures.cpp\n    src/GeneticAlgorithm.cpp\n    src/GradientOptimizer.cpp\n)\n\nif(CMAKE_CUDA_COMPILER)\n    message(STATUS \"CUDA compiler found: ${CMAKE_CUDA_COMPILER}\")\n    enable_language(CUDA)\n    \n    # Suppress warning about FindCUDA being deprecated (CMP0146)\n    cmake_policy(SET CMP0146 OLD)\n    find_package(CUDA REQUIRED)\n\n    # Add .cu file to sources only if CUDA is present\n    list(APPEND SOURCES src/FitnessGPU.cu src/GradientOptimizerGPU.cu)\n\n    # Create Executable\n    add_executable(SymbolicRegressionGP ${SOURCES})\n\n    # Define the macro to enable GPU code paths in C++\n    target_compile_definitions(SymbolicRegressionGP PUBLIC \"USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\")\n\n    # Include CUDA dirs\n    target_include_directories(SymbolicRegressionGP PUBLIC ${CUDA_INCLUDE_DIRS})\n\n    # Link CUDA libraries\n    # Try modern target first, fallback to variables\n    if(TARGET CUDA::cudart)\n        target_link_libraries(SymbolicRegressionGP PUBLIC CUDA::cudart)\n    else()\n        target_link_libraries(SymbolicRegressionGP PUBLIC ${CUDA_LIBRARIES})\n    endif()\n\n    set(CMAKE_CUDA_PROPAGATE_HOST_FLAGS ON)\n    \n    message(STATUS \"Build type: GPU Accelerated\")\nelse()\n    message(STATUS \"CUDA compiler NOT found. Building for CPU only.\")\n    \n    # Create Executable (without .cu file)\n    add_executable(SymbolicRegressionGP ${SOURCES})\n    \n    message(STATUS \"Build type: CPU Only\")\nendif()\n\n# Common settings\ntarget_include_directories(SymbolicRegressionGP PUBLIC src)\n\n# Enlazar las librer\u00edas necesarias.\n# Se usa la variable legacy ${CUDA_LIBRARIES} como m\u00e9todo de compatibilidad\n# ya que el target moderno CUDA::cudart no se est\u00e1 encontrando en este sistema.\ntarget_link_libraries(SymbolicRegressionGP PUBLIC\n    # Enlazar con OpenMP para C++\n    OpenMP::OpenMP_CXX\n\n    # Enlazar con las librer\u00edas de CUDA (m\u00e9todo de compatibilidad)\n    ${CUDA_LIBRARIES}\n)\n\n# Asegura que los flags del compilador de C++ (como /std:c++17) se pasen a nvcc.\nset(CMAKE_CUDA_PROPAGATE_HOST_FLAGS ON)\n\n\n# === OPTIMIZACI\u00d3N: Flags de compilaci\u00f3n agresivos para GCC/Clang (Colab) ===\n# NOTA: Para MSVC+CUDA, dejamos que CMake maneje la optimizaci\u00f3n autom\u00e1ticamente\n#       ya que los flags manuales causan conflictos con nvcc\nif(CMAKE_CXX_COMPILER_ID MATCHES \"GNU|Clang\")\n    target_compile_options(SymbolicRegressionGP PRIVATE\n        $<$<COMPILE_LANGUAGE:CXX>:-O3>              # M\u00e1xima optimizaci\u00f3n\n        $<$<COMPILE_LANGUAGE:CXX>:-march=native>    # Optimizar para CPU actual\n        $<$<COMPILE_LANGUAGE:CXX>:-ffast-math>      # Matem\u00e1ticas r\u00e1pidas\n        $<$<COMPILE_LANGUAGE:CXX>:-funroll-loops>   # Desenrollar loops\n        $<$<COMPILE_LANGUAGE:CXX>:-ftree-vectorize> # Forzar vectorizaci\u00f3n\n    )\n    # Link-time optimization (LTO) para Release\n    if(CMAKE_BUILD_TYPE STREQUAL \"Release\")\n        set_property(TARGET SymbolicRegressionGP PROPERTY INTERPROCEDURAL_OPTIMIZATION TRUE)\n    endif()\nendif()\n# --- Test Suite Executable (Only build if TestOperators.cpp exists) ---\nif(EXISTS \"${CMAKE_CURRENT_SOURCE_DIR}/src/TestOperators.cpp\")\n    add_executable(TestOperators \n        src/TestOperators.cpp\n        src/ExpressionTree.cpp\n        src/Fitness.cpp\n        src/GeneticOperators.cpp\n        src/AdvancedFeatures.cpp\n        src/GeneticAlgorithm.cpp\n        src/GradientOptimizer.cpp\n    )\n\n    if(CMAKE_CUDA_COMPILER)\n        target_compile_definitions(TestOperators PUBLIC \"USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\")\n        target_sources(TestOperators PRIVATE src/FitnessGPU.cu src/GradientOptimizerGPU.cu)\n        target_include_directories(TestOperators PUBLIC ${CUDA_INCLUDE_DIRS})\n        if(TARGET CUDA::cudart)\n            target_link_libraries(TestOperators PUBLIC CUDA::cudart)\n        else()\n            target_link_libraries(TestOperators PUBLIC ${CUDA_LIBRARIES})\n        endif()\n    endif()\n\n    target_include_directories(TestOperators PUBLIC src)\n    target_link_libraries(TestOperators PUBLIC OpenMP::OpenMP_CXX)\n    message(STATUS \"TestOperators target: ENABLED\")\nelse()\n    message(STATUS \"TestOperators target: SKIPPED (src/TestOperators.cpp not found)\")\nendif()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile C++ Engine\n",
        "%cd Code\n",
        "!cmake -B build -S . -DCMAKE_BUILD_TYPE=Release\n",
        "!cmake --build build -j $(nproc)\n",
        "import os\n",
        "if not os.path.exists('build/SymbolicRegressionGP') and not os.path.exists('build/Release/SymbolicRegressionGP'):\n",
        "    print('BUILD FAILURE? Binary not found in expected locations. Listing build dir:')\n",
        "    !ls -R build\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/grammar.py\n",
        "import numpy as np\nfrom scipy.special import gamma as scipy_gamma, gammaln\nimport math\n\n# Supported operators and their arity (number of arguments)\n# Organized by curriculum stage for progressive unlocking\nOPERATORS = {\n    # === STAGE 0: Pure Arithmetic ===\n    '+': 2,\n    '-': 2,\n    '*': 2,\n    '/': 2,\n    \n    # === STAGE 1: Powers ===\n    'pow': 2,\n    'sqrt': 1,\n    \n    # === STAGE 2: Trigonometry ===\n    'sin': 1,\n    'cos': 1,\n    'tan': 1,\n    'asin': 1,\n    'acos': 1,\n    'atan': 1,\n    \n    # === STAGE 3: Transcendental ===\n    'exp': 1,\n    'log': 1,\n    \n    # === STAGE 4: Advanced ===\n    'abs': 1,\n    'neg': 1,\n    'sign': 1,\n    'floor': 1,\n    'ceil': 1,\n    'mod': 2,\n    '%': 2,     # Alias for mod\n    'gamma': 1,\n    'lgamma': 1,\n    \n    # === C++ / GPU Specific Aliases ===\n    'e': 1,     # Alias for exp\n    '!': 1,     # Alias for gamma/factorial\n    'g': 1,     # Alias for lgamma\n    '_': 1,     # Alias for floor\n    'S': 1,     # Alias for asin\n    'C': 1,     # Alias for acos\n    'T': 1,     # Alias for atan\n}\n\n# Operator groups for curriculum control\nOPERATOR_STAGES = {\n    0: ['+', '-', '*', '/'],\n    1: ['+', '-', '*', '/', 'pow', 'sqrt'],\n    2: ['+', '-', '*', '/', 'pow', 'sqrt', 'sin', 'cos', 'tan', 'asin', 'acos', 'atan'],\n    3: ['+', '-', '*', '/', 'pow', 'sqrt', 'sin', 'cos', 'tan', 'asin', 'acos', 'atan', 'exp', 'log'],\n    4: list(OPERATORS.keys()),  # All operators\n}\n\n# Terminal tokens\n# Terminal tokens\nVARIABLES = ['x' + str(i) for i in range(10)] # x0, x1, ..., x9\n# 'C' is a placeholder for learnable constants\nCONSTANTS = ['C', '0', '1', '2', '3', '5', '10', 'pi', 'e']\n\n# Full Vocabulary\nVOCABULARY = list(OPERATORS.keys()) + VARIABLES + CONSTANTS\nTOKEN_TO_ID = {token: i for i, token in enumerate(VOCABULARY)}\nID_TO_TOKEN = {i: token for token, i in TOKEN_TO_ID.items()}\n\n# Special token for start of sequence\nSOS_TOKEN = '<SOS>'\nEOS_TOKEN = '<EOS>'\nPAD_TOKEN = '<PAD>'\n\nclass Node:\n    def __init__(self, value, children=None):\n        self.value = value\n        self.children = children if children else []\n\n    def __repr__(self):\n        if not self.children:\n            return str(self.value)\n        return f\"({self.value} \" + \" \".join([str(c) for c in self.children]) + \")\"\n    \n    def to_infix(self):\n        if not self.children:\n            return str(self.value)\n        \n        op = self.value\n        if len(self.children) == 1:\n            return f\"{op}({self.children[0].to_infix()})\"\n        elif len(self.children) == 2:\n            if op == 'pow':\n                return f\"({self.children[0].to_infix()} ^ {self.children[1].to_infix()})\"\n            elif op == 'mod' or op == '%':\n                return f\"({self.children[0].to_infix()} % {self.children[1].to_infix()})\"\n            return f\"({self.children[0].to_infix()} {op} {self.children[1].to_infix()})\"\n        \n        # Mapping for short tokens to readable infix\n        if op == '!': return f\"gamma({self.children[0].to_infix()})\" # Use gamma for !\n        if op == '_': return f\"floor({self.children[0].to_infix()})\"\n        if op == 'g': return f\"lgamma({self.children[0].to_infix()})\"\n        if op == 'S': return f\"asin({self.children[0].to_infix()})\"\n        if op == 'C': return f\"acos({self.children[0].to_infix()})\"\n        if op == 'T': return f\"atan({self.children[0].to_infix()})\"\n        if op == 'e': return f\"exp({self.children[0].to_infix()})\"\n        \n        return f\"{op}({self.children[0].to_infix()})\"\n    \n    def count_constants(self):\n        \"\"\"Count the number of 'C' placeholders in the tree.\"\"\"\n        count = 1 if self.value == 'C' else 0\n        for child in self.children:\n            count += child.count_constants()\n        return count\n    \n    def get_constant_positions(self, path=None):\n        \"\"\"Returns a list of paths to all 'C' nodes for optimization.\"\"\"\n        if path is None:\n            path = []\n        positions = []\n        if self.value == 'C':\n            positions.append(path.copy())\n        for i, child in enumerate(self.children):\n            positions.extend(child.get_constant_positions(path + [i]))\n        return positions\n\n\nimport ast\n\nclass ExpressionTree:\n    def __init__(self, token_list):\n        \"\"\"\n        Parses a list of tokens in Pre-order traversal (Prefix notation)\n        Example: ['+', 'x', 'sin', 'x'] -> x + sin(x)\n        \"\"\"\n        self.tokens = token_list\n        try:\n            self.root, remaining = self._build_tree(token_list)\n            if remaining:\n                raise ValueError(\"Tokens remained after building tree\")\n            self.is_valid = True\n        except Exception:\n            self.root = None\n            self.is_valid = False\n\n    @classmethod\n    def from_infix(cls, infix_str):\n        \"\"\"\n        Creates an ExpressionTree from a standard infix string (e.g. \"sin(x) + x^2\").\n        Uses Python's ast to parse.\n        \"\"\"\n        # Replacements to make it valid python for AST\n        # 1. Handle postfix factorial '!' which C++ outputs as '(... )!'\n        # We convert '(... )!' to 'gamma(...)'\n        # Iterate until no '!' left\n        processed_str = infix_str\n        while '!' in processed_str:\n            idx = processed_str.find('!')\n            # Helper to find matching paren backwards\n            if idx > 0 and processed_str[idx-1] == ')':\n                paren_count = 1\n                start = idx - 2\n                while start >= 0 and paren_count > 0:\n                    if processed_str[start] == ')':\n                        paren_count += 1\n                    elif processed_str[start] == '(':\n                        paren_count -= 1\n                    start -= 1\n                # start is now 1 char before the matching '('\n                start += 1 \n                # Reconstruct: ... + gamma( + ... + ) + ...\n                # Content includes the parens: ( ... )\n                content = processed_str[start:idx] \n                processed_str = processed_str[:start] + \"gamma\" + content + processed_str[idx+1:]\n            elif idx < len(processed_str) - 1 and processed_str[idx+1] == '(':\n                # Prefix usage !(...) -> gamma(...)\n                processed_str = processed_str[:idx] + \"gamma\" + processed_str[idx+1:]\n            else:\n                # Fallback: Replace ! with gamma if it's explicitly used as a function-like token\n                processed_str = processed_str.replace('!', 'gamma', 1)\n\n        # 1b. Handle 'pow' and 'mod' keywords if they leak in\n        processed_str = processed_str.replace(' pow ', ' ^ ') # Be careful not to replace 'power' variable names if any, though we don't have them.\n        processed_str = processed_str.replace(' mod ', ' % ')\n\n        # 2. C++ uses ^ for power, Python uses **. AST parses ^ as BitXor.\n        try:\n            tree = ast.parse(processed_str, mode='eval')\n            tokens = cls._ast_to_prefix(tree.body)\n            return cls(tokens)\n        except Exception as e:\n            print(f\"Error parsing infix: {e} | Original: {infix_str} | Processed: {processed_str}\")\n            return cls([]) # Invalid\n\n    @staticmethod\n    def _ast_to_prefix(node):\n        if isinstance(node, ast.BinOp):\n            # Map operators\n            op_map = {\n                ast.Add: '+', ast.Sub: '-', ast.Mult: '*', ast.Div: '/',\n                ast.BitXor: 'pow', ast.Pow: 'pow', ast.Mod: 'mod'\n            }\n            op_type = type(node.op)\n            if op_type in op_map:\n                return [op_map[op_type]] + ExpressionTree._ast_to_prefix(node.left) + ExpressionTree._ast_to_prefix(node.right)\n        \n        elif isinstance(node, ast.UnaryOp):\n            op_map = {ast.USub: 'neg', ast.UAdd: None} # Ignore unary +\n            op_type = type(node.op)\n            if op_type == ast.USub:\n                # Check directly if it's a number to collapse \"-5\"\n                if isinstance(node.operand, ast.Constant) and isinstance(node.operand.value, (int, float)):\n                    return [str(-node.operand.value)]\n                return ['neg'] + ExpressionTree._ast_to_prefix(node.operand)\n            elif op_type == ast.UAdd:\n                 return ExpressionTree._ast_to_prefix(node.operand)\n\n        elif isinstance(node, ast.Call):\n            # Functions like sin(x)\n            func_id = node.func.id\n            # Allow both standard names and GPU short tokens\n            if func_id in ['sin', 'cos', 'tan', 'asin', 'acos', 'atan', 'exp', 'log', 'sqrt', 'abs', 'floor', 'ceil', 'gamma', 'lgamma', 'sign', 'neg',\n                           'S', 'C', 'T', 'e', 'g', '_']: \n                \n                # Map back to short tokens if used by engine\n                # We assume engine uses short tokens for S, C, T, e, !, _, g\n                token = func_id\n                if func_id == 'asin': token = 'S'\n                if func_id == 'acos': token = 'C'\n                if func_id == 'atan': token = 'T'\n                if func_id == 'exp': token = 'e'\n                if func_id == 'gamma': token = '!'\n                if func_id == 'floor': token = '_'\n                if func_id == 'lgamma': token = 'g'\n                \n                tokens = [token]\n                for arg in node.args:\n                    tokens.extend(ExpressionTree._ast_to_prefix(arg))\n                return tokens\n        \n        elif isinstance(node, ast.Name):\n            # Map 'x' to 'x0' if preferred, or keep as is if using x0 in string\n            if node.id == 'x':\n                return ['x0']\n            return [node.id]\n        \n        elif isinstance(node, ast.Constant): # Python 3.8+\n            return [str(node.value)]\n        elif isinstance(node, ast.Num): # Older python\n            return [str(node.n)]\n\n        raise ValueError(f\"Unsupported AST node: {node}\")\n\n\n    def _build_tree(self, tokens):\n        if not tokens:\n            raise ValueError(\"Empty token list\")\n        \n        token = tokens[0]\n        remaining = tokens[1:]\n        \n        if token in OPERATORS:\n            arity = OPERATORS[token]\n            children = []\n            for _ in range(arity):\n                child, remaining = self._build_tree(remaining)\n                children.append(child)\n            return Node(token, children), remaining\n        elif token in VARIABLES or token in CONSTANTS:\n            return Node(token), remaining\n        else:\n            # Try to parse as float literal\n            try:\n                float(token)\n                return Node(token), remaining\n            except:\n                raise ValueError(f\"Unknown token: {token}\")\n\n    def evaluate(self, x_values, constants=None):\n        \"\"\"\n        Evaluates the expression tree for a given input.\n        x_values: \n            - numpy array of shape (N,) for single variable (x0)\n            - numpy array of shape (features, N) or (N, features) ?? \n              Let's standardize on (features, samples) for easy indexing x[i], \n              OR a dictionary {'x0': array, 'x1': array}.\n        constants: optional dict mapping path tuples to constant values\n        Returns a numpy array of results.\n        \"\"\"\n        if isinstance(x_values, dict):\n             # Extract arrays: expected keys 'x0', 'x1', ...\n             # We pass the dict directly.\n             pass\n        elif isinstance(x_values, np.ndarray):\n            if x_values.ndim == 1:\n                # Single variable x -> x0\n                x_values = {'x0': x_values}\n            elif x_values.ndim == 2:\n                # Shape issue: is it (N, M) or (M, N)?\n                # Usually standard ML is (samples, features).\n                # But for our eval logic `x[0]` returning feature 0 is easier.\n                # So if shape is (samples, features), we transpose or wrap.\n                # Let's assume standard (N_samples, M_features).\n                # Then x_values[:, 0] is x0.\n                inputs = {}\n                n_features = x_values.shape[1]\n                for i in range(n_features):\n                    inputs[f'x{i}'] = x_values[:, i]\n                x_values = inputs\n            else:\n                raise ValueError(f\"Unsupported input shape: {x_values.shape}\")\n        else:\n             x_values = {'x0': np.array(x_values, dtype=np.float64)}\n        \n        # Determine sample size from first key\n        n_samples = len(next(iter(x_values.values())))\n        \n        if not self.is_valid:\n            return np.full(n_samples, np.nan, dtype=np.float64)\n        return self._eval_node(self.root, x_values, constants, path=[])\n\n    def _eval_node(self, node, x, constants=None, path=None):\n        val = node.value\n        \n        # Check for variable\n        if val in x:\n            return x[val].astype(np.float64)\n        if val == 'x': # Backward compatibility\n             if 'x0' in x: return x['x0'].astype(np.float64)\n             # Fallback if x was passed as key 'x'\n             if 'x' in x: return x['x'].astype(np.float64)\n             raise ValueError(\"Variable 'x' not found in input.\")\n        # Get sample size from a variable\n        n_samples = len(next(iter(x.values())))\n        \n        if val == 'pi':\n            return np.full(n_samples, np.pi, dtype=np.float64)\n        if val == 'e' and not node.children:\n            return np.full(n_samples, np.e, dtype=np.float64)\n        if val == 'C':\n            # Check if we have an optimized constant for this position\n            if constants is not None and tuple(path) in constants:\n                return np.full(n_samples, constants[tuple(path)], dtype=np.float64)\n            return np.full(n_samples, 1.0, dtype=np.float64)  # Default constant = 1\n        \n        # Check for numeric constants\n        try:\n            return np.full(n_samples, float(val), dtype=np.float64)\n        except:\n            pass\n            \n        # Recursive evaluation\n        args = []\n        for i, c in enumerate(node.children):\n            args.append(self._eval_node(c, x, constants, path + [i] if path is not None else None))\n        \n        # Operators\n        with np.errstate(divide='ignore', invalid='ignore', over='ignore'):\n            if val == '+': return args[0] + args[1]\n            if val == '-': return args[0] - args[1]\n            if val == '*': return args[0] * args[1]\n            if val == '/': \n                return np.divide(args[0], args[1], out=np.zeros(n_samples, dtype=np.float64), where=args[1]!=0)\n            if val == 'pow':\n                # Safe power\n                return np.power(np.abs(args[0]) + 1e-10, np.clip(args[1], -10, 10))\n            if val == 'mod':\n                return np.mod(args[0], args[1] + 1e-10)\n            if val == 'sin': return np.sin(args[0])\n            if val == 'cos': return np.cos(args[0])\n            if val == 'tan': return np.tan(args[0])\n            \n            if val == 'asin' or val == 'S': \n                # Protected asin: asin(clip(x, -1, 1))\n                return np.arcsin(np.clip(args[0], -1 + 1e-7, 1 - 1e-7))\n            if val == 'acos' or val == 'C': \n                # Protected acos: acos(clip(x, -1, 1))\n                return np.arccos(np.clip(args[0], -1 + 1e-7, 1 - 1e-7))\n            if val == 'atan' or val == 'T': return np.arctan(args[0])\n            \n            if val == 'exp' or val == 'e': \n                return np.exp(np.clip(args[0], -100, 100))\n            if val == 'log': \n                return np.log(np.abs(args[0]) + 1e-10)\n            if val == 'sqrt':\n                return np.sqrt(np.abs(args[0]))\n            if val == 'abs':\n                return np.abs(args[0])\n            if val == 'floor' or val == '_':\n                return np.floor(args[0])\n            if val == 'ceil':\n                return np.ceil(args[0])\n            \n            if val == 'gamma' or val == '!':\n                # Match C++ Protected Gamma/Factorial: tgamma(|x| + 1)\n                # This ensures consistent evaluation for formulas from C++ engine (which uses !)\n                arg = np.abs(args[0]) + 1.0\n                clipped = np.clip(arg, 0.1, 50) # Clip upper bound to avoid overflow\n                return scipy_gamma(clipped)\n            if val == 'lgamma' or val == 'g':\n                # Protected lgamma: lgamma(|x| + 1)\n                arg = np.abs(args[0]) + 1.0\n                # gammaln is safe for large positive numbers, so less aggressive clipping needed for overflow,\n                # but we clip for consistency and to avoid extremely large outputs if followed by exp\n                clipped = np.clip(arg, 0.1, 1000) \n                return gammaln(clipped)\n            if val == 'neg':\n                return -args[0]\n            if val == 'sign':\n                return np.sign(args[0])\n                \n        return np.zeros(n_samples, dtype=np.float64)\n\n    def get_infix(self):\n        if not self.is_valid:\n            return \"Invalid\"\n        return self.root.to_infix()\n    \n    \n\n    def count_constants(self):\n        if not self.is_valid:\n            return 0\n        return self.root.count_constants()\n\n    @staticmethod\n    def generate_random(max_depth=4, num_variables=1, p_terminal=0.3):\n        \"\"\"\n        Generates a random valid ExpressionTree.\n        \"\"\"\n        import random\n        \n        valid_vars = ['x' + str(i) for i in range(num_variables)]\n        # Use module-level OPERATORS\n        ops = list(OPERATORS.keys())\n\n        # Weighted operators (matching C++ OPERATOR_WEIGHTS approximately)\n        # +, -, *, / : High weight (0.2, 0.2, 0.2, 0.15)\n        # ^ : Medium (0.1)\n        # sin, cos: Medium (0.1)\n        # log, exp: Low (0.05)\n        # others: Very Low (0.01)\n        \n        weighted_ops = []\n        op_weights = []\n        for op in ops:\n            weighted_ops.append(op)\n            if op in ['+', '-', '*']: w = 20\n            elif op == '/': w = 15\n            elif op in ['^', 'sin', 'cos']: w = 10\n            elif op in ['log', 'exp']: w = 5\n            else: w = 1 # !, _, g, S, C, T\n            op_weights.append(w)\n            \n        def _gen(depth):\n            if depth >= max_depth or (depth > 1 and random.random() < p_terminal):\n                # Terminal\n                if random.random() < 0.75: # Variable (0.75 matches C++ config)\n                     return Node(random.choice(valid_vars))\n                else: # Constant\n                     # C++ uses range -10 to 10\n                     # We can generate a random float string or use 'C' for optimization\n                     # Using 'C' allows optimize_constants to work later.\n                     # But initially, some random numbers are good.\n                     if random.random() < 0.5:\n                         val = random.uniform(-5.0, 5.0)\n                         return Node(f\"{val:.4f}\")\n                     else:\n                         return Node('C')\n            else:\n                # Operator (Weighted)\n                op = random.choices(weighted_ops, weights=op_weights, k=1)[0]\n                arity = OPERATORS[op]\n                if arity == 1:\n                    return Node(op, [_gen(depth+1)])\n                else:\n                    return Node(op, [_gen(depth+1), _gen(depth+1)])\n        \n        root = _gen(0)\n        # Verify valid infix generation\n        try:\n            return ExpressionTree.from_infix(root.to_infix())\n        except:\n            # Fallback for very unlucky generation\n            return ExpressionTree.from_infix(\"x0\")\n\n\nimport sympy\n\ndef simplify_formula(formula_str):\n    \"\"\"\n    Simplifies a mathematical formula using SymPy.\n    \"\"\"\n    try:\n        # 1. Clean up C++ notation that sympy might not like directly\n        # e.g., 'pi' is fine. 'neg(x)' -> '-x'.\n        # But our infix is usually standard. \n        # C++ 'pow(x,2)' might need conversion to 'x**2' or sympy handles it?\n        # Sympy uses 'Pow'. \n        \n        # Replace common mismatches\n        s_str = formula_str.replace(\"pow(\", \"Pow(\")\n        # s_str = s_str.replace(\"abs(\", \"Abs(\") # Sympy handles abs\n        \n        # Parse\n        expr = sympy.sympify(s_str)\n        \n        # Simplify\n        simplified = sympy.simplify(expr)\n        \n        # Convert back to string\n        # We need to ensure it uses our function names (e.g. sin, cos)\n        # Sympy standard printer is usually good.\n        # But 'Power' is '**'. We used 'hat' or 'pow' in some places?\n        # Our tokenizer supports standard operators. 'x**2' is not standard infix for our parser?\n        # Our Parser supports 'x^2' or 'pow(x,2)'? \n        # AST parser handles '**' -> 'pow'.\n        \n        final_str = str(simplified)\n        return final_str\n        \n    except Exception as e:\n        # Fallback if simplification fails (e.g. unknown functions)\n        return formula_str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/model.py\n",
        "import torch\nimport torch.nn as nn\nimport numpy as np\n\nclass AlphaSymbolicModel(nn.Module):\n    def __init__(self, vocab_size, d_model=128, nhead=4, num_encoder_layers=2, num_decoder_layers=2, max_seq_len=256, input_dim=2):\n        super(AlphaSymbolicModel, self).__init__()\n        \n        self.d_model = d_model\n        self.input_dim = input_dim\n        \n        # 1. Point Encoder: Processes pairs/tuples of (x..., y)\n        self.point_embedding = nn.Linear(input_dim, d_model)\n        \n        # We use a standard Transformer Encoder for the \"Problem Embedding\"\n        # Since points are a set, we don't necessarily need positional encoding, \n        # but the Transformer will process them as a sequence.\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n        self.problem_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n        \n        # 2. Formula Decoder: Generates tokens\n        self.token_embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoder = PositionalEncoding(d_model, max_len=max_seq_len)\n        \n        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n        self.formula_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n        \n        # 3. Heads\n        self.policy_head = nn.Linear(d_model, vocab_size)\n        self.value_head = nn.Sequential(\n            nn.Linear(d_model, 64),\n            nn.ReLU(),\n            nn.Linear(64, 3) # Quantiles: 0.25, 0.50, 0.75\n        )\n        \n    def forward(self, x_values, y_values, formula_input, formula_mask=None):\n        \"\"\"\n        x_values: [batch, num_points]\n        y_values: [batch, num_points]\n        formula_input: [batch, seq_len] (Token IDs)\n        formula_mask: Optional mask for the decoder (causal mask)\n        \"\"\"\n        # -- Problem Encoding --\n        # 1. Ensure dimensions\n        if x_values.dim() == 2:\n            x_values = x_values.unsqueeze(-1) # [batch, num_points, 1]\n        \n        if y_values.dim() == 2:\n            y_values = y_values.unsqueeze(-1) # [batch, num_points, 1]\n            \n        # 2. Stack x and y: [batch, num_points, n_vars + 1]\n        points = torch.cat([x_values, y_values], dim=-1)\n        \n        # 3. Pad to match input_dim (e.g., 11)\n        curr_dim = points.shape[-1]\n        if curr_dim < self.input_dim:\n            pad_size = self.input_dim - curr_dim\n            padding = torch.zeros(points.shape[0], points.shape[1], pad_size, device=points.device)\n            points = torch.cat([points, padding], dim=-1)\n        \n        # Project to d_model\n        points_emb = self.point_embedding(points) # [batch, num_points, d_model]\n        \n        # Encode problem (memory for decoder)\n        memory = self.problem_encoder(points_emb)\n        \n        # -- Formula Decoding --\n        # Embed tokens\n        tgt = self.token_embedding(formula_input) # [batch, seq_len, d_model]\n        tgt = self.pos_encoder(tgt)\n        \n        # Decode\n        # memory is [batch, num_points, d_model]\n        # tgt is [batch, seq_len, d_model]\n        if formula_mask is None:\n             # Create causal mask\n            seq_len = formula_input.size(1)\n            formula_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(formula_input.device)\n\n        output = self.formula_decoder(tgt, memory, tgt_mask=formula_mask)\n        \n        # -- Heads --\n        # Policy: distribution over vocab for each token position\n        logits = self.policy_head(output) # [batch, seq_len, vocab_size]\n        \n        # Value: estimate value from the LAST token's state\n        # (Assuming the last token summarizes the current state)\n        last_token_output = output[:, -1, :] # [batch, d_model]\n        value = self.value_head(last_token_output) # [batch, 1]\n        \n        return logits, value\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        # x: [batch, seq_len, d_model]\n        x = x + self.pe[:, :x.size(1), :]\n        return x\n\nif __name__ == \"__main__\":\n    # Smoke Test\n    vocab_size = 20\n    model = AlphaSymbolicModel(vocab_size=vocab_size, d_model=32)\n    \n    # Dummy data\n    bs = 2\n    points = 10\n    x = torch.randn(bs, points)\n    y = torch.randn(bs, points)\n    \n    # Formula input (start token + some tokens)\n    seq = torch.randint(0, vocab_size, (bs, 5))\n    \n    logits, value = model(x, y, seq)\n    \n    print(\"Logits shape:\", logits.shape) # Should be [2, 5, 20]\n    print(\"Value shape:\", value.shape)   # Should be [2, 1]\n    print(\"Smoke test passed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/environment.py\n",
        "import gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\nfrom core.grammar import VOCABULARY, OPERATORS, TOKEN_TO_ID, ExpressionTree\nfrom data.synthetic_data import DataGenerator\n\nclass SymbolicEnv(gym.Env):\n    def __init__(self, max_length=50):\n        super(SymbolicEnv, self).__init__()\n        \n        self.vocab_size = len(VOCABULARY)\n        self.max_length = max_length\n        self.vocab = VOCABULARY\n        \n        # Action space: Choose a token from the vocabulary\n        self.action_space = spaces.Discrete(self.vocab_size)\n        \n        # Observation space: \n        # 1. Current token sequence (padded)\n        # 2. X values (fixed size for simplicity)\n        # 3. Y values\n        # For this prototype we will expose a dictionary observation\n        self.observation_space = spaces.Dict({\n            \"sequence\": spaces.Box(low=0, high=self.vocab_size, shape=(max_length,), dtype=np.int32),\n            \"x\": spaces.Box(low=-np.inf, high=np.inf, shape=(10,), dtype=np.float32),\n            \"y\": spaces.Box(low=-np.inf, high=np.inf, shape=(10,), dtype=np.float32)\n        })\n        \n        self.data_gen = DataGenerator(max_depth=4)\n        self.current_problem = None\n        self.current_sequence = []\n        self.open_branches = 0\n        \n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        \n        # Generate a new problem (X, Y)\n        # In a real scenario, this could be sampled from a fixed dataset\n        batch = self.data_gen.generate_batch(1, point_count=10)\n        self.current_problem = batch[0]\n        \n        self.current_sequence = []\n        self.open_branches = 1 # Start expecting a root node\n        \n        return self._get_obs(), {}\n\n    def step(self, action_id):\n        token = self.vocab[action_id]\n        self.current_sequence.append(token)\n        \n        # Update open branches\n        if token in OPERATORS:\n            arity = OPERATORS[token]\n            self.open_branches += (arity - 1)\n        else:\n            self.open_branches -= 1\n            \n        term = False\n        trunc = False\n        reward = 0.0\n        \n        # Check completion\n        if self.open_branches == 0:\n            term = True\n            # Tree is complete, evaluate\n            reward = self._calculate_reward()\n        elif self.open_branches < 0:\n            # Should not happen if we mask actions, but for safety\n            term = True\n            reward = -100.0 # Syntax error penalty\n        elif len(self.current_sequence) >= self.max_length:\n            trunc = True\n            reward = -10.0 # Incomplete penalty\n            \n        return self._get_obs(), reward, term, trunc, {}\n\n    def _get_obs(self):\n        # Convert sequence to IDs and pad\n        seq_ids = [TOKEN_TO_ID[t] for t in self.current_sequence]\n        padded_seq = np.zeros(self.max_length, dtype=np.int32)\n        padded_seq[:len(seq_ids)] = seq_ids\n        \n        return {\n            \"sequence\": padded_seq,\n            \"x\": self.current_problem['x'].astype(np.float32),\n            \"y\": self.current_problem['y'].astype(np.float32)\n        }\n\n    def _calculate_reward(self):\n        try:\n            tree = ExpressionTree(self.current_sequence)\n            if not tree.is_valid:\n                return -100.0\n            \n            y_pred = tree.evaluate(self.current_problem['x'])\n            \n            # Root Mean Squared Error (RMSE)\n            mse = np.mean((y_pred - self.current_problem['y'])**2)\n            rmse = np.sqrt(mse)\n            \n            if np.isnan(rmse) or np.isinf(rmse):\n                return -1000.0\n                \n            # Reward is negative RMSE\n            # We want to maximize reward -> minimize RMSE\n            # Normalize or scale? simpler is just -RMSE\n            return -rmse\n            \n        except Exception:\n            return -100.0\n\nif __name__ == \"__main__\":\n    env = SymbolicEnv()\n    obs, _ = env.reset()\n    print(\"Initial Observation Keys:\", obs.keys())\n    \n    # Simulate a few steps for x + x\n    # Prefix: + x x\n    actions = ['+', 'x', 'x']\n    tot_reward = 0\n    for tok in actions:\n        aid = TOKEN_TO_ID[tok]\n        obs, reward, term, trunc, _ = env.step(aid)\n        print(f\"Action: {tok}, Reward: {reward}, Term: {term}, Branches: {env.open_branches}\")\n        tot_reward += reward\n        if term: break\n    \n    print(f\"Total Reward: {tot_reward}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/loss.py\n",
        "\nimport torch\nimport torch.nn as nn\n\nclass QuantileLoss(nn.Module):\n    \"\"\"\n    Quantile Loss (Pinball Loss) for multiple quantiles.\n    \n    Args:\n        quantiles (list): List of quantiles to estimate (e.g. [0.25, 0.5, 0.75])\n    \"\"\"\n    def __init__(self, quantiles=[0.25, 0.5, 0.75]):\n        super().__init__()\n        self.quantiles = quantiles\n        \n    def forward(self, preds, target):\n        \"\"\"\n        preds: [batch, num_quantiles] - Predicted values for each quantile\n        target: [batch, 1] - True scalar target\n        \"\"\"\n        # Ensure target matches batch dim\n        # target shape might be [batch] or [batch, 1]\n        if target.dim() == 1:\n            target = target.unsqueeze(1)\n            \n        loss = 0\n        for i, q in enumerate(self.quantiles):\n            error = target - preds[:, i:i+1]\n            # Pinball loss: max(q * error, (q - 1) * error)\n            # Equivalent to: error * (q - I(error < 0))\n            loss += torch.max(q * error, (q - 1) * error).mean()\n            \n        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/gp_bridge.py\n",
        "import os\nimport subprocess\nimport tempfile\nimport re\nimport time\nimport sys\nfrom typing import List, Optional\nimport numpy as np\n\n# Windows: Disable crash dialog boxes for child processes\nif sys.platform == 'win32':\n    try:\n        import ctypes\n        # SEM_FAILCRITICALERRORS | SEM_NOGPFAULTERRORBOX | SEM_NOOPENFILEERRORBOX\n        SEM_NOGPFAULTERRORBOX = 0x0002\n        SEM_FAILCRITICALERRORS = 0x0001\n        SEM_NOOPENFILEERRORBOX = 0x8000\n        ctypes.windll.kernel32.SetErrorMode(\n            SEM_FAILCRITICALERRORS | SEM_NOGPFAULTERRORBOX | SEM_NOOPENFILEERRORBOX\n        )\n    except Exception:\n        pass  # Silently ignore if ctypes fails\n\nclass GPEngine:\n    def __init__(self, binary_path=None):\n        if binary_path is None:\n            # Default location: Code/build/Release/SymbolicRegressionGP.exe\n            # Assuming we are in AlphaSymbolic/.. root or similar.\n            # Adjust path relative to this file: alphasybolic/core/gp_bridge.py\n            # So binary is at ../../Code/build/Release/SymbolicRegressionGP.exe\n            # Improved Project Root Detection\n            # We look for the \"Code\" directory by walking up from this file.\n            current_dir = os.path.dirname(os.path.abspath(__file__))\n            project_root = None\n            \n            # Walk up up to 5 levels\n            d = current_dir\n            for _ in range(5):\n                if os.path.exists(os.path.join(d, \"Code\")):\n                    project_root = d\n                    break\n                parent = os.path.dirname(d)\n                if parent == d:\n                    break\n                d = parent\n            \n            if project_root:\n                base_dir = project_root\n            else:\n                # Fallback\n                base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\n            # Define candidates based on OS\n            is_windows = os.name == 'nt'\n            search_paths = []\n            \n            if is_windows:\n                 search_paths = [\n                    os.path.join(base_dir, \"Code\", \"build\", \"Release\", \"SymbolicRegressionGP.exe\"),\n                    os.path.join(base_dir, \"Code\", \"build\", \"SymbolicRegressionGP.exe\"),\n                    # Fallbacks\n                     os.path.join(base_dir, \"Code\", \"build\", \"Release\", \"SymbolicRegressionGP\"),\n                 ]\n            else:\n                 # Linux/Mac (Colab) - Prioritize no extension\n                 search_paths = [\n                    os.path.join(base_dir, \"Code\", \"build\", \"SymbolicRegressionGP\"),\n                    os.path.join(base_dir, \"Code\", \"build\", \"Release\", \"SymbolicRegressionGP\"),\n                    # Fallbacks\n                    os.path.join(base_dir, \"Code\", \"build\", \"Release\", \"SymbolicRegressionGP.exe\"),\n                    os.path.join(base_dir, \"Code\", \"build\", \"SymbolicRegressionGP.exe\"),\n                 ]\n\n            self.binary_path = None\n            for p in search_paths:\n                if os.path.exists(p):\n                    self.binary_path = p\n                    break\n            \n            if self.binary_path is None:\n                print(f\"[Warning] GP Binary not found. Checked locations:\")\n                for p in search_paths:\n                    print(f\" - {p}\")\n                # Fallback to the most likely one for the current OS\n                self.binary_path = search_paths[0]\n        else:\n            self.binary_path = binary_path\n\n    def run(self, x_values, y_values: List[float], seeds: List[str] = [], timeout_sec: int = 10) -> Optional[str]:\n        \"\"\"\n        Runs the C++ GP Engine with the given data and seeds.\n        Returns the best formula found as a string, or None if failed.\n        \"\"\"\n        if not os.path.exists(self.binary_path):\n            print(f\"[Error] GP Binary not found at: {self.binary_path}\")\n            return None\n\n        # Create temporary files\n        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as seed_file, \\\n             tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as data_file:\n            \n            # Write Seeds\n            for seed in seeds:\n                seed_file.write(seed + \"\\n\")\n            seed_file_path = seed_file.name\n            \n            # Write Data\n            # Format:\n            # Line 1: x0_1 x0_2 ...\n            # Line 2: x1_1 x1_2 ...\n            # ...\n            # Line M: y1 y2 ...\n            \n            # Handle x_values input structure\n            # Case 1: x_values is a list of lists (matrix) or numpy 2D array [features, samples]\n            # Case 2: x_values is dict {'x0': ..., 'x1': ...}\n            # Case 3: x_values is list (single feature) [samples]\n            \n            x_matrix = []\n            if isinstance(x_values, dict):\n                 # Sort keys to ensure order x0, x1, x2...\n                 sorted_keys = sorted(x_values.keys(), key=lambda k: int(k[1:]) if k[1:].isdigit() else 0)\n                 for k in sorted_keys:\n                     x_matrix.append(x_values[k])\n            elif isinstance(x_values, np.ndarray):\n                if x_values.ndim == 1:\n                    x_matrix.append(x_values)\n                else:\n                     # Check shape. Assume (features, samples) if passed from app loop.\n                     # But verify: if shape is (N, F) and F is small, we probably want to transpose.\n                     # Let's assume input matches logic in grammar.py (features, samples)\n                     # Actually, standard sklearn is (samples, features).\n                     # Let's support both but prioritize features being rows for the file.\n                     # If (samples, features), we transpose.\n                     # Heuristic: if shape[0] > shape[1] and shape[1] < 20, assume (samples, features).\n                     if x_values.shape[0] > x_values.shape[1] and x_values.shape[1] < 50:\n                          # (Samples, Features) -> Transpose to (Features, Samples)\n                          for i in range(x_values.shape[1]):\n                              x_matrix.append(x_values[:, i])\n                     else:\n                          # (Features, Samples)\n                          for i in range(x_values.shape[0]):\n                              x_matrix.append(x_values[i])\n            elif isinstance(x_values, list):\n                 # Check if element is list (matrix)\n                 if len(x_values) > 0 and isinstance(x_values[0], list):\n                      # Heuristic check for (Samples, Features) vs (Features, Samples)\n                      # If we have many rows (samples) and few columns (features), transpose.\n                      rows = len(x_values)\n                      cols = len(x_values[0])\n                      \n                      if rows > cols and cols < 50:\n                           # Transpose list of lists\n                           x_matrix = list(map(list, zip(*x_values)))\n                      else:\n                           # Assumed (Features, Samples) already\n                           x_matrix = x_values\n                 else:\n                      # Single feature\n                      x_matrix.append(x_values)\n\n            # Write X lines\n            for feature_vals in x_matrix:\n                data_file.write(\" \".join(map(str, feature_vals)) + \"\\n\")\n            \n            # Write Y line\n            data_file.write(\" \".join(map(str, y_values)) + \"\\n\")\n            data_file_path = data_file.name\n\n        try:\n            # Run Command\n            cmd = [self.binary_path, \"--seed\", seed_file_path, \"--data\", data_file_path]\n            # print(f\"Running GP Engine: {' '.join(cmd)}\")\n            \n            # Windows-specific: Hide console window and suppress error dialogs\n            startupinfo = None\n            creationflags = 0\n            if os.name == 'nt':  # Windows\n                startupinfo = subprocess.STARTUPINFO()\n                startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n                startupinfo.wShowWindow = subprocess.SW_HIDE\n                # CREATE_NO_WINDOW + Don't show error dialogs\n                creationflags = subprocess.CREATE_NO_WINDOW | 0x08000000  # CREATE_NO_WINDOW | SEM_NOGPFAULTERRORBOX\n            \n            start_time = time.time()\n            result = subprocess.run(\n                cmd, \n                capture_output=True, \n                text=True, \n                timeout=timeout_sec,\n                startupinfo=startupinfo,\n                creationflags=creationflags\n            )\n            \n            output = result.stdout\n            \n            # Parse Output\n            # We look for the LAST occurrence of \"Formula: ...\"\n            # Standard formats:\n            # \"Formula: ((x * x) + 2)\"\n            # \"Final Formula: ...\"\n            \n            best_formula = None\n            # Look for formula lines (case-insensitive)\n            # Priority: \"Final Formula:\" > \"Formula:\" > \"Initial best formula:\"\n            for line in output.splitlines():\n                line_lower = line.lower()\n                if \"formula:\" in line_lower:\n                    # Extract the part after \"formula:\" (case-insensitive split)\n                    idx = line_lower.find(\"formula:\")\n                    if idx != -1:\n                        formula_part = line[idx + len(\"formula:\"):].strip()\n                        if formula_part:\n                            best_formula = formula_part\n                            # Keep looking for better matches (Final Formula is best)\n                            if \"final formula:\" in line_lower:\n                                break  # Final Formula is the best, stop looking\n                        \n            # print(f\"GP Engine finished in {time.time() - start_time:.2f}s\")\n            \n            if best_formula is None:\n                pass # print(f\"[DEBUG] GP Engine Output (Stdout):\\n{output}\")\n                pass # print(f\"[DEBUG] GP Engine Output (Stderr):\\n{result.stderr}\")\n            \n            return best_formula\n\n        except subprocess.TimeoutExpired as e:\n            # print(f\"GP Engine timed out after {timeout_sec}s.\")\n            # Recover output captured so far. \n            # Note: TimeoutExpired.stdout/stderr might be bytes even if text=True was passed to subprocess.run.\n            output = e.stdout if e.stdout else \"\"\n            if isinstance(output, bytes):\n                output = output.decode('utf-8', errors='ignore')\n            \n            error_output = e.stderr if e.stderr else \"\"\n            if isinstance(error_output, bytes):\n                error_output = error_output.decode('utf-8', errors='ignore')\n\n            best_formula = None\n            if output:\n                for line in output.splitlines():\n                    line_lower = line.lower()\n                    if \"formula:\" in line_lower:\n                        idx = line_lower.find(\"formula:\")\n                        if idx != -1:\n                            formula_part = line[idx + len(\"formula:\"):].strip()\n                            if formula_part:\n                                best_formula = formula_part\n                                if \"final formula:\" in line_lower:\n                                    break\n            \n            if best_formula:\n                # print(f\"Recovered best formula from timeout: {best_formula}\")\n                return best_formula\n            \n            if error_output:\n                 pass # print(f\"GP Engine Timeout Stderr: {error_output}\")\n            return None\n\n        except Exception as e:\n            # print(f\"GP Engine failed: {e}\")\n            if hasattr(e, 'stderr') and e.stderr:\n                pass # print(f\"Stderr: {e.stderr}\")\n            return None\n        finally:\n            # Cleanup\n            if os.path.exists(seed_file_path):\n                os.unlink(seed_file_path)\n            if os.path.exists(data_file_path):\n                os.unlink(data_file_path)\n\nif __name__ == \"__main__\":\n    # Test\n    engine = GPEngine()\n    x = [1, 2, 3, 4]\n    y = [1+2, 4+2, 9+2, 16+2] # x^2 + 2\n    seeds = [\"(x * x)\", \"(x + 2)\"]\n    \n    print(\"Testing GPEngine...\")\n    res = engine.run(x, y, seeds)\n    print(f\"Result: {res}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/gpu_engine.py\n",
        "\n# DEPRECATED: Use core.gpu instead\nfrom core.gpu import TensorGeneticEngine\n\n# We map every token to an integer ID.\n# 0 is padding/null.\nPAD_ID = 0\n# 1..N are operators and terminals.\n\nclass GPUGrammar:\n    def __init__(self, num_variables=1):\n        self.token_to_id = {'<PAD>': PAD_ID}\n        self.id_to_token = {PAD_ID: '<PAD>'}\n        self.next_id = 1\n        \n        # Terminals (Variables + Constants)\n        # Only include variables compliant with num_variables\n        self.active_variables = ['x0'] # Always support x0\n        if num_variables > 1:\n            self.active_variables = [f'x{i}' for i in range(num_variables)]\n        elif num_variables == 1:\n            self.active_variables = ['x', 'x0'] # Support both for 1D\n\n        self.terminals = self.active_variables + ['C', '1', '2', '3', '5', 'pi', 'e']\n        for t in self.terminals:\n            self.token_to_id[t] = self.next_id\n            self.id_to_token[self.next_id] = t\n            self.next_id += 1\n            \n        # Operators\n        # Map operator string to ID\n        self.operators = list(OPERATORS.keys())\n        for op in self.operators:\n            self.token_to_id[op] = self.next_id\n            self.id_to_token[self.next_id] = op\n            self.next_id += 1\n            \n        self.vocab_size = self.next_id\n        \n        # Precompute arithmetic mappings for faster lookup in eval loop\n        # We need to know which ID corresponds to which operation type\n        self.op_ids = {op: self.token_to_id[op] for op in self.operators}\n        self.arity = {self.token_to_id[op]: OPERATORS[op] for op in self.operators}\n\nclass TensorGeneticEngine:\n    def __init__(self, device: torch.device = None, pop_size=10000, max_len=30, num_variables=1, max_constants=5, n_islands=5):\n        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.grammar = GPUGrammar(num_variables)\n        \n        # Adjust pop_size to be divisible by n_islands\n        self.n_islands = n_islands\n        if pop_size % n_islands != 0:\n            pop_size = (pop_size // n_islands) * n_islands\n            \n        self.pop_size = pop_size\n        self.island_size = pop_size // n_islands\n        self.max_len = max_len\n        self.num_variables = num_variables\n        self.max_constants = max_constants\n        \n        # Pre-allocate memory for random generation\n        self.terminal_ids = torch.tensor([self.grammar.token_to_id[t] for t in self.grammar.terminals], device=self.device)\n        self.operator_ids = torch.tensor([self.grammar.token_to_id[op] for op in self.grammar.operators], device=self.device)\n        \n        # --- Pre-compute Arity Masks for Safe Mutation ---\n        self.token_arity = torch.zeros(self.grammar.vocab_size + 1, dtype=torch.long, device=self.device)\n        self.arity_0_ids = []\n        self.arity_1_ids = []\n        self.arity_2_ids = []\n        \n        # Terminals (0)\n        for t in self.grammar.terminals:\n            tid = self.grammar.token_to_id[t]\n            self.token_arity[tid] = 0\n            self.arity_0_ids.append(tid)\n            \n        # Operators (1 or 2)\n        for op in self.grammar.operators:\n            tid = self.grammar.token_to_id[op]\n            arity = OPERATORS[op]\n            self.token_arity[tid] = arity\n            if arity == 1: self.arity_1_ids.append(tid)\n            elif arity == 2: self.arity_2_ids.append(tid)\n            \n        self.arity_0_ids = torch.tensor(self.arity_0_ids, device=self.device)\n        self.arity_1_ids = torch.tensor(self.arity_1_ids, device=self.device)\n        self.arity_2_ids = torch.tensor(self.arity_2_ids, device=self.device)\n\n    def optimize_constants(self, population: torch.Tensor, constants: torch.Tensor, x: torch.Tensor, y_target: torch.Tensor, steps=10, lr=0.1):\n        \"\"\"\n        Refine constants using Gradient Descent.\n        population: [K, L]\n        constants: [K, MaxConstants]\n        \"\"\"\n        # Clone constants to leaf tensor with grad\n        optimized_consts = constants.clone().detach().requires_grad_(True)\n        optimizer = torch.optim.Adam([optimized_consts], lr=lr)\n        \n        best_mse = torch.full((population.shape[0],), float('inf'), device=self.device)\n        best_consts = constants.clone().detach() # Fallback\n        \n        for _ in range(steps):\n            optimizer.zero_grad()\n            \n            # Forward (Differentiable)\n            mse, _ = self.evaluate_differentiable(population, optimized_consts, x, y_target)\n            \n            # Mask NaNs (invalid formulas don't train)\n            valid_mask = ~torch.isnan(mse)\n            if not valid_mask.any(): break\n            \n            # Keep best known constants per individual\n            improved = (mse < best_mse) & valid_mask\n            if improved.any():\n                best_mse[improved] = mse[improved].detach()\n                best_consts[improved] = optimized_consts[improved].detach()\n            \n            # Loss = Sum(MSE_valid)\n            loss = mse[valid_mask].sum()\n            \n            if not loss.requires_grad:\n                # This happens if no individual uses 'C' (graph disconnected)\n                break\n                \n            loss.backward()\n            optimizer.step()\n            \n        return best_consts, best_mse\n\n    def infix_to_rpn(self, formulas: List[str]) -> torch.Tensor:\n        \"\"\"\n        Converts a list of infix strings to a padded RPN tensor [B, L].\n        \"\"\"\n        batch_rpn = []\n        for f in formulas:\n            try:\n                # Use shared ExpressionTree to parse infix -> tree -> postfix(ish)\n                # But ExpressionTree is prefix. We need Postfix for stack eval.\n                # Let's do a simple recursive implementation here or leverage parsed tree.\n                tree = ExpressionTree.from_infix(f)\n                if not tree.is_valid:\n                    batch_rpn.append([PAD_ID]*self.max_len)\n                    continue\n                \n                # Conversion: Tree -> Postfix\n                rpn_tokens = []\n                def traverse(node):\n                    if not node: return\n                    for child in node.children:\n                        traverse(child)\n                    rpn_tokens.append(node.value)\n                \n                traverse(tree.root)\n                \n                # Convert to IDs\n                ids = [self.grammar.token_to_id.get(t, PAD_ID) for t in rpn_tokens]\n                # Pad/Truncate\n                if len(ids) > self.max_len:\n                    ids = ids[:self.max_len]\n                else:\n                    ids = ids + [PAD_ID] * (self.max_len - len(ids))\n                batch_rpn.append(ids)\n            except:\n                batch_rpn.append([PAD_ID]*self.max_len)\n                \n        if not batch_rpn:\n             return torch.empty((0, self.max_len), device=self.device, dtype=torch.long)\n        return torch.tensor(batch_rpn, device=self.device, dtype=torch.long)\n\n    def rpn_to_infix(self, rpn_tensor: torch.Tensor) -> str:\n        \"\"\"\n        Decodes a single RPN tensor row back to infix string.\n        \"\"\"\n        ids = rpn_tensor.squeeze().cpu().numpy()\n        stack = []\n        \n        for id in ids:\n            if id == PAD_ID: continue\n            token = self.grammar.id_to_token.get(id, '?')\n            \n            if token in OPERATORS:\n                arity = OPERATORS[token]\n            if token in OPERATORS:\n                arity = OPERATORS[token]\n                if len(stack) < arity: \n                    # Skip invalid op, just like GPU engine does\n                    continue\n                \n                args = [stack.pop() for _ in range(arity)]\n                args.reverse()\n                \n                # Infix string construction\n                if arity == 2:\n                    if token == 'pow': elem = f\"pow({args[0]}, {args[1]})\"\n                    else: elem = f\"({args[0]} {token} {args[1]})\"\n                else:\n                    elem = f\"{token}({args[0]})\"\n                stack.append(elem)\n            else:\n                stack.append(token)\n                \n        if len(stack) >= 1:\n            return stack[-1]\n        # print(f\"DEBUG: RPN Decode Failed. IDs: {ids} Stack: {stack}\")\n        return \"Invalid\"\n\n    def evaluate_batch(self, population: torch.Tensor, x: torch.Tensor, y_target: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Evaluates the RPN population on the GPU.\n        population: [PopSize, MaxLen] (Integers)\n        x: [DataSize] (Floats)\n        y_target: [DataSize] (Floats)\n        \n        Returns: RMSE per individual [PopSize]\n        \"\"\"\n        B, L = population.shape\n        D = x.shape[0]\n        MAX_STACK = 10\n        \n        # Stack: [B, D, MAX_STACK]\n        # We need D because each data point evaluates differently.\n        # But wait, the structure is the same. Just the values differ.\n        # We can treat B*D as the batch dimension for the stack operations to simplify?\n        # PopSize=10k, Data=20 -> 200k items. Easy for GPU.\n        \n        # Reshape inputs for \"Batch of Data Points\"\n        # Effective Batch Size = B * D\n        eff_B = B * D\n        \n        # Expand population to match data: [B, 1, L] -> [B, D, L] -> [B*D, L]\n        pop_expanded = population.unsqueeze(1).expand(-1, D, -1).reshape(eff_B, L)\n        \n        # Expand x to match population: [D] -> [1, D] -> [B, D] -> [B*D]\n        x_expanded = x.unsqueeze(0).expand(B, -1).reshape(eff_B)\n        \n        # Stack tensor: [EffectiveBatch, StackDepth]\n        stack = torch.zeros(eff_B, MAX_STACK, device=self.device, dtype=torch.float32)\n        sp = torch.zeros(eff_B, device=self.device, dtype=torch.long) # Stack pointer (next empty slot)\n        \n        # DEBUG\n        # print(f\"DEBUG: Eval Batch B={B} L={L} EffB={eff_B}\")\n        \n        # Constants lookup (naive)\n        pi_val = torch.tensor(np.pi, device=self.device)\n        e_val = torch.tensor(np.e, device=self.device)\n        \n        # Precompute IDs for speed\n        id_x = self.grammar.token_to_id.get('x', -100)\n        id_x0 = self.grammar.token_to_id.get('x0', -100)\n        id_C = self.grammar.token_to_id.get('C', -100)\n        id_pi = self.grammar.token_to_id.get('pi', -100)\n        id_e = self.grammar.token_to_id.get('e', -100)\n        \n        # Binary Ops\n        op_add = self.grammar.token_to_id.get('+', -100)\n        op_sub = self.grammar.token_to_id.get('-', -100)\n        op_mul = self.grammar.token_to_id.get('*', -100)\n        op_div = self.grammar.token_to_id.get('/', -100)\n        op_pow = self.grammar.token_to_id.get('pow', -100)\n        \n        # Unary Ops\n        # Unary Ops\n        op_sin = self.grammar.token_to_id.get('sin', -100)\n        op_cos = self.grammar.token_to_id.get('cos', -100)\n        op_tan = self.grammar.token_to_id.get('tan', -100)\n        \n        op_asin = self.grammar.token_to_id.get('asin', -100)\n        op_acos = self.grammar.token_to_id.get('acos', -100)\n        op_atan = self.grammar.token_to_id.get('atan', -100)\n        \n        op_exp = self.grammar.token_to_id.get('exp', -100)\n        op_log = self.grammar.token_to_id.get('log', -100)\n        op_sqrt = self.grammar.token_to_id.get('sqrt', -100)\n        op_abs = self.grammar.token_to_id.get('abs', -100)\n        op_neg = self.grammar.token_to_id.get('neg', -100)\n\n        # Loop over RPN tokens\n        for i in range(L):\n            token = pop_expanded[:, i] # [EffectiveBatch]\n            \n            # Mask: Is this row active? (Not PAD)\n            # PAD=0. If PAD, we do nothing (stack remains same)\n            active_mask = (token != PAD_ID)\n            if not active_mask.any(): continue\n            \n            # 1. Handle Operands (Push)\n            # -------------------------\n            # We calculate \"value to push\" for everyone, then apply.\n            push_vals = torch.zeros(eff_B, device=self.device)\n            is_operand = torch.zeros(eff_B, dtype=torch.bool, device=self.device)\n            \n            # x\n            mask = (token == id_x) | (token == id_x0)\n            if mask.any():\n                push_vals[mask] = x_expanded[mask]\n                is_operand = is_operand | mask\n                \n            # Constants\n            mask = (token == id_pi)\n            if mask.any():\n                push_vals[mask] = pi_val\n                is_operand = is_operand | mask\n            \n            mask = (token == id_e)\n            if mask.any():\n                push_vals[mask] = e_val\n                is_operand = is_operand | mask\n                \n            mask = (token == id_C)\n            if mask.any():\n                push_vals[mask] = 1.0 # Default C=1.0 for GPU Search (optimization is hard here)\n                is_operand = is_operand | mask\n                \n            # Numeric Literals (1..5)\n            # (Assuming ids mapped sequentially or we map individually)\n            # Simpler: Check range if mapped sequentially, or just discrete checks\n            for val_str in ['1', '2', '3', '5']:\n                vid = self.grammar.token_to_id.get(val_str, -999)\n                mask = (token == vid)\n                if mask.any():\n                    push_vals[mask] = float(val_str)\n                    is_operand = is_operand | mask\n\n            # Apply Push\n            if is_operand.any():\n                # stack[b, sp[b]] = val\n                # Safe scatter\n                safe_sp = torch.clamp(sp, 0, MAX_STACK-1)\n                stack.scatter_(1, safe_sp.unsqueeze(1), push_vals.unsqueeze(1))\n                # Increment SP\n                sp = sp + is_operand.long()\n\n\n            # 2. Handle Binary Ops (Pop 2, Push 1)\n            # ------------------------------------\n            is_binary = (token == op_add) | (token == op_sub) | (token == op_mul) | (token == op_div) | (token == op_pow)\n            \n            if is_binary.any():\n                # We need at least 2 items. If sp < 2, it's invalid.\n                valid_op = is_binary & (sp >= 2)\n                \n                if valid_op.any():\n                    # Calculate indices safely (clamp to valid range [0, 9] even if invalid row)\n                    # We will mask out the result later, so garbage input is fine, but SEGV isn't.\n                    safe_sp_minus_1 = torch.clamp(sp - 1, 0, MAX_STACK - 1)\n                    safe_sp_minus_2 = torch.clamp(sp - 2, 0, MAX_STACK - 1)\n                    \n                    # Pop B (Top)\n                    idx_b = safe_sp_minus_1.unsqueeze(1)\n                    val_b = stack.gather(1, idx_b).squeeze(1)\n                    \n                    # Pop A (Second)\n                    idx_a = safe_sp_minus_2.unsqueeze(1)\n                    val_a = stack.gather(1, idx_a).squeeze(1)\n                    \n                    res = torch.zeros_like(val_a)\n                    \n                    # Compute\n                    mask = (token == op_add) & valid_op\n                    if mask.any(): res[mask] = val_a[mask] + val_b[mask]\n                    \n                    mask = (token == op_sub) & valid_op\n                    if mask.any(): res[mask] = val_a[mask] - val_b[mask]\n                    \n                    mask = (token == op_mul) & valid_op\n                    if mask.any(): res[mask] = val_a[mask] * val_b[mask]\n                    \n                    mask = (token == op_div) & valid_op\n                    if mask.any(): \n                        # Protected Div\n                        denom = val_b[mask]\n                        denom = torch.where(denom.abs() < 1e-6, torch.tensor(1.0, device=self.device), denom)\n                        res[mask] = val_a[mask] / denom\n                        \n                    mask = (token == op_pow) & valid_op\n                    if mask.any():\n                        # Protected Pow\n                        base = val_a[mask].abs() + 1e-6\n                        expon = torch.clamp(val_b[mask], -10, 10)\n                        res[mask] = torch.pow(base, expon)\n                    \n                    # Push Result (at pos sp-2)\n                    write_val = res\n                    # Write pos must be valid too\n                    write_pos = torch.clamp(sp - 2, 0, MAX_STACK-1)\n                    \n                    # Blend: Only update if valid_op\n                    current_at_pos = stack.gather(1, write_pos.unsqueeze(1)).squeeze(1)\n                    final_write_val = torch.where(valid_op, write_val, current_at_pos)\n                    \n                    stack.scatter_(1, write_pos.unsqueeze(1), final_write_val.unsqueeze(1))\n                    \n                    # Decrement SP by 1 (Pop 2, Push 1 = Net -1)\n                    sp = sp - valid_op.long()\n\n\n            # 3. Handle Unary Ops (Pop 1, Push 1)\n            # -----------------------------------\n            is_unary = (token == op_sin) | (token == op_cos) | (token == op_tan) | \\\n                       (token == op_asin) | (token == op_acos) | (token == op_atan) | \\\n                       (token == op_exp) | (token == op_log) | \\\n                       (token == op_sqrt) | (token == op_abs) | (token == op_neg)\n                       \n            if is_unary.any():\n                valid_op = is_unary & (sp >= 1)\n                \n                if valid_op.any():\n                    # Index safety\n                    safe_sp_minus_1 = torch.clamp(sp - 1, 0, MAX_STACK - 1)\n                    \n                    # Peek Top (at sp-1)\n                    idx_a = safe_sp_minus_1.unsqueeze(1)\n                    val_a = stack.gather(1, idx_a).squeeze(1)\n                    \n                    res = torch.zeros_like(val_a)\n                    \n                    mask = (token == op_sin) & valid_op\n                    if mask.any(): res[mask] = torch.sin(val_a[mask])\n                    \n                    mask = (token == op_cos) & valid_op\n                    if mask.any(): res[mask] = torch.cos(val_a[mask])\n                    \n                    mask = (token == op_tan) & valid_op\n                    if mask.any(): res[mask] = torch.tan(val_a[mask])\n                    \n                    mask = (token == op_asin) & valid_op\n                    if mask.any():\n                        # Clamp for safety\n                        clamped = torch.clamp(val_a[mask], -0.999, 0.999) \n                        res[mask] = torch.asin(clamped)\n                        \n                    mask = (token == op_acos) & valid_op\n                    if mask.any():\n                        clamped = torch.clamp(val_a[mask], -0.999, 0.999)\n                        res[mask] = torch.acos(clamped)\n                        \n                    mask = (token == op_atan) & valid_op\n                    if mask.any(): res[mask] = torch.atan(val_a[mask])\n                    \n                    mask = (token == op_exp) & valid_op\n                    if mask.any(): res[mask] = torch.exp(torch.clamp(val_a[mask], -20, 20))\n                    \n                    mask = (token == op_log) & valid_op\n                    if mask.any(): res[mask] = torch.log(val_a[mask].abs() + 1e-6)\n                    \n                    mask = (token == op_sqrt) & valid_op\n                    if mask.any(): res[mask] = torch.sqrt(val_a[mask].abs())\n                    \n                    mask = (token == op_abs) & valid_op\n                    if mask.any(): res[mask] = torch.abs(val_a[mask])\n                    \n                    mask = (token == op_neg) & valid_op\n                    if mask.any(): res[mask] = -val_a[mask]\n                    \n                    # Overwrite Top\n                    write_pos = safe_sp_minus_1\n                    current_at_pos = stack.gather(1, write_pos.unsqueeze(1)).squeeze(1)\n                    final_write_val = torch.where(valid_op, res, current_at_pos)\n                    \n                    stack.scatter_(1, write_pos.unsqueeze(1), final_write_val.unsqueeze(1))\n                    \n                    # SP stays same\n\n        \n        # End of Loop\n        # Result is at stack[0] (if valid)\n        # Check validity: sp should be 1\n        \n        is_valid = (sp == 1)\n        \n        # Extract result\n        # final_preds: [EffectiveBatch]\n        final_preds = stack[:, 0]\n        \n        # For invalid, set to NaN or huge error\n        final_preds = torch.where(is_valid, final_preds, torch.tensor(float('nan'), device=self.device))\n        \n        # Reshape back to [B, D]\n        # [eff_B] -> [B, D]\n        preds_matrix = final_preds.view(B, D)\n        \n        # Compute RMSE\n        # y_target: [D] -> [1, D] -> [B, D]\n        target_matrix = y_target.unsqueeze(0).expand(B, -1)\n        \n        # MSE: mean over dim 1 (Data points)\n        diff = preds_matrix - target_matrix\n        mse = torch.mean(diff**2, dim=1) # [B]\n        \n        # Handle NaNs (invalid formulas)\n        mse = torch.where(torch.isnan(mse), torch.tensor(1e9, device=self.device), mse)\n        rmse = torch.sqrt(mse)\n        \n        return rmse\n\n    def evaluate_differentiable(self, population: torch.Tensor, constants: torch.Tensor, x: torch.Tensor, y_target: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Autograd-compatible evaluation for constant optimization.\n        Only run this on a small subset (e.g. Top-K) due to memory cost of tracing.\n        \n        population: [B, L] (Long)\n        constants: [B, MaxConstants] (Float, Requires Grad)\n        x: [D]\n        y_target: [D]\n        \n        Returns: (RMSE [B], Predictions [B, D])\n        \"\"\"\n        B, L = population.shape\n        D = x.shape[0]\n        MAX_STACK = 10\n        eff_B = B * D\n        \n        # Reshape inputs\n        pop_expanded = population.unsqueeze(1).expand(-1, D, -1).reshape(eff_B, L)\n        x_expanded = x.unsqueeze(0).expand(B, -1).reshape(eff_B)\n        \n        # Expand constants: [B, K] -> [B, D, K] -> [B*D, K]\n        constants_expanded = constants.unsqueeze(1).expand(-1, D, -1).reshape(eff_B, -1)\n        \n        # Initial State (Functional, no in-place)\n        stack = torch.zeros(eff_B, MAX_STACK, device=self.device, dtype=torch.float32)\n        sp = torch.zeros(eff_B, device=self.device, dtype=torch.long)\n        c_ptr = torch.zeros(eff_B, device=self.device, dtype=torch.long) # Pointer to which constant to use next\n        \n        # Constants lookup\n        pi_val = torch.tensor(np.pi, device=self.device)\n        e_val = torch.tensor(np.e, device=self.device)\n        \n        # IDs\n        id_x = self.grammar.token_to_id.get('x', -100)\n        id_x0 = self.grammar.token_to_id.get('x0', -100)\n        id_C = self.grammar.token_to_id.get('C', -100)\n        id_pi = self.grammar.token_to_id.get('pi', -100)\n        id_e = self.grammar.token_to_id.get('e', -100)\n        \n        # Binary Ops\n        op_add = self.grammar.token_to_id.get('+', -100)\n        op_sub = self.grammar.token_to_id.get('-', -100)\n        op_mul = self.grammar.token_to_id.get('*', -100)\n        op_div = self.grammar.token_to_id.get('/', -100)\n        op_pow = self.grammar.token_to_id.get('pow', -100)\n        \n        # Unary Ops\n        op_sin = self.grammar.token_to_id.get('sin', -100)\n        op_cos = self.grammar.token_to_id.get('cos', -100)\n        op_tan = self.grammar.token_to_id.get('tan', -100)\n        op_asin = self.grammar.token_to_id.get('asin', -100)\n        op_acos = self.grammar.token_to_id.get('acos', -100)\n        op_atan = self.grammar.token_to_id.get('atan', -100)\n        op_exp = self.grammar.token_to_id.get('exp', -100)\n        op_log = self.grammar.token_to_id.get('log', -100)\n        op_sqrt = self.grammar.token_to_id.get('sqrt', -100)\n        op_abs = self.grammar.token_to_id.get('abs', -100)\n        op_neg = self.grammar.token_to_id.get('neg', -100)\n        \n        import torch.nn.functional as F\n\n        for i in range(L):\n            token = pop_expanded[:, i]\n            active_mask = (token != PAD_ID)\n            if not active_mask.any(): continue\n            \n            # --- 1. Push Operands ---\n            push_vals = torch.zeros(eff_B, device=self.device)\n            is_operand = torch.zeros(eff_B, dtype=torch.bool, device=self.device)\n            \n            # x\n            mask = (token == id_x) | (token == id_x0)\n            if mask.any():\n                push_vals = torch.where(mask, x_expanded, push_vals)\n                is_operand = is_operand | mask\n                \n            # Learnable Constants 'C'\n            mask = (token == id_C)\n            if mask.any():\n                # Gather from constants buffer using c_ptr\n                # safe_ptr = c_ptr.clamp(0, K-1)\n                safe_ptr = torch.clamp(c_ptr, 0, constants_expanded.shape[1]-1)\n                \n                # Gather: constants[batch, ptr]\n                # Gather requires [B, 1] index\n                val_c = torch.gather(constants_expanded, 1, safe_ptr.unsqueeze(1)).squeeze(1)\n                \n                push_vals = torch.where(mask, val_c, push_vals)\n                is_operand = is_operand | mask\n                \n                # Update pointer only for those who used C\n                c_ptr = c_ptr + mask.long()\n                \n            # Fixed Constants\n            mask = (token == id_pi)\n            if mask.any():\n                push_vals = torch.where(mask, pi_val, push_vals)\n                is_operand = is_operand | mask\n                \n            mask = (token == id_e)\n            if mask.any():\n                push_vals = torch.where(mask, e_val, push_vals)\n                is_operand = is_operand | mask\n                \n            # Literals\n            for val_str in ['1', '2', '3', '5']:\n                vid = self.grammar.token_to_id.get(val_str, -999)\n                mask = (token == vid)\n                if mask.any():\n                    push_vals = torch.where(mask, torch.tensor(float(val_str), device=self.device), push_vals)\n                    is_operand = is_operand | mask\n            \n            # Update Stack (Functional)\n            if is_operand.any():\n                # One-hot encoding of SP position\n                # safe_sp = sp.clamp(0, MAX_STACK - 1)\n                # target_mask: [B, MAX_STACK]\n                target_mask = F.one_hot(torch.clamp(sp, 0, MAX_STACK-1), num_classes=MAX_STACK).bool()\n                \n                # Logic: if is_operand, replace stack[sp] with push_val\n                # stack_new = stack * (~(is_operand & target_mask)) + push_vals * (is_operand & target_mask)\n                # But is_operand is [B], target_mask is [B, 10].\n                \n                update_mask = target_mask & is_operand.unsqueeze(1) # [B, 10]\n                \n                # Expand push_vals to [B, 10]\n                vals_expanded = push_vals.unsqueeze(1).expand(-1, MAX_STACK)\n                \n                stack = torch.where(update_mask, vals_expanded, stack)\n                sp = sp + is_operand.long()\n                \n            # --- 2. Binary Ops ---\n            is_binary = (token == op_add) | (token == op_sub) | (token == op_mul) | (token == op_div) | (token == op_pow)\n            valid_op = is_binary & (sp >= 2)\n            \n            if valid_op.any():\n                sp_1 = torch.clamp(sp - 1, 0, MAX_STACK-1)\n                sp_2 = torch.clamp(sp - 2, 0, MAX_STACK-1)\n                \n                # Gather operands\n                idx_b = F.one_hot(sp_1, MAX_STACK).bool()\n                val_b = (stack * idx_b).sum(dim=1) # Differentiable gather\n                \n                idx_a = F.one_hot(sp_2, MAX_STACK).bool()\n                val_a = (stack * idx_a).sum(dim=1)\n                \n                res = torch.zeros_like(val_a)\n                \n                mask = (token == op_add) & valid_op\n                if mask.any(): res = torch.where(mask, val_a + val_b, res)\n                \n                mask = (token == op_sub) & valid_op\n                if mask.any(): res = torch.where(mask, val_a - val_b, res)\n                \n                mask = (token == op_mul) & valid_op\n                if mask.any(): res = torch.where(mask, val_a * val_b, res)\n                \n                mask = (token == op_div) & valid_op\n                if mask.any():\n                    denom = torch.where(val_b.abs() < 1e-6, torch.tensor(1.0, device=self.device), val_b)\n                    res = torch.where(mask, val_a / denom, res)\n                    \n                mask = (token == op_pow) & valid_op\n                if mask.any():\n                    base = val_a.abs() + 1e-6\n                    expon = torch.clamp(val_b, -10, 10)\n                    res = torch.where(mask, torch.pow(base, expon), res)\n                \n                # Write back to sp-2\n                write_pos = sp_2\n                target_mask = F.one_hot(write_pos, MAX_STACK).bool()\n                update_mask = target_mask & valid_op.unsqueeze(1)\n                vals_expanded = res.unsqueeze(1).expand(-1, MAX_STACK)\n                \n                stack = torch.where(update_mask, vals_expanded, stack)\n                sp = sp - valid_op.long()\n                \n            # --- 3. Unary Ops ---\n            is_unary = (token == op_sin) | (token == op_cos) | (token == op_tan) | \\\n                       (token == op_asin) | (token == op_acos) | (token == op_atan) | \\\n                       (token == op_exp) | (token == op_log) | \\\n                       (token == op_sqrt) | (token == op_abs) | (token == op_neg)\n            valid_op = is_unary & (sp >= 1)\n            \n            if valid_op.any():\n                sp_1 = torch.clamp(sp - 1, 0, MAX_STACK-1)\n                idx_a = F.one_hot(sp_1, MAX_STACK).bool()\n                val_a = (stack * idx_a).sum(dim=1)\n                \n                res = torch.zeros_like(val_a)\n                \n                mask = (token == op_sin) & valid_op\n                if mask.any(): res = torch.where(mask, torch.sin(val_a), res)\n                \n                mask = (token == op_cos) & valid_op\n                if mask.any(): res = torch.where(mask, torch.cos(val_a), res)\n                \n                mask = (token == op_tan) & valid_op\n                if mask.any(): res = torch.where(mask, torch.tan(val_a), res)\n                \n                mask = (token == op_asin) & valid_op\n                if mask.any():\n                    clamped = torch.clamp(val_a, -0.999, 0.999)\n                    res = torch.where(mask, torch.asin(clamped), res)\n                    \n                mask = (token == op_acos) & valid_op\n                if mask.any():\n                    clamped = torch.clamp(val_a, -0.999, 0.999)\n                    res = torch.where(mask, torch.acos(clamped), res)\n                    \n                mask = (token == op_atan) & valid_op\n                if mask.any(): res = torch.where(mask, torch.atan(val_a), res)\n                \n                mask = (token == op_exp) & valid_op\n                if mask.any(): res = torch.where(mask, torch.exp(torch.clamp(val_a, -20, 20)), res)\n                \n                mask = (token == op_log) & valid_op\n                if mask.any(): res = torch.where(mask, torch.log(val_a.abs() + 1e-6), res)\n                \n                mask = (token == op_sqrt) & valid_op\n                if mask.any(): res = torch.where(mask, torch.sqrt(val_a.abs()), res)\n                \n                mask = (token == op_abs) & valid_op\n                if mask.any(): res = torch.where(mask, torch.abs(val_a), res)\n                \n                mask = (token == op_neg) & valid_op\n                if mask.any(): res = torch.where(mask, -val_a, res)\n                \n                # Write back\n                target_mask = F.one_hot(sp_1, MAX_STACK).bool()\n                update_mask = target_mask & valid_op.unsqueeze(1)\n                vals_expanded = res.unsqueeze(1).expand(-1, MAX_STACK)\n                \n                stack = torch.where(update_mask, vals_expanded, stack)\n\n        # Final Extract\n        is_valid = (sp == 1)\n        final_preds = stack[:, 0]\n        final_preds = torch.where(is_valid, final_preds, torch.tensor(float('nan'), device=self.device))\n        \n        preds_matrix = final_preds.view(B, D)\n        \n        # Loss\n        target_matrix = y_target.unsqueeze(0).expand(B, -1)\n        mse = torch.mean((preds_matrix - target_matrix)**2, dim=1) # [B]\n        \n        # Handling NaNs for gradient? \n        # If NaN, we can't backprop. Mask them out.\n        # But we mostly optimize valid formulas.\n        \n        return mse, preds_matrix\n\n    def run(self, x_data: List[float], y_data: List[float], seeds: List[str], timeout_sec=10) -> Optional[str]:\n        \"\"\"\n        Main entry point.\n        \"\"\"\n    def rpn_to_infix(self, rpn_tensor: torch.Tensor, constants: torch.Tensor = None) -> str:\n        \"\"\"\n        Decodes RPN tensor to Infix string (CPU-style formatting).\n        \"\"\"\n        vocab = self.grammar.id_to_token\n        stack = []\n        const_idx = 0\n        \n        def format_const(val):\n            # Match C++ format_constant\n            if abs(val - round(val)) < 1e-9:\n                return str(int(round(val)))\n            if abs(val) >= 1e6 or abs(val) <= 1e-6:\n                return f\"{val:.8e}\"\n            s = f\"{val:.8f}\"\n            s = s.rstrip('0').rstrip('.')\n            return s if s else \"0\"\n\n        for token_id in rpn_tensor:\n            token_id = token_id.item()\n            if token_id == PAD_ID: break\n            \n            token = vocab.get(token_id, \"\")\n            \n            if token in self.grammar.OPERATORS:\n                arity = self.grammar.token_arity.get(token, 2)\n                if arity == 1:\n                    if not stack: return \"Invalid\"\n                    a = stack.pop()\n                    if token == 's': stack.append(f\"sin({a})\")\n                    elif token == 'c': stack.append(f\"cos({a})\")\n                    elif token == 'l': stack.append(f\"log({a})\")\n                    elif token == 'e': stack.append(f\"exp({a})\")\n                    elif token == 'q': stack.append(f\"sqrt({a})\")\n                    elif token == 'a': stack.append(f\"abs({a})\")\n                    elif token == 'n': stack.append(f\"sign({a})\")\n                    elif token == '_': stack.append(f\"floor({a})\")\n                    elif token == '!': stack.append(f\"({a})!\")\n                    else: stack.append(f\"{token}({a})\")\n                else: # Binary\n                    if len(stack) < 2: return \"Invalid\"\n                    b = stack.pop()\n                    a = stack.pop()\n                    \n                    # Handle A + (-B) -> (A - B)\n                    # Handle 0 - B -> (-B)\n                    if token == '+' and b.startswith(\"-\") and not b.startswith(\"(-\"):\n                         stack.append(f\"({a} - {b[1:]})\")\n                    elif token == '-' and a == \"0\":\n                         stack.append(f\"(-{b})\")\n                    else:\n                         stack.append(f\"({a} {token} {b})\")\n            elif token == 'C':\n                val = 1.0\n                if constants is not None and const_idx < len(constants):\n                    val = constants[const_idx].item()\n                    const_idx += 1\n                stack.append(format_const(val))\n            elif token.startswith('x'):\n                # Handle x0, x1\n                # If token is just 'x', assume x0\n                if token == 'x': stack.append(\"x0\")\n                else: stack.append(token)\n            else:\n                stack.append(str(token))\n                \n        if len(stack) == 1:\n            return stack[0]\n        return \"Invalid\"\n\n\n    def run(self, x_values: List[float], y_targets: List[float], seeds: List[str], timeout_sec=10, callback=None) -> Optional[str]:\n        \"\"\"\n        Main evolutionary loop on GPU.\n        callback: function(gen, best_mse, best_rpn, best_consts, is_new_best) -> None\n        \"\"\"\n        start_time = time.time()\n        \n        # 1. Setup Data\n        x_t = torch.tensor(x_values, device=self.device, dtype=torch.float32)\n        y_t = torch.tensor(y_targets, device=self.device, dtype=torch.float32)\n        \n        if x_t.ndim > 1: x_t = x_t.flatten() \n        if y_t.ndim > 1: y_t = y_t.flatten()\n\n        # print(f\"[GPU Worker] Initializing Tensor Population ({self.pop_size})...\")\n        \n        # --- 0. Target Pattern Detection (\"The Sniper\") ---\n        # Swiftly check for trivial Linear or Geometric patterns\n        if x_t.shape[0] > 2:\n            try:\n                # Prepare X matrix [N, 2] for (slope, intercept)\n                X_mat = torch.stack([x_t, torch.ones_like(x_t)], dim=1)\n                \n                # A. Linear Check (y = mx + c)\n                # Solve: X * [m, c] = y\n                try:\n                    solution = torch.linalg.lstsq(X_mat, y_t).solution\n                    m, c = solution[0].item(), solution[1].item()\n                    y_pred = m * x_t + c\n                    \n                    # Check residuals (Relative Error or R2?)\n                    # Use Normalized RMSE\n                    res_std = torch.std(y_t - y_pred)\n                    y_std = torch.std(y_t)\n                    if y_std > 1e-9 and (res_std / y_std) < 1e-4:\n                        # Found Linear!\n                        # print(f\"[GPU Sniper] Detected Linear Pattern: {m:.4f}*x + {c:.4f}\")\n                        if abs(c) < 1e-5: return f\"({m:.4f} * x)\"\n                        return f\"(({m:.4f} * x) + {c:.4f})\"\n                except: pass\n\n                # B. Geometric Check (y = A * e^(Bx) -> log(y) = log(A) + Bx)\n                if torch.all(y_t > 0):\n                    log_y = torch.log(y_t)\n                    solution_g = torch.linalg.lstsq(X_mat, log_y).solution\n                    B, log_A = solution_g[0].item(), solution_g[1].item()\n                    \n                    y_pred_log = B * x_t + log_A\n                    res_std_log = torch.std(log_y - y_pred_log)\n                    \n                    if res_std_log < 1e-4:\n                        # Found Geometric!\n                        # Formula: exp(log_A + Bx)\n                        # print(f\"[GPU Sniper] Detected Geometric Pattern.\")\n                        return f\"exp(({B:.4f} * x) + {log_A:.4f})\"\n            except Exception as e:\n                pass\n\n        \n        # 2. Initialize Population & Constants\n        seed_tensor = self.infix_to_rpn(seeds)\n        num_seeds = seed_tensor.shape[0]\n        \n        population = torch.zeros(self.pop_size, self.max_len, device=self.device, dtype=torch.long)\n        pop_constants = torch.randn(self.pop_size, self.max_constants, device=self.device) # Learnable Constants\n        \n        # Fill seeds\n        population[:num_seeds] = seed_tensor\n        \n        # Fill rest\n        if num_seeds > 0:\n            remaining = self.pop_size - num_seeds\n            src_indices = torch.randint(0, num_seeds, (remaining,), device=self.device)\n            population[num_seeds:] = seed_tensor[src_indices]\n            \n            mutation_mask = torch.rand(remaining, self.max_len, device=self.device) < 0.3\n            random_tokens = torch.randint(1, self.grammar.vocab_size, (remaining, self.max_len), device=self.device)\n            population[num_seeds:] = torch.where(mutation_mask, random_tokens, population[num_seeds:])\n        else:\n             print(\"[GPU Worker] No seeds provided.\")\n             return None\n\n        best_formula_str = None\n        best_rmse = float('inf')\n        \n        generations = 0\n        COMPLEXITY_PENALTY = 0.01\n        \n        # --- Dynamic Adaptation (\"The Thermostat\") ---\n        stagnation_counter = 0\n        current_mutation_rate = 0.15  # Base rate\n        current_chaos_rate = 0.01     # Base chaos\n        last_improvement_gen = 0\n\n        \n        while time.time() - start_time < timeout_sec:\n            generations += 1\n            \n            # A. Evaluate (Standard)\n            fitness_rmse = self.evaluate_batch(population, x_t, y_t)\n            \n            # Calculate Complexity (Length)\n            # Penalize longer formulas to encourage simplicity (Occam's Razor)\n            lengths = (population != PAD_ID).sum(dim=1).float()\n            # fitness = rmse * (1 + penalty * length) + length * epsilon (for 0-rmse ties)\n            fitness_penalized = fitness_rmse * (1.0 + COMPLEXITY_PENALTY * lengths) + lengths * 1e-6\n            \n            # B. Constant Optimization (Elitism)\n            # Pick Top 50 candidates based on PENALIZED fitness to refine\n            k_opt = 50\n            top_vals, top_indices = torch.topk(fitness_penalized, k_opt, largest=False)\n            \n            # Run Gradient Descent on these constants\n            refined_consts, refined_mse = self.optimize_constants(\n                population[top_indices], \n                pop_constants[top_indices], \n                x_t, y_t, steps=10, lr=0.1\n            )\n            \n            # Write back results\n            pop_constants[top_indices] = refined_consts.detach()\n            fitness_rmse[top_indices] = refined_mse.detach()\n            \n            # Re-calculate penalized fitness for optimized ones\n            refined_lengths = lengths[top_indices]\n            fitness_penalized[top_indices] = refined_mse.detach() * (1.0 + COMPLEXITY_PENALTY * refined_lengths) + refined_lengths * 1e-6\n            \n            # --- Algebraic Simplification (The Cleaner) ---\n            # Every 5 generations, simplify the top elites to remove clutter (x*1 -> x)\n            if generations % 5 == 0:\n                try:\n                    import sympy\n                    # Simplify the optimization candidates (which are already best)\n                    # We operate in-place on the population\n                    for idx_in_top in range(len(top_indices)):\n                        pop_idx = top_indices[idx_in_top]\n                        \n                        # 1. Decode to string (with optimized constants)\n                        rpn = population[pop_idx].unsqueeze(0)\n                        consts = pop_constants[pop_idx]\n                        expr_str = self.rpn_to_infix(rpn, consts)\n                        \n                        if expr_str == \"Invalid\": continue\n                        \n                        # 2. Simplify with SymPy\n                        try:\n                            # Parse and simplify\n                            sym_expr = sympy.sympify(expr_str)\n                            simplified_sym = sympy.simplify(sym_expr)\n                            \n                            # 3. Re-encode to RPN + Constants\n                            new_rpn_ids, new_consts_vals = self.sympy_to_rpn(simplified_sym)\n                            \n                            # Update if valid and fits\n                            if len(new_rpn_ids) <= self.max_len:\n                                # Overwrite population\n                                population[pop_idx] = torch.tensor(new_rpn_ids + [PAD_ID]*(self.max_len - len(new_rpn_ids)), device=self.device)\n                                \n                                # Overwrite constants\n                                new_c_tensor = torch.zeros(self.max_constants, device=self.device)\n                                num_c = min(len(new_consts_vals), self.max_constants)\n                                if num_c > 0:\n                                    new_c_tensor[:num_c] = torch.tensor(new_consts_vals[:num_c], device=self.device)\n                                pop_constants[pop_idx] = new_c_tensor\n                                \n                                # Note: Fitness needs update? \n                                # Simplification should preserve semantics, so RMSE is same. \n                                # But length might decrease, so fitness improves!\n                                # Let's re-eval next generation or now?\n                                # For safety, we leave it. It will be re-evaluated next gen or by selection if we updated lengths.\n                                lengths[pop_idx] = len(new_rpn_ids) # Approximate update\n                                \n                        except Exception as e:\n                            # print(f\"Simplification failed for {expr_str}: {e}\")\n                            pass\n                except ImportError:\n                    pass\n\n            # Check Best (based on Raw RMSE, but maybe Length matters for user? stick to RMSE)\n            min_rmse, min_idx = torch.min(fitness_rmse, dim=0)\n            if min_rmse.item() < best_rmse:\n                best_rmse = min_rmse.item()\n                best_rpn = population[min_idx].unsqueeze(0)\n                best_consts_vec = pop_constants[min_idx]\n                best_formula_str = self.rpn_to_infix(best_rpn, best_consts_vec)\n                # print(f\"[GPU Worker] New Best: {best_formula_str} (RMSE: {best_rmse:.5f})\")\n                \n                if callback:\n                    callback(generations, best_rmse, best_rpn, best_consts_vec, True)\n                \n                # Reset Stagnation\n                stagnation_counter = 0\n                current_mutation_rate = 0.15\n                current_chaos_rate = 0.01\n                last_improvement_gen = generations\n            else:\n                stagnation_counter += 1\n                \n            if callback and (generations % 100 == 0 or generations == 1) and best_rpn is not None:\n                 # Pass current global best\n                 callback(generations, best_rmse, best_rpn, best_consts_vec, False) # False = not new best, just update\n                 \n            # Adaptation Logic\n\n\n                \n            # Adaptation Logic\n            if stagnation_counter > 20:\n                # Boost Mutation/Chaos incrementally\n                current_mutation_rate = min(0.40, current_mutation_rate + 0.02)\n                current_chaos_rate = min(0.05, current_chaos_rate + 0.005)\n                \n            # --- Island Cataclysm (Nuclear Reset) ---\n            if stagnation_counter >= 50:\n                 # print(f\"[GPU Worker] CATACLYSM! Global Stagnation {stagnation_counter}. Resetting population.\")\n                 # Keep Top 1 (min_idx)\n                 # We need to construct a new population where index 0 is best, rest random.\n                 \n                 # 1. Save Best\n                 saved_best_rpn = population[min_idx].clone()\n                 saved_best_c = pop_constants[min_idx].clone()\n                 \n                 # 2. Randomize All\n                 population = torch.randint(1, self.grammar.vocab_size, (self.pop_size, self.max_len), device=self.device)\n                 pop_constants = torch.randn(self.pop_size, self.max_constants, device=self.device)\n                 \n                 # 3. Restore Best at 0\n                 population[0] = saved_best_rpn\n                 pop_constants[0] = saved_best_c\n                 \n                 # 4. Reset Stats\n                 stagnation_counter = 0\n                 current_mutation_rate = 0.15\n                 current_chaos_rate = 0.01\n                 \n                 # Force re-eval? Next loop will evaluate.\n\n\n            \n            # C. Island Selection & Tournament (Vectorized)\n            # 1. Reshape to [NumIslands, IslandSize]\n            view_fit = fitness_penalized.view(self.n_islands, self.island_size)\n            view_pop = population.view(self.n_islands, self.island_size, self.max_len)\n            view_const = pop_constants.view(self.n_islands, self.island_size, self.max_constants)\n\n            # 2. Elitism per Island\n            k_elite_island = max(1, int(self.island_size * 0.1))\n            # topk returns indices relative to the island\n            elite_vals, elite_local_idx = torch.topk(view_fit, k_elite_island, dim=1, largest=False)\n\n            # Gather Elites\n            # Expansion for gather: [Islands, K, L]\n            gather_idx_pop = elite_local_idx.unsqueeze(-1).expand(-1, -1, self.max_len)\n            elites_pop = torch.gather(view_pop, 1, gather_idx_pop)\n            \n            gather_idx_c = elite_local_idx.unsqueeze(-1).expand(-1, -1, self.max_constants)\n            elites_c = torch.gather(view_const, 1, gather_idx_c)\n\n            # 3. Tournament for Offspring\n            num_offspring = self.island_size - k_elite_island\n            \n            # Generate random pairs of indices [Islands, NumOffspring]\n            p1_idx = torch.randint(0, self.island_size, (self.n_islands, num_offspring), device=self.device)\n            p2_idx = torch.randint(0, self.island_size, (self.n_islands, num_offspring), device=self.device)\n            \n            # Compare fitness\n            f1 = torch.gather(view_fit, 1, p1_idx)\n            f2 = torch.gather(view_fit, 1, p2_idx)\n            \n            winner_idx = torch.where(f1 < f2, p1_idx, p2_idx)\n            \n            # Gather Winners\n            gather_idx_win_pop = winner_idx.unsqueeze(-1).expand(-1, -1, self.max_len)\n            winners_pop = torch.gather(view_pop, 1, gather_idx_win_pop)\n            \n            gather_idx_win_c = winner_idx.unsqueeze(-1).expand(-1, -1, self.max_constants)\n            winners_c = torch.gather(view_const, 1, gather_idx_win_c)\n            \n            # 4. Migration (Every 10 gens, Ring Topology)\n            # Inject neighbor's elites into the worst slots of current offspring\n            if generations % 10 == 0 and self.n_islands > 1:\n                # Rotate elites: Island i gets elites from i-1 (or i+1 if we roll pos)\n                migrants_pop = torch.roll(elites_pop, shifts=1, dims=0)\n                migrants_c = torch.roll(elites_c, shifts=1, dims=0)\n                \n                # Replace last k_elite spots in WINNERS (weakest offspring? actually tournament winners are random quality)\n                # But acceptable to just replace.\n                if num_offspring >= k_elite_island:\n                    winners_pop[:, -k_elite_island:] = migrants_pop\n                    winners_c[:, -k_elite_island:] = migrants_c\n            \n            # D. Mutation (On Offspring Only)\n            \n            # 1. Safe Arity-Preserving Mutation (Dynamic Rate)\n            mask = torch.rand(winners_pop.shape, device=self.device) < current_mutation_rate\n            current_arities = self.token_arity[winners_pop]\n            \n            # Arity 0 -> Arity 0\n            if len(self.arity_0_ids) > 0:\n                noise_0 = self.arity_0_ids[torch.randint(0, len(self.arity_0_ids), winners_pop.shape, device=self.device)]\n                winners_pop = torch.where(mask & (current_arities == 0), noise_0, winners_pop)\n                \n            # Arity 1 -> Arity 1\n            if len(self.arity_1_ids) > 0:\n                noise_1 = self.arity_1_ids[torch.randint(0, len(self.arity_1_ids), winners_pop.shape, device=self.device)]\n                winners_pop = torch.where(mask & (current_arities == 1), noise_1, winners_pop)\n                \n            # Arity 2 -> Arity 2\n            if len(self.arity_2_ids) > 0:\n                noise_2 = self.arity_2_ids[torch.randint(0, len(self.arity_2_ids), winners_pop.shape, device=self.device)]\n                winners_pop = torch.where(mask & (current_arities == 2), noise_2, winners_pop)\n                \n            # 2. Chaos Mutation (Structure changing, Low Rate: 1%)\n            chaos_mask = torch.rand(winners_pop.shape, device=self.device) < current_chaos_rate\n            chaos_noise = torch.randint(1, self.grammar.vocab_size, winners_pop.shape, device=self.device)\n            winners_pop = torch.where(chaos_mask, chaos_noise, winners_pop)\n            \n            # Constant Mutation\n            c_noise = torch.randn_like(winners_c) * 0.1\n            winners_c = winners_c + c_noise\n            \n            # 5. Reconstruct Population\n            # Concat [Elites, Offspring] -> [Islands, Size, L]\n            next_pop_view = torch.cat([elites_pop, winners_pop], dim=1)\n            next_c_view = torch.cat([elites_c, winners_c], dim=1)\n            \n            # Flatten to [PopSize, L]\n            population = next_pop_view.view(self.pop_size, self.max_len)\n            pop_constants = next_c_view.view(self.pop_size, self.max_constants)\n            \n        print(f\"[GPU Worker] Finished. Gens: {generations}. Best RMSE: {best_rmse:.5f}\")\n        return best_formula_str\n\n    def sympy_to_rpn(self, sym_expr) -> Tuple[List[int], List[float]]:\n        \"\"\"\n        Converts a SymPy expression to RPN token IDs and a list of constants.\n        \"\"\"\n        import sympy\n        \n        rpn_ids = []\n        constants = []\n        \n        def visit(node):\n            if node.is_Number:\n                val = float(node)\n                # Check for simple constants\n                if node == sympy.pi:\n                    rpn_ids.append(self.grammar.token_to_id['pi'])\n                elif node == sympy.E:\n                    rpn_ids.append(self.grammar.token_to_id['e'])\n                elif val == 1.0 and '1' in self.grammar.token_to_id:\n                     rpn_ids.append(self.grammar.token_to_id['1'])\n                elif val == 2.0 and '2' in self.grammar.token_to_id:\n                     rpn_ids.append(self.grammar.token_to_id['2'])\n                elif val == 3.0 and '3' in self.grammar.token_to_id:\n                     rpn_ids.append(self.grammar.token_to_id['3'])\n                elif val == 5.0 and '5' in self.grammar.token_to_id:\n                     rpn_ids.append(self.grammar.token_to_id['5'])\n                else:\n                    # Generic Constant -> C\n                    rpn_ids.append(self.grammar.token_to_id['C'])\n                    constants.append(val)\n            elif node.is_Symbol:\n                name = str(node)\n                if name in self.grammar.token_to_id:\n                    rpn_ids.append(self.grammar.token_to_id[name])\n                else:\n                    # Variable mismatch or unknown\n                    rpn_ids.append(self.grammar.token_to_id.get('x', 0)) # Fallback\n            elif isinstance(node, sympy.Add):\n                # Sympy Add is n-ary. Convert to chain of binary adds.\n                # A + B + C -> A B + C +\n                args = node.args\n                visit(args[0])\n                for i in range(1, len(args)):\n                    visit(args[i])\n                    rpn_ids.append(self.grammar.token_to_id['+'])\n            elif isinstance(node, sympy.Mul):\n                # A * B * C -> A B * C *\n                args = node.args\n                visit(args[0])\n                for i in range(1, len(args)):\n                    visit(args[i])\n                    rpn_ids.append(self.grammar.token_to_id['*'])\n            elif isinstance(node, sympy.Pow):\n                visit(node.base)\n                visit(node.exp)\n                rpn_ids.append(self.grammar.token_to_id['pow'])\n            elif isinstance(node, sympy.sin):\n                visit(node.args[0])\n                rpn_ids.append(self.grammar.token_to_id['sin'])\n            elif isinstance(node, sympy.cos):\n                visit(node.args[0])\n                rpn_ids.append(self.grammar.token_to_id['cos'])\n            elif isinstance(node, sympy.tan):\n                visit(node.args[0])\n                rpn_ids.append(self.grammar.token_to_id['tan'])\n            elif isinstance(node, sympy.exp):\n                visit(node.args[0])\n                rpn_ids.append(self.grammar.token_to_id['exp'])\n            elif isinstance(node, sympy.log):\n                visit(node.args[0])\n                rpn_ids.append(self.grammar.token_to_id['log'])\n            # Add other functions as needed (asin, acos, etc.)\n            else:\n                 # Fallback for unknown\n                 # Check if it is a known function by string\n                 func_name = str(node.func)\n                 if func_name in self.grammar.token_to_id:\n                      visit(node.args[0]) # Assumes unary\n                      rpn_ids.append(self.grammar.token_to_id[func_name])\n                 else:\n                     # raise ValueError(f\"Unknown node: {node}\")\n                     # Fallback to ignore? Or try to approximate?\n                     # If we raise, the simplification block catches it and aborts.\n                     raise ValueError(f\"Unknown node: {node}\")\n        \n        visit(sym_expr)\n        return rpn_ids, constants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/__init__.py\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/gpu/__init__.py\n",
        "from .engine import TensorGeneticEngine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/gpu/benchmark.py\n",
        "\"\"\"\nPerformance Benchmarking Suite for GPU GP Engine.\n\nStandard benchmark problems for evaluating symbolic regression performance.\n\"\"\"\nimport torch\nimport numpy as np\nimport time\nfrom typing import List, Dict, Tuple, Optional\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass BenchmarkResult:\n    \"\"\"Result from a single benchmark run.\"\"\"\n    problem_name: str\n    target_formula: str\n    found_formula: Optional[str]\n    rmse: float\n    exact_match: bool\n    time_seconds: float\n    generations: int\n\n\nclass BenchmarkSuite:\n    \"\"\"\n    Standard symbolic regression benchmark problems.\n    \n    Includes problems from:\n    - Nguyen benchmark suite\n    - Keijzer benchmarks\n    - Custom problems\n    \"\"\"\n    \n    # Standard benchmark problems: (name, formula, x_range, n_points)\n    PROBLEMS = {\n        # Nguyen benchmarks\n        'nguyen-1': ('x^3 + x^2 + x', (-1, 1), 20),\n        'nguyen-2': ('x^4 + x^3 + x^2 + x', (-1, 1), 20),\n        'nguyen-3': ('x^5 + x^4 + x^3 + x^2 + x', (-1, 1), 20),\n        'nguyen-4': ('x^6 + x^5 + x^4 + x^3 + x^2 + x', (-1, 1), 20),\n        'nguyen-5': ('sin(x^2)*cos(x) - 1', (-1, 1), 20),\n        'nguyen-6': ('sin(x) + sin(x + x^2)', (-1, 1), 20),\n        'nguyen-7': ('log(x+1) + log(x^2+1)', (0, 2), 20),\n        'nguyen-8': ('sqrt(x)', (0, 4), 20),\n        \n        # Keijzer benchmarks (simpler)\n        'keijzer-1': ('x^3/5 + x^2/2 - x', (-3, 3), 20),\n        'keijzer-4': ('x^3 * exp(-x) * cos(x) * sin(x)', (0, 10), 20),\n        \n        # Simple polynomials\n        'poly-1': ('x^2', (-5, 5), 20),\n        'poly-2': ('x^3 - 2*x', (-3, 3), 20),\n        'poly-3': ('2*x^2 + 3*x + 1', (-5, 5), 20),\n        \n        # Trigonometric\n        'trig-1': ('sin(x)', (-3.14, 3.14), 20),\n        'trig-2': ('cos(x)*sin(x)', (-3.14, 3.14), 20),\n        \n        # Mixed\n        'mixed-1': ('x*sin(x)', (-5, 5), 20),\n        'mixed-2': ('sqrt(x)*log(x+1)', (0.1, 10), 20),\n    }\n    \n    def __init__(self, engine_factory):\n        \"\"\"\n        Args:\n            engine_factory: Function that creates a TensorGeneticEngine instance\n        \"\"\"\n        self.engine_factory = engine_factory\n        self.results: List[BenchmarkResult] = []\n    \n    def generate_data(self, formula: str, x_range: Tuple[float, float], n_points: int) -> Tuple[List[float], List[float]]:\n        \"\"\"Generate x,y data from a formula string.\"\"\"\n        import math\n        \n        x_vals = np.linspace(x_range[0], x_range[1], n_points).tolist()\n        y_vals = []\n        \n        for x in x_vals:\n            try:\n                # Safe eval with math functions\n                y = eval(formula.replace('^', '**'), {\"x\": x, \"sin\": math.sin, \"cos\": math.cos, \n                                                        \"tan\": math.tan, \"exp\": math.exp, \n                                                        \"log\": math.log, \"sqrt\": math.sqrt,\n                                                        \"pi\": math.pi, \"e\": math.e})\n                y_vals.append(float(y))\n            except:\n                y_vals.append(0.0)\n        \n        return x_vals, y_vals\n    \n    def run_benchmark(self, problem_name: str, timeout_sec: float = 10) -> BenchmarkResult:\n        \"\"\"Run a single benchmark problem.\"\"\"\n        if problem_name not in self.PROBLEMS:\n            raise ValueError(f\"Unknown problem: {problem_name}\")\n        \n        formula, x_range, n_points = self.PROBLEMS[problem_name]\n        x_vals, y_vals = self.generate_data(formula, x_range, n_points)\n        \n        engine = self.engine_factory()\n        \n        start_time = time.time()\n        result = engine.run(x_vals, y_vals, [], timeout_sec=timeout_sec)\n        elapsed = time.time() - start_time\n        \n        # Calculate RMSE of found solution\n        rmse = float('inf')\n        if result:\n            try:\n                # Evaluate found formula\n                import math\n                found_y = []\n                for x in x_vals:\n                    try:\n                        y = eval(result.replace('^', '**'), \n                                {\"x\": x, \"x0\": x, \"sin\": math.sin, \"cos\": math.cos,\n                                 \"tan\": math.tan, \"exp\": math.exp, \"log\": math.log, \n                                 \"sqrt\": math.sqrt, \"abs\": abs, \"pi\": math.pi, \"e\": math.e})\n                        found_y.append(float(y))\n                    except:\n                        found_y.append(float('inf'))\n                \n                mse = sum((a-b)**2 for a,b in zip(y_vals, found_y)) / len(y_vals)\n                rmse = mse ** 0.5\n            except:\n                pass\n        \n        # Check exact match (simplified comparison)\n        exact_match = rmse < 1e-6\n        \n        bench_result = BenchmarkResult(\n            problem_name=problem_name,\n            target_formula=formula,\n            found_formula=result,\n            rmse=rmse,\n            exact_match=exact_match,\n            time_seconds=elapsed,\n            generations=0  # Would need to track in engine\n        )\n        \n        self.results.append(bench_result)\n        return bench_result\n    \n    def run_suite(self, problem_names: List[str] = None, timeout_sec: float = 10, \n                  callback=None) -> Dict[str, BenchmarkResult]:\n        \"\"\"\n        Run a suite of benchmark problems.\n        \n        Args:\n            problem_names: List of problems to run (default: all)\n            timeout_sec: Timeout per problem\n            callback: Optional progress callback\n            \n        Returns:\n            Dict mapping problem name to result\n        \"\"\"\n        if problem_names is None:\n            problem_names = list(self.PROBLEMS.keys())\n        \n        results = {}\n        for i, name in enumerate(problem_names):\n            if callback:\n                callback(f\"Running {name} ({i+1}/{len(problem_names)})\")\n            \n            results[name] = self.run_benchmark(name, timeout_sec)\n        \n        return results\n    \n    def get_summary(self) -> Dict:\n        \"\"\"Get summary statistics of benchmark results.\"\"\"\n        if not self.results:\n            return {}\n        \n        n_exact = sum(1 for r in self.results if r.exact_match)\n        avg_rmse = np.mean([r.rmse for r in self.results if r.rmse < float('inf')])\n        avg_time = np.mean([r.time_seconds for r in self.results])\n        \n        return {\n            'n_problems': len(self.results),\n            'n_exact_matches': n_exact,\n            'success_rate': n_exact / len(self.results) * 100,\n            'avg_rmse': avg_rmse,\n            'avg_time_seconds': avg_time,\n        }\n    \n    def print_report(self):\n        \"\"\"Print a formatted benchmark report.\"\"\"\n        print(\"\\n\" + \"=\"*70)\n        print(\"GPU GP ENGINE BENCHMARK REPORT\")\n        print(\"=\"*70)\n        \n        for result in self.results:\n            status = \"\u2713\" if result.exact_match else \"\u2717\"\n            print(f\"\\n{status} {result.problem_name}\")\n            print(f\"  Target: {result.target_formula}\")\n            print(f\"  Found:  {result.found_formula or 'None'}\")\n            print(f\"  RMSE:   {result.rmse:.6e}\")\n            print(f\"  Time:   {result.time_seconds:.2f}s\")\n        \n        summary = self.get_summary()\n        print(\"\\n\" + \"-\"*70)\n        print(f\"SUMMARY: {summary.get('n_exact_matches', 0)}/{summary.get('n_problems', 0)} exact matches\")\n        print(f\"Success Rate: {summary.get('success_rate', 0):.1f}%\")\n        print(f\"Average RMSE: {summary.get('avg_rmse', 0):.6e}\")\n        print(f\"Average Time: {summary.get('avg_time_seconds', 0):.2f}s\")\n        print(\"=\"*70)\n\n\ndef create_benchmark_suite(device=None, pop_size=1000):\n    \"\"\"Factory function to create a benchmark suite.\"\"\"\n    from . import TensorGeneticEngine\n    \n    def factory():\n        return TensorGeneticEngine(device=device, pop_size=pop_size, n_islands=4)\n    \n    return BenchmarkSuite(factory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/gpu/config.py\n",
        "import math\n\nclass GpuGlobals:\n    # ============================================================\n    #                  PAR\u00c1METROS GLOBALES\n    # ============================================================\n\n    # ----------------------------------------\n    # Datos del Problema (Regresi\u00f3n Simb\u00f3lica)\n    # ----------------------------------------\n    USE_LOG_TRANSFORMATION = True\n\n    # ----------------------------------------\n    # Configuraci\u00f3n General del Algoritmo Gen\u00e9tico\n    # ----------------------------------------\n    FORCE_CPU_MODE = False # Si es True, usa CPU aunque CUDA est\u00e9 disponible\n    \n    # Tama\u00f1o de poblaci\u00f3n - M\u00c1XIMO para RTX 3050 (4GB VRAM)\n    POP_SIZE = 5000       # Agresivo - usa ~3GB VRAM\n    GENERATIONS = 100000  # M\u00e1s generaciones\n    NUM_ISLANDS = 8       # M\u00e1xima diversidad\n    MIN_POP_PER_ISLAND = 10\n\n    # --- F\u00f3rmula Inicial ---\n    USE_INITIAL_FORMULA = False\n    INITIAL_FORMULA_STRING = \"log((x1+exp((((((1.28237193+((x0+2.59195138)+8.54688985))*x0)+(log((((x2/-0.99681346)-(x0-8.00219939))/(0.35461932-x2)))+(x0+(88.95319019/((x0+x0)+x0)))))-x1)/((exp(exp(((exp(x2)*(1.39925709/x0))^exp(x0))))+0.76703064)*6.05423753)))))\"\n\n    # ----------------------------------------\n    # Par\u00e1metros del Modelo de Islas\n    # ----------------------------------------\n    MIGRATION_INTERVAL = 100\n    MIGRATION_SIZE = 50\n\n    # ----------------------------------------\n    # Par\u00e1metros de Generaci\u00f3n Inicial de \u00c1rboles\n    # ----------------------------------------\n    MAX_TREE_DEPTH_INITIAL = 8\n    TERMINAL_VS_VARIABLE_PROB = 0.75\n    CONSTANT_MIN_VALUE = -10.0\n    CONSTANT_MAX_VALUE = 10.0\n    CONSTANT_INT_MIN_VALUE = -10\n    CONSTANT_INT_MAX_VALUE = 10\n    USE_HARD_DEPTH_LIMIT = True\n    MAX_TREE_DEPTH_HARD_LIMIT = 30  # M\u00c1XIMO - expresiones muy complejas\n\n    # ----------------------------------------\n    # Par\u00e1metros de Operadores Gen\u00e9ticos (Configuraci\u00f3n de Operadores)\n    # ----------------------------------------\n    USE_OP_PLUS     = True\n    USE_OP_MINUS    = True\n    USE_OP_MULT     = True\n    USE_OP_DIV      = True\n    USE_OP_POW      = True\n    USE_OP_MOD      = False\n    USE_OP_SIN      = False\n    USE_OP_COS      = False\n    USE_OP_LOG      = True\n    USE_OP_EXP      = True\n    USE_OP_FACT     = False\n    USE_OP_FLOOR    = False\n    USE_OP_GAMMA    = True\n    USE_OP_ASIN     = False\n    USE_OP_ACOS     = False\n    USE_OP_ATAN     = False\n\n    # Pesos de Operadores (Order: +, -, *, /, ^, %, s, c, l, e, !, _, g, S, C, T)\n    OPERATOR_WEIGHTS = [\n        0.20 * (1.0 if USE_OP_PLUS else 0.0),\n        0.20 * (1.0 if USE_OP_MINUS else 0.0),\n        0.20 * (1.0 if USE_OP_MULT else 0.0),\n        0.15 * (1.0 if USE_OP_DIV else 0.0),\n        0.10 * (1.0 if USE_OP_POW else 0.0),\n        0.02 * (1.0 if USE_OP_MOD else 0.0),\n        0.10 * (1.0 if USE_OP_SIN else 0.0),\n        0.10 * (1.0 if USE_OP_COS else 0.0),\n        0.05 * (1.0 if USE_OP_LOG else 0.0),\n        0.05 * (1.0 if USE_OP_EXP else 0.0),\n        0.01 * (1.0 if USE_OP_FACT else 0.0),\n        0.01 * (1.0 if USE_OP_FLOOR else 0.0),\n        0.01 * (1.0 if USE_OP_GAMMA else 0.0),\n        0.01 * (1.0 if USE_OP_ASIN else 0.0),\n        0.01 * (1.0 if USE_OP_ACOS else 0.0),\n        0.01 * (1.0 if USE_OP_ATAN else 0.0)\n    ]\n\n    # ----------------------------------------\n    # Par\u00e1metros de Operadores Gen\u00e9ticos (Mutaci\u00f3n, Cruce, Selecci\u00f3n)\n    # ----------------------------------------\n    BASE_MUTATION_RATE = 0.30\n    BASE_ELITE_PERCENTAGE = 0.15\n    DEFAULT_CROSSOVER_RATE = 0.60\n    DEFAULT_TOURNAMENT_SIZE = 4\n    MAX_TREE_DEPTH_MUTATION = 8\n    MUTATE_INSERT_CONST_PROB = 0.6\n    MUTATE_INSERT_CONST_INT_MIN = 1\n    MUTATE_INSERT_CONST_INT_MAX = 5\n    MUTATE_INSERT_CONST_FLOAT_MIN = 0.5\n    MUTATE_INSERT_CONST_FLOAT_MAX = 5.0\n\n    # ----------------------------------------\n    # Par\u00e1metros de Fitness y Evaluaci\u00f3n\n    # ----------------------------------------\n    COMPLEXITY_PENALTY = 0.01\n    USE_RMSE_FITNESS = True\n    FITNESS_ORIGINAL_POWER = 1.3\n    FITNESS_PRECISION_THRESHOLD = 0.001\n    FITNESS_PRECISION_BONUS = 0.0001\n    FITNESS_EQUALITY_TOLERANCE = 1e-9\n    EXACT_SOLUTION_THRESHOLD = 1e-8\n\n    # ----------------------------------------\n    # Fitness Ponderado (Weighted Fitness)\n    # ----------------------------------------\n    USE_WEIGHTED_FITNESS = False\n    WEIGHTED_FITNESS_EXPONENT = 0.25\n\n    # ----------------------------------------\n    # Par\u00e1metros de Caracter\u00edsticas Avanzadas\n    # ----------------------------------------\n    STAGNATION_LIMIT = 50\n    GLOBAL_STAGNATION_LIMIT = 100\n    STAGNATION_RANDOM_INJECT_PERCENT = 0.1\n    PARAM_MUTATE_INTERVAL = 50\n    PATTERN_RECORD_FITNESS_THRESHOLD = 10.0\n    PATTERN_MEM_MIN_USES = 3\n    PATTERN_INJECT_INTERVAL = 10\n    PATTERN_INJECT_PERCENT = 0.05\n    PARETO_MAX_FRONT_SIZE = 50\n    \n    SIMPLIFY_NEAR_ZERO_TOLERANCE = 1e-9\n    SIMPLIFY_NEAR_ONE_TOLERANCE = 1e-9\n    LOCAL_SEARCH_ATTEMPTS = 30\n    \n    USE_SIMPLIFICATION = True\n    USE_ISLAND_CATACLYSM = True\n    USE_LEXICASE_SELECTION = True\n    USE_PARETO_SELECTION = True  # NSGA-II multi-objective (error vs complexity)\n    USE_WEIGHTED_FITNESS = False  # Enable to weight fitness cases (e.g., by difficulty)\n\n    # ----------------------------------------\n    # Otros Par\u00e1metros\n    # ----------------------------------------\n    PROGRESS_REPORT_INTERVAL = 100\n    FORCE_INTEGER_CONSTANTS = False\n    \n    # Control de Duplicados\n    PREVENT_DUPLICATES = True\n    DUPLICATE_RETRIES = 10\n    INF = float('inf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/gpu/engine.py\n",
        "\nimport torch\nimport numpy as np\nimport time\nfrom typing import List, Tuple, Optional\nfrom core.grammar import OPERATORS, VARIABLES, CONSTANTS, ExpressionTree\nfrom .formatting import format_const\nfrom .sniper import Sniper\nfrom .config import GpuGlobals\nfrom .pareto import ParetoOptimizer\nfrom .pattern_memory import PatternMemory\n\n# SymPy for simplification\ntry:\n    import sympy\n    from sympy import symbols, sympify, simplify, nsimplify, Float\n    from sympy.parsing.sympy_parser import parse_expr\n    SYMPY_AVAILABLE = True\nexcept ImportError:\n    SYMPY_AVAILABLE = False\n\n# --- GPU GRAMMAR ENCODING (RPN / Postfix) ---\nPAD_ID = 0\n\nclass GPUGrammar:\n    def __init__(self, num_variables=1):\n        self.token_to_id = {'<PAD>': PAD_ID}\n        self.id_to_token = {PAD_ID: '<PAD>'}\n        self.next_id = 1\n        \n        # Terminals (Variables + Constants)\n        self.active_variables = ['x0'] # Always support x0\n        if num_variables > 1:\n            self.active_variables = [f'x{i}' for i in range(num_variables)]\n        elif num_variables == 1:\n            self.active_variables = ['x', 'x0'] \n\n        self.terminals = self.active_variables + ['C', '1', '2', '3', '5'] # Removed pi, e to avoid collision\n        for t in self.terminals:\n            self.token_to_id[t] = self.next_id\n            self.id_to_token[self.next_id] = t\n            self.next_id += 1\n            \n        # Operators\n        self.operators = []\n        if GpuGlobals.USE_OP_PLUS:  self.operators.append('+')\n        if GpuGlobals.USE_OP_MINUS: self.operators.append('-')\n        if GpuGlobals.USE_OP_MULT:  self.operators.append('*')\n        if GpuGlobals.USE_OP_DIV:   self.operators.append('/')\n        if GpuGlobals.USE_OP_POW:   self.operators.append('pow')\n        if GpuGlobals.USE_OP_MOD:   self.operators.append('%')\n        if GpuGlobals.USE_OP_SIN:   self.operators.append('sin')\n        if GpuGlobals.USE_OP_COS:   self.operators.append('cos')\n        if GpuGlobals.USE_OP_LOG:   self.operators.append('log')\n        if GpuGlobals.USE_OP_EXP:   self.operators.append('e')\n        if GpuGlobals.USE_OP_FACT:  self.operators.append('!') # tgamma\n        # if GpuGlobals.USE_OP_FLOOR: self.operators.append('_') # Not mapped in default?\n        if GpuGlobals.USE_OP_GAMMA: self.operators.append('g')\n        if GpuGlobals.USE_OP_ASIN:  self.operators.append('S')\n        if GpuGlobals.USE_OP_ACOS:  self.operators.append('C')\n        if GpuGlobals.USE_OP_ATAN:  self.operators.append('T')\n        \n        # Always active standard ops? Or add globals for them?\n        # Assuming these are always available or tracked by globals?\n        # Globals.h doesn't seem to have toggles for sqrt/abs/neg explicitly in the list I saw?\n        # Wait, I saw USE_OP_SIN, etc.\n        # I'll add them unconditionally for now or check globals?\n        # Globals.h doesn't list sqrt/abs/neg toggles. So they are likely always on or implicit.\n        self.operators.append('sqrt')\n        self.operators.append('abs')\n        self.operators.append('neg')\n        self.operators.append('_') # Floor, adding it back since I saw it in C++ kernel logic!\n\n        for op in self.operators:\n            self.token_to_id[op] = self.next_id\n            self.id_to_token[self.next_id] = op\n            self.next_id += 1\n            \n        self.vocab_size = self.next_id\n        \n        self.op_ids = {op: self.token_to_id[op] for op in self.operators}\n        self.token_arity = {}\n        for op in self.operators:\n            tid = self.token_to_id[op]\n            self.token_arity[op] = OPERATORS[op] \n            \n    def get_subtree_span(self, rpn_ids: List[int], root_idx: int) -> Tuple[int, int]:\n        \"\"\"\n        Finds the span (start_idx, end_idx) of the subtree rooted at root_idx in RPN.\n        Scanning backwards from root_idx.\n        Returns indices inclusive [start, end].\n        \"\"\"\n        if root_idx < 0 or root_idx >= len(rpn_ids): return (-1, -1)\n        \n        # Get Arity of root\n        root_id = rpn_ids[root_idx]\n        if root_id == PAD_ID: return (root_idx, root_idx)\n        \n        token = self.id_to_token.get(root_id, \"\")\n        required_args = self.token_arity.get(token, 0)\n        \n        current_idx = root_idx - 1\n        for _ in range(required_args):\n            start, _ = self.get_subtree_span(rpn_ids, current_idx)\n            if start == -1: return (-1, -1) # Error\n            current_idx = start - 1\n            \n        return (current_idx + 1, root_idx)\n\nclass TensorGeneticEngine:\n    def __init__(self, device=None, pop_size=None, max_len=30, num_variables=1, max_constants=5, n_islands=None):\n        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # Defaults from Globals\n        if pop_size is None: pop_size = GpuGlobals.POP_SIZE\n        if n_islands is None: n_islands = GpuGlobals.NUM_ISLANDS\n        \n        self.grammar = GPUGrammar(num_variables)\n        \n        self.n_islands = n_islands\n        if pop_size % n_islands != 0:\n            pop_size = (pop_size // n_islands) * n_islands\n            \n        self.pop_size = pop_size\n        self.island_size = pop_size // n_islands\n        self.max_len = max_len\n        self.num_variables = num_variables\n        self.max_constants = max_constants\n        \n        # Pre-allocate memory for random generation\n        self.terminal_ids = torch.tensor([self.grammar.token_to_id[t] for t in self.grammar.terminals], device=self.device)\n        self.operator_ids = torch.tensor([self.grammar.token_to_id[op] for op in self.grammar.operators], device=self.device)\n        \n        # --- Pre-compute Arity Masks for Safe Mutation ---\n        self.token_arity = torch.zeros(self.grammar.vocab_size + 1, dtype=torch.long, device=self.device)\n        self.arity_0_ids = []\n        self.arity_1_ids = []\n        self.arity_2_ids = []\n        \n        # Terminals (0)\n        # Note: self.grammar.terminals is fixed in grammar class currently.\n        # Ideally Grammar should also take GpuGlobals into account.\n        for t in self.grammar.terminals:\n            tid = self.grammar.token_to_id[t]\n            self.token_arity[tid] = 0\n            self.arity_0_ids.append(tid)\n            \n        # Operators (1 or 2)\n        for op in self.grammar.operators:\n            tid = self.grammar.token_to_id[op]\n            arity = OPERATORS[op]\n            self.token_arity[tid] = arity\n            if arity == 1: self.arity_1_ids.append(tid)\n            elif arity == 2: self.arity_2_ids.append(tid)\n            \n        self.arity_0_ids = torch.tensor(self.arity_0_ids, device=self.device)\n        self.arity_1_ids = torch.tensor(self.arity_1_ids, device=self.device)\n        self.arity_2_ids = torch.tensor(self.arity_2_ids, device=self.device)\n        \n        # The Sniper\n        self.sniper = Sniper(self.device)\n        \n        # Pareto Optimizer (NSGA-II)\n        self.pareto = ParetoOptimizer(self.device, GpuGlobals.PARETO_MAX_FRONT_SIZE)\n        \n        # Pattern Memory\n        self.pattern_memory = PatternMemory(\n            self.device, \n            max_patterns=100,\n            fitness_threshold=GpuGlobals.PATTERN_RECORD_FITNESS_THRESHOLD,\n            min_uses=GpuGlobals.PATTERN_MEM_MIN_USES\n        )\n\n\n    def optimize_constants(self, population: torch.Tensor, constants: torch.Tensor, x: torch.Tensor, y_target: torch.Tensor, steps=10, lr=0.1):\n        \"\"\"\n        Refine constants using Gradient Descent (Adam).\n        Returns: (best_constants, best_mse)\n        \"\"\"\n        # Optimize a COPY of constants\n        optimized_consts = constants.clone().detach().requires_grad_(True)\n        optimizer = torch.optim.Adam([optimized_consts], lr=lr)\n        \n        # Track best found during steps (in case it diverges)\n        best_mse = torch.full((population.shape[0],), float('inf'), device=self.device, dtype=torch.float64)\n        best_consts = constants.clone().detach() \n        \n        for _ in range(steps):\n            optimizer.zero_grad()\n            \n            # Forward pass (differentiable if we implemented soft operations, \n            # but standard ops are differentiable in PyTorch!)\n            # Evaluator returns RMSE, but we want MSE for gradients usually, or just minimize RMSE.\n            # evaluate_batch returns RMSE [PopSize].\n            # Problem: evaluate_batch uses scatter_ (in-place) which might break gradients if not careful.\n            # However, for simple constant optimization, we might need a \"soft\" stack or ignore in-place issues if PyTorch handles them.\n            # Let's try standard evaluate_batch. If scatter breaks, we might need a rewriting.\n            # ACTUALLY: scatter_ IS differentiable for values, but not indices (indices are fixed by RPN).\n            # So this SHOULD work.\n            \n            rmse = self.evaluate_batch(population, x, y_target, optimized_consts)\n            \n            # Loss = Sum of RMSEs (to optimize all in parallel)\n            # We filter NaNs\n            valid_mask = ~torch.isnan(rmse)\n            if not valid_mask.any(): break\n            \n            # Update bests\n            current_mse = rmse**2 # Approximation since we returned RMSE\n            improved = (current_mse < best_mse) & valid_mask\n            if improved.any():\n                best_mse[improved] = current_mse[improved].detach()\n                best_consts[improved] = optimized_consts[improved].detach()\n            \n            loss = rmse[valid_mask].sum()\n            \n            if not loss.requires_grad: \n                # This happens if formula has no 'C' or operations detach graph\n                break\n                \n            loss.backward()\n            optimizer.step()\n            \n        return best_consts, torch.sqrt(best_mse)\n\n    def local_search(self, population: torch.Tensor, constants: torch.Tensor, \n                     x: torch.Tensor, y: torch.Tensor, \n                     top_k: int = 10, attempts: int = None) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Hill climbing: try single-token mutations on top individuals, keep improvements.\n        \n        Args:\n            population: [PopSize, L] RPN tensors\n            constants: [PopSize, MaxC] constants\n            x: Input data\n            y: Target data  \n            top_k: Number of top individuals to apply local search\n            attempts: Number of mutation attempts per individual (default: LOCAL_SEARCH_ATTEMPTS)\n        \n        Returns:\n            (improved_population, improved_constants)\n        \"\"\"\n        if attempts is None:\n            attempts = GpuGlobals.LOCAL_SEARCH_ATTEMPTS\n        \n        pop_out = population.clone()\n        const_out = constants.clone()\n        \n        # Get top K individuals by fitness\n        fitness = self.evaluate_batch(population, x, y, constants)\n        _, top_idx = torch.topk(fitness, top_k, largest=False)\n        \n        for idx in top_idx:\n            idx = idx.item()\n            current_rpn = population[idx:idx+1]\n            current_const = constants[idx:idx+1]\n            current_fit = fitness[idx].item()\n            \n            best_rpn = current_rpn.clone()\n            best_const = current_const.clone()\n            best_fit = current_fit\n            \n            # Try random single-token mutations\n            for _ in range(attempts):\n                # Mutate with high rate (1 token expected change)\n                mutant = self.mutate_population(current_rpn, mutation_rate=0.15)\n                \n                # Evaluate mutant\n                mutant_fit = self.evaluate_batch(mutant, x, y, current_const)[0].item()\n                \n                if mutant_fit < best_fit:\n                    best_rpn = mutant.clone()\n                    best_fit = mutant_fit\n            \n            # Update if improved\n            if best_fit < current_fit:\n                pop_out[idx] = best_rpn[0]\n                # Also optimize constants for the improved individual\n                opt_const, _ = self.optimize_constants(best_rpn, best_const, x, y, steps=5)\n                const_out[idx] = opt_const[0]\n        \n        return pop_out, const_out\n\n    def simplify_expression(self, rpn_tensor: torch.Tensor, constants: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, bool]:\n        \"\"\"\n        Simplify an RPN expression using SymPy.\n        \n        Args:\n            rpn_tensor: [L] tensor of token IDs (single individual)\n            constants: [MaxC] tensor of constant values\n        \n        Returns:\n            (simplified_rpn, new_constants, success)\n        \"\"\"\n        if not SYMPY_AVAILABLE or not GpuGlobals.USE_SIMPLIFICATION:\n            return rpn_tensor, constants, False\n        \n        try:\n            # 1. Convert RPN to infix string\n            infix = self.rpn_to_infix(rpn_tensor, constants)\n            if infix == \"Invalid\" or not infix:\n                return rpn_tensor, constants, False\n            \n            # 2. Prepare SymPy symbols\n            sym_vars = {f'x{i}': symbols(f'x{i}') for i in range(self.num_variables)}\n            sym_vars['x'] = sym_vars.get('x0', symbols('x0'))  # Alias\n            \n            # 3. Parse to SymPy (handle operator conversions)\n            expr_str = infix\n            expr_str = expr_str.replace('^', '**')  # Power\n            expr_str = expr_str.replace('lgamma', 'loggamma')\n            \n            # Try parsing\n            try:\n                expr = parse_expr(expr_str, local_dict=sym_vars)\n            except:\n                # Fallback to sympify\n                expr = sympify(expr_str, locals=sym_vars)\n            \n            # 4. Simplify\n            simplified = simplify(expr)\n            \n            # 5. Rationalize constants (e.g., 0.5 -> 1/2)\n            simplified = nsimplify(simplified, tolerance=1e-6, rational=True)\n            \n            # 6. Convert back to infix string\n            simplified_str = str(simplified)\n            \n            # 7. If simplification made it longer, abort\n            if len(simplified_str) > len(infix) * 1.5:\n                return rpn_tensor, constants, False\n            \n            # 8. Convert to our format (** -> ^, etc.)\n            simplified_str = simplified_str.replace('**', ' ^ ')\n            simplified_str = simplified_str.replace('loggamma', 'lgamma')\n            \n            # 9. Convert back to RPN\n            new_rpn = self.infix_to_rpn([simplified_str])\n            if new_rpn.shape[0] == 0 or (new_rpn[0] == PAD_ID).all():\n                return rpn_tensor, constants, False\n            \n            # 10. Extract new constants from simplified expression\n            # For now, we initialize with zeros (optimizer will refine)\n            new_consts = torch.zeros(self.max_constants, device=self.device, dtype=torch.float64)\n            \n            # Count 'C' tokens in new RPN\n            id_C = self.grammar.token_to_id.get('C', -1)\n            n_consts = (new_rpn[0] == id_C).sum().item()\n            \n            # Try to extract numeric constants from simplified_str\n            import re\n            numbers = re.findall(r'[-+]?\\d*\\.?\\d+', simplified_str)\n            for i, num in enumerate(numbers[:min(n_consts, self.max_constants)]):\n                try:\n                    new_consts[i] = float(num)\n                except:\n                    pass\n            \n            return new_rpn[0], new_consts, True\n            \n        except Exception as e:\n            # Simplification failed, return original\n            return rpn_tensor, constants, False\n\n    def simplify_population(self, population: torch.Tensor, constants: torch.Tensor, top_k: int = None) -> Tuple[torch.Tensor, torch.Tensor, int]:\n        \"\"\"\n        Simplify top K individuals in the population.\n        \n        Args:\n            population: [PopSize, L] RPN tensors\n            constants: [PopSize, MaxC] constant tensors\n            top_k: Number of individuals to simplify (default: 10% of population)\n        \n        Returns:\n            (new_population, new_constants, n_simplified)\n        \"\"\"\n        if not SYMPY_AVAILABLE or not GpuGlobals.USE_SIMPLIFICATION:\n            return population, constants, 0\n        \n        if top_k is None:\n            top_k = max(1, int(population.shape[0] * 0.1))\n        \n        n_simplified = 0\n        pop_out = population.clone()\n        const_out = constants.clone()\n        \n        for i in range(min(top_k, population.shape[0])):\n            new_rpn, new_consts, success = self.simplify_expression(population[i], constants[i])\n            if success:\n                pop_out[i] = new_rpn\n                const_out[i] = new_consts\n                n_simplified += 1\n        \n        return pop_out, const_out, n_simplified\n\n    def migrate_islands(self, population: torch.Tensor, constants: torch.Tensor, fitness: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Perform ring migration between islands.\n        \n        Top MIGRATION_SIZE individuals from each island migrate to the next island (ring topology),\n        replacing the worst individuals in the destination.\n        \n        Args:\n            population: [PopSize, L] RPN tensors\n            constants: [PopSize, MaxC] constant tensors\n            fitness: [PopSize] fitness scores (lower is better)\n        \n        Returns:\n            (new_population, new_constants)\n        \"\"\"\n        if self.n_islands <= 1:\n            return population, constants\n        \n        pop_out = population.clone()\n        const_out = constants.clone()\n        \n        island_size = self.island_size\n        mig_size = min(GpuGlobals.MIGRATION_SIZE, island_size // 2)  # Don't migrate more than half\n        \n        for island in range(self.n_islands):\n            # Source island\n            src_start = island * island_size\n            src_end = src_start + island_size\n            \n            # Destination island (ring: island+1 mod n_islands)\n            dst_island = (island + 1) % self.n_islands\n            dst_start = dst_island * island_size\n            dst_end = dst_start + island_size\n            \n            # Get fitness for source island\n            src_fitness = fitness[src_start:src_end]\n            \n            # Get indices of best individuals in source (lowest fitness)\n            _, best_idx_local = torch.topk(src_fitness, mig_size, largest=False)\n            best_idx_global = best_idx_local + src_start\n            \n            # Get fitness for destination island\n            dst_fitness = fitness[dst_start:dst_end]\n            \n            # Get indices of worst individuals in destination (highest fitness)\n            _, worst_idx_local = torch.topk(dst_fitness, mig_size, largest=True)\n            worst_idx_global = worst_idx_local + dst_start\n            \n            # Migrate: replace worst in destination with best from source\n            pop_out[worst_idx_global] = population[best_idx_global]\n            const_out[worst_idx_global] = constants[best_idx_global]\n        \n        return pop_out, const_out\n\n    def deduplicate_population(self, population: torch.Tensor, constants: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, int]:\n        \"\"\"\n        Remove duplicate individuals from the population.\n        \n        Duplicates are identified by hashing their RPN token sequence.\n        Duplicates are replaced with **FRESH RANDOM INDIVIDUALS** (not mutated clones).\n        \n        Args:\n            population: [PopSize, L] RPN tensors\n            constants: [PopSize, MaxC] constant tensors\n        \n        Returns:\n            (new_population, new_constants, n_replaced)\n        \"\"\"\n        if not GpuGlobals.PREVENT_DUPLICATES:\n            return population, constants, 0\n        \n        pop_size = population.shape[0]\n        pop_cpu = population.cpu().numpy()\n        \n        # Hash each individual\n        seen_hashes = {}\n        duplicate_indices = []\n        \n        for i in range(pop_size):\n            # Create hash from non-padding tokens\n            tokens = pop_cpu[i]\n            non_pad = tokens[tokens != PAD_ID]\n            hash_key = tuple(non_pad.tolist())\n            \n            if hash_key in seen_hashes:\n                duplicate_indices.append(i)\n            else:\n                seen_hashes[hash_key] = i\n        \n        n_dups = len(duplicate_indices)\n        if n_dups == 0:\n            return population, constants, 0\n        \n        # Replace duplicates with fresh random individuals\n        pop_out = population.clone()\n        const_out = constants.clone()\n        \n        # Generate N fresh random trees\n        fresh_pop = self._generate_random_population(n_dups)\n        fresh_consts = torch.randn(n_dups, constants.shape[1], device=self.device, dtype=torch.float64)\n        \n        # Assign to duplicate slots\n        # duplicate_indices is a list, convert to tensor?\n        # Actually indexing with list works in PyTorch if converted to tensor or list.\n        # But `fresh_pop` is [n_dups, L].\n        pop_out[duplicate_indices] = fresh_pop\n        const_out[duplicate_indices] = fresh_consts\n        \n        return pop_out, const_out, n_dups\n\n    def tarpeian_control(self, population: torch.Tensor, fitness: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Tarpeian bloat control: randomly penalize oversized individuals.\n        \n        Individuals longer than 1.5x average length have 50% chance of \n        receiving very bad fitness, pushing them out of selection.\n        \n        Args:\n            population: [PopSize, L] RPN tensors\n            fitness: [PopSize] current fitness values\n            \n        Returns:\n            Modified fitness tensor\n        \"\"\"\n        lengths = (population != PAD_ID).sum(dim=1).float()\n        avg_len = lengths.mean()\n        \n        # Find oversized individuals (> 1.5x average)\n        oversized = lengths > avg_len * 1.5\n        \n        # Randomly penalize 50% of oversized\n        random_mask = torch.rand(population.shape[0], device=self.device) < 0.5\n        penalize_mask = oversized & random_mask\n        \n        # Apply penalty\n        fitness_out = fitness.clone()\n        fitness_out[penalize_mask] = 1e30  # Large but within float32 range\n        \n        return fitness_out\n\n    def shrink_mutation(self, individual: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Apply shrinking mutation - removes a subtree and replaces with terminal.\n        \"\"\"\n        ind_cpu = individual.cpu().numpy()\n        non_pad = ind_cpu[ind_cpu != PAD_ID]\n        \n        if len(non_pad) < 3:\n            return individual\n        \n        # Pick a random operator position\n        operator_positions = []\n        for i, token_id in enumerate(non_pad):\n            token = self.grammar.id_to_token.get(token_id, \"\")\n            if token in self.grammar.operators:\n                operator_positions.append(i)\n        \n        if not operator_positions:\n            return individual\n        \n        target_pos = np.random.choice(operator_positions)\n        span = self.grammar.get_subtree_span(non_pad.tolist(), target_pos)\n        \n        if span[0] == -1:\n            return individual\n        \n        # Replace subtree with random terminal\n        terminal_id = self.terminal_ids[torch.randint(len(self.terminal_ids), (1,))].item()\n        new_tokens = list(non_pad[:span[0]]) + [terminal_id] + list(non_pad[target_pos+1:])\n        new_tokens = new_tokens[:self.max_len]\n        new_tokens = new_tokens + [PAD_ID] * (self.max_len - len(new_tokens))\n        \n        return torch.tensor(new_tokens, device=self.device, dtype=individual.dtype)\n\n\n    def mutate_population(self, population: torch.Tensor, mutation_rate: float) -> torch.Tensor:\n        \"\"\"\n        Performs arity-safe mutation on the population.\n        \"\"\"\n        # Create mutation mask\n        mask = torch.rand_like(population, dtype=torch.float32) < mutation_rate\n        # Don't mutate padding\n        mask = mask & (population != PAD_ID)\n        \n        # We need to know arity of current tokens to replace them with same arity\n        # self.token_arity has shape [VocabSize+1]\n        # Gather arity for each token in population\n        current_arities = self.token_arity[population]\n        \n        # New Reference:\n        # Arity 0 -> Sample from arity_0_ids\n        # Arity 1 -> Sample from arity_1_ids\n        # Arity 2 -> Sample from arity_2_ids\n        \n        # We can prepare 3 tensors of random replacements, one for each arity type, same shape as pop\n        # Ideally only generate for needed spots, but fully generating is easier for vectorized code.\n        \n        # Random replacements for Arity 0\n        if len(self.arity_0_ids) > 0:\n            rand_idx_0 = torch.randint(0, len(self.arity_0_ids), population.shape, device=self.device)\n            replacements_0 = self.arity_0_ids[rand_idx_0]\n        else:\n            replacements_0 = population\n            \n        # Random replacements for Arity 1\n        if len(self.arity_1_ids) > 0:\n             rand_idx_1 = torch.randint(0, len(self.arity_1_ids), population.shape, device=self.device)\n             replacements_1 = self.arity_1_ids[rand_idx_1]\n        else:\n             replacements_1 = population\n\n        # Random replacements for Arity 2\n        if len(self.arity_2_ids) > 0:\n             rand_idx_2 = torch.randint(0, len(self.arity_2_ids), population.shape, device=self.device)\n             replacements_2 = self.arity_2_ids[rand_idx_2]\n        else:\n             replacements_2 = population\n             \n        # Apply Logic\n        mutated_pop = population.clone()\n        \n        # Mask for Arity 0 mutations\n        mask_0 = mask & (current_arities == 0)\n        mutated_pop = torch.where(mask_0, replacements_0, mutated_pop)\n        \n        # Mask for Arity 1 mutations\n        mask_1 = mask & (current_arities == 1)\n        mutated_pop = torch.where(mask_1, replacements_1, mutated_pop)\n        \n        # Mask for Arity 2 mutations\n        mask_2 = mask & (current_arities == 2)\n        mutated_pop = torch.where(mask_2, replacements_2, mutated_pop)\n        \n        return mutated_pop\n\n    def _get_subtree_ranges(self, population: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Calculates the start index of the subtree ending at each position.\n        Returns tensor [B, L] where value is start_index, or -1 if invalid/padding.\n        Optimized for RPN logic on GPU.\n        \"\"\"\n        B, L = population.shape\n        subtree_starts = torch.full((B, L), -1, device=self.device, dtype=torch.long)\n        \n        # 1. Map tokens to Arity Change\n        # Variables/Consts: +1\n        # Binary: -1 (Pop 2 Push 1 -> Net -1)\n        # Unary: 0 (Pop 1 Push 1 -> Net 0)\n        \n        # We need a fast lookup.\n        # Construct Arity Table\n        # Default 1 (Operand)\n        arities = torch.ones_like(population, dtype=torch.long) \n        \n        # Binary (-1)\n        op_add = self.grammar.token_to_id.get('+', -100); op_sub = self.grammar.token_to_id.get('-', -100)\n        op_mul = self.grammar.token_to_id.get('*', -100); op_div = self.grammar.token_to_id.get('/', -100)\n        op_pow = self.grammar.token_to_id.get('pow', -100); op_mod = self.grammar.token_to_id.get('%', -100)\n        \n        mask_bin = (population == op_add) | (population == op_sub) | (population == op_mul) | \\\n                   (population == op_div) | (population == op_pow) | (population == op_mod)\n        arities[mask_bin] = -1\n        \n        # Unary (0)\n        op_sin = self.grammar.token_to_id.get('sin', -100); op_cos = self.grammar.token_to_id.get('cos', -100)\n        # ... (add all unary)\n        # Simplified: If it's not binary and it's an operator, it's unary?\n        # Better: Assume all Ops are <= some ID? No.\n        # Explicit list is safer.\n        unary_tokens = ['sin','cos','tan','S','C','T','e','log','sqrt','abs','neg','!','_','g']\n        unary_ids = [self.grammar.token_to_id.get(t, -999) for t in unary_tokens]\n        # Make tensor of unary ids\n        # Ideally this table is precomputed. For now, on the fly is ok.\n        \n        mask_unary = torch.zeros_like(population, dtype=torch.bool)\n        for uid in unary_ids:\n             mask_unary = mask_unary | (population == uid)\n        arities[mask_unary] = 0\n        \n        # Padding: -999 (Invalid)\n        arities[population == PAD_ID] = -999\n        \n        # 2. To find start of subtree ending at 'end', we scan backwards until cumulative sum is +1.\n        # Since scanning backwards is hard in vector, we can scan loops?\n        # Max depth isn't huge.\n        \n        # Alternatively: \"Stack Depth at step i\".\n        # Subtree at 'end' corresponds to the interval [start, end] where\n        # Depth(start-1) = D\n        # Depth(end) = D + 1\n        # And min_depth(start...end) >= D\n        \n        # Let's compute cumulative sum (Stack Depth Profile)\n        # cum_arity[i] is depth AFTER processing token i.\n        # arities for this: PAD=0? No, PAD breaks it.\n        # Let's mask PAD for cumsum.\n        \n        safe_arities = arities.clone()\n        safe_arities[population == PAD_ID] = 0\n        depths = torch.cumsum(safe_arities, dim=1)\n        \n        # The scan logic is still tricky O(L^2) across batch.\n        # For L=30, iterating i from 0 to L is fast.\n        \n        for i in range(L):\n            # If position i is PAD, skip\n            is_pad = (population[:, i] == PAD_ID)\n            \n            # We want to find 'start' such that sum(arities[start...i]) == 1\n            # Which means depths[i] - depths[start-1] == 1\n            # implies depths[start-1] = depths[i] - 1.\n            # And for all k in start...i, depths[k] >= depths[start-1] (validity).\n            \n            target_depth = depths[:, i] - 1\n            \n            # Search backwards from i\n            # We can vectorize this search over B by iterating j downwards\n            current_start = torch.full((B,), -1, device=self.device, dtype=torch.long)\n            found = torch.zeros(B, dtype=torch.bool, device=self.device)\n            \n            # Optimization: Pre-calculate validity masks?\n            # Brute force backwards for L=30 is fine.\n            for j in range(i, -1, -1):\n                # Check condition for batch\n                # d[j-1] == target?\n                # Actually, depth[j-1] is depth BEFORE processing j.\n                # If j=0, depth[-1] = 0.\n                \n                prev_depth = depths[:, j-1] if j > 0 else torch.zeros(B, device=self.device)\n                \n                # Match condition: prev_depth == target_depth\n                match = (prev_depth == target_depth)\n                \n                # Check if we violated lower bound in between?\n                # Implicitly, if we hit match first time going backwards, it's the minimal subtree.\n                # We update 'found' mask.\n                \n                new_found = match & (~found)\n                current_start[new_found] = j\n                found = found | new_found\n            \n            # Store valid starts for this end position 'i'\n            # Only valid if not PAD and found\n            valid_i = (~is_pad) & found\n            subtree_starts[valid_i, i] = current_start[valid_i]\n            \n        return subtree_starts\n\n    def crossover_population(self, parents: torch.Tensor, crossover_rate: float) -> torch.Tensor:\n        \"\"\"\n        Performs subtree crossover on the population (Two-Child Crossover).\n        parents: [PopSize, L] (assumed valid RPNs)\n        Uses GPU to find subtrees, CPU to splice (faster than irregular GPU scatter).\n        \"\"\"\n        pop_size, length = parents.shape\n        # We process pairs. Shuffle.\n        indices = torch.randperm(pop_size, device=self.device)\n        \n        # Number of crossover operations\n        n_pairs = int(pop_size * 0.5 * crossover_rate)\n        if n_pairs == 0:\n            return parents.clone()\n        \n        # Get ranges for ALL (Parallel GPU Scan)\n        subtree_starts = self._get_subtree_ranges(parents)\n        \n        # Move relevant data to CPU for splicing\n        parents_cpu = parents.detach().cpu().numpy()\n        starts_cpu = subtree_starts.detach().cpu().numpy()\n        indices_cpu = indices.detach().cpu().numpy()\n        \n        offspring_cpu = parents_cpu.copy()\n        \n        # Create batches of crossover\n        MAX_LEN = self.max_len\n        PAD = PAD_ID\n        \n        for k in range(n_pairs):\n            idx_a = indices_cpu[2*k]\n            idx_b = indices_cpu[2*k+1]\n            \n            pA = parents_cpu[idx_a]\n            pB = parents_cpu[idx_b]\n            starts_A = starts_cpu[idx_a]\n            starts_B = starts_cpu[idx_b]\n            \n            # Valid root points are where starts_X != -1\n            cand_A = np.where(starts_A != -1)[0]\n            cand_B = np.where(starts_B != -1)[0]\n            \n            if len(cand_A) == 0 or len(cand_B) == 0: continue\n            \n            # Pick random crossover points\n            end_A = np.random.choice(cand_A)\n            end_B = np.random.choice(cand_B)\n            \n            start_A = starts_A[end_A]\n            start_B = starts_B[end_B]\n            \n            # --- Child 1: A takes B ---\n            # New A = A[:startA] + B[startB:endB+1] + A[endA+1:]\n            \n            part1 = pA[:start_A]\n            part2 = pB[start_B : end_B+1]\n            part3 = pA[end_A+1:]\n            \n            new_gene_a = np.concatenate([part1, part2, part3])\n            \n            # Validate Length A\n            valid_len_a = len(new_gene_a)\n            if valid_len_a > MAX_LEN:\n                # Check real length (last non-pad index)\n                non_pad = np.where(new_gene_a != PAD)[0]\n                if len(non_pad) == 0: real_len = 0\n                else: real_len = non_pad[-1] + 1\n                \n                if real_len <= MAX_LEN:\n                    # Fits if truncated\n                    truncated = np.full(MAX_LEN, PAD, dtype=pA.dtype)\n                    truncated[:real_len] = new_gene_a[:real_len]\n                    offspring_cpu[idx_a] = truncated\n            else:\n                # Fits, need to pad\n                padded = np.full(MAX_LEN, PAD, dtype=pA.dtype)\n                padded[:valid_len_a] = new_gene_a\n                offspring_cpu[idx_a] = padded\n\n            # --- Child 2: B takes A ---\n            # New B = B[:startB] + A[startA:endA+1] + B[endB+1:]\n            \n            part1_b = pB[:start_B]\n            part2_b = pA[start_A : end_A+1]\n            part3_b = pB[end_B+1:]\n            \n            new_gene_b = np.concatenate([part1_b, part2_b, part3_b])\n            \n            # Validate Length B\n            valid_len_b = len(new_gene_b)\n            if valid_len_b > MAX_LEN:\n                # Check real length\n                non_pad = np.where(new_gene_b != PAD)[0]\n                if len(non_pad) == 0: real_len = 0\n                else: real_len = non_pad[-1] + 1\n                \n                if real_len <= MAX_LEN:\n                    truncated = np.full(MAX_LEN, PAD, dtype=pB.dtype)\n                    truncated[:real_len] = new_gene_b[:real_len]\n                    offspring_cpu[idx_b] = truncated\n            else:\n                padded = np.full(MAX_LEN, PAD, dtype=pB.dtype)\n                padded[:valid_len_b] = new_gene_b\n                offspring_cpu[idx_b] = padded\n                \n        return torch.tensor(offspring_cpu, device=self.device, dtype=torch.long)\n\n\n\n\n    def infix_to_rpn(self, formulas: List[str]) -> torch.Tensor:\n        \"\"\"\n        Converts a list of infix strings to a padded RPN tensor [B, L].\n        \"\"\"\n        batch_rpn = []\n        for f in formulas:\n            try:\n                tree = ExpressionTree.from_infix(f)\n                if not tree.is_valid:\n                    batch_rpn.append([PAD_ID]*self.max_len)\n                    continue\n                \n                rpn_tokens = []\n                def traverse(node):\n                    if not node: return\n                    for child in node.children:\n                        traverse(child)\n                    rpn_tokens.append(node.value)\n                \n                traverse(tree.root)\n                ids = [self.grammar.token_to_id.get(t, PAD_ID) for t in rpn_tokens]\n                if len(ids) > self.max_len:\n                    ids = ids[:self.max_len]\n                else:\n                    ids = ids + [PAD_ID] * (self.max_len - len(ids))\n                batch_rpn.append(ids)\n            except:\n                batch_rpn.append([PAD_ID]*self.max_len)\n                \n        if not batch_rpn:\n             return torch.empty((0, self.max_len), device=self.device, dtype=torch.long)\n        return torch.tensor(batch_rpn, device=self.device, dtype=torch.long)\n\n\n    def rpn_to_infix(self, rpn_tensor: torch.Tensor, constants: torch.Tensor = None) -> str:\n        \"\"\"\n        Decodes RPN tensor to Infix string (CPU-style formatting).\n        \"\"\"\n        if rpn_tensor.ndim > 1:\n            rpn_tensor = rpn_tensor.view(-1)\n            \n        vocab = self.grammar.id_to_token\n        stack = []\n        const_idx = 0\n        \n        for token_id in rpn_tensor:\n\n            token_id = token_id.item()\n            if token_id == PAD_ID: continue\n            \n            token = vocab.get(token_id, \"\")\n            \n            if token in self.grammar.operators:\n                arity = self.grammar.token_arity.get(token, 2)\n                if arity == 1:\n                    if not stack: return \"Invalid\"\n                    a = stack.pop()\n                    if token == 's': stack.append(f\"sin({a})\")\n                    elif token == 'c': stack.append(f\"cos({a})\")\n                    elif token == 'l': stack.append(f\"log({a})\")\n                    elif token == 'e' or token == 'exp': stack.append(f\"exp({a})\")\n                    elif token == 'q' or token == 'sqrt': stack.append(f\"sqrt({a})\")\n                    elif token == 'a' or token == 'abs': stack.append(f\"abs({a})\")\n                    elif token == 'n' or token == 'sign': stack.append(f\"sign({a})\")\n                    elif token == 'neg': stack.append(f\"neg({a})\")\n                    elif token == '_' or token == 'floor': stack.append(f\"floor({a})\")\n                    elif token == '!' or token == 'gamma': stack.append(f\"gamma({a})\")\n                    elif token == 'g' or token == 'lgamma': stack.append(f\"lgamma({a})\")\n                    elif token == 'S' or token == 'asin': stack.append(f\"asin({a})\")\n                    elif token == 'C' or token == 'acos': stack.append(f\"acos({a})\")\n                    elif token == 'T' or token == 'atan': stack.append(f\"atan({a})\")\n                    else: stack.append(f\"{token}({a})\")\n                else: # Binary\n                    if len(stack) < 2: return \"Invalid\"\n                    b = stack.pop()\n                    a = stack.pop()\n                    \n                    if token == '+' and b.startswith(\"-\") and not b.startswith(\"(-\"):\n                         stack.append(f\"({a} - {b[1:]})\")\n                    elif token == '-' and a == \"0\":\n                         stack.append(f\"(-{b})\")\n                    elif token == 'pow':\n                         stack.append(f\"({a} ^ {b})\")\n                    elif token == 'mod':\n                         stack.append(f\"({a} % {b})\")\n                    else:\n                         stack.append(f\"({a} {token} {b})\")\n            elif token == 'C':\n                val = 1.0\n                if constants is not None and const_idx < len(constants):\n                    val = constants[const_idx].item()\n                    const_idx += 1\n                stack.append(format_const(val))\n            elif token.startswith('x'):\n                if token == 'x': stack.append(\"x0\")\n                else: stack.append(token)\n            else:\n                stack.append(str(token))\n                \n        if len(stack) == 1:\n            return stack[0]\n        return \"Invalid\"\n    \n    def get_tree_size(self, rpn_tensor: torch.Tensor) -> int:\n        \"\"\"\n        Returns number of non-pad nodes.\n        \"\"\"\n        return (rpn_tensor != PAD_ID).sum().item()\n    \n\n\n    def _run_vm(self, population: torch.Tensor, x: torch.Tensor, constants: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Internal VM interpreter to evaluate RPN population on the GPU.\n        Returns: (final_predictions, stack_pointer)\n        \"\"\"\n        B, L = population.shape\n        D = x.shape[0]\n        MAX_STACK = 10\n        eff_B = B * D\n        \n        pop_expanded = population.unsqueeze(1).expand(-1, D, -1).reshape(eff_B, L)\n        const_expanded = None\n        if constants is not None:\n             const_expanded = constants.unsqueeze(1).expand(-1, D, -1).reshape(eff_B, -1)\n             \n        if x.ndim == 1:\n            x_expanded = x.unsqueeze(0).expand(B, -1).reshape(eff_B, 1)\n        else:\n            x_expanded = x.unsqueeze(0).expand(B, -1, -1).reshape(eff_B, x.shape[1])\n            \n        stack = torch.zeros(eff_B, MAX_STACK, device=self.device, dtype=torch.float64)\n        sp = torch.zeros(eff_B, device=self.device, dtype=torch.long)\n        const_counters = torch.zeros(eff_B, device=self.device, dtype=torch.long)\n        \n        # NEW: Error tracking\n        has_error = torch.zeros(eff_B, dtype=torch.bool, device=self.device)\n        \n        pi_val = torch.tensor(np.pi, device=self.device, dtype=torch.float64)\n        e_val = torch.tensor(np.e, device=self.device, dtype=torch.float64)\n\n        # IDs\n        id_C = self.grammar.token_to_id.get('C', -100)\n        id_pi = self.grammar.token_to_id.get('pi', -100)\n        id_e = self.grammar.token_to_id.get('e', -100)\n        \n        op_add = self.grammar.token_to_id.get('+', -100); op_sub = self.grammar.token_to_id.get('-', -100)\n        op_mul = self.grammar.token_to_id.get('*', -100); op_div = self.grammar.token_to_id.get('/', -100)\n        op_pow = self.grammar.token_to_id.get('pow', -100); op_mod = self.grammar.token_to_id.get('%', -100)\n        op_sin = self.grammar.token_to_id.get('sin', -100); op_cos = self.grammar.token_to_id.get('cos', -100)\n        op_tan = self.grammar.token_to_id.get('tan', -100)\n        op_asin = self.grammar.token_to_id.get('S', -100); op_acos = self.grammar.token_to_id.get('C', -100); op_atan = self.grammar.token_to_id.get('T', -100)\n        op_exp = self.grammar.token_to_id.get('e', -100); op_log = self.grammar.token_to_id.get('log', -100)\n        op_sqrt = self.grammar.token_to_id.get('sqrt', -100); op_abs = self.grammar.token_to_id.get('abs', -100); op_neg = self.grammar.token_to_id.get('neg', -100)\n        op_fact = self.grammar.token_to_id.get('!', -100); op_floor = self.grammar.token_to_id.get('_', -100); op_gamma = self.grammar.token_to_id.get('g', -100)\n        \n        var_ids = [self.grammar.token_to_id.get(v, -100) for v in self.grammar.active_variables]\n        id_x_legacy = self.grammar.token_to_id.get('x', -100)\n\n        for i in range(L):\n            token = pop_expanded[:, i]\n            active_mask = (token != PAD_ID)\n            if not active_mask.any(): continue\n            \n            push_vals = torch.zeros(eff_B, device=self.device, dtype=torch.float64)\n            is_operand = torch.zeros(eff_B, dtype=torch.bool, device=self.device)\n            \n            # Variables\n            mask = (token == id_x_legacy)\n            if mask.any():\n                push_vals[mask] = x_expanded[mask, 0]\n                is_operand = is_operand | mask\n                \n            for v_idx, vid in enumerate(var_ids):\n                mask = (token == vid)\n                if mask.any():\n                    v_col = v_idx if v_idx < x_expanded.shape[1] else 0\n                    push_vals[mask] = x_expanded[mask, v_col]\n                    is_operand = is_operand | mask\n            \n            mask = (token == id_pi)\n            if mask.any(): push_vals[mask] = pi_val; is_operand = is_operand | mask\n            mask = (token == id_e)\n            if mask.any(): push_vals[mask] = e_val; is_operand = is_operand | mask\n                \n            mask = (token == id_C)\n            if mask.any():\n                if const_expanded is not None:\n                     safe_idx = torch.clamp(const_counters, 0, const_expanded.shape[1]-1)\n                     c_vals = const_expanded.gather(1, safe_idx.unsqueeze(1)).squeeze(1)\n                     push_vals[mask] = c_vals[mask]\n                     const_counters[mask] += 1\n                else:\n                     push_vals[mask] = 1.0 \n                is_operand = is_operand | mask\n            \n            for val_str in ['1', '2', '3', '5']:\n                vid = self.grammar.token_to_id.get(val_str, -999)\n                mask = (token == vid)\n                if mask.any():\n                    push_vals[mask] = float(val_str)\n                    is_operand = is_operand | mask\n                    \n            if is_operand.any():\n                safe_sp = torch.clamp(sp, 0, MAX_STACK-1)\n                stack = stack.scatter(1, safe_sp.unsqueeze(1), push_vals.unsqueeze(1))\n                sp = sp + is_operand.long()\n                \n            # Binary\n            is_binary = (token == op_add) | (token == op_sub) | (token == op_mul) | (token == op_div) | (token == op_pow) | (token == op_mod)\n            \n            enough_stack = (sp >= 2)\n            valid_op = is_binary & enough_stack\n            \n            has_error = has_error | (is_binary & ~enough_stack)\n            \n            if valid_op.any():\n                idx_b = torch.clamp(sp - 1, 0, MAX_STACK - 1).unsqueeze(1); val_b = stack.gather(1, idx_b).squeeze(1)\n                idx_a = torch.clamp(sp - 2, 0, MAX_STACK - 1).unsqueeze(1); val_a = stack.gather(1, idx_a).squeeze(1)\n                res = torch.zeros_like(val_a)\n                \n                m = (token == op_add) & valid_op; res[m] = val_a[m] + val_b[m]\n                m = (token == op_sub) & valid_op; res[m] = val_a[m] - val_b[m]\n                m = (token == op_mul) & valid_op; res[m] = val_a[m] * val_b[m]\n                m = (token == op_div) & valid_op\n                if m.any(): \n                    d = val_b[m]; bad = d.abs() < 1e-9; sd = torch.where(bad, torch.tensor(1.0, device=self.device, dtype=torch.float64), d)\n                    out = val_a[m] / sd; out[bad] = 1e150; res[m] = out\n                m = (token == op_mod) & valid_op\n                if m.any():\n                    d = val_b[m]; bad = d.abs() < 1e-9; sd = torch.where(bad, torch.tensor(1.0, device=self.device, dtype=torch.float64), d)\n                    out = torch.fmod(val_a[m], sd); out[bad] = 1e150; res[m] = out\n                m = (token == op_pow) & valid_op; \n                if m.any(): res[m] = torch.pow(val_a[m], val_b[m])\n                \n                wp = torch.clamp(sp - 2, 0, MAX_STACK-1)\n                curr = stack.gather(1, wp.unsqueeze(1)).squeeze(1)\n                fw = torch.where(valid_op, res, curr)\n                stack = stack.scatter(1, wp.unsqueeze(1), fw.unsqueeze(1)); sp = sp - valid_op.long()\n                \n            # Unary\n            is_unary = (token == op_sin) | (token == op_cos) | (token == op_tan) | \\\n                       (token == op_asin) | (token == op_acos) | (token == op_atan) | \\\n                       (token == op_exp) | (token == op_log) | \\\n                       (token == op_sqrt) | (token == op_abs) | (token == op_neg) | \\\n                       (token == op_fact) | (token == op_floor) | (token == op_gamma)\n            \n            enough_stack = (sp >= 1)\n            valid_op = is_unary & enough_stack\n            \n            has_error = has_error | (is_unary & ~enough_stack)\n            \n            if valid_op.any():\n                idx_a = torch.clamp(sp - 1, 0, MAX_STACK - 1).unsqueeze(1); val_a = stack.gather(1, idx_a).squeeze(1); res = torch.zeros_like(val_a)\n                m = (token == op_sin) & valid_op; res[m] = torch.sin(val_a[m])\n                m = (token == op_cos) & valid_op; res[m] = torch.cos(val_a[m])\n                m = (token == op_tan) & valid_op; res[m] = torch.tan(val_a[m])\n                m = (token == op_log) & valid_op\n                if m.any(): \n                    inv = val_a[m]; s = inv > 1e-9; out = torch.full_like(inv, 1e150); out[s] = torch.log(inv[s]); res[m] = out\n                m = (token == op_exp) & valid_op\n                if m.any(): \n                    inv = val_a[m]; s = inv <= 700.0; out = torch.full_like(inv, 1e150); out[s] = torch.exp(inv[s]); res[m] = out\n                m = (token == op_sqrt) & valid_op; res[m] = torch.sqrt(val_a[m].abs())\n                m = (token == op_abs) & valid_op; res[m] = torch.abs(val_a[m])\n                m = (token == op_neg) & valid_op; res[m] = -val_a[m]\n                m = (token == op_asin) & valid_op; res[m] = torch.asin(torch.clamp(val_a[m], -1.0, 1.0))\n                m = (token == op_acos) & valid_op; res[m] = torch.acos(torch.clamp(val_a[m], -1.0, 1.0))\n                m = (token == op_atan) & valid_op; res[m] = torch.atan(val_a[m])\n                m = (token == op_floor) & valid_op; res[m] = torch.floor(val_a[m])\n                m = (token == op_fact) & valid_op\n                if m.any():\n                    inv = val_a[m]; u = (inv < 0) | (inv > 170.0); out = torch.full_like(inv, 1e150)\n                    si = inv.clone(); si[u] = 1.0; vc = torch.special.gamma(si + 1.0); out[~u] = vc[~u]; res[m] = out\n                m = (token == op_gamma) & valid_op\n                if m.any():\n                    inv = val_a[m]; u = (inv <= -1.0); out = torch.full_like(inv, 1e150)\n                    si = inv.clone(); si[u] = 1.0; vc = torch.special.gammaln(si + 1.0); out[~u] = vc[~u]; res[m] = out\n\n                wp = torch.clamp(sp - 1, 0, MAX_STACK-1); curr = stack.gather(1, wp.unsqueeze(1)).squeeze(1)\n                fw = torch.where(valid_op, res, curr); stack = stack.scatter(1, wp.unsqueeze(1), fw.unsqueeze(1))\n        return stack[:, 0], sp, has_error\n\n    def evaluate_batch(self, population: torch.Tensor, x: torch.Tensor, y_target: torch.Tensor, constants: torch.Tensor = None) -> torch.Tensor:\n        \"\"\"\n        Evaluates the RPN population on the GPU.\n        Returns: RMSE per individual [PopSize]\n        \"\"\"\n        B, L = population.shape\n        D = x.shape[0]\n        \n        final_preds, sp, has_error = self._run_vm(population, x, constants)\n        \n        is_valid = (sp == 1) & (~has_error)\n        # Use parity with C++: if not valid or nan/inf, penalty 1e300\n        final_preds = torch.where(is_valid & ~torch.isnan(final_preds) & ~torch.isinf(final_preds), \n                                  final_preds, \n                                  torch.tensor(1e300, device=self.device, dtype=torch.float64))\n                                  \n        preds_matrix = final_preds.view(B, D)\n        target_matrix = y_target.unsqueeze(0).expand(B, -1)\n        mse = torch.mean((preds_matrix - target_matrix)**2, dim=1)\n        \n        # Guard against MSE itself being Inf/NaN after mean\n        rmse = torch.sqrt(torch.where(torch.isnan(mse) | torch.isinf(mse), \n                                      torch.tensor(1e150, device=self.device, dtype=torch.float64), \n                                      mse))\n        return rmse\n\n        \n        # Precompute IDs\n        id_C = self.grammar.token_to_id.get('C', -100)\n        id_pi = self.grammar.token_to_id.get('pi', -100)\n        id_e = self.grammar.token_to_id.get('e', -100)\n        \n        # Op IDs\n        op_add = self.grammar.token_to_id.get('+', -100)\n        op_sub = self.grammar.token_to_id.get('-', -100)\n        op_mul = self.grammar.token_to_id.get('*', -100)\n        op_div = self.grammar.token_to_id.get('/', -100)\n        op_pow = self.grammar.token_to_id.get('pow', -100)\n        op_mod = self.grammar.token_to_id.get('%', -100)\n        \n        op_sin = self.grammar.token_to_id.get('sin', -100)\n        op_cos = self.grammar.token_to_id.get('cos', -100)\n        op_tan = self.grammar.token_to_id.get('tan', -100)\n        op_asin = self.grammar.token_to_id.get('S', -100)\n        op_acos = self.grammar.token_to_id.get('C', -100)\n        op_atan = self.grammar.token_to_id.get('T', -100)\n        op_exp = self.grammar.token_to_id.get('e', -100) # 'e' is the operator token\n        op_log = self.grammar.token_to_id.get('log', -100)\n        op_sqrt = self.grammar.token_to_id.get('sqrt', -100)\n        op_abs = self.grammar.token_to_id.get('abs', -100)\n        op_neg = self.grammar.token_to_id.get('neg', -100)\n        \n        op_fact = self.grammar.token_to_id.get('!', -100)\n        op_floor = self.grammar.token_to_id.get('_', -100)\n        op_gamma = self.grammar.token_to_id.get('g', -100)\n        \n        # Cache Variable IDs\n        # We know self.grammar.active_variables list\n        var_ids = [self.grammar.token_to_id.get(v, -100) for v in self.grammar.active_variables]\n        # x0 -> index 0, x1 -> index 1...\n        # Also 'x' usually maps to x0\n        id_x_legacy = self.grammar.token_to_id.get('x', -100)\n\n        for i in range(L):\n            token = pop_expanded[:, i]\n            active_mask = (token != PAD_ID)\n            if not active_mask.any(): continue\n            \n            # --- 1. Push ---\n            push_vals = torch.zeros(eff_B, device=self.device, dtype=torch.float64)\n            is_operand = torch.zeros(eff_B, dtype=torch.bool, device=self.device)\n            \n            # Variables\n            # Check legacy 'x'\n            mask = (token == id_x_legacy)\n            if mask.any():\n                push_vals[mask] = x_expanded[mask, 0]\n                is_operand = is_operand | mask\n                \n            # Check x0, x1, x2...\n            for v_idx, vid in enumerate(var_ids):\n                mask = (token == vid)\n                if mask.any():\n                    # If inputs have enough columns, use them. If not, fallback to 0 or error?\n                    # We assume x_expanded shape matches grammar requirements.\n                    if v_idx < x_expanded.shape[1]:\n                        push_vals[mask] = x_expanded[mask, v_idx]\n                        is_operand = is_operand | mask\n            \n            mask = (token == id_pi)\n            if mask.any():\n                push_vals[mask] = pi_val\n                is_operand = is_operand | mask\n            \n\n            mask = (token == id_e)\n            if mask.any():\n                push_vals[mask] = e_val\n                is_operand = is_operand | mask\n                \n            mask = (token == id_C)\n            if mask.any():\n                if const_expanded is not None:\n                     # Gather constants based on current counter\n                     # const_expanded: [eff_B, MaxC]\n                     # const_counters: [eff_B]\n                     # We need to clamp counter to MaxC-1 to avoid error, though valid RPN shouldn't exceed\n                     safe_idx = torch.clamp(const_counters, 0, const_expanded.shape[1]-1)\n                     \n                     c_vals = const_expanded.gather(1, safe_idx.unsqueeze(1)).squeeze(1)\n                     push_vals[mask] = c_vals[mask]\n                     \n                     # Increment counter where C was used\n                     const_counters[mask] += 1\n                else:\n                     push_vals[mask] = 1.0 \n                     \n                is_operand = is_operand | mask\n            \n            # Literals\n            for val_str in ['1', '2', '3', '5']:\n                vid = self.grammar.token_to_id.get(val_str, -999)\n                mask = (token == vid)\n                if mask.any():\n                    push_vals[mask] = float(val_str)\n                    is_operand = is_operand | mask\n                    \n\n            if is_operand.any():\n                safe_sp = torch.clamp(sp, 0, MAX_STACK-1)\n                # Out-of-place scatter for autograd safety\n                stack = stack.scatter(1, safe_sp.unsqueeze(1), push_vals.unsqueeze(1))\n                sp = sp + is_operand.long()\n                \n            # --- 2. Binary ---\n            is_binary = (token == op_add) | (token == op_sub) | (token == op_mul) | (token == op_div) | (token == op_pow)\n            valid_op = is_binary & (sp >= 2)\n            \n            if valid_op.any():\n                idx_b = torch.clamp(sp - 1, 0, MAX_STACK - 1).unsqueeze(1)\n                val_b = stack.gather(1, idx_b).squeeze(1)\n                \n                idx_a = torch.clamp(sp - 2, 0, MAX_STACK - 1).unsqueeze(1)\n                val_a = stack.gather(1, idx_a).squeeze(1)\n                \n                res = torch.zeros_like(val_a)\n                \n                mask = (token == op_add) & valid_op\n                if mask.any(): res[mask] = val_a[mask] + val_b[mask]\n                \n                mask = (token == op_sub) & valid_op\n                if mask.any(): res[mask] = val_a[mask] - val_b[mask]\n                \n                mask = (token == op_mul) & valid_op\n                if mask.any(): res[mask] = val_a[mask] * val_b[mask]\n                \n                mask = (token == op_div) & valid_op\n                if mask.any(): \n                    denom = val_b[mask]\n                    # C++: if (fabs(right) < 1e-9) { result = GPU_MAX_DOUBLE; }\n                    # We implement parity:\n                    bad_denom = denom.abs() < 1e-9\n                    \n                    # We compute safe division where possible\n                    safe_denom = torch.where(bad_denom, torch.tensor(1.0, device=self.device, dtype=torch.float64), denom)\n                    out_div = val_a[mask] / safe_denom\n                    \n                    # Apply penalty for bad denom\n                    # We use 1e300 as GPU_MAX_DOUBLE proxy (or just 1e15 to avoid inf issues in float64?)\n                    # C++ uses DBL_MAX typically which is ~1e308. \n                    out_div[bad_denom] = 1e300\n                    res[mask] = out_div\n                    \n                mask = (token == op_mod) & valid_op\n                if mask.any():\n                    denom = val_b[mask]\n                    bad_denom = denom.abs() < 1e-9\n                    safe_denom = torch.where(bad_denom, torch.tensor(1.0, device=self.device, dtype=torch.float64), denom)\n                    # C++: fmod\n                    out_mod = torch.fmod(val_a[mask], safe_denom)\n                    out_mod[bad_denom] = 1e300\n                    res[mask] = out_mod\n                    \n                mask = (token == op_pow) & valid_op\n                if mask.any():\n                    # No artificial clamping for float64 unless extremely huge to avoid NaN propagation immediately\n                    # C++ just does pow(l, r)\n                    # But we can protect against complex numbers (negative base ^ float exp) -> NaN\n                    base = val_a[mask]\n                    expon = val_b[mask]\n                    # If base < 0 and exponent is not integer loop, result is NaN. \n                    # We can protect base like C++ protected ops sometimes do, or just let it be NaN (yielding INF fitness)\n                    res[mask] = torch.pow(base, expon)\n                \n                write_pos = torch.clamp(sp - 2, 0, MAX_STACK-1)\n                current_at_pos = stack.gather(1, write_pos.unsqueeze(1)).squeeze(1)\n                final_write_val = torch.where(valid_op, res, current_at_pos)\n                \n                # Out-of-place scatter\n                stack = stack.scatter(1, write_pos.unsqueeze(1), final_write_val.unsqueeze(1))\n                sp = sp - valid_op.long()\n                \n            # --- 3. Unary ---\n            is_unary = (token == op_sin) | (token == op_cos) | (token == op_tan) | \\\n                       (token == op_asin) | (token == op_acos) | (token == op_atan) | \\\n                       (token == op_exp) | (token == op_log) | \\\n                       (token == op_sqrt) | (token == op_abs) | (token == op_neg) | \\\n                       (token == op_fact) | (token == op_floor) | (token == op_gamma)\n            valid_op = is_unary & (sp >= 1)\n            \n            if valid_op.any():\n                idx_a = torch.clamp(sp - 1, 0, MAX_STACK - 1).unsqueeze(1)\n                val_a = stack.gather(1, idx_a).squeeze(1)\n                res = torch.zeros_like(val_a)\n                \n                mask = (token == op_sin) & valid_op\n                if mask.any(): res[mask] = torch.sin(val_a[mask])\n                \n                mask = (token == op_cos) & valid_op\n                if mask.any(): res[mask] = torch.cos(val_a[mask])\n                \n                mask = (token == op_log) & valid_op\n                if mask.any(): \n                    # C++: (val <= 1e-9) ? GPU_MAX_DOUBLE : log(val)\n                    # We use a large value for error\n                    inp = val_a[mask]\n                    safe_mask = inp > 1e-9\n                    # Where unsafe, we put a huge value. But we must set res.\n                    # We compute log everywhere but replace bad ones? Or select?\n                    out = torch.full_like(inp, 1e300) # GPU_MAX_DOUBLE proxy\n                    out[safe_mask] = torch.log(inp[safe_mask])\n                    res[mask] = out\n                \n                mask = (token == op_exp) & valid_op\n                if mask.any(): \n                    # C++: (val > 700.0) ? GPU_MAX_DOUBLE : exp(val)\n                    inp = val_a[mask]\n                    safe_mask = inp <= 700.0\n                    out = torch.full_like(inp, 1e300)\n                    out[safe_mask] = torch.exp(inp[safe_mask])\n                    res[mask] = out\n                \n                mask = (token == op_sqrt) & valid_op\n                if mask.any(): res[mask] = torch.sqrt(val_a[mask].abs())\n                \n                mask = (token == op_abs) & valid_op\n                if mask.any(): res[mask] = torch.abs(val_a[mask])\n                \n                mask = (token == op_neg) & valid_op\n                if mask.any(): res[mask] = -val_a[mask]\n                \n                mask = (token == op_tan) & valid_op\n                if mask.any(): res[mask] = torch.tan(val_a[mask])\n                \n                mask = (token == op_asin) & valid_op\n                if mask.any(): \n                    # C++ protected: asin(clamp(x, -1, 1)) (actually S op code)\n                    # But if we want standard behavior or protected?\n                    # The C++ code for 'asin' (op 'S' in tree string, but 'asin' in kernel?) \n                    # Kernel uses standard asin but our engine usually protects domain.\n                    # Let's use protection [-1, 1]\n                    res[mask] = torch.asin(torch.clamp(val_a[mask], -1.0, 1.0))\n                \n                mask = (token == op_acos) & valid_op\n                if mask.any(): res[mask] = torch.acos(torch.clamp(val_a[mask], -1.0, 1.0))\n                \n                mask = (token == op_atan) & valid_op\n                if mask.any(): res[mask] = torch.atan(val_a[mask])\n\n                mask = (token == op_floor) & valid_op\n                if mask.any(): res[mask] = torch.floor(val_a[mask])\n                \n                mask = (token == op_fact) & valid_op\n                if mask.any():\n                     # C++: (val < 0 || val > 170.0) ? GPU_MAX_DOUBLE : tgamma(val + 1.0);\n                     inp = val_a[mask]\n                     # tgamma(n+1) = n!\n                     # We use lgamma and exp to be safe? or torch.special.gamma?\n                     # torch.special.gamma is 'tgamma' equivalent.\n                     # Protection\n                     unsafe = (inp < 0) | (inp > 170.0)\n                     out = torch.full_like(inp, 1e300)\n                     \n                     # Only compute safe to avoid NaN/Inf in gradients or runtime\n                     safe_inp = inp.clone()\n                     safe_inp[unsafe] = 1.0 # dummy\n                     \n                     val_computed = torch.special.gamma(safe_inp + 1.0)\n                     out[~unsafe] = val_computed[~unsafe]\n                     res[mask] = out\n\n                mask = (token == op_gamma) & valid_op\n                if mask.any():\n                     # C++: (val <= -1.0) ? GPU_MAX_DOUBLE : lgamma(val + 1.0); \n                     # Wait, snippet said lgamma(val+1). Usually 'gamma' op is just gamma function?\n                     # C++ snippet: case 'g': result = (val <= -1.0) ? GPU_MAX_DOUBLE : lgamma(val + 1.0); \n                     # This seems to be Log-Gamma of (x+1)? Or is it Gamma? \n                     # 'lgamma' function usually computes log(|gamma(x)|).\n                     # The snippet explicitly says lgamma. So GPU op 'g' is log-gamma.\n                     inp = val_a[mask]\n                     unsafe = (inp <= -1.0)\n                     out = torch.full_like(inp, 1e300)\n                     \n                     safe_inp = inp.clone()\n                     safe_inp[unsafe] = 1.0\n                     \n                     val_computed = torch.special.gammaln(safe_inp + 1.0) # lgamma matches gammaln in torch\n                     out[~unsafe] = val_computed[~unsafe]\n                     res[mask] = out\n\n                write_pos = torch.clamp(sp - 1, 0, MAX_STACK-1)\n                current_at_pos = stack.gather(1, write_pos.unsqueeze(1)).squeeze(1)\n                final_write_val = torch.where(valid_op, res, current_at_pos)\n                \n                # Out-of-place scatter\n                stack = stack.scatter(1, write_pos.unsqueeze(1), final_write_val.unsqueeze(1))\n\n        \n        is_valid = (sp == 1)\n        final_preds = stack[:, 0]\n        final_preds = torch.where(is_valid, final_preds, torch.tensor(float('nan'), device=self.device, dtype=torch.float64))\n        preds_matrix = final_preds.view(B, D)\n        target_matrix = y_target.unsqueeze(0).expand(B, -1)\n        mse = torch.mean((preds_matrix - target_matrix)**2, dim=1)\n        rmse = torch.sqrt(torch.where(torch.isnan(mse), torch.tensor(1e300, device=self.device, dtype=torch.float64), mse))\n        return rmse\n\n    def evaluate_differentiable(self, population: torch.Tensor, constants: torch.Tensor, x: torch.Tensor, y_target: torch.Tensor):\n        import torch.nn.functional as F\n        return self.evaluate_batch(population, x, y_target), torch.zeros_like(x).expand(population.shape[0], -1) \n\n    def evaluate_batch_full(self, population: torch.Tensor, x: torch.Tensor, y_target: torch.Tensor, constants: torch.Tensor = None) -> torch.Tensor:\n        \"\"\"\n        Returns full error matrix [Pop, D]. \n        Used for Lexicase Selection.\n        \"\"\"\n        B, L = population.shape\n        D = x.shape[0]\n        \n        final_preds, sp = self._run_vm(population, x, constants)\n        is_valid = (sp == 1)\n        \n        # Penalize invalid/nan/inf with 1e300\n        final_preds = torch.where(is_valid & ~torch.isnan(final_preds) & ~torch.isinf(final_preds), \n                                  final_preds, \n                                  torch.tensor(1e300, device=self.device, dtype=torch.float64))\n        \n        preds_matrix = final_preds.view(B, D)\n        target_matrix = y_target.unsqueeze(0).expand(B, -1)\n        abs_err = torch.abs(preds_matrix - target_matrix)\n        \n        # Guard against Inf in abs_err (e.g. pred - target where one is huge)\n        abs_err = torch.where(torch.isnan(abs_err) | torch.isinf(abs_err), \n                              torch.tensor(1e300, device=self.device, dtype=torch.float64), \n                              abs_err)\n        return abs_err\n\n    def compute_case_weights(self, errors: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute case weights based on difficulty (variance of errors across population).\n        \n        Cases with higher variance are considered harder and get higher weights.\n        \n        Args:\n            errors: [PopSize, n_cases] error matrix\n            \n        Returns:\n            [n_cases] weights normalized to sum to 1\n        \"\"\"\n        # Variance across population per case\n        case_variance = torch.var(errors, dim=0)\n        \n        # Normalize to weights (higher variance -> higher weight)\n        weights = case_variance / (case_variance.sum() + 1e-9)\n        \n        return weights\n\n    def weighted_rmse(self, errors: torch.Tensor, weights: torch.Tensor = None) -> torch.Tensor:\n        \"\"\"\n        Compute weighted RMSE across cases.\n        \n        Args:\n            errors: [PopSize, n_cases] absolute error matrix\n            weights: [n_cases] optional case weights (default: uniform)\n            \n        Returns:\n            [PopSize] weighted RMSE per individual\n        \"\"\"\n        if weights is None or not GpuGlobals.USE_WEIGHTED_FITNESS:\n            # Standard RMSE\n            return torch.sqrt((errors ** 2).mean(dim=1))\n        \n        # Weighted mean squared error\n        weighted_mse = (errors ** 2 * weights.unsqueeze(0)).sum(dim=1)\n        return torch.sqrt(weighted_mse)\n\n    def lexicase_selection(self, population: torch.Tensor, errors: torch.Tensor, n_select: int) -> torch.Tensor:\n        \"\"\"\n        Selects n_select parents using Tournament Lexicase Selection.\n        errors: [PopSize, n_cases] (Absolute Error)\n        \"\"\"\n        # Lexicase is slow if running on full population for every selection.\n        # \"Tournament Lexicase\": Pick random subset, run lexicase on it to find 1 winner. Repeat.\n        \n        # Optimized implementation:\n        # We need n_select winners.\n        # For each winner:\n        # 1. Pick pool (size ~50?)\n        # 2. Shuffle cases\n        # 3. Filter loop\n        \n        # Since we cannot easily loop inside tensor ops, we might need a custom kernel or CPU loop.\n        # Lexicase is inherently sequential on cases.\n        # CPU Loop over n_select is feasible if n_select is not huge (e.g. 1000).\n        \n        pop_size, n_cases = errors.shape\n        pool_size = 50\n        \n        selected_indices = []\n        \n        # Errors to CPU for logic\n        errors_cpu = errors.detach().cpu().numpy()\n        \n        for _ in range(n_select):\n            # 1. Pool\n            candidates = np.random.randint(0, pop_size, pool_size)\n            \n            # 2. Shuffle cases\n            cases = np.random.permutation(n_cases)\n            \n            active_cands = candidates\n            \n            for case_idx in cases:\n                # Get errors for active candidates at this case\n                # errors_cpu[active_cands, case_idx]\n                case_errs = errors_cpu[active_cands, case_idx]\n                min_err = np.min(case_errs)\n                \n                # Epsilon (MAD or simple)\n                epsilon = max(min_err * 0.1, 1e-9)\n                \n                # Filter\n                survivors_mask = case_errs <= (min_err + epsilon)\n                active_cands = active_cands[survivors_mask]\n                \n                if len(active_cands) == 1:\n                    break\n            \n            # Pick random survivor\n            winner = np.random.choice(active_cands)\n            selected_indices.append(winner)\n            \n        return population[selected_indices]\n\n        \n\n    def _generate_random_population(self, size: int) -> torch.Tensor:\n        \"\"\"\n        Helper to generate random RPN population of given size.\n        \"\"\"\n        formulas = []\n        # Generate full population (slower but ensures diversity)\n        for _ in range(size):\n            try:\n                # Generate random valid tree\n                tree = ExpressionTree.generate_random(max_depth=GpuGlobals.MAX_TREE_DEPTH_INITIAL, num_variables=self.num_variables)\n                formulas.append(tree.get_infix())\n            except:\n                formulas.append(\"x0\")\n        \n        # Convert to RPN\n        return self.infix_to_rpn(formulas)\n\n    def initialize_population(self) -> torch.Tensor:\n        \"\"\"\n        Generates a population of VALID random formulas.\n        \"\"\"\n        return self._generate_random_population(self.pop_size)\n\n    def cataclysm_population(self, population: torch.Tensor, constants: torch.Tensor, fitness_rmse: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Hard Reset: Keep Top 10% Elites, replace the rest with new random individuals.\n        Called when diversity collapses (too many duplicates).\n        \"\"\"\n        B = population.shape[0]\n        n_elites = int(B * 0.10)\n        n_random = B - n_elites\n        \n        # Sort by fitness (RMSE ascending is better)\n        sorted_indices = torch.argsort(fitness_rmse)\n        elite_indices = sorted_indices[:n_elites]\n        \n        # Keep Elites\n        elites = population[elite_indices]\n        elite_consts = constants[elite_indices]\n        \n        # Generate fresh randoms\n        new_pop = self._generate_random_population(n_random)\n        new_consts = torch.zeros((n_random, constants.shape[1]), device=self.device, dtype=torch.float64)\n        \n        # Combine\n        final_pop = torch.cat([elites, new_pop], dim=0)\n        final_consts = torch.cat([elite_consts, new_consts], dim=0)\n        \n        return final_pop, final_consts\n\n    def detect_patterns(self, targets: List[float]) -> List[str]:\n        \"\"\"\n        Detects simple patterns (Arithmetic, Geometric) in 1D targets.\n        Returns a list of seed formulas.\n        \"\"\"\n        if len(targets) < 3: return []\n        \n        seeds = []\n        \n        # 1. Arithmetic: y = a + d*x  (assuming x=0, 1, 2...)\n        # We need to know X to be sure, but let's assume standard index for simple detection\n        # or just check diffs.\n        \n        diffs = np.diff(targets)\n        if np.allclose(diffs, diffs[0], atol=1e-5):\n            d = diffs[0]\n            a = targets[0] # assuming x0=0 start? \n            # If x starts at 1, then y = a + d*(x-1) = (a-d) + d*x\n            # We construct a generic candidate 'C + C*x0'\n            # We can just let the optimizer find constants if we give the structure.\n            seeds.append(\"(C + (C * x0))\") \n            \n        # 2. Geometric: y = a * r^x\n        # Check ratios\n        if not np.any(np.abs(targets) < 1e-9):\n            ratios = targets[1:] / targets[:-1]\n            if np.allclose(ratios, ratios[0], atol=1e-5):\n                seeds.append(\"(C * (C ^ x0))\")\n                \n        # 3. Constant\n        if np.allclose(targets, targets[0], atol=1e-5):\n             seeds.append(\"C\")\n             \n        # 4. Fibonacci-ish? (Last 2 sum)\n        # 5. Sinusoidal?\n        \n        return seeds\n\n    def run(self, x_values: List[float], y_targets: List[float], seeds: List[str], timeout_sec=10, callback=None) -> Optional[str]:\n        \"\"\"\n        Main evolutionary loop.\n        \"\"\"\n        start_time = time.time()\n        \n        # 1. Setup Data\n        if GpuGlobals.USE_LOG_TRANSFORMATION:\n            print(\"Info: Log Transformation is ON (Target = ln(Y)).\")\n            y_np = np.array(y_targets)\n            x_np = np.array(x_values)\n            mask = y_np > 1e-9 # Parity with C++ log protection\n            if not mask.all():\n                print(f\"Warning: Filtering out {(~mask).sum()} zero or negative data points for log transformation.\")\n                y_np = y_np[mask]\n                x_np = x_np[mask]\n            y_targets = np.log(y_np).tolist()\n            x_values = x_np.tolist()\n\n        x_t = torch.tensor(x_values, device=self.device, dtype=torch.float64)\n        y_t = torch.tensor(y_targets, device=self.device, dtype=torch.float64)\n        \n        # Flatten only if strictly 1 variable and input is weirdly shaped?\n        # If num_variables > 1, we expect x_t to be [N, Vars]\n        if self.num_variables == 1:\n             if x_t.ndim > 1: x_t = x_t.flatten()\n        \n        if y_t.ndim > 1: y_t = y_t.flatten()\n\n        # The Sniper\n        sniper_res = self.sniper.run(x_values, y_targets)\n        if sniper_res: return sniper_res\n        \n        print(\"[GPU Worker] Initializing Tensor Population...\")\n        \n        # 2. Init Population\n        population = self.initialize_population()\n        # population[:, 0] = torch.randint(...) # No longer needed, as we have valid RPNs\n\n \n        \n        # Seeds\n        if seeds:\n            seed_tensors = self.infix_to_rpn(seeds)\n            k_seeds = seed_tensors.shape[0]\n            if k_seeds > 0:\n                population[:k_seeds] = seed_tensors\n        \n        # --- Pattern Detection ---\n        # Detect standard sequences (Arithmetic, Geometric)\n        pattern_seeds = self.detect_patterns(y_targets)\n        if pattern_seeds:\n            print(f\"[GPU Worker] Detected patterns: {pattern_seeds}\")\n            pat_tensors = self.infix_to_rpn(pattern_seeds)\n            k_pats = pat_tensors.shape[0]\n            if k_pats > 0:\n                 # Insert after user seeds\n                 offset = len(seeds) if seeds else 0\n                 population[offset:offset+k_pats] = pat_tensors\n\n        pop_constants = torch.randn(self.pop_size, self.max_constants, device=self.device, dtype=torch.float64)\n        \n        # Stats\n        best_rmse = float('inf')\n        best_rpn = None\n        best_consts_vec = None\n        \n        stagnation_counter = 0\n        current_mutation_rate = GpuGlobals.BASE_MUTATION_RATE\n        \n        generations = 0\n        COMPLEXITY_PENALTY = GpuGlobals.COMPLEXITY_PENALTY\n        max_generations = GpuGlobals.GENERATIONS\n\n        # Loop until: fitness ~0, OR max generations, OR timeout\n        while generations < max_generations:\n            # Check timeout (optional, set timeout_sec=None to disable)\n            if timeout_sec and (time.time() - start_time) >= timeout_sec:\n                print(f\"[GPU] Timeout after {generations} generations\")\n                break\n                \n            generations += 1\n            \n\n            # Eval (Fast Scan)\n            fitness_rmse = self.evaluate_batch(population, x_t, y_t, pop_constants)\n            \n            # --- Constant Optimization (Top K) ---\n            # Optimize top 200 candidates to refine their constants\n            k_opt = min(self.pop_size, 200)\n            \n            # Find candidates (using penalized fitness or raw rmse?)\n            # Raw RMSE is better for optimization target\n            _, top_idx = torch.topk(fitness_rmse, k_opt, largest=False)\n            \n            # Extract subset\n            opt_pop = population[top_idx]\n            opt_consts = pop_constants[top_idx]\n            \n            # Optimize (Gradient Descent)\n            # Use fewer steps to keep speed up? 10 is fine.\n            refined_consts, refined_mse = self.optimize_constants(\n                opt_pop, opt_consts, x_t, y_t, steps=10, lr=0.1\n            )\n            \n            # Update population constants\n            pop_constants[top_idx] = refined_consts\n            \n            # Update fitness for optimized individuals (optional but good for accurate tracking)\n            # refined_mse is actually RMSE from the function\n            fitness_rmse[top_idx] = refined_mse\n            \n            # Re-evaluate penalties? Length doesn't change.\n            # But we can just leave it for next gen or update fitness_penalized here.\n            # Let's update Penalized so Elitism picks the improved versions instantly.\n            # We need lengths for these\n            # lengths is [PopSize], so we pick top_idx\n            # fitness_penalized[top_idx] = refined_mse * (1.0 + lengths[top_idx] * COMPLEXITY_PENALTY)\n            \n            # Run Adam\n            # refined_consts, refined_rmse = self.optimize_constants(opt_pop, opt_consts, x_t, y_t, steps=15)\n            \n            # Update Population\n            # We must update the original tensors.\n            # 1. Update Constants\n            # pop_constants[top_idx] = refined_consts\n            # 2. Update Fitness Scores (Evaluation was implicit in optimize)\n            # But wait, fitness_rmse is [PopSize].\n            # We update the scores.\n            # fitness_rmse[top_idx] = refined_rmse\n            \n            # --- Selection ---\n            lengths = (population != PAD_ID).sum(dim=1).float()\n            fitness_penalized = fitness_rmse * (1.0 + COMPLEXITY_PENALTY * lengths) + lengths * 1e-6\n            \n            # --- Tarpeian Bloat Control ---\n            fitness_penalized = self.tarpeian_control(population, fitness_penalized)\n            \n            # Select Best\n            min_rmse, min_idx = torch.min(fitness_rmse, dim=0)\n            if min_rmse.item() < best_rmse:\n                best_rmse = min_rmse.item()\n                best_rpn = population[min_idx].clone()\n                best_consts_vec = pop_constants[min_idx].clone()\n                best_island_idx = (min_idx.item() // self.island_size) \n                \n                if callback:\n                    callback(generations, best_rmse, best_rpn, best_consts_vec, True, best_island_idx)\n                \n                stagnation_counter = 0\n                current_mutation_rate = GpuGlobals.BASE_MUTATION_RATE\n            else:\n                stagnation_counter += 1\n            \n            if callback and (generations % GpuGlobals.PROGRESS_REPORT_INTERVAL == 0 or generations == 1) and best_rpn is not None:\n                 callback(generations, best_rmse, best_rpn, best_consts_vec, False, -1)\n\n            # --- Island Migration ---\n            if self.n_islands > 1 and generations % GpuGlobals.MIGRATION_INTERVAL == 0:\n                population, pop_constants = self.migrate_islands(population, pop_constants, fitness_rmse)\n\n            # Cataclysm\n            if stagnation_counter >= GpuGlobals.STAGNATION_LIMIT:\n                 saved_best_rpn = best_rpn.clone()\n                 saved_best_c = best_consts_vec.clone()\n                 population = self.initialize_population()\n                 pop_constants = torch.randn(self.pop_size, self.max_constants, device=self.device, dtype=torch.float64)\n                 population[0] = saved_best_rpn\n                 pop_constants[0] = saved_best_c\n                 stagnation_counter = 0\n                 current_mutation_rate = GpuGlobals.BASE_MUTATION_RATE\n                 continue\n            \n            # --- Dynamic Mutation Rate ---\n            if stagnation_counter > 10:\n                current_mutation_rate = min(0.4, GpuGlobals.BASE_MUTATION_RATE + (stagnation_counter - 10) * 0.01)\n            else:\n                current_mutation_rate = GpuGlobals.BASE_MUTATION_RATE\n\n\n            # --- NEW ADVANCED EVOLUTION STEP ---\n            # 1. Elitism\n            # 2. Crossover (Lexicase Sel)\n            # 3. Mutation (Tournament Sel)\n            # 4. Uniqueness Check\n            \n            next_pop_list = []\n            next_const_list = []\n            \n            # 1. Elitism (Top 5% using Pareto or Fitness)\n            k_elite = max(1, int(self.pop_size * 0.05))\n            \n            if GpuGlobals.USE_PARETO_SELECTION:\n                # Use NSGA-II to select elite individuals balancing error vs complexity\n                complexity = lengths  # Tree size as complexity\n                elite_idx = self.pareto.select(population, fitness_rmse, complexity, k_elite)\n            else:\n                # Standard fitness-based elitism\n                _, elite_idx = torch.topk(fitness_penalized, k_elite, largest=False)\n            \n            elites = population[elite_idx]\n            elites_c = pop_constants[elite_idx]\n            next_pop_list.append(elites)\n            next_const_list.append(elites_c)\n            \n            remaining_slots = self.pop_size - k_elite\n            \n            # 2. Crossover (Using Lexicase if costly or standard if fast?)\n            # Lexicase is costly. Let's compute FULL errors only if using Lexicase.\n            # Use Lexicase for Crossover Parents (Standard GP practice)\n            \n            # 2. Crossover Parents (GPU Tournament for Speed)\n            # 2. Crossover Parents (GPU Tournament for Speed)\n            n_crossover = int(remaining_slots * GpuGlobals.DEFAULT_CROSSOVER_RATE)\n            n_mutation = remaining_slots - n_crossover\n            \n            if n_crossover > 0:\n                idx_cross = torch.randint(0, self.pop_size, (n_crossover, GpuGlobals.DEFAULT_TOURNAMENT_SIZE), device=self.device)\n                best_in_tourn = torch.argmin(fitness_penalized[idx_cross], dim=1)\n                global_idx_cross = idx_cross.gather(1, best_in_tourn.unsqueeze(1)).squeeze(1)\n                \n                # We need PAIRS of parents. This logic selects N individuals.\n                # crossover_population internally shuffles and pairs them.\n                parents_cross = population[global_idx_cross]\n                consts_cross = pop_constants[global_idx_cross]\n                \n                # Perform crossover (Vectorized)\n                off_cross = self.crossover_population(parents_cross, crossover_rate=1.0) # Rate 1.0 because we already selected size\n                next_pop_list.append(off_cross)\n                next_const_list.append(consts_cross)\n            \n            # 3. Mutation Parents (Tournament)\n            if n_mutation > 0:\n                idx_mut = torch.randint(0, self.pop_size, (n_mutation, GpuGlobals.DEFAULT_TOURNAMENT_SIZE), device=self.device)\n                best_in_tourn = torch.argmin(fitness_penalized[idx_mut], dim=1)\n                global_idx_mut = idx_mut.gather(1, best_in_tourn.unsqueeze(1)).squeeze(1)\n                \n                parents_mut = population[global_idx_mut]\n                consts_mut = pop_constants[global_idx_mut]\n                \n                off_mut = self.mutate_population(parents_mut, current_mutation_rate)\n                next_pop_list.append(off_mut)\n                next_const_list.append(consts_mut)\n            \n            \n            # Concatenate\n            next_pop = torch.cat(next_pop_list, dim=0)\n            next_c = torch.cat(next_const_list, dim=0)\n            \n            population = next_pop[:self.pop_size]\n            pop_constants = next_c[:self.pop_size]\n\n            # --- Deduplication (Aggressive: Every generation to force diversity) ---\n            if GpuGlobals.PREVENT_DUPLICATES and generations % 1 == 0:\n                population, pop_constants, n_dups = self.deduplicate_population(population, pop_constants)\n                # Silent - only log if many duplicates\n                if n_dups > self.pop_size * 0.1:\n                    print(f\"[GPU] Removed {n_dups} duplicates (Fresh Randoms Injected)\")\n            \n            # Debug: Report Valid Count\n            if generations % 5 == 0:\n                 valid_cnt = (fitness_rmse < 1e9).sum().item()\n                 print(f\"[GPU] Gen {generations}: Valid Individuals = {valid_cnt}/{self.pop_size}\")\n                    \n                # TRIGGER CATACLYSM if > 90% are duplicates - REMOVED (Redundant with Fresh Random Injection)\n                # if n_dups > self.pop_size * 0.9 and generations > 20: \n                #     print(f\"!!! CATACLYSM TRIGGERED (Duplicates: {n_dups}/{self.pop_size}) !!!\")\n                #     print(\"!!! Resetting 90% of population with fresh DNA !!!\")\n                #     population, pop_constants = self.cataclysm_population(population, pop_constants, fitness_rmse)\n\n            # --- Simplification (Reduced Frequency: Every 500 generations) ---\n            if GpuGlobals.USE_SIMPLIFICATION and generations % 500 == 0:\n                # Simplify top 50 individuals\n                population, pop_constants, n_simp = self.simplify_population(population, pop_constants, top_k=50)\n                if n_simp > 0 and callback:\n                    print(f\"[GPU] Simplified {n_simp} expressions\")\n\n            # --- Pattern Memory (DISABLED FOR SPEED TEST) ---\n            # Record successful subtrees from current population\n            # self.pattern_memory.record_subtrees(population, fitness_rmse, self.grammar)\n            \n            # Inject patterns periodically\n            # if generations % GpuGlobals.PATTERN_INJECT_INTERVAL == 0:\n            #     population, pop_constants, n_inj = self.pattern_memory.inject_into_population(\n            #         population, pop_constants, self.grammar, \n            #         percent=GpuGlobals.PATTERN_INJECT_PERCENT\n            #     )\n\n            # --- Local Search (DISABLED FOR SPEED TEST) ---\n            # if generations % 100 == 0:\n            #     population, pop_constants = self.local_search(\n            #         population, pop_constants, x_t, y_t, \n            #         top_k=10, attempts=GpuGlobals.LOCAL_SEARCH_ATTEMPTS\n            #     )\n            \n            if best_rmse < 1e-7:\n                 return self.rpn_to_infix(best_rpn, best_consts_vec)\n                 \n        if best_rpn is not None:\n             return self.rpn_to_infix(best_rpn, best_consts_vec)\n        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/gpu/ensemble.py\n",
        "\"\"\"\nEnsemble / Coevolution Support for GPU GP Engine.\n\nProvides utilities to:\n- Run multiple GP engines in parallel\n- Combine results from multiple runs\n- Share best individuals between runs (coevolution)\n\"\"\"\nimport torch\nimport numpy as np\nfrom typing import List, Tuple, Optional, Callable\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport time\n\n\nclass EnsembleRunner:\n    \"\"\"\n    Runs multiple GP engines and combines their results.\n    \n    Supports:\n    - Parallel execution of multiple runs\n    - Hall of Fame aggregation across runs\n    - Best solution selection with Pareto consideration\n    \"\"\"\n    \n    def __init__(self, engine_factory: Callable, n_runs: int = 5, \n                 share_best: bool = True, share_interval: int = 100):\n        \"\"\"\n        Args:\n            engine_factory: Function that creates a TensorGeneticEngine instance\n            n_runs: Number of parallel runs\n            share_best: Whether to share best solutions between runs\n            share_interval: Generations between sharing best solutions\n        \"\"\"\n        self.engine_factory = engine_factory\n        self.n_runs = n_runs\n        self.share_best = share_best\n        self.share_interval = share_interval\n        \n        # Hall of Fame: list of (formula_str, rmse, complexity)\n        self.hall_of_fame: List[Tuple[str, float, int]] = []\n        self.max_hof_size = 20\n        \n    def run_single(self, engine, x_values, y_targets, seeds, timeout_sec, run_id) -> Tuple[Optional[str], float, int]:\n        \"\"\"\n        Run a single GP engine.\n        \n        Returns:\n            (best_formula, best_rmse, run_id)\n        \"\"\"\n        try:\n            result = engine.run(x_values, y_targets, seeds, timeout_sec=timeout_sec)\n            \n            # Get fitness from last evaluation\n            if hasattr(engine, 'best_rmse'):\n                rmse = engine.best_rmse\n            else:\n                rmse = float('inf')\n            \n            return (result, rmse, run_id)\n        except Exception as e:\n            print(f\"[Ensemble] Run {run_id} failed: {e}\")\n            return (None, float('inf'), run_id)\n    \n    def run_ensemble(self, x_values: List[float], y_targets: List[float], \n                     seeds: List[str] = None, timeout_sec: float = 10,\n                     callback: Callable = None) -> str:\n        \"\"\"\n        Run ensemble of GP engines and return best result.\n        \n        Args:\n            x_values: Input data\n            y_targets: Target data\n            seeds: Optional seed formulas\n            timeout_sec: Timeout per run\n            callback: Optional progress callback\n            \n        Returns:\n            Best formula found across all runs\n        \"\"\"\n        if seeds is None:\n            seeds = []\n        \n        # Create engines\n        engines = [self.engine_factory() for _ in range(self.n_runs)]\n        \n        results = []\n        best_formula = None\n        best_rmse = float('inf')\n        \n        # Run sequentially (parallel would require careful GPU memory management)\n        for i, engine in enumerate(engines):\n            if callback:\n                callback(f\"Running engine {i+1}/{self.n_runs}\")\n            \n            # Use shared seeds from Hall of Fame\n            shared_seeds = seeds.copy()\n            if self.share_best and self.hall_of_fame:\n                top_hof = [f for f, _, _ in self.hall_of_fame[:5]]\n                shared_seeds.extend(top_hof)\n            \n            result, rmse, _ = self.run_single(engine, x_values, y_targets, \n                                              shared_seeds, timeout_sec, i)\n            \n            if result:\n                results.append((result, rmse))\n                \n                # Update Hall of Fame\n                complexity = len(result) if result else 0\n                self._add_to_hof(result, rmse, complexity)\n                \n                if rmse < best_rmse:\n                    best_rmse = rmse\n                    best_formula = result\n        \n        if callback:\n            callback(f\"Ensemble complete. Best RMSE: {best_rmse:.6f}\")\n        \n        return best_formula\n    \n    def _add_to_hof(self, formula: str, rmse: float, complexity: int):\n        \"\"\"Add formula to Hall of Fame if it's good enough.\"\"\"\n        if formula is None:\n            return\n        \n        # Check if already in HoF\n        for existing_formula, _, _ in self.hall_of_fame:\n            if existing_formula == formula:\n                return\n        \n        # Add\n        self.hall_of_fame.append((formula, rmse, complexity))\n        \n        # Sort by (rmse, complexity) - lexicographic\n        self.hall_of_fame.sort(key=lambda x: (x[1], x[2]))\n        \n        # Trim to max size\n        if len(self.hall_of_fame) > self.max_hof_size:\n            self.hall_of_fame = self.hall_of_fame[:self.max_hof_size]\n    \n    def get_pareto_front(self) -> List[Tuple[str, float, int]]:\n        \"\"\"\n        Get Pareto-optimal solutions from Hall of Fame.\n        \n        Returns:\n            List of (formula, rmse, complexity) tuples on the Pareto front\n        \"\"\"\n        if not self.hall_of_fame:\n            return []\n        \n        pareto = []\n        for formula, rmse, complexity in self.hall_of_fame:\n            is_dominated = False\n            for other_formula, other_rmse, other_complexity in self.hall_of_fame:\n                if other_formula == formula:\n                    continue\n                # Check if other dominates this\n                if other_rmse <= rmse and other_complexity <= complexity:\n                    if other_rmse < rmse or other_complexity < complexity:\n                        is_dominated = True\n                        break\n            \n            if not is_dominated:\n                pareto.append((formula, rmse, complexity))\n        \n        return pareto\n    \n    def get_best(self) -> Optional[str]:\n        \"\"\"Get best formula from Hall of Fame.\"\"\"\n        if not self.hall_of_fame:\n            return None\n        return self.hall_of_fame[0][0]\n\n\ndef create_ensemble_runner(device=None, pop_size=1000, n_runs=5):\n    \"\"\"\n    Factory function to create an EnsembleRunner.\n    \n    Args:\n        device: Torch device\n        pop_size: Population size per run\n        n_runs: Number of runs\n        \n    Returns:\n        EnsembleRunner instance\n    \"\"\"\n    from . import TensorGeneticEngine\n    \n    def factory():\n        return TensorGeneticEngine(device=device, pop_size=pop_size, n_islands=4)\n    \n    return EnsembleRunner(factory, n_runs=n_runs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/gpu/formatting.py\n",
        "\ndef format_const(val: float) -> str:\n    \"\"\"\n    Format a constant float to string matching CPU engine rules:\n    - Integer-like values -> \"3\"\n    - Extreme values (>=1e6 or <=1e-6) -> Scientific \"1.23456789e+09\"\n    - Normal values -> Fixed \"1.23456789\", trimmed trailing zeros and dot.\n    \"\"\"\n    if abs(val - round(val)) < 1e-9:\n        return str(int(round(val)))\n    if abs(val) >= 1e6 or abs(val) <= 1e-6:\n        return f\"{val:.8e}\"\n    s = f\"{val:.8f}\"\n    s = s.rstrip('0').rstrip('.')\n    return s if s else \"0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/gpu/pareto.py\n",
        "\"\"\"\nPareto Optimization (NSGA-II) for GPU GP Engine.\n\nImplements multi-objective optimization balancing:\n- Objective 1: Error (RMSE) - minimize\n- Objective 2: Complexity (tree size) - minimize\n\"\"\"\nimport torch\nimport numpy as np\nfrom typing import List, Tuple\n\n\nclass ParetoOptimizer:\n    \"\"\"\n    NSGA-II style Pareto optimizer for symbolic regression.\n    \n    Objectives:\n        - fitness: RMSE (lower is better)\n        - complexity: number of tokens (lower is better)\n    \"\"\"\n    \n    def __init__(self, device: torch.device, max_front_size: int = 50):\n        self.device = device\n        self.max_front_size = max_front_size\n    \n    def dominates(self, obj_a: Tuple[float, float], obj_b: Tuple[float, float]) -> bool:\n        \"\"\"\n        Check if solution A dominates solution B (both objectives <= and at least one <).\n        \"\"\"\n        a_fit, a_comp = obj_a\n        b_fit, b_comp = obj_b\n        \n        # A dominates B if A is <= B in all objectives and < in at least one\n        at_least_one_better = (a_fit < b_fit) or (a_comp < b_comp)\n        not_worse = (a_fit <= b_fit) and (a_comp <= b_comp)\n        \n        return not_worse and at_least_one_better\n    \n    def non_dominated_sort(self, fitness: torch.Tensor, complexity: torch.Tensor) -> List[List[int]]:\n        \"\"\"\n        Perform non-dominated sorting on the population.\n        \n        Args:\n            fitness: [PopSize] RMSE values\n            complexity: [PopSize] tree sizes\n            \n        Returns:\n            List of fronts, where each front is a list of indices\n        \"\"\"\n        n = fitness.shape[0]\n        fitness_cpu = fitness.cpu().numpy()\n        complexity_cpu = complexity.cpu().numpy()\n        \n        # For each individual, count how many dominate it\n        domination_count = np.zeros(n, dtype=np.int32)\n        dominated_by = [[] for _ in range(n)]  # Who each individual dominates\n        \n        for i in range(n):\n            for j in range(i + 1, n):\n                obj_i = (fitness_cpu[i], complexity_cpu[i])\n                obj_j = (fitness_cpu[j], complexity_cpu[j])\n                \n                if self.dominates(obj_i, obj_j):\n                    dominated_by[i].append(j)\n                    domination_count[j] += 1\n                elif self.dominates(obj_j, obj_i):\n                    dominated_by[j].append(i)\n                    domination_count[i] += 1\n        \n        # Build fronts\n        fronts = []\n        current_front = []\n        \n        # First front: individuals with domination_count = 0\n        for i in range(n):\n            if domination_count[i] == 0:\n                current_front.append(i)\n        \n        while current_front:\n            fronts.append(current_front)\n            next_front = []\n            \n            for i in current_front:\n                for j in dominated_by[i]:\n                    domination_count[j] -= 1\n                    if domination_count[j] == 0:\n                        next_front.append(j)\n            \n            current_front = next_front\n        \n        return fronts\n    \n    def crowding_distance(self, front: List[int], fitness: torch.Tensor, complexity: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Calculate crowding distance for individuals in a front.\n        \n        Args:\n            front: List of indices in this front\n            fitness: [PopSize] RMSE values\n            complexity: [PopSize] tree sizes\n            \n        Returns:\n            [len(front)] crowding distances\n        \"\"\"\n        n = len(front)\n        if n <= 2:\n            return torch.full((n,), float('inf'), device=self.device)\n        \n        distances = torch.zeros(n, device=self.device, dtype=torch.float64)\n        \n        # For each objective\n        for obj_vals in [fitness, complexity]:\n            # Get values for this front\n            front_vals = obj_vals[front].cpu().numpy()\n            \n            # Sort by objective\n            sorted_idx = np.argsort(front_vals)\n            \n            # Boundary points get infinite distance\n            distances[sorted_idx[0]] = float('inf')\n            distances[sorted_idx[-1]] = float('inf')\n            \n            # Normalize by range\n            obj_range = front_vals[sorted_idx[-1]] - front_vals[sorted_idx[0]]\n            if obj_range < 1e-9:\n                continue\n            \n            # Calculate crowding distance for interior points\n            for i in range(1, n - 1):\n                distances[sorted_idx[i]] += (front_vals[sorted_idx[i + 1]] - front_vals[sorted_idx[i - 1]]) / obj_range\n        \n        return distances\n    \n    def select(self, population: torch.Tensor, fitness: torch.Tensor, complexity: torch.Tensor, n_select: int) -> torch.Tensor:\n        \"\"\"\n        Select n_select individuals using NSGA-II selection.\n        \n        Args:\n            population: [PopSize, L] RPN tensors\n            fitness: [PopSize] RMSE values\n            complexity: [PopSize] tree sizes\n            n_select: Number of individuals to select\n            \n        Returns:\n            [n_select] tensor of selected indices\n        \"\"\"\n        # Non-dominated sorting\n        fronts = self.non_dominated_sort(fitness, complexity)\n        \n        selected = []\n        \n        for front in fronts:\n            if len(selected) + len(front) <= n_select:\n                # Add entire front\n                selected.extend(front)\n            else:\n                # Need to select subset using crowding distance\n                remaining = n_select - len(selected)\n                \n                # Calculate crowding distance\n                distances = self.crowding_distance(front, fitness, complexity)\n                \n                # Select by highest crowding distance\n                _, sorted_idx = torch.sort(distances, descending=True)\n                for i in range(remaining):\n                    selected.append(front[sorted_idx[i].item()])\n                \n                break\n        \n        return torch.tensor(selected, device=self.device, dtype=torch.long)\n    \n    def get_pareto_front(self, fitness: torch.Tensor, complexity: torch.Tensor) -> List[int]:\n        \"\"\"\n        Get indices of individuals in the Pareto front.\n        \n        Args:\n            fitness: [PopSize] RMSE values\n            complexity: [PopSize] tree sizes\n            \n        Returns:\n            List of indices in the Pareto front\n        \"\"\"\n        fronts = self.non_dominated_sort(fitness, complexity)\n        \n        if not fronts:\n            return []\n        \n        front = fronts[0]\n        \n        # Limit size\n        if len(front) > self.max_front_size:\n            distances = self.crowding_distance(front, fitness, complexity)\n            _, sorted_idx = torch.sort(distances, descending=True)\n            front = [front[sorted_idx[i].item()] for i in range(self.max_front_size)]\n        \n        return front\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/gpu/pattern_memory.py\n",
        "\"\"\"\nPattern Memory System for GPU GP Engine.\n\nStores successful subtrees/patterns and injects them into the population\nto accelerate convergence by reusing proven building blocks.\n\"\"\"\nimport torch\nimport numpy as np\nfrom typing import List, Dict, Tuple, Optional\nfrom collections import defaultdict\n\n\nclass PatternMemory:\n    \"\"\"\n    Memory system that stores successful formula patterns (subtrees).\n    \n    Patterns with good fitness scores are recorded and can be injected\n    into the population to share successful building blocks.\n    \"\"\"\n    \n    def __init__(self, device: torch.device, max_patterns: int = 100, \n                 fitness_threshold: float = 10.0, min_uses: int = 3):\n        \"\"\"\n        Args:\n            device: Torch device\n            max_patterns: Maximum number of patterns to store\n            fitness_threshold: Only record patterns from individuals with fitness below this\n            min_uses: Minimum uses before a pattern is considered \"useful\"\n        \"\"\"\n        self.device = device\n        self.max_patterns = max_patterns\n        self.fitness_threshold = fitness_threshold\n        self.min_uses = min_uses\n        \n        # Pattern storage: hash -> (pattern_rpn, count, best_fitness)\n        self.patterns: Dict[tuple, Tuple[List[int], int, float]] = {}\n        \n        # Usage stats\n        self.total_recorded = 0\n        self.total_injected = 0\n    \n    def record_subtrees(self, population: torch.Tensor, fitness: torch.Tensor, \n                        grammar, min_size: int = 3, max_size: int = 10):\n        \"\"\"\n        Extract and record successful subtrees from the population.\n        \n        Args:\n            population: [PopSize, L] RPN tensors\n            fitness: [PopSize] RMSE values\n            grammar: GPUGrammar for subtree extraction\n            min_size: Minimum subtree size to record\n            max_size: Maximum subtree size to record\n        \"\"\"\n        pop_cpu = population.cpu().numpy()\n        fit_cpu = fitness.cpu().numpy()\n        \n        # Only look at individuals with good fitness\n        good_mask = fit_cpu < self.fitness_threshold\n        good_indices = np.where(good_mask)[0]\n        \n        for idx in good_indices[:50]:  # Limit to prevent slowdown\n            rpn = pop_cpu[idx]\n            fit = fit_cpu[idx]\n            \n            # Find all subtrees\n            subtrees = self._extract_subtrees(rpn, grammar, min_size, max_size)\n            \n            for subtree in subtrees:\n                self._record_pattern(subtree, fit)\n    \n    def _extract_subtrees(self, rpn: np.ndarray, grammar, min_size: int, max_size: int) -> List[List[int]]:\n        \"\"\"\n        Extract all valid subtrees from an RPN expression.\n        \"\"\"\n        subtrees = []\n        \n        # Find non-pad length\n        non_pad = rpn[rpn != 0]\n        if len(non_pad) < min_size:\n            return subtrees\n        \n        # Try each position as potential subtree root\n        for root_idx in range(len(non_pad)):\n            span = grammar.get_subtree_span(non_pad.tolist(), root_idx)\n            if span[0] == -1:\n                continue\n            \n            start, end = span\n            size = end - start + 1\n            \n            if min_size <= size <= max_size:\n                subtree = non_pad[start:end+1].tolist()\n                subtrees.append(subtree)\n        \n        return subtrees\n    \n    def _record_pattern(self, pattern: List[int], fitness: float):\n        \"\"\"\n        Record a pattern in memory.\n        \"\"\"\n        key = tuple(pattern)\n        \n        if key in self.patterns:\n            rpn, count, best_fit = self.patterns[key]\n            self.patterns[key] = (rpn, count + 1, min(best_fit, fitness))\n        else:\n            if len(self.patterns) >= self.max_patterns:\n                # Evict least used pattern\n                self._evict_least_useful()\n            \n            self.patterns[key] = (pattern, 1, fitness)\n            self.total_recorded += 1\n    \n    def _evict_least_useful(self):\n        \"\"\"\n        Remove the least useful pattern (lowest count, highest fitness).\n        \"\"\"\n        if not self.patterns:\n            return\n        \n        # Score: higher is worse (low count, high fitness)\n        def score(item):\n            key, (rpn, count, best_fit) = item\n            return -count + best_fit / 100.0\n        \n        worst_key = max(self.patterns.items(), key=score)[0]\n        del self.patterns[worst_key]\n    \n    def get_useful_patterns(self, n: int = 10) -> List[List[int]]:\n        \"\"\"\n        Get the top N most useful patterns.\n        \n        Args:\n            n: Number of patterns to return\n            \n        Returns:\n            List of RPN patterns (as lists of token IDs)\n        \"\"\"\n        # Filter by min_uses\n        useful = [(k, v) for k, v in self.patterns.items() if v[1] >= self.min_uses]\n        \n        # Sort by usefulness: high count, low fitness\n        useful.sort(key=lambda x: (-x[1][1], x[1][2]))\n        \n        return [list(k) for k, v in useful[:n]]\n    \n    def inject_into_population(self, population: torch.Tensor, constants: torch.Tensor,\n                                grammar, percent: float = 0.05) -> Tuple[torch.Tensor, torch.Tensor, int]:\n        \"\"\"\n        Inject useful patterns into the population by replacing some individuals.\n        \n        Args:\n            population: [PopSize, L] RPN tensors\n            constants: [PopSize, MaxC] constants\n            grammar: GPUGrammar for token lookup\n            percent: Fraction of population to replace\n            \n        Returns:\n            (new_population, new_constants, n_injected)\n        \"\"\"\n        patterns = self.get_useful_patterns(20)\n        if not patterns:\n            return population, constants, 0\n        \n        pop_size, max_len = population.shape\n        n_inject = max(1, int(pop_size * percent))\n        n_inject = min(n_inject, len(patterns) * 2)  # Don't inject more than we have variety\n        \n        pop_out = population.clone()\n        const_out = constants.clone()\n        \n        # Inject at random positions (avoid elites at front)\n        inject_start = int(pop_size * 0.1)  # Skip first 10% (elites)\n        inject_positions = torch.randint(inject_start, pop_size, (n_inject,))\n        \n        for i, pos in enumerate(inject_positions):\n            pattern = patterns[i % len(patterns)]\n            \n            # Pad pattern to max_len\n            padded = pattern + [0] * (max_len - len(pattern))\n            padded = padded[:max_len]\n            \n            pop_out[pos] = torch.tensor(padded, device=self.device, dtype=population.dtype)\n            \n            # Random constants for the pattern\n            const_out[pos] = torch.randn_like(const_out[pos]) * 0.5\n        \n        self.total_injected += n_inject\n        return pop_out, const_out, n_inject\n    \n    def get_stats(self) -> Dict:\n        \"\"\"\n        Get pattern memory statistics.\n        \"\"\"\n        return {\n            'n_patterns': len(self.patterns),\n            'total_recorded': self.total_recorded,\n            'total_injected': self.total_injected,\n            'useful_count': sum(1 for v in self.patterns.values() if v[1] >= self.min_uses)\n        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/gpu/sniper.py\n",
        "\nimport torch\nimport numpy as np\nfrom .formatting import format_const\n\nclass Sniper:\n    \"\"\"\n    'The Sniper': Special Pattern Detection Unit.\n    Quickly identifies simple patterns (Linear, Geometric) before evolution begins.\n    \"\"\"\n    def __init__(self, device):\n        self.device = device\n        \n    def check_linear(self, x_t, y_t):\n        \"\"\"\n        Check for y = m*x + c\n        Returns formula string if found, else None\n        \"\"\"\n        try:\n            # Solve [x, 1] * [m, c]^T = y\n            # A: [N, 2]\n            ones = torch.ones_like(x_t)\n            A = torch.stack([x_t, ones], dim=1)\n            \n            # Least squares\n            # solution = (A^T A)^-1 A^T y\n            solution = torch.linalg.lstsq(A, y_t).solution\n            m = solution[0].item()\n            c = solution[1].item()\n            \n            # Predict\n            y_pred = m * x_t + c\n            mse = torch.mean((y_pred - y_t)**2)\n            \n            # Threshold (1e-6)\n            if mse < 1e-6:\n                m_str = format_const(m)\n                c_str = format_const(c)\n                # Formats: \n                # (m * x) + c if c > 0\n                # (m * x) - |c| if c < 0\n                term = f\"({m_str} * x)\"\n                if c >= 0:\n                    return f\"({term} + {c_str})\"\n                else:\n                    return f\"({term} - {format_const(abs(c))})\"\n        except:\n            pass\n        return None\n\n    def check_geometric(self, x_t, y_t):\n        \"\"\"\n        Check for y = A * exp(B*x) -> log(y) = log(A) + B*x\n        Returns formula string if found, else None\n        \"\"\"\n        try:\n            if (y_t <= 0).any(): return None\n            \n            log_y = torch.log(y_t)\n            \n            # Solve [x, 1] * [B, log_A]^T = log_y\n            ones = torch.ones_like(x_t)\n            A_mat = torch.stack([x_t, ones], dim=1)\n            \n            solution = torch.linalg.lstsq(A_mat, log_y).solution\n            B = solution[0].item()\n            log_A = solution[1].item()\n            A_val = np.exp(log_A)\n            \n            # Predict\n            y_pred = A_val * torch.exp(B * x_t)\n            \n            # Check relative error for geometric? Or log-MSE?\n            # Let's check MSE of original\n            mse = torch.mean((y_pred - y_t)**2)\n            \n            if mse < 1e-4: # Slightly looser for exponential\n                # Formula: exp(B*x + log_A) if we want pure exp form? \n                # Or A * exp(B*x)?\n                # Our grammar supports exp inside.\n                # Let's use exp(B*x + log_A) which is exp(log(y)) = y.\n                # A * exp(Bx) = exp(lnA + Bx).\n                # GPU grammar usually prefers: exp( (B*x) + lnA )\n                \n                b_str = format_const(B)\n                ln_a_str = format_const(log_A)\n                \n                inner = f\"({b_str} * x)\"\n                if log_A >= 0:\n                    inner = f\"({inner} + {ln_a_str})\"\n                else:\n                     inner = f\"({inner} - {format_const(abs(log_A))})\"\n                     \n                return f\"exp({inner})\"\n        except:\n            pass\n        return None\n\n    def run(self, x_data, y_data):\n        \"\"\"\n        Run all checks.\n        x_data, y_data: CPU lists or arrays.\n        \"\"\"\n        try:\n            x_t = torch.tensor(x_data, device=self.device, dtype=torch.float32).flatten()\n            y_t = torch.tensor(y_data, device=self.device, dtype=torch.float32).flatten()\n            \n            res = self.check_linear(x_t, y_t)\n            if res: \n                print(f\"[The Sniper] Detected Linear Pattern: {res}\")\n                return res\n            \n            res = self.check_geometric(x_t, y_t)\n            if res:\n                print(f\"[The Sniper] Detected Geometric Pattern: {res}\")\n                return res\n            \n        except Exception as e:\n            # print(f\"[The Sniper] Failed: {e}\")\n            pass\n        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/data/synthetic_data.py\n",
        "import numpy as np\nimport random\nfrom core.grammar import VOCABULARY, OPERATORS, VARIABLES, CONSTANTS, ExpressionTree\nfrom data.augmentation import augment_formula_tokens\n\nclass DataGenerator:\n    def __init__(self, max_depth=5, population_size=1000, allowed_operators=None, num_variables=1):\n        self.max_depth = max_depth\n        self.population_size = population_size\n        self.num_variables = num_variables\n        self.vocab = VOCABULARY\n        # Use subset of variables based on num_variables\n        self.active_variables = VARIABLES[:num_variables]\n        \n        # Pre-compute terminal vs operator lists\n        self.terminals = self.active_variables + CONSTANTS\n        if allowed_operators:\n            self.operators = [op for op in allowed_operators if op in OPERATORS]\n        else:\n            self.operators = list(OPERATORS.keys())\n\n    def generate_random_tree(self, max_depth, current_depth=0):\n        if current_depth >= max_depth:\n            # Balanced Terminal Selection: 50% variable, 50% constant\n            if random.random() < 0.5:\n                return [random.choice(self.active_variables)]\n            else:\n                return [random.choice(CONSTANTS)]\n        \n        # Decide if terminal or operator\n        # Higher probability of operator at shallow depths\n        if random.random() < 0.7: \n            op = random.choice(self.operators)\n            arity = OPERATORS[op]\n            tokens = [op]\n            for _ in range(arity):\n                tokens.extend(self.generate_random_tree(max_depth, current_depth + 1))\n            return tokens\n        else:\n            # Balanced Terminal Selection: 40% var, 30% C, 30% numbers\n            r = random.random()\n            if r < 0.4:\n                return [random.choice(self.active_variables)]\n            elif r < 0.7:\n                return ['C']\n            else:\n                return [random.choice([c for c in CONSTANTS if c != 'C'])]\n\n    def generate_batch(self, batch_size, point_count=10, x_range=(-10, 10)):\n        \"\"\"\n        Generates a batch of (X, Y) pairs and their generating formulas.\n        \"\"\"\n        data = []\n        \n        while len(data) < batch_size:\n            # Generate random formula\n            tokens = self.generate_random_tree(self.max_depth)\n            tree = ExpressionTree(tokens)\n            \n            if not tree.is_valid:\n                continue\n            \n            # Ensure variables are present (90% of the time, check any active variable)\n            if not any(v in tokens for v in self.active_variables) and random.random() < 0.9:\n                continue\n                \n            # Generate random X points\n            # If num_variables > 1, shape (point_count, num_variables)\n            # If num_variables == 1, shape (point_count,) or (point_count, 1) - but maintain compat\n            if self.num_variables > 1:\n                x_values = np.random.uniform(x_range[0], x_range[1], (point_count, self.num_variables))\n                # Sorting 2D array by first col just for consistent indexing? or keep random?\n                # Maybe sort by first column for visualization\n                x_values = x_values[x_values[:, 0].argsort()]\n            else:\n                x_values = np.random.uniform(x_range[0], x_range[1], point_count)\n                # Sort X for cleaner visualization/learning\n                x_values.sort()\n            \n            # Randomize 'C' values if present\n            c_positions = tree.root.get_constant_positions()\n            constant_vals = {}\n            for pos in c_positions:\n                # Expanded range: -20 to 20. Favor 1.0 occasionally\n                val = random.uniform(-20, 20) if random.random() > 0.1 else 1.0\n                constant_vals[tuple(pos)] = val\n            \n            # Calculate Y with randomized constants\n            y_values = tree.evaluate(x_values, constants=constant_vals)\n            \n            # Check for validity (no NaNs, Infs, or extremely large values)\n            if np.any(np.isnan(y_values)) or np.any(np.isinf(y_values)):\n                continue\n            if np.max(np.abs(y_values)) > 1e4: # Reject too large numbers (1e6 causes NaN gradients)\n                continue\n            if np.std(y_values) < 1e-6: # Reject flat lines (too simple)\n                 # Optionally keep some, but mostly we want interesting curves\n                 if random.random() > 0.1: continue\n\n            data.append({\n                'tokens': tokens,\n                'infix': tree.get_infix(),\n                'x': x_values,\n                'y': y_values\n            })\n            \n        return data\n\n    def generate_structured_tree(self, complexity=1, input_node='x0'):\n        \"\"\"\n        Recursively builds a structured, human-like formula.\n        Respects self.operators.\n        \"\"\"\n        # Base cases\n        if complexity <= 0:\n            # Randomly choose between active_variables, C and constants\n            r = random.random()\n            if r < 0.4: return [random.choice(self.active_variables)]\n            if r < 0.7: return ['C']\n            return [random.choice([c for c in CONSTANTS if c != 'C'])]\n            \n        # Filter available structures based on allowed operators\n        available_structures = []\n        \n        # Arithmetic needed: +, -, *\n        if any(op in self.operators for op in ['+', '-', '*']):\n            available_structures.append('arithmetic')\n            \n        # Poly needed: pow\n        if 'pow' in self.operators:\n            available_structures.append('poly')\n            \n        # Trig needed: sin, cos, asin, acos, atan\n        if any(op in self.operators for op in ['sin', 'cos', 'asin', 'acos', 'atan']):\n            available_structures.append('trig')\n            \n        # Exp/Log needed\n        if 'exp' in self.operators or 'log' in self.operators:\n            available_structures.append('exp_log')\n            \n        # Composition needs enough variety\n        if len(self.operators) > 4 and complexity > 1:\n             available_structures.append('composition')\n        \n        # Fallback if nothing allowed matches (shouldn't happen with proper init)\n        if not available_structures:\n            return input_node if isinstance(input_node, list) else [input_node]\n\n        choice = random.choice(available_structures)\n        \n        if choice == 'poly':\n            # a*x + b or a*x^2 + b\n            a = str(random.randint(1, 5))\n            b = str(random.randint(-5, 5))\n            power = random.choice(['1', '2', '3'])\n            if power == '1':\n                term = ['*', a] + (input_node if isinstance(input_node, list) else [input_node])\n                return ['+', ] + term + [b]\n            else:\n                base = input_node if isinstance(input_node, list) else [input_node]\n                pow_term = ['pow'] + base + [power]\n                term = ['*', a] + pow_term\n                return ['+', ] + term + [b]\n                \n        elif choice == 'trig':\n            # Filter trig ops that are allowed\n            ops = [op for op in ['sin', 'cos', 'asin', 'acos', 'atan'] if op in self.operators]\n            if not ops: return input_node # Should be caught by structure check\n            func = random.choice(ops)\n            val = input_node if isinstance(input_node, list) else [input_node]\n            return [func] + val\n            \n        elif choice == 'exp_log':\n            ops = [op for op in ['exp', 'log'] if op in self.operators]\n            if not ops: return input_node\n            func = random.choice(ops)\n            val = input_node if isinstance(input_node, list) else [input_node]\n            return [func] + val\n            \n        elif choice == 'arithmetic':\n            # Allow mixing variables in arithmetic nodes\n            left_input = input_node if random.random() < 0.6 else random.choice(self.active_variables)\n            right_input = input_node if random.random() < 0.6 else random.choice(self.active_variables)\n            left = self.generate_structured_tree(complexity - 1, left_input)\n            right = self.generate_structured_tree(complexity - 1, right_input)\n            ops = [op for op in ['+', '-', '*'] if op in self.operators]\n            if not ops: return input_node\n            op = random.choice(ops)\n            return [op] + left + right\n            \n        elif choice == 'composition':\n            inner = self.generate_structured_tree(complexity - 1, input_node)\n            outer = self.generate_structured_tree(1, inner)\n            return outer\n            \n        return [input_node]\n\n    def generate_inverse_batch(self, batch_size, point_count=10, x_range=(-5, 5)):\n        \"\"\"\n        Generates complex, structured formulas using the new engine.\n        \"\"\"\n        data = []\n        attempts = 0\n        \n        while len(data) < batch_size and attempts < batch_size * 5:\n            attempts += 1\n            # Random complexity capped by max_depth\n            complexity = random.randint(1, max(1, self.max_depth - 1))\n            \n            try:\n                # Use random variable as starting seed if needed, but structured tree handles selection at leaves\n                tokens = self.generate_structured_tree(complexity, random.choice(self.active_variables))\n                \n                # Convert numeric strings to 'C' placeholders if needed\n                # But here we want the GROUND TRUTH tokens with numbers for checking?\n                # The model predicts tokens. 'C' is for optimization.\n                # If we train \"End-to-End\" (predict 3*x), we keep numbers.\n                # If we train \"Symbolic\" (predict C*x), we swap.\n                # The original code swapped numbers to 'C'. Let's check VOCABULARY.\n                # '1','2','3' are in VOCABULARY. So we can keep small integers.\n                # Large integers -> 'C'.\n                \n                final_tokens = []\n                for t in tokens:\n                    if t in self.vocab:\n                        final_tokens.append(t)\n                    else:\n                        # If it's a number not in vocab, map to C?\n                        # Or just nearest constant?\n                        # For now, simplistic mapping:\n                        try:\n                            val = float(t)\n                            if abs(val - round(val)) < 0.01 and str(int(round(val))) in self.vocab:\n                                final_tokens.append(str(int(round(val))))\n                            else:\n                                final_tokens.append('C')\n                        except:\n                            final_tokens.append('C')\n\n                # --- DATA AUGMENTATION ---\n                if random.random() < 0.3:\n                    final_tokens = augment_formula_tokens(final_tokens)\n                # -------------------------\n                \n                tree = ExpressionTree(final_tokens)\n                if not tree.is_valid:\n                    continue\n                \n                # Ensure variables are present (90% of the time)\n                if not any(v in final_tokens for v in self.active_variables) and random.random() < 0.9:\n                    continue\n                    \n                # Check constraints (depth, length)\n                if len(final_tokens) > 30: # Limit length\n                    continue\n\n                # Generate X points\n                # Use safer range for complex funcs\n                # Exp/Pow grow very fast, so we constrain X to avoid float overflow\n                range_limit = x_range\n                if 'exp' in final_tokens or 'pow' in final_tokens:\n                    range_limit = (-2, 2)\n                elif 'log' in final_tokens or 'sqrt' in final_tokens:\n                    range_limit = (0.1, 5)\n\n                if self.num_variables > 1:\n                    x_safe = np.linspace(range_limit[0], range_limit[1], point_count)\n                    # For multivar, linspace per column or random?\n                    # Let's use random uniform for coverage in multivar space\n                    x_safe = np.random.uniform(range_limit[0], range_limit[1], (point_count, self.num_variables))\n                else:\n                    x_safe = np.linspace(range_limit[0], range_limit[1], point_count)\n                \n                # Randomize 'C' values if present\n                c_positions = tree.root.get_constant_positions()\n                constant_vals = {}\n                for pos in c_positions:\n                    # Expanded range: -20 to 20\n                    val = random.uniform(-20, 20) if random.random() > 0.1 else 1.0\n                    constant_vals[tuple(pos)] = val\n                \n                y_values = tree.evaluate(x_safe, constants=constant_vals)\n                \n                # Quality Control\n                if np.any(np.isnan(y_values)) or np.any(np.isinf(y_values)):\n                    continue\n                if np.max(np.abs(y_values)) > 1e4: # Relaxed limit\n                    continue\n                if np.std(y_values) < 0.01: # Too flat\n                    continue\n                \n                data.append({\n                    'tokens': final_tokens,\n                    'infix': tree.get_infix(),\n                    'x': x_safe,\n                    'y': y_values\n                })\n            except Exception:\n                continue\n                \n        return data\n\n# Quick test if run directly\nif __name__ == \"__main__\":\n    gen = DataGenerator(max_depth=4)\n    batch = gen.generate_batch(5)\n    for item in batch:\n        print(f\"Formula: {item['infix']}\")\n        print(f\"Tokens: {item['tokens']}\")\n        print(f\"Y sample: {item['y'][:3]}...\")\n        print(\"-\" * 20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/data/benchmark_data.py\n",
        "import numpy as np\n\n# Standard Benchmark Problems\n# Levels: 1 (Easy), 2 (Medium), 3 (Hard)\n\nBENCHMARK_SUITE = [\n    # --- Level 1: Polynomials & Basic Arithmetic ---\n    {\n        'id': 'p1',\n        'name': 'Lineal',\n        'formula_str': '2.5 * x + 1.0',\n        'lambda': lambda x: 2.5 * x + 1.0,\n        'domain': (-10, 10),\n        'points': 20,\n        'level': 1\n    },\n    {\n        'id': 'p2',\n        'name': 'Cuadratica Simple',\n        'formula_str': 'x * x',\n        'lambda': lambda x: x**2,\n        'domain': (-5, 5),\n        'points': 20,\n        'level': 1\n    },\n    {\n        'id': 'p3',\n        'name': 'Polinomio Cubico',\n        'formula_str': 'x**3 + x**2',\n        'lambda': lambda x: x**3 + x**2,\n        'domain': (-3, 3),\n        'points': 20,\n        'level': 1\n    },\n    \n    # --- Level 2: Trigonometric & Transcendental ---\n    {\n        'id': 'p4',\n        'name': 'Seno Basico',\n        'formula_str': 'sin(x)',\n        'lambda': lambda x: np.sin(x),\n        'domain': (-np.pi, np.pi),\n        'points': 30,\n        'level': 2\n    },\n    {\n        'id': 'p5',\n        'name': 'Coseno Desplazado',\n        'formula_str': 'cos(x) + 1',\n        'lambda': lambda x: np.cos(x) + 1,\n        'domain': (-np.pi, np.pi),\n        'points': 30,\n        'level': 2\n    },\n    {\n        'id': 'p6',\n        'name': 'Exponencial Simple',\n        'formula_str': 'exp(x)',\n        'lambda': lambda x: np.exp(x),\n        'domain': (-2, 2), # Small domain to avoid explosion\n        'points': 20,\n        'level': 2\n    },\n    \n    # --- Level 3: Physics / Complex ---\n    {\n        'id': 'p7',\n        'name': 'Damped Oscillation',\n        'formula_str': 'exp(-x) * sin(2*x)',\n        'lambda': lambda x: np.exp(-x) * np.sin(2*x),\n        'domain': (0, 4),\n        'points': 40,\n        'level': 3\n    },\n    {\n        'id': 'p8',\n        'name': 'Gaussian',\n        'formula_str': 'exp(-x**2)',\n        'lambda': lambda x: np.exp(-x**2),\n        'domain': (-3, 3),\n        'points': 30,\n        'level': 3\n    },\n    {\n        'id': 'p9',\n        'name': 'Nguyen-3 (x^3 + x^2 + x)',\n        'formula_str': 'x**3 + x**2 + x',\n        'lambda': lambda x: x**3 + x**2 + x,\n        'domain': (-2, 2),\n        'points': 20,\n        'level': 3\n    },\n    {\n        'id': 'p10',\n        'name': 'Rational Function',\n        'formula_str': 'x / (1 + x**2)',\n        'lambda': lambda x: x / (1 + x**2),\n        'domain': (-4, 4),\n        'points': 30,\n        'level': 3\n    }\n]\n\ndef get_benchmark_data(problem_id):\n    \"\"\"Returns (x, y) for a specific problem ID.\"\"\"\n    for p in BENCHMARK_SUITE:\n        if p['id'] == problem_id:\n            x = np.linspace(p['domain'][0], p['domain'][1], p['points'])\n            y = p['lambda'](x)\n            return x, y, p\n    return None, None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/data/augmentation.py\n",
        "\nimport random\nfrom core.grammar import OPERATORS\n\ndef augment_formula_tokens(tokens):\n    \"\"\"\n    Applies mathematical invariants to generate an equivalent formula structure.\n    Acts as 'Data Augmentation' for symbolic regression.\n    \n    Supported Transformations:\n    1. Commutativity: (+) and (*)\n       e.g. [+ a b] -> [+ b a]\n    2. Identity:\n       e.g. x -> [+ x 0], x -> [* x 1] (Rarely used to avoid bloat, but useful for robustness)\n    3. Inverse operations (Conceptually):\n       Not implemented directly on tokens without tree parsing, \n       so we focus on purely structural swaps that don't change value.\n    \n    Args:\n        tokens (list): List of tokens in Prefix notation.\n    \n    Returns:\n        list: A new list of tokens representing an equivalent formula.\n    \"\"\"\n    if not tokens:\n        return []\n\n    # Helper to parse prefix expression into a tree-like structure (recursive)\n    def parse_prefix(token_list):\n        if not token_list:\n            return None, []\n        \n        root = token_list[0]\n        remaining = token_list[1:]\n        \n        if root in OPERATORS:\n            try:\n                arity = OPERATORS[root]\n                children = []\n                for _ in range(arity):\n                    child, remaining = parse_prefix(remaining)\n                    children.append(child)\n                return {'val': root, 'children': children}, remaining\n            except:\n                 # Fallback for malformed\n                return {'val': root, 'children': []}, remaining\n        else:\n            # Terminal\n            return {'val': root, 'children': []}, remaining\n\n    # Helper to flatten tree back to tokens\n    def flatten(node):\n        res = [node['val']]\n        for child in node['children']:\n            res.extend(flatten(child))\n        return res\n\n    # 1. Parse\n    try:\n        tree, _ = parse_prefix(tokens)\n    except:\n        return list(tokens) # Fail safe\n\n    # 2. Augment Recursive\n    def augment_recursive(node):\n        # First augment children\n        for i in range(len(node['children'])):\n            node['children'][i] = augment_recursive(node['children'][i])\n            \n        val = node['val']\n        children = node['children']\n        \n        # Transformation: Commutativity\n        if val in ['+', '*'] and len(children) == 2:\n            if random.random() < 0.5:\n                # Swap children\n                node['children'] = [children[1], children[0]]\n        \n        # Transformation: (- a b) -> (+ a (- b)) ? Too complex for tokens only without 'neg'\n        # Transformation: (+ x x) -> (* x 2) ?\n        if val == '+' and len(children) == 2:\n            # Check deep equality is hard, but simple check:\n            if flatten(children[0]) == flatten(children[1]):\n                if random.random() < 0.3:\n                    # Convert x + x -> x * 2\n                    return {'val': '*', 'children': [children[0], {'val': '2', 'children': []}]}\n\n        return node\n\n    # 3. Apply\n    augmented_tree = augment_recursive(tree)\n    \n    # 4. Flatten\n    return flatten(augmented_tree)\n\nif __name__ == \"__main__\":\n    # Test\n    # Formula: (+ x y) -> prefix ['+', 'x', 'y']\n    t1 = ['+', 'x', 'y']\n    print(f\"Original: {t1} -> Aug: {augment_formula_tokens(t1)}\")\n    \n    # Formula: (* (+ a b) c)\n    t2 = ['*', '+', 'a', 'b', 'c']\n    print(f\"Original: {t2} -> Aug: {augment_formula_tokens(t2)}\")\n    \n    # Formula: (+ x x)\n    t3 = ['+', 'x', 'x']\n    print(f\"Original: {t3} -> Aug: {augment_formula_tokens(t3)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/data/expanded_benchmarks.py\n",
        "\nimport pandas as pd\nimport numpy as np\nimport os\n\ndef load_expanded_feynman_subset(csv_path=\"data/benchmarks/FeynmanEquations.csv\", limit=50):\n    \"\"\"\n    Loads equations from the Feynman dataset and projects them to 1D.\n    Strategies for projection:\n    - Fix all variables except the first one to 1.0.\n    \"\"\"\n    \n    if not os.path.exists(csv_path):\n        print(f\"Warning: {csv_path} not found.\")\n        return []\n\n    try:\n        df = pd.read_csv(csv_path)\n    except Exception as e:\n        print(f\"Error reading CSV: {e}\")\n        return []\n    \n    problems = []\n    \n    # Filter for reasonable complexity (e.g., # variables <= 3 for now to ensure 1D projection makes sense)\n    # We can be bolder, but let's start safe.\n    # Actually, let's take everything and just project.\n    \n    count = 0\n    for idx, row in df.iterrows():\n        if limit is not None and count >= limit:\n            break\n            \n        try:\n            row_id = row['Filename']\n            formula_raw = str(row['Formula'])\n            num_vars = int(row['# variables'])\n            \n            # Extract variable names\n            var_names = []\n            for i in range(1, 11):\n                v_col = f'v{i}_name'\n                if v_col in row and pd.notna(row[v_col]):\n                    var_names.append(row[v_col])\n            \n            # Projection Logic\n            # We treat the first variable as 'x' and the rest as constants = 1.0\n            # We need to construct a python-evaluable string where other vars are replaced by 1.0\n            \n            target_var = var_names[0]\n            formula_1d = formula_raw\n            \n            # Replace other variables with \"1.0\"\n            for other_var in var_names[1:]:\n                # Simple replace might be dangerous if variable names are substrings of others\n                # But Feynman dataset usually uses distinct names like m, v, theta, sigma\n                # Better: use a context dict for eval, but we need a string for the model target?\n                # Actually, our model needs a target string that uses 'x'.\n                pass\n                \n            # Create a closure-like logic for evaluation\n            # We will store the full formula and the fixed context\n            fixed_context = {v: 1.0 for v in var_names[1:]}\n            \n            problems.append({\n                \"id\": row_id,\n                \"name\": f\"Feynman {row_id}\",\n                \"original_formula\": formula_raw,\n                \"target_var\": target_var,\n                \"fixed_context\": fixed_context,\n                \"description\": f\"Projected 1D (varying {target_var}, others fixed to 1.0)\"\n            })\n            count += 1\n            \n        except Exception as e:\n            continue\n            \n    return problems\n\ndef evaluate_projected_formula(formula, target_var, x_val, fixed_context):\n    \"\"\"\n    Evaluates the formula with x_val assigned to target_var, and others fixed.\n    \"\"\"\n    # math context\n    ctx = {\n        'exp': np.exp, 'sin': np.sin, 'cos': np.cos, 'sqrt': np.sqrt, 'log': np.log, \n        'pi': np.pi, 'theta': 1.0, 'sigma': 1.0 # Defaults\n    }\n    \n    # Constants from fixed_context\n    ctx.update(fixed_context)\n    \n    # Target variable\n    ctx[target_var] = x_val\n    \n    try:\n        return eval(formula, {}, ctx)\n    except Exception as e:\n        return np.full_like(x_val, np.nan)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/data/__init__.py\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/search/mcts.py\n",
        "import math\nimport numpy as np\nimport torch\nimport copy\nfrom core.grammar import VOCABULARY, TOKEN_TO_ID, OPERATORS, ExpressionTree, VARIABLES, OPERATOR_STAGES\nfrom utils.optimize_constants import optimize_constants\nfrom utils.data_utils import normalize_batch\n\nclass MCTSNode:\n    def __init__(self, tokens, parent=None, prior=0.0):\n        self.tokens = tokens\n        self.parent = parent\n        self.children = {}\n        self.visit_count = 0\n        self.value_sum = 0.0\n        self.prior = prior\n        self.is_expanded = False\n        \n        # for parallel search\n        self.virtual_loss = 0.0\n        self.virtual_visits = 0\n\n    @property\n    def value(self):\n        count = self.visit_count + self.virtual_visits\n        if count == 0:\n            return 0.0\n        # Combine real value and virtual loss\n        # Virtual loss is SUBTRACTED to discourage visits\n        return (self.value_sum - self.virtual_loss) / count\n\n    def ucb_score(self, c_puct=1.0):\n        count = self.visit_count + self.virtual_visits\n        parent_count = self.parent.visit_count + self.parent.virtual_visits if self.parent else 1\n        \n        if self.parent is None:\n            return 0.0\n            \n        u = c_puct * self.prior * math.sqrt(parent_count) / (1 + count)\n        return self.value + u\n\n    @property\n    def complexity(self):\n        \"\"\"Estimate complexity (length of formula).\"\"\"\n        return len(self.tokens)\n\nclass MCTS:\n    def __init__(self, model, device, grammar=None, c_puct=1.0, n_simulations=100, max_simulations=None, max_depth=50, complexity_lambda=0.1, max_len=200, batch_size=8, curriculum_stage=None, num_variables=1):\n        self.model = model\n        self.device = device\n        self.grammar = grammar\n        self.c_puct = c_puct\n        self.num_variables = num_variables\n        \n        # Handle backwards compatibility for max_simulations\n        if max_simulations is not None:\n            self.n_simulations = max_simulations\n        else:\n            self.n_simulations = n_simulations\n            \n        self.max_depth = max_depth\n        self.complexity_lambda = complexity_lambda\n        self.max_len = max_len\n        self.min_value = -float('inf')\n        self.max_value = float('inf')\n        self.vocab_size = len(VOCABULARY)\n        self.sos_id = self.vocab_size\n        self.batch_size = batch_size\n        \n        # Curriculum stage for operator filtering\n        self.curriculum_stage = curriculum_stage\n        self._build_allowed_tokens()\n        \n        # Pareto Front: List of {'tokens':, 'rmse':, 'complexity':, 'formula':}\n        self.pareto_front = []\n        \n        # Virtual loss constant usually 1-3\n        self.v_loss_const = 3.0\n    \n    def _build_allowed_tokens(self):\n        \"\"\"Build set of allowed token indices based on curriculum stage and num_variables.\"\"\"\n        # Terminals are always allowed, but respect num_variables\n        allowed = set(['C', '0', '1', '2', '3', '5', '10', 'pi', 'e'])\n        \n        # Determine allowed variables\n        if self.num_variables == 1:\n            allowed.update(['x', 'x0'])\n        else:\n            allowed.update(VARIABLES[:self.num_variables])\n        \n        # Add operators based on curriculum stage\n        if self.curriculum_stage is not None and self.curriculum_stage in OPERATOR_STAGES:\n            allowed.update(OPERATOR_STAGES[self.curriculum_stage])\n        else:\n            # No stage = all operators allowed\n            allowed.update(OPERATORS.keys())\n        \n        # Convert to indices\n        self.allowed_token_indices = set()\n        for token in allowed:\n            if token in TOKEN_TO_ID:\n                self.allowed_token_indices.add(TOKEN_TO_ID[token])\n        \n    def search(self, x_values, y_values, num_simulations=None):\n        \"\"\"\n        Run MCTS (Parallel/Batched) to find the best formula.\n        \"\"\"\n        self.pareto_front = [] # Reset Pareto Front for new search\n        root = MCTSNode(tokens=[])\n        \n        # Initial expansion (single)\n        self._expand_batch([root], x_values, y_values)\n        \n        best_rmse = float('inf')\n        best_formula = None\n        best_tokens = None\n        \n        limit = num_simulations if num_simulations is not None else self.n_simulations\n        \n        # Loop in batches\n        # Ensure we do at least 1 batch\n        num_batches = max(1, (limit + self.batch_size - 1) // self.batch_size)\n        \n        for _ in range(num_batches): \n            leaves = []\n            \n            # 1. Selection (find N leaves)\n            for _ in range(self.batch_size):\n                node = root\n                depth = 0\n                \n                # Selection loop\n                while node.is_expanded and node.children and depth < self.max_depth:\n                    node = max(node.children.values(), key=lambda n: n.ucb_score(self.c_puct))\n                    \n                    # Apply virtual loss to discourage re-selection in same batch\n                    node.virtual_loss += self.v_loss_const\n                    node.virtual_visits += 1\n                    depth += 1\n                \n                # Check if valid leaf to expand\n                if depth < self.max_depth and not node.is_expanded:\n                    # Avoid duplicates in batch (simple check)\n                    if node not in leaves:\n                        leaves.append(node)\n                else:\n                    pass\n            \n            if not leaves:\n                # If no leaves found (tree fully explored or locked), standard MCTS usually continues or stops.\n                # We can just break or continue backprop of terminals.\n                if root.visit_count > limit: break \n                continue\n                \n            # 2. Batch Expansion & Evaluation\n            values = self._expand_batch(leaves, x_values, y_values)\n            \n            # 3. Backpropagation\n            for node, val in zip(leaves, values):\n                # Check for best solution found\n                if self._is_complete_tree(node.tokens):\n                    # For completed formulas, we calculate REAL RMSE\n                    try:\n                        # Evaluar\n                        # Importar aqu\u00ed para evitar circular imports si es necesario\n                        from utils.optimize_constants import optimize_constants\n                        \n                        # 1. Optimizar constants (Crucial para Accuracy)\n                        # Esto es \"Phase 1\" de TPSR (constantes en las hojas)\n                        # Por simplicidad en esta iteraci\u00f3n, asumimos que 'evaluate_formula' ya hace algo o usamos el string directo.\n                        # Idealmente llamar\u00edamos a BFGS aqu\u00ed.\n                        \n                        # Use existing _evaluate_formula to get RMSE and optimized constants\n                        tree = ExpressionTree(node.tokens)\n                        optimized_constants, real_rmse = optimize_constants(tree, x_values, y_values)\n                        \n                        # Get y_pred using the optimized constants\n                        y_pred = tree.evaluate(x_values, constants=optimized_constants)\n                        \n                        # Check dimensions\n                        if y_pred.shape != y_values.shape:\n                            # If shapes don't match, it's an invalid evaluation\n                            final_val = 0.0\n                        else:\n                            # 2. Calcular Reward TPSR (Hybrid Accuracy + Complexity)\n                            # R = 1 / (1 + NMSE) + lambda * exp(-len/L)\n                            \n                            mse = np.mean((y_pred - y_values)**2)\n                            var_y = np.var(y_values)\n                            if var_y < 1e-9: var_y = 1.0 # Avoid division by zero\n                            \n                            nmse = mse / var_y\n                            \n                            # Evitar NMSE gigantes\n                            if np.isnan(nmse) or np.isinf(nmse):\n                                nmse = 1e9\n                            \n                            r_acc = 1.0 / (1.0 + nmse)\n                            \n                            # Penalizaci\u00f3n por complejidad\n                            token_len = len(node.tokens)\n                            L = self.max_len # Max length del modelo\n                            \n                            r_cplx = self.complexity_lambda * np.exp(-token_len / L)\n                            \n                            # Suma y Normalizaci\u00f3n (para mantener rango 0-1)\n                            # El m\u00e1ximo te\u00f3rico es (1.0 + lambda). Dividimos por eso.\n                            raw_reward = r_acc + r_cplx\n                            final_val = raw_reward / (1.0 + self.complexity_lambda)\n\n                        # Update best formula based on RMSE (for reporting, not for MCTS value)\n                        if real_rmse < best_rmse:\n                            best_rmse = real_rmse\n                            best_tokens = node.tokens\n                            best_formula = ExpressionTree(node.tokens).get_infix()\n                        \n                        # Update Pareto Front\n                        # Complexity = len(tokens) (or could use count_constants + nodes)\n                        complexity = len(node.tokens)\n                        self._update_pareto_front(node.tokens, real_rmse, complexity, ExpressionTree(node.tokens).get_infix())\n\n                    except Exception as e:\n                        # print(f\"Error evaluating formula: {e}\")\n                        final_val = 0.0 # Invalid formula gets 0 reward\n                else:\n                    final_val = val\n                \n                # The following lines were part of the user's instruction but contained syntax errors and undefined variables.\n                # They are commented out to maintain a syntactically correct and functional document.\n                # If these lines were intended to be added, please provide a complete and correct snippet.\n                #\n                # # Construir vector de probabilidades\n                # probs = np.zeros(self.vocab_size, dtype=np.float32)\n                # for token_id, count in counts.items():\n                #     probs[token_id] = count / total_visits_count += 1\n                \n                curr = node\n                while curr is not None:\n                    curr.visit_count += 1\n                    curr.value_sum += final_val\n                    \n                    # Revert virtual loss for parent and above\n                    # Since we added to PARENT's child (which is curr), \n                    # and we traverse Up...\n                    # Wait, logic: We selected CHILD. Virtual loss was added TO CHILD (curr).\n                    # So we must remove it from curr.\n                    if curr.virtual_visits > 0:\n                        curr.virtual_loss -= self.v_loss_const\n                        curr.virtual_visits -= 1\n                            \n                    curr = curr.parent\n        \n        # After search, force cleanup of any residual virtual loss (safety)\n        # (Not strictly needed if logic is perfect, but good practice in complex async MCTS)\n        \n        return {\n            'tokens': best_tokens,\n            'formula': best_formula,\n            'rmse': best_rmse,\n            'root': root,\n            'pareto_front': self.pareto_front\n        }\n\n    def _update_pareto_front(self, tokens, rmse, complexity, formula_str):\n        \"\"\"\n        Update the Pareto Front with a new solution.\n        Keep solutions that are not dominated by any other solution.\n        Solution A dominates B if:\n        A.rmse <= B.rmse AND A.complexity <= B.complexity AND (A.rmse < B.rmse OR A.complexity < B.complexity)\n        \"\"\"\n        # Create candidate\n        candidate = {'tokens': tokens, 'rmse': rmse, 'complexity': complexity, 'formula': formula_str}\n        \n        # Check if dominated by existing\n        is_dominated = False\n        to_remove = []\n        \n        for existing in self.pareto_front:\n            # Check if existing dominates candidate\n            if (existing['rmse'] <= candidate['rmse'] and \n                existing['complexity'] <= candidate['complexity'] and \n                (existing['rmse'] < candidate['rmse'] or existing['complexity'] < candidate['complexity'])):\n                is_dominated = True\n                break\n                \n            # Check if candidate dominates existing\n            if (candidate['rmse'] <= existing['rmse'] and \n                candidate['complexity'] <= existing['complexity'] and \n                (candidate['rmse'] < existing['rmse'] or candidate['complexity'] < existing['complexity'])):\n                to_remove.append(existing)\n        \n        if not is_dominated:\n            # Remove dominated existing solutions\n            for item in to_remove:\n                self.pareto_front.remove(item)\n            \n            # Add candidate\n            self.pareto_front.append(candidate)\n            # Sort by RMSE for easier viewing\n            self.pareto_front.sort(key=lambda x: x['rmse'])\n\n    def _expand_batch(self, nodes, x_values, y_values):\n        \"\"\"\n        Batched expansion. Returns list of values.\n        \"\"\"\n        if not nodes:\n            return []\n            \n        # Prepare inputs\n        # CRITICAL: Normalize data before feeding to model (Model expects [-1, 1] range)\n        # We need to wrap single items in list to use normalize_batch, then unpack or use directly if batch logic allows.\n        # normalize_batch takes lists of arrays.\n        \n        # NOTE: normalize_batch expects list of numpy arrays. _expand_batch takes single x_values/y_values which are reused across batch?\n        # x_values is passed to search(). \n        # If search() was called with raw data, we must normalize it here for the MODEL only.\n        # But optimize_constants expects RAW data. \n        # So we keep x_values raw, and create normalized tensors for the model.\n        \n        # normalize_batch input: list of arrays. output: list of arrays.\n        norm_x_list, norm_y_list = normalize_batch([x_values], [y_values])\n        norm_x = norm_x_list[0]\n        norm_y = norm_y_list[0]\n        \n        x_tensor = torch.tensor(norm_x, dtype=torch.float32).unsqueeze(0).to(self.device)\n        y_tensor = torch.tensor(norm_y, dtype=torch.float32).unsqueeze(0).to(self.device)\n        \n        # Repeat X/Y for batch\n        batch_size = len(nodes)\n        x_batch = x_tensor.repeat(batch_size, 1, 1).squeeze(1) # [batch, points]\n        y_batch = y_tensor.repeat(batch_size, 1, 1).squeeze(1) # [batch, points]\n        \n        # Prepare sequences\n        # Find max len\n        max_len = 0\n        seqs = []\n        for n in nodes:\n            s = [self.sos_id] + [TOKEN_TO_ID[t] for t in n.tokens]\n            seqs.append(s)\n            max_len = max(max_len, len(s))\n            \n        # Pad and stack\n        input_tensor = torch.full((batch_size, max_len), self.sos_id, dtype=torch.long).to(self.device)\n        for i, s in enumerate(seqs):\n            input_tensor[i, :len(s)] = torch.tensor(s, dtype=torch.long)\n            \n        # Inference\n        with torch.no_grad():\n            logits, value_preds = self.model(x_batch, y_batch, input_tensor)\n            \n        # Process results\n        values = []\n        \n        # To CPU numpy for probability processing\n        probs_batch = torch.softmax(logits[:, -1, :self.vocab_size], dim=1).cpu().numpy()\n        value_preds = value_preds.cpu().numpy() # [batch, 3]\n        \n        for i, node in enumerate(nodes):\n            # 1. Store Value (Median for now)\n            # value_preds is [batch, 3] -> (Pessimistic, Median, Optimistic)\n            # We use Median (index 1) for standard UCB.\n            val_pred = value_preds[i, 1] \n            val = float(np.clip(val_pred, 0.0, 1.0))\n            values.append(val)\n            \n            # 2. Expand children\n            node_probs = probs_batch[i]\n            valid_next = self._get_valid_next_tokens(node.tokens)\n            \n            for idx in valid_next:\n                token = VOCABULARY[idx]\n                prior = node_probs[idx]\n                child = MCTSNode(tokens=node.tokens + [token], parent=node, prior=prior)\n                node.children[token] = child\n            \n            node.is_expanded = True\n            \n        return values\n\n    def _get_valid_next_tokens(self, tokens):\n        \"\"\"Grammar check + curriculum filtering.\"\"\"\n        open_slots = 1\n        for t in tokens:\n            if t in OPERATORS:\n                open_slots += OPERATORS[t] - 1\n            else:\n                open_slots -= 1\n        \n        if open_slots <= 0:\n            return []\n        \n        # Filter by curriculum-allowed tokens\n        return list(self.allowed_token_indices)\n\n    def _is_complete_tree(self, tokens):\n        if not tokens: return False\n        try:\n            tree = ExpressionTree(tokens)\n            # Basic validation\n            if len(tokens) > self.max_depth * 2: return False\n            return tree.is_valid\n        except:\n            return False\n\n    def _evaluate_formula(self, tokens, x, y):\n        try:\n            tree = ExpressionTree(tokens)\n            _, rmse = optimize_constants(tree, x, y)\n            return rmse\n        except:\n            return 1e9\n\n    def get_training_examples(self, root):\n        \"\"\"\n        Extrae ejemplos de entrenamiento del \u00e1rbol generado.\n        Retorna: lista de (state_tokens, policy_probs, value_target)\n        \"\"\"\n        examples = []\n        queue = [root]\n        \n        while queue:\n            node = queue.pop(0)\n            if node.visit_count < 1: \n                continue\n            \n            # Policy Target (Pi)\n            # Distribuci\u00f3n de visitas de los hijos\n            counts = {}\n            total_visits = 0\n            has_children = False\n            \n            for token_id, child in node.children.items():\n                # child key is token STRING or ID?\n                # In _expand_batch: node.children[token] = child.\n                # token = VOCABULARY[idx] (String).\n                # So keys are strings.\n                # But we need ID for probabilities array index.\n                if token_id in TOKEN_TO_ID:\n                    tid = TOKEN_TO_ID[token_id]\n                    counts[tid] = child.visit_count\n                    total_visits += child.visit_count\n                    queue.append(child)\n                    has_children = True\n            \n            if not has_children or total_visits == 0:\n                continue\n                \n            # Construir vector de probabilidades\n            probs = np.zeros(self.vocab_size, dtype=np.float32)\n            for tid, count in counts.items():\n                probs[tid] = count / total_visits\n            \n            # Value Target (V)\n            # Usamos el Q-value (valor esperado) del nodo como target para el Value Head.\n            # Q = value_sum / visit_count\n            v = node.value_sum / node.visit_count\n            \n            # State: node.tokens (lista de ids?)\n            # node.tokens is list of strings (from VOCABULARY).\n            # self_play.py expects tokens as strings in ReplayBuffer.add.\n            examples.append((node.tokens, probs, v))\n            \n        return examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/search/beam_search.py\n",
        "\"\"\"\nBeam Search for AlphaSymbolic.\nExplores multiple formula candidates in parallel, keeping top-K at each step.\n\"\"\"\nimport torch\nimport numpy as np\nfrom core.grammar import VOCABULARY, OPERATORS, TOKEN_TO_ID, ExpressionTree, OPERATOR_STAGES\nfrom utils.optimize_constants import optimize_constants\nfrom utils.data_utils import normalize_batch\n\nclass BeamSearch:\n    def __init__(self, model, device, beam_width=10, max_length=30, curriculum_stage=None, num_variables=1):\n        self.model = model\n        self.device = device\n        self.beam_width = beam_width\n        self.max_length = max_length\n        self.num_variables = num_variables\n        self.vocab_size = len(VOCABULARY)\n        self.sos_id = self.vocab_size  # SOS token ID\n        \n        # Build token mask\n        # 1. Start with everything allowed (mask = 0)\n        # 2. disallow variables outside num_variables range\n        mask = torch.zeros(self.vocab_size, device=device)\n        \n        # Determine allowed variables\n        from core.grammar import VARIABLES\n        if num_variables == 1:\n            allowed_vars = set(['x', 'x0'])\n        else:\n            allowed_vars = set(VARIABLES[:num_variables])\n            \n        disallowed_vars = [v for v in VARIABLES if v not in allowed_vars]\n        if 'x' not in allowed_vars: disallowed_vars.append('x')\n\n        for v in disallowed_vars:\n            if v in TOKEN_TO_ID:\n                mask[TOKEN_TO_ID[v]] = float('-inf')\n\n        # 3. Apply curriculum limits if set\n        if curriculum_stage is not None:\n            allowed_ops = set(OPERATOR_STAGES.get(curriculum_stage, list(OPERATORS.keys())))\n            # Disallow operators not in the current stage\n            for op in OPERATORS:\n                if op not in allowed_ops:\n                    mask[TOKEN_TO_ID[op]] = float('-inf')\n        \n        # Only set self.token_mask if there are actually restricted tokens\n        if torch.any(mask != 0):\n            self.token_mask = mask\n        else:\n            self.token_mask = None\n        \n    def search(self, x_values, y_values, return_partial=False):\n        \"\"\"\n        Beam Search to find the best formula structure.\n        \"\"\"\n        # Prepare data once\n        # Normalize data for model inference\n        # normalize_batch input: list of arrays. We wrap x_values/y_values in list.\n        norm_x_list, norm_y_list = normalize_batch([x_values], [y_values])\n        norm_x = norm_x_list[0]\n        norm_y = norm_y_list[0]\n        \n        x_tensor = torch.tensor(norm_x, dtype=torch.float32).unsqueeze(0).to(self.device) # [1, points, vars] or [1, points] if 1D-normalized\n        y_tensor = torch.tensor(norm_y, dtype=torch.float32).unsqueeze(0).to(self.device) # [1, points]\n        \n        # Each element in beams is just the sequence of tokens (list of strings)\n        # We track scores and open branches in parallel lists or a list of dicts\n        beams = [{'seq': [], 'log_prob': 0.0, 'open': 1}]\n        \n        completed = []\n        \n        for step in range(self.max_length):\n            if not beams:\n                break\n                \n            # Filter valid beams just in case\n            active_beams = [b for b in beams if b['open'] > 0]\n            if not active_beams:\n                break\n                \n            # Prepare batch for model\n            # Batch size = number of active beams\n            batch_size = len(active_beams)\n            \n            # Expand X and Y to match batch size [batch, points, vars]\n            # Use expand with *shape to handle arbitrary dimensions (1D or Multi-Var)\n            # x_tensor shape is [1, points, vars] or [1, points]\n            # We want [batch_size, points, vars]\n            \n            # Use repeat or expand. Expand is strictly view, safer:\n            x_batch = x_tensor.expand(batch_size, *x_tensor.shape[1:])\n            y_batch = y_tensor.expand(batch_size, *y_tensor.shape[1:])\n            \n            # Prepare input sequences [batch, current_seq_len]\n            # Must prepend SOS token\n            seqs = [[self.sos_id] + [TOKEN_TO_ID[t] for t in b['seq']] for b in active_beams]\n            input_tensor = torch.tensor(seqs, dtype=torch.long).to(self.device)\n            \n            # Single model call for all beams\n            with torch.no_grad():\n                logits, _ = self.model(x_batch, y_batch, input_tensor)\n            \n            # Logits shape: [batch, seq_len, vocab_size]\n            # We want the last token's probabilities\n            last_token_logits = logits[:, -1, :self.vocab_size]\n            \n            # Apply curriculum mask if set\n            if self.token_mask is not None:\n                last_token_logits = last_token_logits + self.token_mask\n\n\n            \n            log_probs = torch.log_softmax(last_token_logits, dim=-1) # [batch, vocab]\n            \n            # --- Repetition Penalty (Simple) ---\n            # If the same token was generated recently, penalize it slightly.\n            # This prevents 10 ////////// loops.\n            penalty_factor = 2.0  # Reduce log_prob (which is negative) by absolute amount or multiplier?\n            # Log probs are negative (e.g. -0.1). Making them MORE negative penalizes.\n            # If we multiply by 1.2, -0.1 becomes -0.12 (lower probability).\n            \n            for i, beam in enumerate(active_beams):\n                if beam['seq']:\n                     # Get last token ID\n                    last_token = beam['seq'][-1]\n                    if last_token in TOKEN_TO_ID:\n                        last_id = TOKEN_TO_ID[last_token]\n                        # Penalize current step logits for this token\n                        # If log_prob is close to 0 (high prob), e.g. -0.01 -> -0.012\n                        # If log_prob is -10 (low prob), -> -12\n                        # Check bounds to avoid NaN if -inf\n                        if log_probs[i, last_id] > -1e9:\n                             log_probs[i, last_id] *= 1.5 \n            # -----------------------------------\n            \n            # We need to find the top-K candidates ACROSS current beams?\n            # Standard beam search: expand all, then prune to K\n            \n            all_candidates = []\n            \n            # Get top-K for EACH beam to avoid explosion (e.g. top 2*width)\n            k_per_beam = min(self.beam_width, self.vocab_size)\n            beam_topk_scores, beam_topk_indices = torch.topk(log_probs, k_per_beam, dim=-1)\n            \n            # Move to CPU for processing logic\n            beam_topk_scores = beam_topk_scores.cpu().numpy()\n            beam_topk_indices = beam_topk_indices.cpu().numpy()\n            \n            for i, beam in enumerate(active_beams):\n                for score, idx in zip(beam_topk_scores[i], beam_topk_indices[i]):\n                    token = VOCABULARY[idx]\n                    new_seq = beam['seq'] + [token]\n                    \n                    # Calculate new open branches\n                    if token in OPERATORS:\n                        new_open = beam['open'] + OPERATORS[token] - 1\n                    else:\n                        new_open = beam['open'] - 1\n                    \n                    if new_open < 0:\n                        continue\n                        \n                    all_candidates.append({\n                        'seq': new_seq,\n                        'log_prob': beam['log_prob'] + score,\n                        'open': new_open\n                    })\n            \n            # Global prune: keep top beam_width\n            all_candidates.sort(key=lambda x: x['log_prob'], reverse=True)\n            beams = all_candidates[:self.beam_width]\n            \n            # Check for completions\n            still_active = []\n            for b in beams:\n                if b['open'] == 0:\n                    completed.append(b)\n                else:\n                    still_active.append(b)\n            \n            beams = still_active\n            # If we filled up on completions, we might still want to explore? \n            # Usually we keep exploring until all beams are done or max length\n            if len(completed) >= self.beam_width:\n                 # Optional: early exit if we found enough good candidates\n                 pass\n\n        # Evaluate results\n        scored_results = []\n        for beam in completed:\n            tree = ExpressionTree(beam['seq'])\n            if tree.is_valid:\n                constants, rmse = optimize_constants(tree, x_values, y_values)\n                scored_results.append({\n                    'tokens': beam['seq'],\n                    'log_prob': beam['log_prob'],\n                    'rmse': rmse,\n                    'constants': constants,\n                    'formula': tree.get_infix()\n                })\n        \n        scored_results.sort(key=lambda x: x['rmse'])\n        \n        # If no results and return_partial is requested, return the best incomplete beam\n        if not scored_results and return_partial and beams:\n            # Take the beam with highest probability\n            best_beam = beams[0] \n            # Construct a partial result\n            # We can't optimize constants or get a valid infix easily, but we can show tokens\n            scored_results.append({\n                'tokens': best_beam['seq'],\n                'log_prob': best_beam['log_prob'],\n                'rmse': float('inf'),\n                'constants': {},\n                'formula': \"Partial: \" + \" \".join(best_beam['seq']) + \"...\"\n            })\n            \n        return scored_results\n\n\ndef beam_solve(target_x, target_y, model, device, beam_width=20, max_length=25, num_variables=1):\n    \"\"\"\n    Solve symbolic regression using beam search.\n    \"\"\"\n    searcher = BeamSearch(model, device, beam_width=beam_width, max_length=max_length, num_variables=num_variables)\n    results = searcher.search(target_x, target_y)\n    \n    if not results:\n        return None\n        \n    return results  # Return all results for Pareto analysis\n\n\nif __name__ == \"__main__\":\n    from core.model import AlphaSymbolicModel\n    \n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    VOCAB_SIZE = len(VOCABULARY)\n    \n    model = AlphaSymbolicModel(vocab_size=VOCAB_SIZE + 1, d_model=64).to(DEVICE)\n    try:\n        model.load_state_dict(torch.load(\"alpha_symbolic_model.pth\", map_location=DEVICE, weights_only=True))\n    except:\n        print(\"Model not found, using random weights\")\n    model.eval()\n    \n    # Test\n    x_test = np.linspace(-5, 5, 20).astype(np.float64)\n    y_test = 2 * x_test + 3\n    \n    print(\"Running Beam Search...\")\n    results = beam_solve(x_test, y_test, model, DEVICE, beam_width=10)\n    \n    print(f\"\\nFound {len(results)} valid formulas:\")\n    for i, r in enumerate(results[:5]):\n        print(f\"  {i+1}. {r['formula']} (RMSE: {r['rmse']:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/search/hybrid_search.py\n",
        "import time\nimport torch\nimport numpy as np\nfrom typing import List, Dict, Any, Optional\nimport concurrent.futures\nimport os\n\nfrom core.gp_bridge import GPEngine\nfrom search.beam_search import BeamSearch, beam_solve\nfrom core.grammar import ExpressionTree\ntry:\n    from core.gpu import TensorGeneticEngine\nexcept ImportError:\n    TensorGeneticEngine = None\n    print(\"Warning: Could not import TensorGeneticEngine (PyTorch/CUDA missing?)\")\n\ndef _run_gp_worker(args):\n    \"\"\"\n    Worker function for Parallel GP execution.\n    args: (x_list, y_list, seeds_chunk, gp_timeout, gp_binary_path)\n    \"\"\"\n    x_list, y_list, seeds, timeout, binary_path = args\n    import numpy as np\n    from core.grammar import ExpressionTree\n    engine = GPEngine(binary_path=binary_path)\n    # Give each worker a slight timeout variance to avoid file lock collisions if using temp files\n    # or just to spread load. But GPEngine handles unique tmp files so it should be fine.\n    \n    # Run GP\n    result = engine.run(x_list, y_list, seeds, timeout_sec=timeout)\n    \n    # Evaluate immediately if result found to return RMSE for comparison\n    if result:\n        try:\n             # Basic RMSE check for the worker's champion\n            tree = ExpressionTree.from_infix(result)\n            y_pred = tree.evaluate(np.array(x_list))\n            mse = np.mean((np.array(y_list) - y_pred)**2)\n            rmse = np.sqrt(mse)\n            return {'formula': result, 'rmse': rmse, 'status': 'success'}\n        except:\n            return {'formula': result, 'rmse': 999.0, 'status': 'eval_error'}\n    \n    return {'formula': None, 'rmse': 1e9, 'status': 'failed'}\n\ndef hybrid_solve(\n    x_values: np.ndarray,\n    y_values: np.ndarray,\n    model: torch.nn.Module,\n    device: torch.device,\n    beam_width: int = 50,\n    gp_timeout: int = 10,\n    gp_binary_path: Optional[str] = None,\n    max_workers: int = 4,\n    num_variables: int = 1,\n    extra_seeds: Optional[List[str]] = None\n) -> Dict[str, Any]:\n    \"\"\"\n    Solves Symbolic Regression using a Hybrid Neuro-Evolutionary approach with Parallel GP.\n    \"\"\"\n    \n    # print(f\"--- Starting Alpha-GP Hybrid Search (Parallel Workers={max_workers}, Vars={num_variables}) ---\")\n    start_time = time.time()\n    \n    # 1. Neural Beam Search (Phase 1)\n    # print(f\"[Phase 1] Neural Beam Search (Width={beam_width})...\")\n    neural_results = beam_solve(x_values, y_values, model, device, beam_width=beam_width, num_variables=num_variables)\n    \n    seeds = []\n    \n    # Inject Extra Seeds (Feedback Loop)\n    if extra_seeds:\n        pass # print(f\"[Phase 1] Injecting {len(extra_seeds)} external seeds (Feedback Loop).\")\n        seeds.extend(extra_seeds)\n        \n    if neural_results:\n        pass # print(f\"[Phase 1] Found {len(neural_results)} candidates.\")\n        seen_formulas = set()\n        for res in neural_results:\n            f_str = res['formula']\n            if f_str.startswith(\"Partial\"): continue\n            if f_str not in seen_formulas:\n                seeds.append(f_str)\n                seen_formulas.add(f_str)\n        \n        # print(f\"[Phase 1] Generated {len(seeds)} unique seeds for GP.\")\n        if len(seeds) > 0:\n            print(f\"Top Seed NN: {seeds[0]}\")\n    else:\n        print(\"[Phase 1] No valid candidates found. Falling back to pure GP.\")\n\n    # 2. GP Refinement (Phase 2 - Heterogeneous CPU + GPU)\n    # print(f\"[Phase 2] Genetic Refinement (Timeout={gp_timeout}s)...\")\n    \n    x_list = x_values.tolist() if hasattr(x_values, 'tolist') else list(x_values)\n    y_list = y_values.tolist() if hasattr(y_values, 'tolist') else list(y_values)\n    \n    # Resources managed dynamically by max_workers\n\n    results = []\n    futures = []\n\n    # A. Launch CPU Workers (Background)\n    # ---------------------------------\n    # ---------------------------------\n    # Prepare chunks for ALL workers\n    cpu_seeds = list(seeds) # Copy\n    cpu_chunks = []\n    \n    if max_workers > 0:\n        if not cpu_seeds:\n            cpu_chunks = [[] for _ in range(max_workers)]\n        else:\n            # Distribute seeds round-robin\n            cpu_chunks = [[] for _ in range(max_workers)]\n            for i, seed in enumerate(cpu_seeds):\n                cpu_chunks[i % max_workers].append(seed)\n\n    # print(f\"[Phase 2] Launching {max_workers} Parallel GP Workers (C++ SOTA)...\")\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=max(1, max_workers)) as executor:\n        for chunk in cpu_chunks:\n            args = (x_list, y_list, chunk, gp_timeout, gp_binary_path)\n            futures.append(executor.submit(_run_gp_worker, args))\n\n\n        # C. Collect CPU Results\n        # ----------------------\n        for future in concurrent.futures.as_completed(futures):\n            try:\n                res = future.result()\n                if res['status'] == 'success' or res['status'] == 'eval_error':\n                    res['worker'] = 'CPU'\n                    results.append(res)\n            except Exception as e:\n                print(f\"Worker exception: {e}\")\n\n    total_time = time.time() - start_time\n\n    # Find best result across all workers\n    best_result = None\n    best_rmse = float('inf')\n    \n    for res in results:\n        if res['formula'] and res['rmse'] < best_rmse:\n            best_rmse = res['rmse']\n            best_result = res['formula']\n            \n    if best_result:\n        # print(f\"--- Hybrid Search Completed in {total_time:.2f}s ---\")\n        # print(f\"Best Formula (Parallel): {best_result} (RMSE: {best_rmse:.5f})\")\n        \n        return {\n            'formula': best_result,\n            'rmse': best_rmse,\n            'source': 'Alpha-GP Hybrid',\n            'time': total_time,\n            'seeds_tried': seeds if seeds else []\n        }\n    else:\n        print(f\"--- Hybrid Search Failed (All workers failed) ---\")\n        return {\n            'formula': None,\n            'rmse': 1e9,\n            'source': 'Alpha-GP Hybrid',\n            'time': total_time,\n            'seeds_tried': seeds if seeds else []\n        }\n\nif __name__ == \"__main__\":\n    # Test\n    class MockModel(torch.nn.Module):\n        def forward(self, x, y, seq):\n            bs, seq_len = seq.shape\n            vocab = 20\n            return torch.randn(bs, seq_len, vocab), None\n\n    print(\"Testing Parallel Hybrid Search...\")\n    x = np.linspace(-5, 5, 20)\n    y = x**2 - 5\n    try:\n        # Important: must protect entry point for multiprocessing on Windows\n        res = hybrid_solve(x, y, MockModel(), torch.device(\"cpu\"), beam_width=5, max_workers=2)\n        print(res)\n    except Exception as e:\n        print(f\"Test failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/search/pareto.py\n",
        "\"\"\"\nPareto Front Manager for AlphaSymbolic.\nMaintains a set of non-dominated solutions (accuracy vs complexity).\n\"\"\"\nimport numpy as np\nfrom core.grammar import ExpressionTree\n\nclass ParetoSolution:\n    def __init__(self, tokens, rmse, complexity, formula_str, constants=None):\n        self.tokens = tokens\n        self.rmse = rmse  # Lower is better\n        self.complexity = complexity  # Lower is better (number of nodes)\n        self.formula = formula_str\n        self.constants = constants or {}\n        \n    def dominates(self, other):\n        \"\"\"Returns True if self dominates other (better in all objectives).\"\"\"\n        # Self dominates other if:\n        # - Self is at least as good in all objectives\n        # - Self is strictly better in at least one objective\n        at_least_as_good = (self.rmse <= other.rmse) and (self.complexity <= other.complexity)\n        strictly_better = (self.rmse < other.rmse) or (self.complexity < other.complexity)\n        return at_least_as_good and strictly_better\n    \n    def __repr__(self):\n        return f\"ParetoSolution(rmse={self.rmse:.4f}, complexity={self.complexity}, formula='{self.formula}')\"\n\n\nclass ParetoFront:\n    def __init__(self, max_size=50):\n        self.solutions = []\n        self.max_size = max_size\n        \n    def add(self, solution):\n        \"\"\"\n        Attempts to add a solution to the Pareto front.\n        Returns True if added, False if dominated.\n        \"\"\"\n        # Check if new solution is dominated by any existing\n        for existing in self.solutions:\n            if existing.dominates(solution):\n                return False  # New solution is dominated\n        \n        # Remove any solutions dominated by the new one\n        self.solutions = [s for s in self.solutions if not solution.dominates(s)]\n        \n        # Add the new solution\n        self.solutions.append(solution)\n        \n        # Enforce max size by removing worst solutions\n        if len(self.solutions) > self.max_size:\n            # Sort by a combined score and keep top max_size\n            self.solutions.sort(key=lambda s: s.rmse + 0.01 * s.complexity)\n            self.solutions = self.solutions[:self.max_size]\n        \n        return True\n    \n    def add_from_results(self, results_list):\n        \"\"\"\n        Add multiple results from beam search or MCTS.\n        results_list: list of dicts with 'tokens', 'rmse', 'constants', 'formula'\n        \"\"\"\n        added = 0\n        for r in results_list:\n            tree = ExpressionTree(r['tokens'])\n            complexity = len(r['tokens'])  # Simple complexity = token count\n            \n            sol = ParetoSolution(\n                tokens=r['tokens'],\n                rmse=r['rmse'],\n                complexity=complexity,\n                formula_str=r['formula'],\n                constants=r.get('constants', {})\n            )\n            \n            if self.add(sol):\n                added += 1\n        \n        return added\n    \n    def get_best_by_rmse(self):\n        \"\"\"Returns the solution with lowest RMSE.\"\"\"\n        if not self.solutions:\n            return None\n        return min(self.solutions, key=lambda s: s.rmse)\n    \n    def get_simplest(self):\n        \"\"\"Returns the solution with lowest complexity.\"\"\"\n        if not self.solutions:\n            return None\n        return min(self.solutions, key=lambda s: s.complexity)\n    \n    def get_balanced(self, alpha=0.5):\n        \"\"\"\n        Returns a balanced solution.\n        alpha: weight for RMSE (1-alpha for complexity)\n        \"\"\"\n        if not self.solutions:\n            return None\n        \n        # Normalize scores\n        rmse_vals = [s.rmse for s in self.solutions]\n        comp_vals = [s.complexity for s in self.solutions]\n        \n        min_rmse, max_rmse = min(rmse_vals), max(rmse_vals) + 1e-10\n        min_comp, max_comp = min(comp_vals), max(comp_vals) + 1e-10\n        \n        def score(s):\n            norm_rmse = (s.rmse - min_rmse) / (max_rmse - min_rmse)\n            norm_comp = (s.complexity - min_comp) / (max_comp - min_comp)\n            return alpha * norm_rmse + (1 - alpha) * norm_comp\n        \n        return min(self.solutions, key=score)\n    \n    def summary(self):\n        \"\"\"Print a summary of the Pareto front.\"\"\"\n        print(f\"\\n=== Pareto Front ({len(self.solutions)} solutions) ===\")\n        for i, sol in enumerate(sorted(self.solutions, key=lambda s: s.rmse)[:10]):\n            print(f\"  {i+1}. RMSE={sol.rmse:.6f}, Nodes={sol.complexity}, Formula: {sol.formula}\")\n\n\n# Quick test\nif __name__ == \"__main__\":\n    front = ParetoFront()\n    \n    # Add some test solutions\n    solutions = [\n        ParetoSolution(['x'], 10.0, 1, \"x\"),\n        ParetoSolution(['+', 'x', '1'], 5.0, 3, \"(x + 1)\"),\n        ParetoSolution(['*', '2', 'x'], 3.0, 3, \"(2 * x)\"),\n        ParetoSolution(['+', '*', '2', 'x', '3'], 0.5, 5, \"((2 * x) + 3)\"),\n        ParetoSolution(['+', '*', '*', '2', 'x', 'x', '+', 'x', '1'], 0.1, 9, \"complicated\"),\n    ]\n    \n    for sol in solutions:\n        added = front.add(sol)\n        print(f\"Added {sol.formula}: {added}\")\n    \n    front.summary()\n    \n    print(f\"\\nBest by RMSE: {front.get_best_by_rmse()}\")\n    print(f\"Simplest: {front.get_simplest()}\")\n    print(f\"Balanced: {front.get_balanced()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/search/__init__.py\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/training/train.py\n",
        "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom core.model import AlphaSymbolicModel\nfrom data.synthetic_data import DataGenerator\nfrom core.grammar import VOCABULARY, TOKEN_TO_ID, ExpressionTree\n\ndef validate(model, val_data, device, vocab_size):\n    model.eval()\n    total_loss = 0\n    total_token_acc = 0\n    valid_formulas = 0\n    total_samples = len(val_data)\n    \n    ce_loss_fn = nn.CrossEntropyLoss(ignore_index=-1)\n    \n    # Prepare batch\n    x_list = [d['x'] for d in val_data]\n    y_list = [d['y'] for d in val_data]\n    token_ids_list = [[TOKEN_TO_ID[t] for t in d['tokens']] for d in val_data]\n    \n    max_len = max(len(s) for s in token_ids_list)\n    SOS_ID = vocab_size\n    \n    decoder_input = torch.full((total_samples, max_len + 1), SOS_ID, dtype=torch.long)\n    targets = torch.full((total_samples, max_len + 1), -1, dtype=torch.long)\n    \n    for i, seq in enumerate(token_ids_list):\n        l = len(seq)\n        decoder_input[i, 1:l+1] = torch.tensor(seq, dtype=torch.long)\n        targets[i, :l] = torch.tensor(seq, dtype=torch.long)\n        \n    x_tensor = torch.tensor(np.array(x_list), dtype=torch.float32).to(device)\n    y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32).to(device)\n    decoder_input = decoder_input.to(device)\n    targets = targets.to(device)\n    \n    with torch.no_grad():\n        logits, _ = model(x_tensor, y_tensor, decoder_input)\n        \n        # Loss\n        loss = ce_loss_fn(logits.view(-1, vocab_size + 1), targets.view(-1))\n        total_loss = loss.item()\n        \n        # Accuracy\n        preds = torch.argmax(logits, dim=-1) # [batch, seq_len]\n        \n        # Token accuracy (mask padding)\n        mask = targets != -1\n        correct = (preds == targets) & mask\n        total_token_acc = correct.sum().float() / mask.sum().float()\n        \n        # Formula Validity (reconstruct and check)\n        # We need to strip EOS/Padding and stop at first end token if we had one, \n        # but here we just check if the sequence *as predicted* makes sense?\n        # Actually, let's just check the ground truth reconstruction for now or \n        # ideally we should run greedy search to generate a formula and check THAT.\n        # Checking \"teacher forced\" predictions for validity is less useful.\n        # fast check:\n        pass \n\n    model.train()\n    return total_loss, total_token_acc.item()\n\ndef train_supervised():\n    # Hyperparameters\n    BATCH_SIZE = 32\n    EPOCHS = 100 \n    LR = 1e-4\n    VOCAB_SIZE = len(VOCABULARY)\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    print(f\"Using device: {DEVICE}\")\n    \n    # Model\n    model = AlphaSymbolicModel(vocab_size=VOCAB_SIZE + 1, d_model=64).to(DEVICE)\n    optimizer = optim.Adam(model.parameters(), lr=LR)\n    ce_loss_fn = nn.CrossEntropyLoss(ignore_index=-1)\n    \n    # Data Generator\n    data_gen = DataGenerator(max_depth=4)\n    \n    # Generate Fixed Validation Set\n    print(\"Generating validation set...\")\n    val_data = data_gen.generate_batch(100) # 100 validation samples\n    \n    SOS_ID = VOCAB_SIZE\n    model.train()\n    \n    for epoch in range(EPOCHS):\n        # 1. Generate Training Batch\n        batch_data = data_gen.generate_batch(BATCH_SIZE)\n        if not batch_data: continue\n        \n        # Prepare inputs\n        x_list = [d['x'] for d in batch_data]\n        y_list = [d['y'] for d in batch_data]\n        token_ids_list = [[TOKEN_TO_ID[t] for t in d['tokens']] for d in batch_data]\n        \n        max_len = max(len(s) for s in token_ids_list)\n        \n        decoder_input = torch.full((BATCH_SIZE, max_len + 1), SOS_ID, dtype=torch.long)\n        targets = torch.full((BATCH_SIZE, max_len + 1), -1, dtype=torch.long)\n        \n        for i, seq in enumerate(token_ids_list):\n            l = len(seq)\n            decoder_input[i, 1:l+1] = torch.tensor(seq, dtype=torch.long)\n            targets[i, :l] = torch.tensor(seq, dtype=torch.long)\n            \n        x_tensor = torch.tensor(np.array(x_list), dtype=torch.float32).to(DEVICE)\n        y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n        decoder_input = decoder_input.to(DEVICE)\n        targets = targets.to(DEVICE)\n        \n        # Forward\n        logits, value_pred = model(x_tensor, y_tensor, decoder_input)\n        loss = ce_loss_fn(logits.view(-1, VOCAB_SIZE + 1), targets.view(-1))\n        \n        # Optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if epoch % 10 == 0:\n            val_loss, val_acc = validate(model, val_data, DEVICE, VOCAB_SIZE)\n            print(f\"Epoch {epoch}: Train Loss = {loss.item():.4f} | Val Loss = {val_loss:.4f} | Val Acc = {val_acc:.2%}\")\n            \n    # Save model\n    torch.save(model.state_dict(), \"alpha_symbolic_model.pth\")\n    print(\"Training complete. Model saved.\")\n\nif __name__ == \"__main__\":\n    try:\n        train_supervised()\n    except Exception as e:\n        print(f\"Training failed: {e}\")\n        import traceback\n        traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/training/train_enhanced.py\n",
        "\"\"\"\nEnhanced Training Script for AlphaSymbolic.\nIncludes:\n- Curriculum Learning (simple formulas first, then complex operators)\n- Value Network Training (not just policy)\n- Proper Loss Weighting\n- Regularization (dropout, weight decay)\n- Learning Rate Scheduling (OneCycleLR)\n- Gradient Accumulation\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nimport numpy as np\nimport time\n\nfrom core.model import AlphaSymbolicModel\nfrom data.synthetic_data import DataGenerator\nfrom core.grammar import VOCABULARY, TOKEN_TO_ID\n\ndef train_enhanced(epochs=1000, batch_size=64, curriculum=True, save_interval=100, accum_steps=4):\n    \"\"\"\n    Enhanced training with curriculum learning, value head, OneCycleLR, and Gradient Accumulation.\n    effective_batch_size = batch_size * accum_steps\n    \"\"\"\n    VOCAB_SIZE = len(VOCABULARY)\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    print(\"=\"*60)\n    print(\"AlphaSymbolic Enhanced Training\")\n    print(\"=\"*60)\n    print(f\"Device: {DEVICE}\")\n    print(f\"Vocabulary Size: {VOCAB_SIZE}\")\n    print(f\"Epochs: {epochs}\")\n    print(f\"Physical Batch Size: {batch_size}\")\n    print(f\"Accumulation Steps: {accum_steps}\")\n    print(f\"Effective Batch Size: {batch_size * accum_steps}\")\n    print(f\"Curriculum Learning: {curriculum}\")\n    \n    # Model with dropout\n    model = AlphaSymbolicModel(\n        vocab_size=VOCAB_SIZE + 1, \n        d_model=128,  # Larger model\n        nhead=4,\n        num_encoder_layers=3,\n        num_decoder_layers=3\n    ).to(DEVICE)\n    \n    # Count parameters\n    total_params = sum(p.numel() for p in model.parameters())\n    print(f\"Total Parameters: {total_params:,}\")\n    \n    # Optimizer with weight decay\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01) # Higher LR for OneCycle\n    \n    # Learning rate scheduler\n    # Steps per epoch is actually 1 because we generate data on fly? \n    # No, usually OneCycle needs total steps: epochs * steps_per_epoch\n    # Here one \"epoch\" is one batch generation loop?\n    # In the original code, `range(epochs)` ran one batch per loop iteration.\n    # So total_steps = epochs.\n    scheduler = OneCycleLR(\n        optimizer, \n        max_lr=1e-3, \n        total_steps=epochs, \n        pct_start=0.3, \n        div_factor=25, \n        final_div_factor=1000\n    )\n    \n    # Loss functions\n    ce_loss_fn = nn.CrossEntropyLoss(ignore_index=-1)\n    mse_loss_fn = nn.MSELoss()\n    \n    # SOS token\n    SOS_ID = VOCAB_SIZE\n    \n    # Curriculum Levels\n    # Level 0: Basic arithmetic\n    # Level 1: Division included\n    # Level 2: All (Trig, Exp, Log)\n    op_levels = [\n        ['+', '-', '*'],\n        ['+', '-', '*', '/'],\n        None # All\n    ]\n    \n    # Training loop\n    model.train()\n    start_time = time.time()\n    best_loss = float('inf')\n    \n    optimizer.zero_grad() # Initialize gradients to zero before the loop\n    \n    for epoch in range(epochs):\n        # Determine Curriculum Level\n        if curriculum:\n            progress = epoch / epochs\n            # Depth: 2 -> 6\n            current_depth = int(2 + progress * 4)\n            # Operators\n            if progress < 0.3:\n                current_ops = op_levels[0]\n            elif progress < 0.6:\n                current_ops = op_levels[1]\n            else:\n                current_ops = op_levels[2]\n        else:\n            current_depth = 5\n            current_ops = None\n        \n        # Generate batch with current difficulty\n        data_gen = DataGenerator(max_depth=current_depth, allowed_operators=current_ops)\n        batch_data = data_gen.generate_batch(batch_size)\n        \n        if len(batch_data) < batch_size // 2:\n            continue\n        \n        actual_batch = len(batch_data)\n        \n        # Prepare data\n        x_list = [d['x'] for d in batch_data]\n        y_list = [d['y'] for d in batch_data]\n        token_ids_list = [[TOKEN_TO_ID[t] for t in d['tokens']] for d in batch_data]\n        \n        # Pad sequences\n        max_len = max(len(s) for s in token_ids_list)\n        \n        # Decoder input: [SOS, tokens...]\n        decoder_input = torch.full((actual_batch, max_len + 1), SOS_ID, dtype=torch.long)\n        targets = torch.full((actual_batch, max_len + 1), -1, dtype=torch.long)  # -1 = padding\n        \n        # Target values (negative normalized RMSE, scaled to [-1, 1])\n        # For synthetic data, the \"perfect\" formula exists, so we use length penalty as proxy\n        value_targets = torch.zeros(actual_batch, 1)\n        \n        for i, seq in enumerate(token_ids_list):\n            l = len(seq)\n            decoder_input[i, 1:l+1] = torch.tensor(seq, dtype=torch.long)\n            targets[i, :l] = torch.tensor(seq, dtype=torch.long)\n            # Value target: These are ground truth sequences, so they lead to the correct solution.\n            # We assign a high value (1.0) to represent that this is a \"winning\" path.\n            # Ideally, we could dampen it slightly for earlier steps (gamma discount), \n            # but 1.0 is standard for \"optimal path\" in supervised pretraining.\n            value_targets[i] = 1.0\n        \n        x_tensor = torch.tensor(np.array(x_list), dtype=torch.float32).to(DEVICE)\n        y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n        decoder_input = decoder_input.to(DEVICE)\n        targets = targets.to(DEVICE)\n        value_targets = value_targets.to(DEVICE)\n        \n        # Forward pass\n        logits, value_pred = model(x_tensor, y_tensor, decoder_input)\n        \n        # Policy loss (cross-entropy)\n        policy_loss = ce_loss_fn(logits.view(-1, VOCAB_SIZE + 1), targets.view(-1))\n        \n        # Value loss (MSE)\n        # Value loss (ensure dimensions match)\n        if value_pred.shape != value_targets.shape:\n             value_pred = value_pred.view_as(value_targets)\n             \n        value_loss = mse_loss_fn(value_pred, value_targets)\n        \n        # Combined loss\n        loss = policy_loss + 0.5 * value_loss\n        \n        # Gradient Accumulation\n        # Normalize loss by accum_steps to keep magnitude same\n        loss = loss / accum_steps\n        loss.backward()\n        \n        if (epoch + 1) % accum_steps == 0:\n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n        \n        # Logging (multiply loss back by accum_steps for display)\n        if epoch % 50 == 0:\n            elapsed = time.time() - start_time\n            lr = scheduler.get_last_lr()[0]\n            real_loss = loss.item() * accum_steps\n            ops_name = \"All\" if current_ops is None else str(len(current_ops))\n            print(f\"Epoch {epoch:4d} | Loss: {real_loss:.4f} (P: {policy_loss.item():.4f}, V: {value_loss.item():.4f}) | LR: {lr:.2e} | Depth: {current_depth} | Ops: {ops_name} | Time: {elapsed:.1f}s\")\n        \n        # Save checkpoint\n        if epoch % save_interval == 0 and epoch > 0:\n            real_loss = loss.item() * accum_steps\n            if real_loss < best_loss:\n                best_loss = real_loss\n                torch.save(model.state_dict(), \"alpha_symbolic_model.pth\")\n                print(f\"  -> Saved checkpoint (best loss: {best_loss:.4f})\")\n    \n    # Final save\n    torch.save(model.state_dict(), \"alpha_symbolic_model.pth\")\n    total_time = time.time() - start_time\n    print(f\"\\nTraining complete! Total time: {total_time:.1f}s\")\n    print(f\"Model saved to: alpha_symbolic_model.pth\")\n    \n    return model\n\n\nif __name__ == \"__main__\":\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"Train AlphaSymbolic\")\n    parser.add_argument(\"--epochs\", type=int, default=500, help=\"Number of epochs\")\n    parser.add_argument(\"--batch\", type=int, default=32, help=\"Batch size (physical)\")\n    parser.add_argument(\"--accum\", type=int, default=4, help=\"Gradient accumulation steps\")\n    parser.add_argument(\"--no-curriculum\", action=\"store_true\", help=\"Disable curriculum learning\")\n    args = parser.parse_args()\n    \n    train_enhanced(\n        epochs=args.epochs,\n        batch_size=args.batch,\n        curriculum=not args.no_curriculum,\n        accum_steps=args.accum\n    )\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/training/self_play.py\n",
        "\"\"\"\nSelf-Play AlphaZero Loop for AlphaSymbolic.\nThe model improves by learning from its own search results.\n\nProcess:\n1. Generate problems (synthetic or from memory)\n2. Use MCTS to find best formulas\n3. Store successful (state, action, value) tuples with priority\n4. Train network on this experience using weighted sampling\n5. Repeat\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport time\nimport os\nfrom collections import deque\nimport random\nimport sys\nimport os\n\n# Add project root to sys.path\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nfrom core.model import AlphaSymbolicModel\nfrom core.grammar import VOCABULARY, TOKEN_TO_ID\nfrom data.synthetic_data import DataGenerator\nfrom search.mcts import MCTS\nfrom data.pattern_memory import PatternMemory\nfrom core.loss import QuantileLoss\n\n\nclass ReplayBuffer:\n    \"\"\"Experience replay buffer for storing (state, policy, value) tuples with priority.\"\"\"\n    \n    def __init__(self, capacity=50000):\n        self.buffer = deque(maxlen=capacity)\n        self.priorities = deque(maxlen=capacity)\n    \n    def add(self, x_data, y_data, tokens, policy, value):\n        \"\"\"\n        Add an experience tuple (state, policy, value).\n        \"\"\"\n        self.buffer.append({\n            'x': x_data,\n            'y': y_data, \n            'tokens': tokens,\n            'policy': policy,\n            'value': value\n        })\n        # Priority: could be based on error, but for now uniform/FIFO\n        # We'll use value magnitude + small noise for diversity if needed\n        # For now, just append 1.0\n        self.priorities.append(1.0)\n    \n    def sample(self, batch_size):\n        \"\"\"Sample a batch.\"\"\"\n        if len(self.buffer) < batch_size:\n            return list(self.buffer)\n        \n        # Uniform sampling for now (simplify)\n        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n        return [self.buffer[i] for i in indices]\n    \n    def __len__(self):\n        return len(self.buffer)\n\n\nclass AlphaZeroLoop:\n    def __init__(self, model_path=\"alpha_symbolic_model.pth\", fresh_start=False):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.vocab_size = len(VOCABULARY)\n        self.model_path = model_path\n\n        # Handle fresh start\n        if fresh_start and os.path.exists(self.model_path):\n            os.remove(self.model_path)\n            print(\"Previous model deleted. Starting fresh.\")\n        \n        # Model\n        self.model = AlphaSymbolicModel(\n            vocab_size=self.vocab_size + 1,\n            d_model=128,\n            nhead=4,\n            num_encoder_layers=3,\n            num_decoder_layers=3\n        ).to(self.device)\n        \n        self.model_path = model_path\n        self.load_model()\n        \n        # Optimizer\n        self.optimizer = optim.AdamW(self.model.parameters(), lr=1e-4, weight_decay=0.01)\n        \n        # Replay buffer\n        self.replay = ReplayBuffer(capacity=100000)\n        \n        # Data generator for new problems\n        self.data_gen = DataGenerator(max_depth=5)\n        \n        # Pattern memory\n        self.memory = PatternMemory()\n        \n        # Search (MCTS)\n        # TPSR params: complexity_lambda=0.1\n        self.searcher = MCTS(self.model, self.device, max_simulations=50, max_depth=25, complexity_lambda=0.1)\n        \n        # Statistics\n        self.stats = {\n            'iterations': 0,\n            'best_rmse': float('inf'),\n            'avg_rmse': deque(maxlen=100)\n        }\n    \n    def load_model(self):\n        if os.path.exists(self.model_path):\n            try:\n                self.model.load_state_dict(torch.load(self.model_path, map_location=self.device, weights_only=True))\n                print(f\"Loaded model from {self.model_path}\")\n            except:\n                print(\"Could not load model, using fresh weights\")\n    \n    def save_model(self):\n        torch.save(self.model.state_dict(), self.model_path)\n    \n    def self_play_episode(self, num_problems=10):\n        \"\"\"\n        Generate problems, solve them with MCTS, store experiences (AlphaZero style).\n        \"\"\"\n        self.model.eval()\n        \n        experiences = []\n        \n        # Generate problems\n        problems = self.data_gen.generate_batch(num_problems)\n        \n        for prob in problems:\n            x_data = prob['x'].astype(np.float64)\n            y_data = prob['y'].astype(np.float64)\n            \n            # Search for solution via MCTS\n            result = self.searcher.search(x_data, y_data)\n            \n            # Extract training examples from the tree (Polcy, Value)\n            # result['root'] is now available\n            if 'root' in result:\n                examples = self.searcher.get_training_examples(result['root'])\n                for (tokens, policy, value) in examples:\n                    self.replay.add(x_data, y_data, tokens, policy, value)\n            \n            if result['tokens']:\n                experiences.append(result['rmse'])\n                \n                # Update pattern memory\n                if result['formula'] and result['rmse'] < 1.0: # Only good ones\n                     self.memory.record(result['tokens'], result['rmse'], result['formula'])\n                \n                # Track statistics\n                if result['rmse'] < self.stats['best_rmse']:\n                    self.stats['best_rmse'] = result['rmse']\n                self.stats['avg_rmse'].append(result['rmse'])\n        \n        return experiences\n    \n    def train_step(self, batch_size=32):\n        \"\"\"\n        Train on experiences from replay buffer.\n        \"\"\"\n        if len(self.replay) < batch_size:\n            return None\n        \n        self.model.train()\n        \n        batch = self.replay.sample(batch_size)\n        \n        # Prepare batch\n        SOS_ID = self.vocab_size\n        \n        x_list = [exp['x'] for exp in batch]\n        y_list = [exp['y'] for exp in batch]\n        token_lists = [exp['tokens'] for exp in batch]\n        policy_targets = [exp['policy'] for exp in batch]\n        value_targets_list = [exp['value'] for exp in batch]\n        \n        # Pad sequences\n        max_len = max(len(t) for t in token_lists)\n        \n        decoder_input = torch.full((len(batch), max_len + 1), SOS_ID, dtype=torch.long)\n        # Policy target is distribution [batch, vocab]\n        # But wait, MCTS returns policy for the NEXT token from current state.\n        # Our model predicts sequence. \n        # Standard AZ: (State) -> (Policy, Value).\n        # Wrapper: (Input X, Y, Partial Formula) -> (Next Token Dist, Value).\n        # So we align 'decoder_input' = tokens. \n        # Target for Policy head at the LAST step is 'policy_targets'.\n        \n        # The 'tokens' in replay buffer IS the state (partial sequence).\n        # So decoder_input should be exactly that.\n        \n        for i, tokens in enumerate(token_lists):\n            ids = [TOKEN_TO_ID[t] for t in tokens] # Assuming tokens are strings? \n            # Wait, MCTSNode.tokens are IDs or Strings?\n            # Grammar says VOCABULARY list. MCTS seems to store tokens as strings or IDs?\n            # MCTS code uses TOKEN_TO_ID when calling model. So MCTSNode.tokens likely Strings?\n            # Let's verify MCTSNode usage.\n            # MCTSNode init: tokens=[]. expand: names from VOCABULARY.\n            # So they are strings.\n            \n            l = len(ids)\n            decoder_input[i, 1:l+1] = torch.tensor(ids, dtype=torch.long)\n            \n        # Targets\n        # Policy: [batch, vocab_size]\n        policy_target_tensor = torch.tensor(np.array(policy_targets), dtype=torch.float32).to(self.device)\n        value_target_tensor = torch.tensor(np.array(value_targets_list), dtype=torch.float32).unsqueeze(1).to(self.device)\n        \n        # To device\n        x_tensor = torch.tensor(np.array(x_list), dtype=torch.float32).to(self.device)\n        y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32).to(self.device)\n        decoder_input = decoder_input.to(self.device)\n        \n        # Forward\n        logits, value_pred = self.model(x_tensor, y_tensor, decoder_input)\n        \n        # Losses\n        # Policy Loss: KLDiv between predicted distribution (logits of last token) and target distribution\n        # logits: [batch, seq_len, vocab]\n        # We only care about the LAST token prediction because that matches the MCTS state\n        # The 'decoder_input' has length L+1 (SOS + tokens).\n        # The prediction for the 'next' token is at the last position.\n        \n        # Gather last step logits\n        # We need to pick the logits corresponding to the end of each sequence?\n        # Since we padded, we must be careful.\n        # 'decoder_input' is padded with SOS? No, standard padding?\n        # I initialized with SOS_ID.\n        # The effective length for user i is len(tokens)+1.\n        # So we take logits[i, len(tokens), :] ??\n        # Or did I pad with something else?\n        \n        last_logits = []\n        for i, tokens in enumerate(token_lists):\n            idx = len(tokens) # index of last real token (SOS is at 0)\n            # sequences: SOS, T1, T2 ...\n            # Input: SOS, T1\n            # Output at pos 1: Pred for T2.\n            # State was [T1]. \n            # Correct.\n            last_logits.append(logits[i, idx, :self.vocab_size])\n            \n        last_logits = torch.stack(last_logits)\n        \n        # Log Softmax for KLDiv\n        log_probs = torch.log_softmax(last_logits, dim=1)\n        \n        # KLDivLoss(input, target) expects log_probs as input\n        policy_loss = nn.KLDivLoss(reduction='batchmean')(log_probs, policy_target_tensor)\n        \n        # Value Loss (Quantile Regression)\n        # value_pred: [batch, 3], value_target_tensor: [batch, 1]\n        value_loss = QuantileLoss()(value_pred, value_target_tensor)\n        \n        total_loss = policy_loss + value_loss # Weighting? 1.0 each for now\n        \n        # Backward\n        self.optimizer.zero_grad()\n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n        self.optimizer.step()\n        \n        return {\n            'total': total_loss.item(),\n            'policy': policy_loss.item(),\n            'value': value_loss.item()\n        }\n    \n    def run(self, iterations=100, problems_per_iter=20, train_steps_per_iter=10, \n            save_interval=10, verbose=True):\n        \"\"\"\n        Main AlphaZero loop.\n        \"\"\"\n        if verbose:\n            print(\"=\"*60)\n            print(\"AlphaZero Self-Play Loop (MCTS Enhanced)\")\n            print(\"=\"*60)\n            print(f\"Device: {self.device}\")\n            print(f\"Iterations: {iterations}\")\n            print(f\"Problems per iteration: {problems_per_iter}\")\n        \n        start_time = time.time()\n        \n        for i in range(iterations):\n            self.stats['iterations'] = i + 1\n            \n            # Self-play phase\n            rmses = self.self_play_episode(problems_per_iter)\n            \n            # Training phase\n            losses = []\n            for _ in range(train_steps_per_iter):\n                loss = self.train_step()\n                if loss:\n                    losses.append(loss)\n            \n            # Logging\n            if verbose and (i + 1) % 5 == 0:\n                avg_rmse = np.mean(list(self.stats['avg_rmse'])) if self.stats['avg_rmse'] else 0\n                avg_loss = np.mean([l['total'] for l in losses]) if losses else 0\n                elapsed = time.time() - start_time\n                \n                print(f\"Iter {i+1:4d} | Buffer: {len(self.replay):5d} | \"\n                      f\"Avg RMSE: {avg_rmse:.4f} | Best: {self.stats['best_rmse']:.4f} | \"\n                      f\"Loss: {avg_loss:.4f} | Time: {elapsed:.1f}s\")\n            \n            # Save model\n            if (i + 1) % save_interval == 0:\n                self.save_model()\n                self.memory.save()\n                if verbose:\n                    print(f\"  -> Checkpoint saved\")\n        \n        # Final save\n        self.save_model()\n        self.memory.save()\n        \n        total_time = time.time() - start_time\n        if verbose:\n            print(f\"\\nSelf-play complete! Total time: {total_time:.1f}s\")\n            print(f\"Final buffer size: {len(self.replay)}\")\n            print(f\"Best RMSE achieved: {self.stats['best_rmse']:.6f}\")\n\n\nif __name__ == \"__main__\":\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"AlphaZero Self-Play\")\n    parser.add_argument(\"--iterations\", type=int, default=50)\n    parser.add_argument(\"--problems\", type=int, default=10)\n    parser.add_argument(\"--train-steps\", type=int, default=5)\n    args = parser.parse_args()\n    \n    loop = AlphaZeroLoop()\n    loop.run(\n        iterations=args.iterations,\n        problems_per_iter=args.problems,\n        train_steps_per_iter=args.train_steps\n    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/training/__init__.py\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/ui/app_core.py\n",
        "\"\"\"\nCore state and model management for AlphaSymbolic Gradio App.\n\"\"\"\nimport torch\nimport os\nfrom core.model import AlphaSymbolicModel\nfrom core.grammar import VOCABULARY\n\nfrom collections import deque\nimport time\n\n# Global state\nMODEL = None\nDEVICE = None\nTRAINING_STATUS = {\"running\": False, \"epoch\": 0, \"loss\": 0, \"message\": \"Listo\"}\nSTOP_TRAINING = False  # Flag to request training stop\n\ndef request_stop_training():\n    \"\"\"Request training to stop gracefully.\"\"\"\n    global STOP_TRAINING\n    STOP_TRAINING = True\n    return \"\u23f9\ufe0f Deteniendo entrenamiento...\"\n\ndef should_stop_training():\n    \"\"\"Check if training should stop.\"\"\"\n    return STOP_TRAINING\n\ndef reset_stop_flag():\n    \"\"\"Reset the stop flag (call at start of training).\"\"\"\n    global STOP_TRAINING\n    STOP_TRAINING = False\n\n# Hall of Shame: Rolling buffer of recent failures\n# Format: {'time': str, 'target': str, 'predicted': str, 'loss': float, 'stage': str}\nTRAINING_ERRORS = deque(maxlen=20)\n\ndef add_training_error(target, predicted, loss, stage):\n    \"\"\"Add an error to the Hall of Shame.\"\"\"\n    TRAINING_ERRORS.append({\n        'time': time.strftime(\"%H:%M:%S\"),\n        'target': target,\n        'predicted': predicted,\n        'loss': float(loss),\n        'stage': stage\n    })\n\ndef get_training_errors():\n    \"\"\"Get list of errors for the UI.\"\"\"\n    return list(TRAINING_ERRORS)\n\nMODEL_PRESETS = {\n    'lite': {'d_model': 128, 'nhead': 4, 'num_encoder_layers': 3, 'num_decoder_layers': 3},\n    'pro': {'d_model': 256, 'nhead': 8, 'num_encoder_layers': 6, 'num_decoder_layers': 6}\n}\nCURRENT_PRESET = 'lite'\n\ndef get_device(force_cpu=False):\n    \"\"\"Get the best available device (CUDA > MPS > CPU).\"\"\"\n    if force_cpu:\n        return torch.device(\"cpu\")\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    return torch.device(\"cpu\")\n\ndef set_device(use_gpu=True):\n    \"\"\"Set the device (GPU or CPU).\"\"\"\n    global DEVICE, MODEL\n    new_device = get_device(force_cpu=not use_gpu)\n    \n    if MODEL is not None and DEVICE != new_device:\n        MODEL = MODEL.to(new_device)\n    \n    DEVICE = new_device\n    return get_device_info()\n\ndef get_device_info():\n    \"\"\"Get device info string.\"\"\"\n    global DEVICE\n    if DEVICE is None:\n        DEVICE = get_device()\n    \n    if DEVICE.type == \"cuda\":\n        return f\"CUDA ({torch.cuda.get_device_name(0)})\"\n    elif DEVICE.type == \"mps\":\n        return \"MPS (Apple Silicon)\"\n    else:\n        return \"CPU\"\n\ndef load_model(force_reload=False, preset_name=None):\n    \"\"\"Load or reload the model.\"\"\"\n    global MODEL, DEVICE, CURRENT_PRESET\n    \n    if preset_name:\n        CURRENT_PRESET = preset_name\n    \n    if DEVICE is None:\n        DEVICE = get_device()\n    \n    VOCAB_SIZE = len(VOCABULARY)\n    config = MODEL_PRESETS[CURRENT_PRESET]\n    \n    print(f\"Loading Model [{CURRENT_PRESET.upper()}]...\")\n    MODEL = AlphaSymbolicModel(\n        vocab_size=VOCAB_SIZE + 1, \n        d_model=config['d_model'], \n        nhead=config['nhead'],\n        num_encoder_layers=config['num_encoder_layers'], \n        num_decoder_layers=config['num_decoder_layers'],\n        max_seq_len=256,\n        input_dim=11\n    ).to(DEVICE)\n    \n    filename = f\"alpha_symbolic_model_{CURRENT_PRESET}.pth\"\n    status = f\"Nuevo modelo ({CURRENT_PRESET})\" # Default status\n    \n    # Check Drive for backup IF on colab and main file doesn't exist or is older?\n    # Simple strategy: prioritize local, but if local missing, check Drive.\n    drive_path = \"/content/drive/MyDrive/AlphaSymbolic_Models\"\n    drive_filename = os.path.join(drive_path, filename)\n    \n    local_path = os.path.join(\"models\", filename)\n    \n    source_file = None\n    if os.path.exists(local_path):\n        source_file = local_path\n    elif os.path.exists(filename): # Legacy location\n        source_file = filename\n    elif os.path.exists(drive_filename):\n        print(f\"\ud83d\udce6 Local model missing. Loading from Drive: {drive_filename}\")\n        source_file = drive_filename\n\n    if source_file:\n        try:\n            state_dict = torch.load(source_file, map_location=DEVICE, weights_only=True)\n            \n            # Check for NaNs\n            has_nans = False\n            for k, v in state_dict.items():\n                if torch.isnan(v).any() or torch.isinf(v).any():\n                    has_nans = True\n                    break\n            \n            if has_nans:\n                # print(f\"\u26a0\ufe0f Modelo corrupto detectado (NaNs) en {source_file}. Eliminando.\")\n                # try:\n                #    os.remove(source_file)\n                #    print(\"\u2705 Archivo corrupto eliminado.\")\n                # except OSError as e:\n                #    print(f\"Error al eliminar archivo: {e}\")\n                status = \"\u26a0\ufe0f Advertencia: NaNs detectados (pero no borrado)\"\n            else:\n                # Handle resizing of Positional Encoding (e.g. 50 -> 256)\n                if 'pos_encoder.pe' in state_dict:\n                    saved_pe_shape = state_dict['pos_encoder.pe'].shape\n                    model_pe_shape = MODEL.pos_encoder.pe.shape\n                    if saved_pe_shape != model_pe_shape:\n                        print(f\"\u26a0\ufe0f Resizing Positional Encoding from {saved_pe_shape[1]} to {model_pe_shape[1]}. Resetting buffer.\")\n                        del state_dict['pos_encoder.pe']\n                        MODEL.load_state_dict(state_dict, strict=False)\n                    else:\n                        MODEL.load_state_dict(state_dict)\n                else:\n                    MODEL.load_state_dict(state_dict)\n                    \n                MODEL.eval()\n                status = f\"Modelo cargado ({CURRENT_PRESET})\"\n                \n        except RuntimeError as e:\n            print(f\"\u26a0\ufe0f Error de compatibilidad ({e}). Iniciando modelo fresco.\")\n            status = f\"Nuevo modelo ({CURRENT_PRESET})\"\n        except Exception as e:\n            print(f\"Error cargando: {e}\")\n            status = \"Sin modelo pre-entrenado\"\n    \n    return status, get_device_info()\n\ndef get_model():\n    \"\"\"Get the current model, loading if needed.\"\"\"\n    global MODEL, DEVICE\n    if MODEL is None:\n        load_model()\n    return MODEL, DEVICE\n\ndef save_model():\n    \"\"\"Save the current model and all associated data (formulas, patterns).\"\"\"\n    global MODEL, CURRENT_PRESET\n    if MODEL is not None:\n        filename = f\"alpha_symbolic_model_{CURRENT_PRESET}.pth\"\n        local_path = os.path.join(\"models\", filename)\n        torch.save(MODEL.state_dict(), local_path)\n        \n        # Backup to Google Drive if available\n        if os.path.exists(\"/content/drive\"):\n            import shutil\n            drive_path = \"/content/drive/MyDrive/AlphaSymbolic_Models\"\n            try:\n                os.makedirs(drive_path, exist_ok=True)\n                \n                # 1. Backup Model\n                drive_filename = os.path.join(drive_path, filename)\n                shutil.copy(local_path, drive_filename)\n                \n                # 2. Backup Formula Data\n                FILES_TO_BACKUP = [\n                    ('top_formulas.csv', 'top_formulas.csv'),\n                    ('pattern_memory.json', 'pattern_memory.json'),\n                    ('results/learned_formulas.csv', 'learned_formulas.csv'),\n                    ('top_5_detailed_report.csv', 'top_5_detailed_report.csv')\n                ]\n                \n                for src, name in FILES_TO_BACKUP:\n                    if os.path.exists(src):\n                        shutil.copy(src, os.path.join(drive_path, name))\n                        \n                print(f\"\u2705 Data & Model backed up to Drive: {drive_path}\")\n            except Exception as e:\n                print(f\"\u26a0\ufe0f Error al respaldar en Drive: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/ui/app_search.py\n",
        "\"\"\"\nSearch/Solve functions for AlphaSymbolic Gradio App.\nSupports both Beam Search and MCTS.\n\"\"\"\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport time\nimport gradio as gr\n\nfrom core.grammar import ExpressionTree\nfrom search.beam_search import BeamSearch\nfrom search.mcts import MCTS\nfrom search.hybrid_search import hybrid_solve\nfrom utils.simplify import simplify_tree\nfrom search.pareto import ParetoFront\nfrom utils.detect_pattern import detect_pattern\nfrom utils.optimize_constants import optimize_constants, substitute_constants\nfrom ui.app_core import get_model\n\n\ndef parse_data(x_str, y_str):\n    \"\"\"\n    Parse input strings.\n    Supports:\n    1. 1D Comma-separated: \"1, 2, 3\"\n    2. 2D Multi-line/Semi-colon: \"1,2; 3,4\" or \"1 2\\n3 4\"\n    \"\"\"\n    try:\n        # Pre-process: standardize separators\n        x_str = x_str.strip()\n        y_str = y_str.strip()\n        \n        # Check for multi-line or semi-colon (Multi-Variable)\n        is_multivar = '\\n' in x_str or ';' in x_str\n        \n        if is_multivar:\n            # Split into rows\n            rows = [r.strip() for r in x_str.replace(';', '\\n').split('\\n') if r.strip()]\n            # Parse each row\n            x_data = []\n            for r in rows:\n                # Handle comma or space\n                vals = [float(v) for v in r.replace(',', ' ').split()]\n                x_data.append(vals)\n            x = np.array(x_data, dtype=np.float64)\n            \n            # Y should also be checked, usually 1D but input might be multi-line\n            y_data = [float(v) for v in y_str.replace(';', '\\n').replace(',', ' ').split()]\n            y = np.array(y_data, dtype=np.float64)\n            \n        else:\n            # Legacy 1D\n            x = np.array([float(v.strip()) for v in x_str.split(',')], dtype=np.float64)\n            y = np.array([float(v.strip()) for v in y_str.split(',')], dtype=np.float64)\n            \n            # Ensure X is (N, 1) or (N,) depending on usage. \n            # Logic mostly expects (N,) for 1D, but model needs (N, 1).\n            # Let's keep (N,) for 1D to not break existing plots, handling shape later.\n        \n        if len(x) != len(y):\n            return None, None, f\"Error: Cantidad de muestras X ({len(x)}) != Y ({len(y)})\"\n            \n        return x, y, None\n    except Exception as e:\n        return None, None, f\"Error parseando datos: {str(e)}\"\n\n\ndef create_fit_plot(x, y, y_pred, formula):\n    \"\"\"Create a plot showing data vs prediction.\"\"\"\n    fig, ax = plt.subplots(figsize=(8, 5), facecolor='#1a1a2e')\n    ax.set_facecolor('#1a1a2e')\n    \n    # Check dimensions\n    if x.ndim > 1 and x.shape[1] > 1:\n        # Multi-Variable: Parity Plot (Real vs Predicted)\n        ax.scatter(y, y_pred, color='#4ade80', s=100, edgecolors='white', alpha=0.7)\n        \n        # Perfect fit line\n        min_val = min(y.min(), y_pred.min())\n        max_val = max(y.max(), y_pred.max())\n        ax.plot([min_val, max_val], [min_val, max_val], '--', color='white', alpha=0.5, label='Ideal')\n        \n        ax.set_xlabel('Valor Real (Target)', color='white')\n        ax.set_ylabel('Prediccion', color='white')\n        ax.set_title(f'Multi-Variable: {x.shape[1]} Features', color='white', fontweight='bold')\n        \n    else:\n        # 1D: Standard X vs Y\n        # Flatten if needed\n        if x.ndim > 1: x = x.flatten()\n        \n        ax.scatter(x, y, color='#00d4ff', s=100, label='Datos Reales', zorder=3, edgecolors='white', linewidth=1)\n        \n        sort_idx = np.argsort(x)\n        ax.plot(x[sort_idx], y_pred[sort_idx], color='#ff6b6b', linewidth=3, label='Prediccion', zorder=2)\n        \n        ax.set_xlabel('X', color='white', fontsize=12)\n        ax.set_ylabel('Y', color='white', fontsize=12)\n        ax.set_title('Ajuste de la Formula', color='white', fontsize=14, fontweight='bold')\n        ax.legend(facecolor='#16213e', edgecolor='#00d4ff', labelcolor='white')\n\n    ax.tick_params(colors='white')\n    ax.grid(True, alpha=0.2, color='white')\n    \n    for spine in ax.spines.values():\n        spine.set_color('#00d4ff')\n    \n    plt.tight_layout()\n    return fig\n\n\ndef solve_formula(x_str, y_str, beam_width, search_method, max_workers=4, progress=gr.Progress()):\n    \"\"\"Main solving function with search method selection.\"\"\"\n    x, y, error = parse_data(x_str, y_str)\n    if error:\n        return error, None, \"\", \"\", \"\"\n    \n    MODEL, DEVICE = get_model()\n    \n    progress(0.1, desc=f\"Analizando patron... [{DEVICE.type.upper()}]\")\n    pattern = detect_pattern(x, y)\n    \n    progress(0.3, desc=f\"Buscando formulas ({search_method})... [{DEVICE.type.upper()}]\")\n    start_time = time.time()\n    \n    results = []\n    \n    num_vars = 1 if x.ndim == 1 else x.shape[1]\n    \n    if search_method == \"Alpha-GP Hybrid\":\n        # Using hybrid search\n        progress(0.4, desc=\"Fase 1: Neural Beam Search...\")\n        # Note: Hybrid search handles its own phases printing, but we want UI updates.\n        # We pass beam_width. gp_timeout is increased to 30s to allow convergence on complex problems.\n        hybrid_res = hybrid_solve(x, y, MODEL, DEVICE, beam_width=int(beam_width), gp_timeout=30, num_variables=num_vars, max_workers=max_workers)\n        \n        if hybrid_res and hybrid_res.get('formula'):\n            progress(0.9, desc=\"Procesando resultados GP...\")\n            # Convert infix string back to tokens for consistency\n            tree = ExpressionTree.from_infix(hybrid_res['formula'])\n            if tree.is_valid:\n                 # Evaluate RMSE roughly (GP result should be good, but let's confirm)\n                 # Optimization is already done by GP, but we might want to fine-tune \n                 # or at least extract constants if they are numbers in the string.\n                 # The string from GP has numbers like 2.345 embedded.\n                 # optimize_constants expects a tree with 'C' placeholders if we want to re-optimize.\n                 # But GP output is fully instantiated.\n                 # So we just evaluate.\n                 \n                 y_pred_check = tree.evaluate(x)\n                 rmse_check = np.sqrt(np.mean((y_pred_check - y)**2))\n                 \n                 results = [{\n                     'tokens': tree.tokens,\n                     'formula': tree.get_infix(),\n                     'rmse': rmse_check,\n                     'constants': {} # Constants are baked into the formula string\n                 }]\n    \n    elif search_method == \"Beam Search\":\n        searcher = BeamSearch(MODEL, DEVICE, beam_width=int(beam_width), max_length=25, num_variables=num_vars)\n        results = searcher.search(x, y)\n    else:  # MCTS\n        mcts = MCTS(MODEL, DEVICE, max_simulations=int(beam_width) * 10, num_variables=num_vars)\n        result = mcts.search(x, y)\n        if result and result.get('tokens'):\n            tokens = result['tokens']\n            tree = ExpressionTree(tokens)\n            if tree.is_valid:\n                constants, rmse = optimize_constants(tree, x, y)\n                results = [{\n                    'tokens': tokens,\n                    'formula': tree.get_infix(),\n                    'rmse': rmse,\n                    'constants': constants\n                }]\n    \n    search_time = time.time() - start_time\n    \n    if not results:\n        return \"No se encontraron formulas validas\", None, \"\", \"\", \"\"\n    \n    progress(0.7, desc=\"Optimizando constantes...\")\n    pareto = ParetoFront()\n    pareto.add_from_results(results)\n    best = pareto.get_best_by_rmse()\n    \n    if not best:\n        return \"Error en optimizacion\", None, \"\", \"\", \"\"\n    \n    progress(0.9, desc=\"Procesando...\") \n    tree = ExpressionTree(best.tokens)\n    \n    # Use the stored formula string directly (this is what GP/search found)\n    display_formula = best.formula\n    \n    # If we have constants to substitute (Beam Search / MCTS with C placeholders)\n    if best.constants:\n        try:\n            positions = tree.root.get_constant_positions()\n            raw_infix = tree.get_infix()\n            display_formula = substitute_constants(raw_infix, best.constants, positions)\n        except:\n            pass\n    \n    # Try to simplify algebraically (x0 + x0 -> 2*x0, etc.)\n    try:\n        simplified = simplify_tree(tree)\n        # Only use simplified if it:\n        # 1. Is valid (not just a number, not \"Invalid\")\n        # 2. Still contains a variable (x or x0-x9)  \n        # 3. Is shorter or similar length\n        if simplified and simplified != \"Invalid\":\n            has_variable = any(v in simplified for v in ['x', 'x0', 'x1', 'x2', 'x3'])\n            is_not_just_number = not simplified.replace('.', '').replace('-', '').isdigit()\n            if has_variable and is_not_just_number:\n                display_formula = simplified\n    except:\n        pass\n    \n    y_pred = tree.evaluate(x, constants=best.constants)\n    \n    fig = create_fit_plot(x, y, y_pred, display_formula)\n    \n    # Format results\n    result_html = f\"\"\"\n    <div style=\"background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); padding: 20px; border-radius: 15px; border: 2px solid #00d4ff;\">\n        <h2 style=\"color: #00d4ff; margin: 0; font-size: 24px;\">Formula Encontrada</h2>\n        <div style=\"background: #0f0f23; padding: 15px; border-radius: 10px; margin: 15px 0; border-left: 4px solid #ff6b6b;\">\n            <code style=\"color: #ff6b6b; font-size: 28px; font-weight: bold;\">{display_formula}</code>\n        </div>\n        <div style=\"display: grid; grid-template-columns: repeat(4, 1fr); gap: 10px;\">\n            <div style=\"background: #0f0f23; padding: 10px; border-radius: 8px; text-align: center;\">\n                <span style=\"color: #888;\">RMSE</span><br>\n                <span style=\"color: #00d4ff; font-size: 16px; font-weight: bold;\">{best.rmse:.6f}</span>\n            </div>\n            <div style=\"background: #0f0f23; padding: 10px; border-radius: 8px; text-align: center;\">\n                <span style=\"color: #888;\">Nodos</span><br>\n                <span style=\"color: #00d4ff; font-size: 16px; font-weight: bold;\">{best.complexity}</span>\n            </div>\n            <div style=\"background: #0f0f23; padding: 10px; border-radius: 8px; text-align: center;\">\n                <span style=\"color: #888;\">Tiempo</span><br>\n                <span style=\"color: #00d4ff; font-size: 16px; font-weight: bold;\">{search_time:.2f}s</span>\n            </div>\n            <div style=\"background: #0f0f23; padding: 10px; border-radius: 8px; text-align: center;\">\n                <span style=\"color: #888;\">Metodo</span><br>\n                <span style=\"color: #4ade80; font-size: 16px; font-weight: bold;\">{search_method}</span>\n            </div>\n        </div>\n        <div style=\"margin-top: 15px; padding: 10px; background: #0f0f23; border-radius: 8px;\">\n            <span style=\"color: #888;\">Patron:</span> \n            <span style=\"color: #ffd93d;\">{pattern['type']}</span> \n            <span style=\"color: #666;\">({pattern['confidence']:.0%})</span>\n            <span style=\"color: #888; margin-left: 20px;\">Device:</span>\n            <span style=\"color: #4ade80;\">{DEVICE.type.upper()}</span>\n        </div>\n    \"\"\"\n    \n    # Add constants if any\n    # Add constants if any\n    if best.constants:\n        # Sort and format cleanly\n        sorted_items = sorted(best.constants.items(), key=lambda x: str(x[0]))\n        clean_consts = []\n        for i, (k, v) in enumerate(sorted_items):\n            clean_consts.append(f\"C_{i+1}: {v:.4f}\")\n        const_str = \"  |  \".join(clean_consts)\n        \n        result_html += f\"\"\"\n        <div style=\"margin-top: 10px; padding: 10px; background: #0f0f23; border-radius: 8px; border-left: 3px solid #ffd93d;\">\n            <span style=\"color: #888;\">Constantes:</span>\n            <span style=\"color: #fff; font-family: monospace; margin-left: 10px;\">{const_str}</span>\n        </div>\n        \"\"\"\n        \n    result_html += \"</div>\"\n    \n    # Predictions table\n    pred_html = '<table style=\"width: 100%; border-collapse: collapse; background: #1a1a2e; border-radius: 10px; overflow: hidden;\">'\n    pred_html += '<tr style=\"background: #16213e;\"><th style=\"padding: 10px; color: #00d4ff;\">X</th><th style=\"color: #00d4ff;\">Pred</th><th style=\"color: #00d4ff;\">Real</th><th style=\"color: #00d4ff;\">Delta</th></tr>'\n    for i in range(min(50, len(y))):\n        delta = abs(y_pred[i] - y[i])\n        color = \"#4ade80\" if delta < 0.1 else \"#fbbf24\" if delta < 1 else \"#ef4444\"\n        \n        # Display X nicely\n        x_val_str = \"\"\n        if x.ndim > 1 and x.shape[1] > 1:\n             x_val_str = f\"[{', '.join([f'{v:.1f}' for v in x[i]])}]\"\n        else:\n             xv = x[i] if x.ndim == 1 else x[i,0]\n             x_val_str = f\"{xv:.2f}\"\n             \n        pred_html += f'<tr style=\"border-bottom: 1px solid #333;\"><td style=\"padding: 8px; color: white; text-align: center;\">{x_val_str}</td><td style=\"color: white; text-align: center;\">{y_pred[i]:.4f}</td><td style=\"color: white; text-align: center;\">{y[i]:.4f}</td><td style=\"color: {color}; text-align: center; font-weight: bold;\">{delta:.4f}</td></tr>'\n    pred_html += '</table>'\n    \n    # Alternatives\n    alt_html = '<div style=\"background: #1a1a2e; padding: 15px; border-radius: 10px;\">'\n    alt_html += '<h4 style=\"color: #00d4ff; margin-top: 0;\">Alternativas</h4>'\n    for i, sol in enumerate(pareto.solutions[:4]):\n        alt_html += f'<div style=\"padding: 5px 10px; margin: 5px 0; background: #0f0f23; border-radius: 5px; border-left: 3px solid {\"#00d4ff\" if i == 0 else \"#666\"};\"><code style=\"color: {\"#ff6b6b\" if i == 0 else \"#888\"};\">{sol.formula}</code> <span style=\"color: #666; font-size: 12px;\">RMSE: {sol.rmse:.4f}</span></div>'\n    alt_html += '</div>'\n    \n    return result_html, fig, pred_html, alt_html, display_formula\n\n\ndef generate_example(tipo):\n    \"\"\"Generate example data.\"\"\"\n    if tipo == \"lineal\":\n        x = np.linspace(1, 10, 10)\n        y = 2 * x + 3\n    elif tipo == \"cuadratico\":\n        x = np.linspace(-5, 5, 11)\n        y = x**2 + 1\n    elif tipo == \"trig\":\n        x = np.linspace(0, 6.28, 20)\n        y = np.sin(x)\n    elif tipo == \"exp\":\n        x = np.linspace(0, 5, 15)\n        y = 2 * np.exp(0.5 * x)\n    else:\n        x = np.linspace(1, 10, 10)\n        y = 2 * x + 3\n    \n    return \", \".join([f\"{v:.2f}\" for v in x]), \", \".join([f\"{v:.4f}\" for v in y])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/ui/app_training.py\n",
        "\"\"\"\nTraining functions for AlphaSymbolic Gradio App.\nWith proper data normalization.\n\"\"\"\nimport os\nimport torch\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport gradio as gr\nfrom collections import deque\nimport random\nimport time\nimport csv\nimport datetime\n\nfrom core.grammar import VOCABULARY, TOKEN_TO_ID, OPERATORS, OPERATOR_STAGES, VARIABLES\nfrom data.synthetic_data import DataGenerator\nfrom ui.app_core import get_model, save_model, TRAINING_STATUS, add_training_error, should_stop_training, reset_stop_flag\nfrom core.loss import QuantileLoss\nfrom search.hybrid_search import hybrid_solve\nfrom core.grammar import ExpressionTree, simplify_formula\nfrom utils.data_utils import normalize_batch\n\n\ndef get_allowed_token_mask(stage, vocab_size, device):\n    \"\"\"\n    Creates a mask tensor for token logits.\n    Allowed tokens = 1.0, Disallowed = 0.0 (for multiplication mask)\n    Or returns indices of allowed tokens for -inf masking.\n    \"\"\"\n    allowed_ops = OPERATOR_STAGES.get(stage, list(OPERATORS.keys()))\n    \n    # All terminals are always allowed + VARIABLES\n    allowed_tokens = set(['C', '0', '1', '2', '3', '5', '10', 'pi', 'e'])\n    allowed_tokens.update(VARIABLES) # IMPORTANT! Don't forget variables\n    allowed_tokens.update(allowed_ops)\n    \n    # Build mask\n    mask = torch.zeros(vocab_size + 1, device=device)  # +1 for SOS token\n    for token in allowed_tokens:\n        if token in TOKEN_TO_ID:\n            mask[TOKEN_TO_ID[token]] = 1.0\n    mask[vocab_size] = 1.0  # SOS always allowed\n    \n    return mask\n\n\n# Normalization moved to utils.data_utils\n\n\n\ndef train_basic(epochs, batch_size, point_count=10, num_variables=1, progress=gr.Progress()):\n    \"\"\"Basic training with synthetic data.\"\"\"\n    global TRAINING_STATUS\n    \n    if TRAINING_STATUS[\"running\"]:\n        return \"Entrenamiento ya en progreso\", None\n    \n    TRAINING_STATUS[\"running\"] = True\n    \n    try:\n        MODEL, DEVICE = get_model()\n        \n        MODEL.train()\n        optimizer = torch.optim.AdamW(MODEL.parameters(), lr=1e-4, weight_decay=0.01)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(epochs), eta_min=1e-6)\n        ce_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n        \n        VOCAB_SIZE = len(VOCABULARY)\n        SOS_ID = VOCAB_SIZE\n        \n        data_gen = DataGenerator(max_depth=4, num_variables=int(num_variables))\n        losses = []\n        \n        for epoch in range(int(epochs)):\n            progress((epoch + 1) / epochs, desc=f\"Epoca {epoch+1}/{int(epochs)} [{DEVICE.type.upper()}]\")\n            \n            # Mix of inverse (known formulas) + random data (AlphaTensor-style)\n            half_batch = int(batch_size) // 2\n            batch_inverse = data_gen.generate_inverse_batch(half_batch, point_count=int(point_count))\n            batch_random = data_gen.generate_batch(int(batch_size) - half_batch, point_count=int(point_count))\n            batch = batch_inverse + batch_random\n            if len(batch) < 2:\n                continue\n            \n            x_list = [d['x'] for d in batch]\n            y_list = [d['y'] for d in batch]\n            \n            # Normalize data\n            x_list, y_list = normalize_batch(x_list, y_list)\n            \n            token_lists = [[TOKEN_TO_ID[t] for t in d['tokens']] for d in batch]\n            \n            max_len = max(len(s) for s in token_lists)\n            decoder_input = torch.full((len(batch), max_len + 1), SOS_ID, dtype=torch.long)\n            targets = torch.full((len(batch), max_len + 1), -1, dtype=torch.long)\n            \n            for i, seq in enumerate(token_lists):\n                decoder_input[i, 1:len(seq)+1] = torch.tensor(seq, dtype=torch.long)\n                targets[i, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n            \n            x_tensor = torch.tensor(np.array(x_list), dtype=torch.float32).to(DEVICE)\n            y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n            decoder_input = decoder_input.to(DEVICE)\n            targets = targets.to(DEVICE)\n            \n            # Forward\n            optimizer.zero_grad()\n            logits, _ = MODEL(x_tensor, y_tensor, decoder_input)\n            loss = ce_loss(logits.view(-1, VOCAB_SIZE + 1), targets.view(-1))\n            \n            # Skip if loss is NaN\n            if torch.isnan(loss) or torch.isinf(loss):\n                continue\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(MODEL.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            \n            losses.append(loss.item())\n        \n        save_model()\n        MODEL.eval()\n        TRAINING_STATUS[\"running\"] = False\n        \n        if not losses:\n            return \"Error: No se pudo calcular loss (revisar datos)\", None\n        \n        fig = create_loss_plot(losses, \"Entrenamiento Basico\")\n        \n        result = f\"\"\"\n        <div style=\"background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); padding: 20px; border-radius: 15px; border: 2px solid #4ade80;\">\n            <h2 style=\"color: #4ade80; margin: 0;\">Entrenamiento Completado</h2>\n            <p style=\"color: white;\">Epocas: {int(epochs)} | Loss Final: {losses[-1]:.4f}</p>\n            <p style=\"color: #00d4ff;\">Dispositivo: {DEVICE.type.upper()}</p>\n        </div>\n        \"\"\"\n        return result, fig\n        \n    except Exception as e:\n        TRAINING_STATUS[\"running\"] = False\n        return f\"Error: {str(e)}\", None\n\n\ndef train_curriculum(epochs, batch_size, point_count=10, num_variables=1, progress=gr.Progress()):\n    \"\"\"Curriculum Learning - starts simple, increases difficulty gradually.\"\"\"\n    global TRAINING_STATUS\n    \n    if TRAINING_STATUS[\"running\"]:\n        return \"Entrenamiento ya en progreso\", None\n    \n    TRAINING_STATUS[\"running\"] = True\n    \n    try:\n        MODEL, DEVICE = get_model()\n        \n        MODEL.train()\n        optimizer = torch.optim.AdamW(MODEL.parameters(), lr=5e-5, weight_decay=0.01)  # Lower LR\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2)\n        ce_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n        \n        VOCAB_SIZE = len(VOCABULARY)\n        SOS_ID = VOCAB_SIZE\n        quantile_loss_fn = QuantileLoss().to(DEVICE)\n        losses = []\n        \n        for epoch in range(int(epochs)):\n            # Curriculum: slow progression\n            # Stage 1 (0-50%): depth 2-3, 80% inverse data\n            # Stage 2 (50-80%): depth 3-4, 50% inverse data  \n            # Stage 3 (80-100%): depth 4-5, 20% inverse data\n            progress_pct = epoch / epochs\n            \n            if progress_pct < 0.5:\n                current_depth = 2 + int(progress_pct * 2)  # 2-3\n                inverse_ratio = 0.8\n            elif progress_pct < 0.8:\n                current_depth = 3 + int((progress_pct - 0.5) * 3.3)  # 3-4\n                inverse_ratio = 0.5\n            else:\n                current_depth = 4 + int((progress_pct - 0.8) * 5)  # 4-5\n                inverse_ratio = 0.2\n            \n            progress((epoch + 1) / epochs, desc=f\"Epoca {epoch+1}/{int(epochs)} (prof: {current_depth}, inv: {inverse_ratio:.0%}) [{DEVICE.type.upper()}]\")\n            \n            data_gen = DataGenerator(max_depth=current_depth, num_variables=int(num_variables))\n            \n            # Mix inverse + random based on curriculum stage\n            n_inverse = int(batch_size * inverse_ratio)\n            n_random = int(batch_size) - n_inverse\n            \n            batch_inverse = data_gen.generate_inverse_batch(max(1, n_inverse), point_count=int(point_count)) if n_inverse > 0 else []\n            batch_random = data_gen.generate_batch(max(1, n_random), point_count=int(point_count)) if n_random > 0 else []\n            batch = batch_inverse + batch_random\n            if len(batch) < 2:\n                continue\n            \n            x_list = [d['x'] for d in batch]\n            y_list = [d['y'] for d in batch]\n            x_list, y_list = normalize_batch(x_list, y_list)\n            \n            token_lists = [[TOKEN_TO_ID[t] for t in d['tokens']] for d in batch]\n            \n            max_len = max(len(s) for s in token_lists)\n            decoder_input = torch.full((len(batch), max_len + 1), SOS_ID, dtype=torch.long)\n            targets = torch.full((len(batch), max_len + 1), -1, dtype=torch.long)\n            \n            for i, seq in enumerate(token_lists):\n                decoder_input[i, 1:len(seq)+1] = torch.tensor(seq, dtype=torch.long)\n                targets[i, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n            \n            x_tensor = torch.tensor(np.array(x_list), dtype=torch.float32).to(DEVICE)\n            y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n            decoder_input = decoder_input.to(DEVICE)\n            targets = targets.to(DEVICE)\n            \n            optimizer.zero_grad()\n            logits, value_pred = MODEL(x_tensor, y_tensor, decoder_input)\n            \n            # Policy Loss\n            loss_policy = ce_loss(logits.view(-1, VOCAB_SIZE + 1), targets.view(-1))\n            \n            # Value Loss\n            # For supervised learning, these are \"perfect\" solutions, so Value Target = 1.0 (as a scalar per batch item)\n            value_targets = torch.ones((len(batch), 1), device=DEVICE)\n            loss_value = quantile_loss_fn(value_pred, value_targets)\n            \n            # Combined Loss\n            loss = loss_policy + 0.5 * loss_value\n            \n            if torch.isnan(loss) or torch.isinf(loss):\n                continue\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(MODEL.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            \n            losses.append(loss.item())\n        \n        save_model()\n        MODEL.eval()\n        TRAINING_STATUS[\"running\"] = False\n        \n        if not losses:\n            return \"Error: No se pudo calcular loss\", None\n        \n        fig = create_loss_plot(losses, \"Curriculum Learning\")\n        \n        result = f\"\"\"\n        <div style=\"background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); padding: 20px; border-radius: 15px; border: 2px solid #00d4ff;\">\n            <h2 style=\"color: #00d4ff; margin: 0;\">Curriculum Learning Completado</h2>\n            <p style=\"color: white;\">Epocas: {int(epochs)} | Loss Final: {losses[-1]:.4f}</p>\n            <p style=\"color: #888;\">Profundidad maxima: 6 | Dispositivo: {DEVICE.type.upper()}</p>\n        </div>\n        \"\"\"\n        return result, fig\n        \n    except Exception as e:\n        TRAINING_STATUS[\"running\"] = False\n        return f\"Error: {str(e)}\", None\n\n\ndef train_self_play(iterations, problems_per_iter, point_count=10, num_variables=1, progress=gr.Progress()):\n    \"\"\"AlphaZero Self-Play loop.\"\"\"\n    global TRAINING_STATUS\n    \n    if TRAINING_STATUS[\"running\"]:\n        return \"Entrenamiento ya en progreso\", None\n    \n    TRAINING_STATUS[\"running\"] = True\n    reset_stop_flag()  # Reset stop flag at start\n    \n    try:\n        MODEL, DEVICE = get_model()\n        \n        from search.mcts import MCTS\n        \n        optimizer = torch.optim.AdamW(MODEL.parameters(), lr=5e-5, weight_decay=0.01)\n        # Scheduler: Reduce LR when plateauing to help convergence\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15, min_lr=1e-6)\n        \n        # Losses for AlphaZero\n        # Policy: KLDiv (comparing distributions)\n        # Value: Quantile Loss (3 Quantiles)\n        kl_loss = torch.nn.KLDivLoss(reduction='batchmean')\n        quantile_loss_fn = QuantileLoss()\n        \n        VOCAB_SIZE = len(VOCABULARY)\n        SOS_ID = VOCAB_SIZE\n        \n        replay_buffer = deque(maxlen=20000)\n        \n        # Adaptive Curriculum State\n        current_depth = 2\n        data_gen = DataGenerator(max_depth=current_depth, num_variables=int(num_variables))\n        \n        # MCTS for A100: Increase batch size and simulations significantly\n        # Adjusted for RTX 3050/i5: Batch 64 is smoother (less CPU wait)\n        # Initialize with Stage 0 (Arithmetic only)\n        curriculum_stage = 0\n        searcher = MCTS(MODEL, DEVICE, max_simulations=500, complexity_lambda=0.1, batch_size=64, curriculum_stage=curriculum_stage, num_variables=int(num_variables))\n        \n        rmses = []\n        losses = []\n        total_gp_corrections = 0  # Track GP expert corrections\n        best_avg_rmse = float('inf')\n        \n        start_time = time.time()\n        \n        for iteration in range(int(iterations)):\n            # Check for stop request\n            if should_stop_training():\n                print(\"\u23f9\ufe0f Training stopped by user\")\n                break\n            # ETA Calculation\n            elapsed = time.time() - start_time\n            if iteration > 0:\n                avg_time_per_iter = elapsed / iteration\n                remaining_iters = int(iterations) - iteration\n                eta_seconds = remaining_iters * avg_time_per_iter\n                \n                # Format ETA\n                if eta_seconds > 3600:\n                    eta_str = f\"{eta_seconds/3600:.1f}h\"\n                elif eta_seconds > 60:\n                    eta_str = f\"{eta_seconds/60:.0f}m\"\n                else:\n                    eta_str = f\"{eta_seconds:.0f}s\"\n            else:\n                eta_str = \"Calculando...\"\n\n            # Adaptive Curriculum Check\n            # Stages: 0=Arithmetic, 1=Poly, 2=Trig, 3=Adv, 4=Complex\n            CURRICULUM_LEVELS = [\n                {'depth': 1, 'ops': ['+', '-', '*', '/']},\n                {'depth': 2, 'ops': ['+', '-', '*', '/']},\n                {'depth': 3, 'ops': ['+', '-', '*', '/', 'pow', 'sqrt']},\n                {'depth': 4, 'ops': ['+', '-', '*', '/', 'pow', 'sqrt', 'sin', 'cos']},\n                {'depth': 5, 'ops': None} # All\n            ]\n            \n\n            recent_rmse = np.mean(rmses[-20:]) if len(rmses) >= 20 else 1.0\n            \n            # Graduation condition: RMSE < 0.1 stable\n            if len(rmses) > 20 and recent_rmse < 0.1 and curriculum_stage < len(CURRICULUM_LEVELS) - 1:\n                curriculum_stage += 1\n                stage_info = CURRICULUM_LEVELS[curriculum_stage]\n                data_gen = DataGenerator(max_depth=stage_info['depth'], allowed_operators=stage_info['ops'], num_variables=int(num_variables))\n                # Recreate MCTS with new curriculum stage for operator filtering\n                searcher = MCTS(MODEL, DEVICE, max_simulations=500, complexity_lambda=0.1, batch_size=64, curriculum_stage=curriculum_stage, num_variables=int(num_variables))\n                print(f\"*** Curriculum Level Up! Stage {curriculum_stage} ({stage_info['depth']}, {stage_info['ops']}) ***\")\n                # Clear buffer to avoid training on old easy data? Maybe keep some for replay.\n            \n            # Ensure data_gen is initialized at start\n            if iteration == 0:\n                stage_info = CURRICULUM_LEVELS[0]\n                data_gen = DataGenerator(max_depth=stage_info['depth'], allowed_operators=stage_info['ops'], num_variables=int(num_variables))\n\n            stage_name = [\"Arithmetic\", \"Polynomials\", \"Trigonometry\", \"Advanced\", \"Complex\"][curriculum_stage]\n            \n            # Safe access to current_lr\n            curr_lr_disp = optimizer.param_groups[0]['lr']\n            msg = f\"Iter {iteration+1}/{int(iterations)} [{stage_name}] RMSE:{recent_rmse:.3f} LR:{curr_lr_disp:.1e} | ETA: {eta_str}\"\n            progress((iteration + 1) / iterations, desc=msg)\n            \n            # Active Learning / Hard Mining Phase\n            MODEL.eval()\n            \n            # Generate a large pool of candidates candidates to find the \"hard\" ones\n            pool_size = problems_per_iter * 3  # Generate 3x more than we need\n            candidates = data_gen.generate_inverse_batch(pool_size, point_count=int(point_count))\n            \n            if not candidates:\n                continue\n                \n            # Quick forward pass to estimate difficulty (Loss)\n            # We want to train on problems where the model currently FAILS (High Loss)\n            hard_problems = []\n            \n            with torch.no_grad():\n                # Process in chunks to avoid OOM\n                chunk_size = 32\n                for i in range(0, len(candidates), chunk_size):\n                    chunk = candidates[i:i+chunk_size]\n                    \n                    x_list = [d['x'] for d in chunk]\n                    y_list = [d['y'] for d in chunk]\n                    x_list, y_list = normalize_batch(x_list, y_list)\n                    \n                    token_lists = [[TOKEN_TO_ID.get(t, TOKEN_TO_ID['C']) for t in d['tokens']] for d in chunk]\n                    max_len = max(len(s) for s in token_lists)\n                    \n                    # Prepare tensors\n                    dec_in = torch.full((len(chunk), max_len + 1), SOS_ID, dtype=torch.long).to(DEVICE)\n                    targets = torch.full((len(chunk), max_len + 1), -1, dtype=torch.long).to(DEVICE)\n                    \n                    for j, seq in enumerate(token_lists):\n                        dec_in[j, 1:len(seq)+1] = torch.tensor(seq, dtype=torch.long)\n                        targets[j, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n                        \n                    x_tensor = torch.tensor(np.array(x_list), dtype=torch.float32).to(DEVICE)\n                    y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n                    \n                    logits, _ = MODEL(x_tensor, y_tensor, dec_in)\n                    \n                    # Calculate loss per item\n                    # CrossEntropy usually aggregates, so we use reduction='none'\n                    loss_f = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='none')\n                    raw_losses = loss_f(logits.view(-1, VOCAB_SIZE + 1), targets.view(-1))\n                    \n                    # Reshape back to [Batch, Seq] to sum/mean per sample\n                    raw_losses = raw_losses.view(len(chunk), -1)\n                    # Average loss per non-padded token\n                    mask = (targets != -1)\n                    sample_losses = (raw_losses * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n                    \n                    for j, loss_val in enumerate(sample_losses):\n                        # Store (Loss, Problem)\n                        hard_problems.append((loss_val.item(), chunk[j]))\n            \n            # Sort by difficulty (Loss descending)\n            hard_problems.sort(key=lambda x: x[0], reverse=True)\n            \n            # Stabilization: Mix Hardest (70%) + Random Examples (30%)\n            # This prevents \"Catastrophic Forgetting\" of simpler patterns\n            n_hard = int(problems_per_iter * 0.7)\n            n_random = int(problems_per_iter) - n_hard\n            \n            # Top K hardest\n            selected_hard = [p[1] for p in hard_problems[:n_hard]]\n            \n            # Random selection from the rest of the pool (to keep variety)\n            remaining_pool = [p[1] for p in hard_problems[n_hard:]]\n            selected_random = random.sample(remaining_pool, min(n_random, len(remaining_pool))) if remaining_pool else []\n            \n            selected_problems = selected_hard + selected_random\n            \n            avg_pool_loss = np.mean([p[0] for p in hard_problems])\n            top_loss = np.mean([p[0] for p in hard_problems[:n_hard]]) if n_hard > 0 else 0\n            \n            print(f\"Active Learning: Pool Loss {avg_pool_loss:.3f} -> Selected Mix (Hard:{top_loss:.3f})\")\n\n            # --- HALL OF SHAME CAPTURE ---\n            # Capture what the model predicts for the top 3 hardest failures\n            try:\n                top_failures = hard_problems[:3]\n                x_fail = [p[1]['x'].astype(np.float64) for p in top_failures]\n                y_fail = [p[1]['y'].astype(np.float64) for p in top_failures]\n                target_formulas = [p[1]['infix'] for p in top_failures]\n                fail_losses = [p[0] for p in top_failures]\n                \n                # Simple Greedy Decode to see what it predicts\n                from search.beam_search import BeamSearch\n                # Use beam search with width 1 (Greedy) for speed, with curriculum mask\n                bs = BeamSearch(MODEL, DEVICE, beam_width=1, max_length=20, curriculum_stage=curriculum_stage, num_variables=int(num_variables))\n                \n                for i in range(len(top_failures)):\n                    try:\n                        # Decode\n                        # Enable return_partial to see what the model is thinking if it fails\n                        res = bs.search(x_fail[i], y_fail[i], return_partial=True)\n                        if not res:\n                            pred_formula = \"Search Empty (No Tokens)\"\n                        else:\n                            pred_formula = res[0]['formula']\n                            \n                        # Detect Looping (e.g. \"10 / / / / / /\")\n                        # Basic heuristic: check if last 10 chars contain > 80% same char or repeating pattern\n                        if len(pred_formula) > 20:\n                            # Check for repeating slashes or other single chars\n                            if pred_formula.count('/') > 10 and pred_formula.endswith('/ .'): \n                                 pred_formula = pred_formula[:20] + \" ... [Loop Detected]\"\n                            elif \" / / / \" in pred_formula:\n                                 pred_formula = pred_formula.split(\" / / / \")[0] + \" ... [Loop Detected]\"\n                        \n                        add_training_error(\n                            target=target_formulas[i],\n                            predicted=pred_formula,\n                            loss=fail_losses[i],\n                            stage=stage_name\n                        )\n                    except Exception as e:\n                        print(f\"HoS Inner Error: {e}\")\n                        add_training_error(\n                            target=target_formulas[i],\n                            predicted=f\"CRASH: {str(e)[:20]}\",\n                            loss=fail_losses[i],\n                            stage=stage_name\n                        )\n            except Exception as e:\n                import traceback\n                print(f\"HoS Outer Error: {e}\")\n                traceback.print_exc()\n\n            # --- MCTS SOLVE + GP EXPERT CORRECTION ---\n            gp_corrections = 0\n            nn_successes = 0\n            \n            for prob in selected_problems:\n                x_data = prob['x'].astype(np.float64)\n                y_data = prob['y'].astype(np.float64)\n                target_tokens = prob.get('tokens', [])  # Known answer for inverse problems\n                \n                try:\n                    # 1. Neural Network attempts to solve\n                    result = searcher.search(x_data, y_data)\n                    nn_rmse = result.get('rmse', float('inf'))\n                    \n                    # 2. Check if NN succeeded or failed\n                    NN_SUCCESS_THRESHOLD = 0.1  # RMSE threshold for \"good enough\"\n                    \n                    if nn_rmse < NN_SUCCESS_THRESHOLD:\n                        # NN succeeded - store its examples\n                        nn_successes += 1\n                        if 'root' in result:\n                            examples = searcher.get_training_examples(result['root'])\n                            for (tokens, policy, value) in examples:\n                                replay_buffer.append({\n                                    'x': x_data, 'y': y_data,\n                                    'tokens': tokens,\n                                    'policy': policy,\n                                    'value': value,\n                                    'source': 'NN'\n                                })\n                        rmses.append(nn_rmse)\n                    else:\n                        # 3. NN FAILED - GP Engine to the rescue!\n                        # Use hybrid_solve with INCREASED timeout for better formulas\n                        try:\n                            gp_result = hybrid_solve(\n                                x_data, y_data, MODEL, DEVICE,\n                                beam_width=20,\n                                gp_timeout=15,    # Increased from 5s to 15s\n                                num_variables=int(num_variables)\n                            )\n                            \n                            if gp_result and gp_result.get('formula'):\n                                # Convert GP formula to tokens\n                                tree = ExpressionTree.from_infix(gp_result['formula'])\n                                if tree.is_valid and tree.tokens:\n                                    # Calculate actual RMSE of GP solution\n                                    y_pred = tree.evaluate(x_data)\n                                    gp_rmse = np.sqrt(np.mean((y_pred - y_data)**2))\n                                    \n                                    # DYNAMIC ACCEPTANCE CRITERIA\n                                    # Accept if RMSE <= 0.01 (Precision Mode)\n                                    # Since we fixed the GP, we expect exact matches.\n                                    is_decent = gp_rmse <= 0.01\n                                    \n                                    if is_decent and len(tree.tokens) <= 50:\n                                        # Sanitize tokens: replace numeric constants NOT in vocab with 'C'\n                                        sanitized_tokens = []\n                                        for t in tree.tokens:\n                                            if t in TOKEN_TO_ID:\n                                                sanitized_tokens.append(t)\n                                            else:\n                                                try:\n                                                    float(t)\n                                                    sanitized_tokens.append('C')\n                                                except ValueError:\n                                                    sanitized_tokens = None\n                                                    break\n                                        \n                                        if sanitized_tokens and len(sanitized_tokens) > 0:\n                                            # Create \"expert\" policy - uniform over tokens\n                                            policy = np.ones(len(VOCABULARY)) / len(VOCABULARY)\n                                            \n                                            # SCALED REWARD\n                                            # Give higher value for better solutions\n                                            # RMSE 0.0 -> Value 1.0\n                                            # RMSE 0.1 -> Value 0.8\n                                            # RMSE 0.5 -> Value 0.2\n                                            # Formula: max(0.1, 1.0 - (rmse * 1.6))\n                                            reward_value = max(0.1, 1.0 - (gp_rmse * 1.6))\n                                            \n                                            replay_buffer.append({\n                                                'x': x_data, 'y': y_data,\n                                                'tokens': sanitized_tokens,\n                                                'policy': policy,\n                                                'value': reward_value,\n                                                'source': 'GP_EXPERT'\n                                            })\n                                            gp_corrections += 1\n                                            rmses.append(gp_rmse)\n                                            \n                                            print(f\"\ud83d\udcda GP Expert ACCEPTED: {gp_result['formula'][:50]}... (RMSE: {gp_rmse:.4f}, Val: {reward_value:.2f})\")\n                                        else:\n                                             print(f\"\ud83d\udd38 GP Rejected (Sanitization): {gp_result['formula'][:30]}\")\n                                    else:\n                                         print(f\"\ud83d\udd38 GP Rejected (Quality): RMSE {gp_rmse:.4f} vs NN {nn_rmse:.4f}\")\n                        except Exception as gp_err:\n                            # GP failed too - skip this problem\n                            pass\n                        \n                        # Also store NN failure for learning (lower value)\n                        if 'root' in result and result.get('tokens'):\n                            examples = searcher.get_training_examples(result['root'])\n                            for (tokens, policy, value) in examples:\n                                replay_buffer.append({\n                                    'x': x_data, 'y': y_data,\n                                    'tokens': tokens,\n                                    'policy': policy,\n                                    'value': max(0.0, 0.3 - nn_rmse * 0.1),  # Low value for bad solutions\n                                    'source': 'NN_FAIL'\n                                })\n                            rmses.append(nn_rmse)\n                        \n                except Exception as e:\n                    print(f\"Self-play error: {e}\")\n                    continue\n            \n            # Log progress\n            if gp_corrections > 0:\n                total_gp_corrections += gp_corrections\n                print(f\"\ud83c\udfaf Iteration {iteration+1}: NN Success: {nn_successes}, GP Corrections: {gp_corrections}\")\n            \n            # Training phase\n            # To saturate GPU: Increase batch size and number of updates\n            if len(replay_buffer) >= 64:\n                MODEL.train()\n                \n                # Dynamic training steps: Train more if we have more data\n                # AlphaZero ratio usually high (e.g. 10 epochs on new data)\n                # Here we sample from buffer.\n                train_batch_size = 128\n                if len(replay_buffer) < train_batch_size:\n                    train_batch_size = 64\n                \n                # Steps: roughly cover 20% of buffer or at least 10 steps\n                steps = max(10, min(50, len(replay_buffer) // train_batch_size))\n                \n                for _ in range(steps):\n                    batch = random.sample(list(replay_buffer), min(train_batch_size, len(replay_buffer)))\n                    \n                    x_list = [exp['x'] for exp in batch]\n                    y_list = [exp['y'] for exp in batch]\n                    x_list, y_list = normalize_batch(x_list, y_list)\n                    \n                    token_lists = [[TOKEN_TO_ID[t] for t in exp['tokens']] for exp in batch]\n                    policy_targets = [exp['policy'] for exp in batch]\n                    value_targets_list = [exp['value'] for exp in batch]\n                    \n                    max_len = max(len(s) for s in token_lists)\n                    decoder_input = torch.full((len(batch), max_len + 1), SOS_ID, dtype=torch.long)\n                    \n                    # Policy targets (for KLDiv) and Value targets\n                    policy_target_tensor = torch.tensor(np.array(policy_targets), dtype=torch.float32).to(DEVICE)\n                    value_target_tensor = torch.tensor(np.array(value_targets_list), dtype=torch.float32).unsqueeze(1).to(DEVICE)\n                    \n                    for i, seq in enumerate(token_lists):\n                        l = len(seq)\n                        decoder_input[i, 1:l+1] = torch.tensor(seq, dtype=torch.long)\n                    \n                    x_tensor = torch.tensor(np.array(x_list), dtype=torch.float32).to(DEVICE)\n                    y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n                    decoder_input = decoder_input.to(DEVICE)\n                    \n                    optimizer.zero_grad()\n                    logits, value_pred = MODEL(x_tensor, y_tensor, decoder_input)\n                    \n                    # Policy Loss (KL Divergence)\n                    # Get logits for the last token position of each sequence\n                    last_logits = []\n                    for i, seq in enumerate(token_lists):\n                        idx = len(seq) # Post-padding index? No, index in padded tensor.\n                        # decoder_input: [SOS, T1, T2]\n                        # logits: [PredSOS, PredT1, PredT2]\n                        # We want prediction AFTER T2? No.\n                        # MCTS Example: State=[T1, T2]. Policy=Dist for T3.\n                        # Model Input: [SOS, T1, T2]. Output Last: Dist for T3.\n                        # Index is len(seq).\n                        last_logits.append(logits[i, idx, :VOCAB_SIZE])\n                    \n                    last_logits = torch.stack(last_logits)\n                    log_probs = torch.nn.functional.log_softmax(last_logits, dim=1)\n                    \n                    loss_policy = kl_loss(log_probs, policy_target_tensor)\n                    \n                    # Value Loss (Quantile)\n                    loss_value = quantile_loss_fn(value_pred, value_target_tensor)\n                    \n                    # Total Loss\n                    loss = loss_policy + loss_value \n                    \n                    if not (torch.isnan(loss) or torch.isinf(loss)):\n                        loss.backward()\n                        torch.nn.utils.clip_grad_norm_(MODEL.parameters(), 1.0)\n                        optimizer.step()\n                        losses.append(loss.item())\n            \n            # Step Scheduler based on recent Loss\n            if losses:\n                current_loss = np.mean(losses[-10:])\n                scheduler.step(current_loss)\n            \n            current_lr = optimizer.param_groups[0]['lr']\n            \n            # Periodic save\n            if (iteration + 1) % 10 == 0:\n                save_model()\n        \n        save_model()\n        MODEL.eval()\n        TRAINING_STATUS[\"running\"] = False\n        \n        fig = create_selfplay_plot(losses, rmses)\n        \n        avg_rmse = np.mean(rmses[-50:]) if rmses else 0\n        gp_pct = (total_gp_corrections / max(1, len(rmses))) * 100\n        result = f\"\"\"\n        <div style=\"background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); padding: 20px; border-radius: 15px; border: 2px solid #ff6b6b;\">\n            <h2 style=\"color: #ff6b6b; margin: 0;\">Self-Play + GP Expert Completado</h2>\n            <p style=\"color: white;\">Iteraciones: {int(iterations)} | Problemas: {len(rmses)}</p>\n            <p style=\"color: #888;\">RMSE Promedio: {avg_rmse:.4f} | Dispositivo: {DEVICE.type.upper()}</p>\n            <p style=\"color: #4ade80;\">\ud83d\udcda Correcciones GP Expert: {total_gp_corrections} ({gp_pct:.1f}% de problemas)</p>\n        </div>\n        \"\"\"\n        return result, fig\n        \n    except Exception as e:\n        TRAINING_STATUS[\"running\"] = False\n        import traceback\n        print(f\"Self-play error traceback:\")\n        traceback.print_exc()\n        return f\"Error: {str(e)}\", None\n\n\ndef create_loss_plot(losses, title):\n    \"\"\"Create a loss plot with dark theme.\"\"\"\n    plt.close('all')\n    fig, ax = plt.subplots(figsize=(8, 4), facecolor='#1a1a2e')\n    ax.set_facecolor('#1a1a2e')\n    \n    if losses and len(losses) > 0:\n        ax.plot(losses, color='#00d4ff', linewidth=2)\n        ax.set_xlabel('Paso', color='white')\n        ax.set_ylabel('Loss', color='white')\n    else:\n        # Placeholder when no data\n        ax.text(0.5, 0.5, 'Esperando datos...', \n                transform=ax.transAxes, fontsize=16, color='#888',\n                ha='center', va='center')\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n    \n    ax.set_title(title, color='white', fontweight='bold')\n    ax.tick_params(colors='white')\n    ax.grid(True, alpha=0.2)\n    for spine in ax.spines.values():\n        spine.set_color('#00d4ff')\n    plt.tight_layout()\n    return fig\n\n\ndef create_selfplay_plot(losses, rmses):\n    \"\"\"Create dual plot for self-play results.\"\"\"\n    plt.close('all')\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4), facecolor='#1a1a2e')\n    \n    ax1.set_facecolor('#1a1a2e')\n    if losses:\n        ax1.plot(losses, color='#00d4ff', linewidth=2)\n    ax1.set_xlabel('Step', color='white')\n    ax1.set_ylabel('Loss', color='white')\n    ax1.set_title('Policy Loss', color='white', fontweight='bold')\n    ax1.tick_params(colors='white')\n    ax1.grid(True, alpha=0.2)\n    \n    ax2.set_facecolor('#1a1a2e')\n    if rmses:\n        ax2.plot(rmses, color='#ff6b6b', linewidth=1, alpha=0.5)\n        if len(rmses) > 10:\n            ma = np.convolve(rmses, np.ones(10)/10, mode='valid')\n            ax2.plot(range(9, len(rmses)), ma, color='#ff6b6b', linewidth=2)\n    ax2.set_xlabel('Problema', color='white')\n    ax2.set_ylabel('RMSE', color='white')\n    ax2.set_title('RMSE', color='white', fontweight='bold')\n    ax2.tick_params(colors='white')\n    ax2.grid(True, alpha=0.2)\n    \n    for ax in [ax1, ax2]:\n        for spine in ax.spines.values():\n            spine.set_color('#00d4ff')\n    \n    plt.tight_layout()\n    return fig\n\ndef train_supervised(iterations, batch_size=128, point_count=10, progress=gr.Progress()):\n    \"\"\"\n    Massive Supervised Pre-training (Warmup).\n    Focus: Syntax, Basic Arithmetic, Overcoming \"Collapse to Constant\".\n    Speed: High (No MCTS, just random generation + CrossEntropy).\n    \"\"\"\n    global TRAINING_STATUS\n    \n    if TRAINING_STATUS[\"running\"]:\n        return \"Entrenamiento ya en progreso\", None\n    \n    TRAINING_STATUS[\"running\"] = True\n    reset_stop_flag()  # Reset stop flag at start\n    \n    try:\n        MODEL, DEVICE = get_model()\n        \n        MODEL.train()\n        optimizer = torch.optim.AdamW(MODEL.parameters(), lr=1e-4, weight_decay=0.01)\n        # Slower decay: T_max = iterations * 2 keeps LR higher for longer\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(iterations*2), eta_min=1e-6)\n        ce_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n        \n        VOCAB_SIZE = len(VOCABULARY)\n        SOS_ID = VOCAB_SIZE\n        \n        # Progressive Curriculum Stages for Pre-training\n        PRE_CURRICULUM = [\n            {'depth': 2, 'ops': ['+', '-', '*', '/'], 'stage': 0},           # 0-20%: Basic arithmetic (depth 2 for variety)\n            {'depth': 2, 'ops': ['+', '-', '*', '/'], 'stage': 0},           # 20-40%: Deeper arithmetic\n            {'depth': 2, 'ops': ['+', '-', '*', '/', 'pow', 'sqrt'], 'stage': 1},  # 40-60%: Powers\n            {'depth': 3, 'ops': ['+', '-', '*', '/', 'pow', 'sqrt', 'sin', 'cos'], 'stage': 2},  # 60-80%: Trig\n            {'depth': 3, 'ops': None, 'stage': None},  # 80-100%: All ops\n        ]\n        \n        losses = []\n        current_stage_idx = 0\n        # Curriculum for variables: start simple, add complexity\n        VARS_BY_STAGE = [1, 1, 2, 3, 5]  # Max vars per stage\n        stage_info = PRE_CURRICULUM[0]\n        data_gen = DataGenerator(max_depth=stage_info['depth'], allowed_operators=stage_info['ops'], num_variables=1)\n        allowed_mask = get_allowed_token_mask(stage_info['stage'] if stage_info['stage'] is not None else 4, VOCAB_SIZE, DEVICE)\n        \n        start_time = time.time()\n        \n        for i in range(int(iterations)):\n            # Check for stop request\n            if should_stop_training():\n                print(\"\u23f9\ufe0f Pre-training stopped by user\")\n                break\n            \n            # Progressive curriculum: change stage based on progress\n            progress_pct = i / int(iterations)\n            new_stage_idx = min(int(progress_pct * 5), 4)  # 0-4 based on 20% increments\n            \n            if new_stage_idx != current_stage_idx:\n                current_stage_idx = new_stage_idx\n                stage_info = PRE_CURRICULUM[current_stage_idx]\n                # Progressive variable curriculum: stages 0-1 use 1 var, 2 uses 1-2, 3 uses 1-3, 4 uses 1-5\n                max_vars_this_stage = VARS_BY_STAGE[current_stage_idx]\n                iter_num_vars = random.randint(1, max_vars_this_stage)\n                data_gen = DataGenerator(max_depth=stage_info['depth'], allowed_operators=stage_info['ops'], num_variables=iter_num_vars)\n                stage_id = stage_info['stage'] if stage_info['stage'] is not None else 4\n                allowed_mask = get_allowed_token_mask(stage_id, VOCAB_SIZE, DEVICE)\n                stage_name = ['Arithmetic', 'Polynomials', 'Trigonometry', 'Advanced', 'Complex'][new_stage_idx]\n                print(f\"\ud83d\udcda Pre-training: {stage_name} (depth={stage_info['depth']}, max_vars={max_vars_this_stage})\")\n            \n            # ETA\n            elapsed = time.time() - start_time\n            if i > 0:\n                iter_per_sec = i / elapsed\n                remaining = int(iterations) - i\n                eta = remaining / iter_per_sec\n                eta_str = f\"{eta:.0f}s\"\n            else:\n                eta_str = \"...\"\n                \n            current_lr = optimizer.param_groups[0]['lr']\n            stage_name = ['Arithmetic', 'Polynomials', 'Trigonometry', 'Advanced', 'Complex'][current_stage_idx]\n            msg = f\"[{stage_name}] Iter {i+1}/{int(iterations)} Loss:{np.mean(losses[-50:]) if losses else 0:.3f} LR:{current_lr:.1e} ETA:{eta_str}\"\n            progress((i + 1) / iterations, desc=msg)\n            \n            # Variable curriculum: use appropriate range for current stage\n            # IMPORTANT: Set num_variables BEFORE generating batch to ensure uniform dimensions\n            max_vars_this_stage = VARS_BY_STAGE[current_stage_idx]\n            batch_num_vars = random.randint(1, max_vars_this_stage)\n            data_gen.num_variables = batch_num_vars\n            data_gen.active_variables = ['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9'][:batch_num_vars]\n            data_gen.terminals = data_gen.active_variables + ['C', '0', '1', '2', '3', '5', '10', 'pi', 'e']\n            \n            # Generate Random Batch (High Speed)\n            batch = data_gen.generate_batch(int(batch_size), point_count=int(point_count))\n            \n            if not batch:\n                continue\n            \n            x_list = [d['x'] for d in batch]\n            y_list = [d['y'] for d in batch]\n            x_list, y_list = normalize_batch(x_list, y_list)\n            \n            token_lists = [[TOKEN_TO_ID.get(t, TOKEN_TO_ID['C']) for t in d['tokens']] for d in batch]\n            \n            max_len = max(len(s) for s in token_lists)\n            decoder_input = torch.full((len(batch), max_len + 1), SOS_ID, dtype=torch.long)\n            targets = torch.full((len(batch), max_len + 1), -1, dtype=torch.long)\n            \n            for j, seq in enumerate(token_lists):\n                decoder_input[j, 1:len(seq)+1] = torch.tensor(seq, dtype=torch.long)\n                targets[j, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n                \n            # Prepare tensors: x is (batch, points, vars), y is (batch, points, 1)\n            x_tensor = torch.tensor(np.stack(x_list), dtype=torch.float32).to(DEVICE)\n            y_tensor = torch.tensor(np.stack(y_list), dtype=torch.float32).to(DEVICE)\n            if y_tensor.dim() == 2:\n                y_tensor = y_tensor.unsqueeze(-1)\n            decoder_input = decoder_input.to(DEVICE)\n            targets = targets.to(DEVICE)\n            \n\n            \n            optimizer.zero_grad()\n            logits, _ = MODEL(x_tensor, y_tensor, decoder_input)\n            \n            # Apply curriculum mask to prevent learning tokens not yet introduced\n            logits = logits + (1 - allowed_mask.view(1, 1, -1)) * -1e4\n            \n            loss = ce_loss(logits.view(-1, VOCAB_SIZE + 1), targets.view(-1))\n            \n            if not (torch.isnan(loss) or torch.isinf(loss)):\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(MODEL.parameters(), 1.0)\n                optimizer.step()\n                scheduler.step()\n                losses.append(loss.item())\n                \n            if (i+1) % 100 == 0:\n                save_model()\n                \n        save_model()\n        MODEL.eval()\n        TRAINING_STATUS[\"running\"] = False\n        \n        fig = create_loss_plot(losses, \"Pre-Entrenamiento Supervisado\")\n        \n        result = f\"\"\"\n        <div style=\"background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); padding: 20px; border-radius: 15px; border: 2px solid #ffd93d;\">\n            <h2 style=\"color: #ffd93d; margin: 0;\">Escuela Primaria (Warmup) Completada</h2>\n            <p style=\"color: white;\">Iteraciones: {int(iterations)} | Loss Final: {losses[-1]:.4f}</p>\n            <p style=\"color: #888;\">El modelo ha aprendido sintaxis basica.</p>\n        </div>\n        \"\"\"\n        return result, fig\n        \n    except Exception as e:\n        TRAINING_STATUS[\"running\"] = False\n        return f\"Error: {str(e)}\", None\n\n\ndef train_hybrid_feedback_loop(iterations, problems_per_iter=10, gp_timeout=10, max_workers=4, progress=gr.Progress()):\n    \"\"\"\n    Teacher-Student Distillation Loop.\n    1. Find problems where model has high loss.\n    2. Use Hybrid Search (GP) to solve them.\n    3. Train model on GP solutions.\n    \"\"\"\n    global TRAINING_STATUS\n    \n    if TRAINING_STATUS[\"running\"]:\n        return \"Entrenamiento ya en progreso\", None\n    \n    TRAINING_STATUS[\"running\"] = True\n    reset_stop_flag()\n    \n    try:\n        MODEL, DEVICE = get_model()\n        \n        optimizer = torch.optim.AdamW(MODEL.parameters(), lr=5e-5, weight_decay=0.01)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n        \n        VOCAB_SIZE = len(VOCABULARY)\n        SOS_ID = VOCAB_SIZE\n        \n        # Replay buffer for \"Gold Standard\" examples found by GP\n        replay_buffer = deque(maxlen=5000)\n        quantile_loss_fn = QuantileLoss().to(DEVICE)\n        \n        # Randomize num_variables each iteration (see loop)\n        # data_gen will be created per-iteration\n        \n        losses = []\n        gp_successes = 0\n        gp_attempts = 0\n        \n        start_time = time.time()\n        \n        for iteration in range(int(iterations)):\n            if should_stop_training():\n                print(\"\u23f9\ufe0f Feedback Loop stopped\")\n                break\n                \n            elapsed = time.time() - start_time\n            # eta_str = f\"{(int(iterations)-iteration) * (elapsed/(iteration+1) if iteration>0 else 0):.0f}s\"\n            iter_dur = elapsed/(iteration+1) if iteration > 0 else 0\n            eta_seconds = (int(iterations)-iteration) * iter_dur\n            eta_str = f\"{eta_seconds:.0f}s\"\n\n            progress((iteration + 1) / iterations, \n                     desc=f\"Iter {iteration+1}/{int(iterations)} | GP Success: {gp_successes}/{gp_attempts} | Loss: {np.mean(losses[-10:]) if losses else 0:.3f}\")\n            \n            start_time_loop = time.time()\n            \n            # --- PHASE 1: HARD MINING ---\n            MODEL.eval()\n            \n            # Randomize number of variables for this iteration (1-10)\n            iter_num_vars = random.randint(1, 10)\n            data_gen = DataGenerator(max_depth=3, num_variables=iter_num_vars)\n            \n            # Generate candidates\n            pool_size = 50 \n            candidates = data_gen.generate_inverse_batch(pool_size, point_count=10)\n            \n            hard_problems = []\n            \n            # Skip if no valid candidates\n            if not candidates:\n                continue\n                \n            with torch.no_grad():\n                # We want to find problems with HIGH LOSS (model failure)\n                # Quick batch forward\n                x_list = [d['x'] for d in candidates]\n                y_list = [d['y'] for d in candidates]\n                x_list, y_list = normalize_batch(x_list, y_list)\n                \n                token_lists = [[TOKEN_TO_ID.get(t, TOKEN_TO_ID['C']) for t in d['tokens']] for d in candidates]\n                \n                # Filter out empty token lists\n                valid_indices = [i for i, tl in enumerate(token_lists) if len(tl) > 0]\n                if not valid_indices:\n                    continue\n                    \n                token_lists = [token_lists[i] for i in valid_indices]\n                candidates = [candidates[i] for i in valid_indices]\n                x_list = [x_list[i] for i in valid_indices]\n                y_list = [y_list[i] for i in valid_indices]\n                actual_pool_size = len(valid_indices)\n\n                # Sync candidates with normalized/filtered values\n                for k_sync in range(actual_pool_size):\n                    candidates[k_sync]['x'] = x_list[k_sync]\n                    candidates[k_sync]['y'] = y_list[k_sync]\n                \n                max_len = max(len(s) for s in token_lists)\n                \n                dec_in = torch.full((actual_pool_size, max_len + 1), SOS_ID, dtype=torch.long).to(DEVICE)\n                targets = torch.full((actual_pool_size, max_len + 1), -1, dtype=torch.long).to(DEVICE)\n                \n                for j, seq in enumerate(token_lists):\n                    dec_in[j, 1:len(seq)+1] = torch.tensor(seq, dtype=torch.long)\n                    targets[j, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n                \n                # Ensure uniform array shapes\n                try:\n                    x_tensor = torch.tensor(np.stack(x_list), dtype=torch.float32).to(DEVICE)\n                    y_tensor = torch.tensor(np.stack(y_list), dtype=torch.float32).to(DEVICE)\n                    if y_tensor.dim() == 2:\n                        y_tensor = y_tensor.unsqueeze(-1)\n                except Exception as e:\n                    print(f\"Skipping batch due to shape error: {e}\")\n                    continue\n                \n                try:\n                    logits, value_pred = MODEL(x_tensor, y_tensor, dec_in)\n                except Exception as e:\n                    print(f\"Skipping batch due to model error: {e}\")\n                    continue\n                \n                loss_f = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='none')\n                raw_losses = loss_f(logits.view(-1, VOCAB_SIZE + 1), targets.view(-1))\n                raw_losses = raw_losses.view(actual_pool_size, -1)\n                \n                mask = (targets != -1)\n                sample_losses = (raw_losses * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n                \n                # Filter: Keep if loss > 1.0 (arbitrary threshold for \"confused\")\n                for j, loss_val in enumerate(sample_losses):\n                    if loss_val.item() > 0.5: # Lower threshold to catch more\n                        hard_problems.append(candidates[j])\n            \n            # Take top K hardest\n            # Limit GP calls per iter to avoid slowness\n            problems_to_solve = hard_problems[:int(problems_per_iter)]\n            \n            if not problems_to_solve:\n                if (iteration + 1) % 5 == 0 or iteration == 0:\n                    print(f\"Iter {iteration}: Looking for hard problems (found 0 in pool of {actual_pool_size})...\")\n                continue\n\n            # --- PHASE 2: TEACHER SOLVES (GP) ---\n            print(f\"Iter {iteration}: Asking Teacher to solve {len(problems_to_solve)} hard problems...\")\n            \n            for prob in problems_to_solve:\n                gp_attempts += 1\n                try:\n                    # Calculate current stats\n                    current_prob = (gp_attempts % int(problems_per_iter)) + 1\n                    success_rate = (gp_successes / gp_attempts * 100) if gp_attempts > 0 else 0\n                    loss_display = f\"{losses[-1]:.4f}\" if losses else \"---\"\n                    \n                    # Construct Live HTML with glassmorphism design\n                    status_html = f\"\"\"\n                    <div style=\"background: linear-gradient(135deg, rgba(26,26,46,0.95) 0%, rgba(22,33,62,0.95) 100%); \n                                padding: 20px; border-radius: 16px; \n                                border: 1px solid rgba(74,222,128,0.3);\n                                box-shadow: 0 8px 32px rgba(0,0,0,0.3);\n                                backdrop-filter: blur(10px);\">\n                        \n                        <div style=\"display: flex; align-items: center; gap: 12px; margin-bottom: 16px;\">\n                            <span style=\"font-size: 28px;\">\ud83d\ude80</span>\n                            <div>\n                                <h3 style=\"color: #4ade80; margin: 0; font-size: 18px; font-weight: 600;\">Training Hybrid Loop</h3>\n                                <span style=\"color: #888; font-size: 12px;\">Teacher-Student Distillation</span>\n                            </div>\n                            <div style=\"margin-left: auto; background: rgba(74,222,128,0.2); padding: 4px 12px; border-radius: 20px;\">\n                                <span style=\"color: #4ade80; font-size: 14px; font-weight: 500;\">LIVE</span>\n                            </div>\n                        </div>\n                        \n                        <div style=\"display: grid; grid-template-columns: repeat(4, 1fr); gap: 12px; margin-bottom: 16px;\">\n                            <div style=\"background: rgba(255,255,255,0.05); padding: 12px; border-radius: 10px; text-align: center;\">\n                                <div style=\"color: #888; font-size: 11px; text-transform: uppercase;\">Iteraci\u00f3n</div>\n                                <div style=\"color: #fff; font-size: 20px; font-weight: 600;\">{iteration+1}<span style=\"color:#666; font-size:14px;\">/{iterations}</span></div>\n                            </div>\n                            <div style=\"background: rgba(255,255,255,0.05); padding: 12px; border-radius: 10px; text-align: center;\">\n                                <div style=\"color: #888; font-size: 11px; text-transform: uppercase;\">Problema</div>\n                                <div style=\"color: #fff; font-size: 20px; font-weight: 600;\">{current_prob}<span style=\"color:#666; font-size:14px;\">/{int(problems_per_iter)}</span></div>\n                            </div>\n                            <div style=\"background: rgba(255,255,255,0.05); padding: 12px; border-radius: 10px; text-align: center;\">\n                                <div style=\"color: #888; font-size: 11px; text-transform: uppercase;\">GP \u00c9xitos</div>\n                                <div style=\"color: #4ade80; font-size: 20px; font-weight: 600;\">{gp_successes}</div>\n                            </div>\n                            <div style=\"background: rgba(255,255,255,0.05); padding: 12px; border-radius: 10px; text-align: center;\">\n                                <div style=\"color: #888; font-size: 11px; text-transform: uppercase;\">Loss</div>\n                                <div style=\"color: #00d4ff; font-size: 20px; font-weight: 600;\">{loss_display}</div>\n                            </div>\n                        </div>\n                        \n                        <div style=\"display: flex; align-items: center; gap: 8px; padding: 10px; background: rgba(255,217,61,0.1); border-radius: 8px;\">\n                            <span style=\"font-size: 16px;\">\u23f3</span>\n                            <span style=\"color: #ffd93d; font-size: 14px;\">ETA: <strong>{locals().get('eta_str', 'Calculando...')}</strong></span>\n                            <div style=\"flex: 1; height: 4px; background: rgba(255,255,255,0.1); border-radius: 2px; margin-left: 12px;\">\n                                <div style=\"width: {((iteration * int(problems_per_iter) + gp_attempts) / (iterations * problems_per_iter) * 100):.0f}%; height: 100%; background: linear-gradient(90deg, #ffd93d, #4ade80); border-radius: 2px;\"></div>\n                            </div>\n                        </div>\n                        \n                        {locals().get('seeds_html', '')}\n                    </div>\n                    \"\"\"\n                    # Always create a graph (placeholder if empty)\n                    fig = create_loss_plot(losses, \"Training Loss\")\n                    yield status_html, fig\n                    \n                    # Run Hybrid Search (Quick Mode)\n                    # We pass the model so beam search can seed the GP\n                    res = None # Initialize to avoid UnboundLocalError\n                    res = hybrid_solve(\n                        prob['x'], \n                        prob['y'], \n                        MODEL, \n                        DEVICE, \n                        beam_width=10,     # Faster beam\n                        gp_timeout=gp_timeout,\n                        gp_binary_path=None,\n                        max_workers=max_workers,      # Parallel Workers (Mission 1)\n                        num_variables=iter_num_vars\n                    )\n                    \n                    # --- UI UPDATE: LIVE STATS ---\n                    elapsed_total = time.time() - start_time_loop\n                    full_loop_problems = iterations * problems_per_iter\n                    solved_problems_count = (iteration * int(problems_per_iter)) + gp_attempts\n                    if solved_problems_count > 0:\n                        avg_time = elapsed_total / solved_problems_count\n                        remaining = full_loop_problems - solved_problems_count\n                        eta_seconds = remaining * avg_time\n                        eta_str = f\"{int(eta_seconds // 60)}m {int(eta_seconds % 60)}s\"\n                    else:\n                        eta_str = \"Calculando...\"\n\n                    seeds_html = \"\"\n                    if res and 'seeds_tried' in res and res['seeds_tried']:\n                        seeds_html = \"<h4 style='color:#ccc; margin-bottom:5px;'>\ud83d\udd0e Top Seeds per Worker:</h4>\"\n                        seeds_html += \"<div style='display:flex; flex-wrap:wrap; gap:5px; font-size:12px; color:#888;'>\"\n                        for i, s in enumerate(res['seeds_tried']):\n                            seeds_html += f\"<span style='background:#222; padding:3px 6px; border-radius:4px;'>Worker {i+1}: {s[:30]}...</span>\"\n                        seeds_html += \"</div>\"\n                    \n                    gp_rmse = res.get('rmse', 1e6)\n                    if res and res.get('formula') and gp_rmse <= 0.01:\n                        # SUCCESS!\n                        gp_successes += 1\n                        \n                        # Parse formula to tokens\n                        try:\n                            # 1. Parse string to tree\n                            tree = ExpressionTree.from_infix(res['formula'])\n                            # 2. Get tokens\n                            tokens = tree.tokens\n                            \n                            # SCALED REWARD + Efficiency\n                            # 1. Quality Reward (0.2 to 1.0)\n                            quality_reward = max(0.2, 1.0 - (gp_rmse * 1.6))\n                            \n                            # 2. Efficiency Bonus (0.5 to 1.0)\n                            taken_time = res.get('time', 10.0)\n                            efficiency_bonus = 1.0\n                            if taken_time > 5.0:\n                                decay = ((taken_time - 5.0) / 25.0) * 0.5\n                                efficiency_bonus = max(0.5, 1.0 - decay)\n                            \n                            # Final Reward = Quality * Efficiency\n                            final_reward = quality_reward * efficiency_bonus\n\n                            replay_buffer.append({\n                                'x': prob['x'],\n                                'y': prob['y'],\n                                'tokens': tokens,\n                                'source': 'GP_Teacher',\n                                'reward': final_reward\n                            })\n\n                            # --- MISSION 2: PERSISTENCE ---\n                            try:\n                                log_file = os.path.join(\"results\", \"learned_formulas.csv\")\n                                file_exists = os.path.isfile(log_file)\n                                \n                                with open(log_file, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n                                    fieldnames = [\"timestamp\", \"formula\", \"rmse\", \"complexity\", \"source\", \"time_taken\"]\n                                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n                                    \n                                    if not file_exists:\n                                        writer.writeheader()\n                                        \n                                    writer.writerow({\n                                        \"timestamp\": datetime.datetime.now().isoformat(),\n                                        \"formula\": res['formula'],\n                                        \"rmse\": res.get('rmse', 0.0),\n                                        \"complexity\": len(tokens),\n                                        \"source\": \"GP_Teacher\",\n                                        \"time_taken\": res.get('time', 0.0)\n                                    })\n                            except Exception as e:\n                                print(f\"Failed to log formula to CSV: {e}\")\n                            # -------------------------------\n                            \n                        except Exception as e:\n                            print(f\"Failed to tokenize GP result: {e}\")\n                            \n                except Exception as e:\n                    print(f\"GP Hybrid Error: {e}\")\n                \n                # --- FALLBACK: If GP failed, use Original Ground Truth ---\n                # This ensures the model always learns something and the graph updates\n                found_gp_solution = (res and res.get('formula') and res.get('rmse', 1e6) <= 0.01)\n                \n                if not found_gp_solution:\n                    # Clean tokens\n                    original_tokens = [t for t in prob['tokens'] if t in TOKEN_TO_ID]\n                    if len(original_tokens) > 0:\n                        replay_buffer.append({\n                            'x': prob['x'],\n                            'y': prob['y'],\n                            'tokens': original_tokens,\n                            'source': 'Original',\n                            'reward': 1.0  # It is the ground truth\n                        })\n                    \n            # --- PHASE 3: STUDENT TRAINS (NN) ---\n            if len(replay_buffer) > 10:\n                MODEL.train()\n                # Train on batch from buffer\n                batch_size_train = min(len(replay_buffer), 64)\n                \n                # Multiple steps to enforce learning\n                steps = 5\n                \n                for _ in range(steps):\n                    batch = random.sample(list(replay_buffer), batch_size_train)\n                    \n                    x_list = [d['x'] for d in batch]\n                    y_list = [d['y'] for d in batch]\n                    x_list, y_list = normalize_batch(x_list, y_list)\n                    \n                    token_lists = [[TOKEN_TO_ID.get(t, TOKEN_TO_ID['C']) for t in d['tokens']] for d in batch]\n                    max_len = max(len(s) for s in token_lists)\n                    \n                    dec_in = torch.full((batch_size_train, max_len + 1), SOS_ID, dtype=torch.long).to(DEVICE)\n                    targets = torch.full((batch_size_train, max_len + 1), -1, dtype=torch.long).to(DEVICE)\n                    \n                    for j, seq in enumerate(token_lists):\n                        dec_in[j, 1:len(seq)+1] = torch.tensor(seq, dtype=torch.long)\n                        targets[j, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n                        \n                    # DYNAMIC PADDING FOR X (Mixed Dimensions)\n                    # Find max variables in this batch\n                    max_vars = max(x.shape[1] for x in x_list)\n                    points = x_list[0].shape[0]\n                    \n                    # Create padded array (Batch, Points, MaxVars)\n                    x_padded = np.zeros((batch_size_train, points, max_vars), dtype=np.float32)\n                    \n                    for j, x_item in enumerate(x_list):\n                        current_vars = x_item.shape[1]\n                        x_padded[j, :, :current_vars] = x_item\n                        \n                    x_t = torch.tensor(x_padded, dtype=torch.float32).to(DEVICE)\n                    y_t = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n                    dec_in = dec_in.to(DEVICE)\n                    targets = targets.to(DEVICE)\n                    \n                    optimizer.zero_grad()\n                    logits, value_pred = MODEL(x_t, y_t, dec_in)\n                    \n                    # Policy Loss only (Standard Supervised)\n                    # We trust the GP solution is \"Correct\" (Value=1.0)\n                    loss_ce = torch.nn.CrossEntropyLoss(ignore_index=-1)(logits.view(-1, VOCAB_SIZE+1), targets.view(-1))\n                    \n                    # Value Loss (Time-Aware Reward)\n                    # We extract the specific reward for each sample in the batch\n                    # Default to 1.0 (legacy data) if 'reward' is missing\n                    batch_rewards = [d.get('reward', 1.0) for d in batch]\n                    value_targets = torch.tensor(batch_rewards, dtype=torch.float32).to(DEVICE).unsqueeze(1)\n                    loss_val = quantile_loss_fn(value_pred, value_targets)\n                    \n                    loss = loss_ce + 0.1 * loss_val\n                    \n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(MODEL.parameters(), 1.0)\n                    optimizer.step()\n                    \n                    losses.append(loss.item())\n                    \n                scheduler.step(np.mean(losses[-10:]))\n                \n            if (iteration + 1) % 5 == 0:\n                save_model()\n                \n        save_model()\n        MODEL.eval()\n        TRAINING_STATUS[\"running\"] = False\n        \n        fig = create_loss_plot(losses, \"Feedback Loop Loss\")\n        \n        result_html = f\"\"\"\n        <div style=\"background: linear-gradient(135deg, #2c3e50 0%, #000000 100%); padding: 20px; border-radius: 15px; border: 2px solid #f1c40f;\">\n            <h2 style=\"color: #f1c40f; margin: 0;\">Feedback Loop Completado</h2>\n            <p style=\"color: white;\">Iteraciones: {iterations} | GP Success: {gp_successes}/{gp_attempts}</p>\n            <p style=\"color: #bbb;\">Nuevos Ejemplos Generados: {len(replay_buffer)}</p>\n        </div>\n        \"\"\"\n        \n        # Intermediate Yield for Live Updates\n        yield result_html, fig\n        return result_html, fig\n\n    except Exception as e:\n        TRAINING_STATUS[\"running\"] = False\n        import traceback\n        traceback.print_exc()\n        return f\"Error CRITICO: {str(e)}\", None\n\n\ndef train_from_memory(epochs=10, batch_size=32, num_variables=1, progress=gr.Progress()):\n    \"\"\"\n    Train from 'learned_formulas.csv' (Offline RL / Imitation Learning).\n    Re-trains the model on the \"Gold Standard\" discoveries.\n    \"\"\"\n    global TRAINING_STATUS\n    \n    if TRAINING_STATUS[\"running\"]:\n        return \"Entrenamiento ya en progreso\", None\n        \n    log_file = os.path.join(\"results\", \"learned_formulas.csv\")\n    if not os.path.exists(log_file):\n        return \"No se encontr\u00f3 el archivo 'learned_formulas.csv'. Ejecuta primero el Feedback Loop.\", None\n        \n    TRAINING_STATUS[\"running\"] = True\n    \n    try:\n        MODEL, DEVICE = get_model()\n        \n        # Load Data\n        import pandas as pd\n        df = pd.read_csv(log_file)\n        \n        if len(df) < 5:\n             TRAINING_STATUS[\"running\"] = False\n             return f\"Muy pocos datos para entrenar ({len(df)} ejemplos). Necesitas al menos 5.\", None\n             \n        progress(0.1, desc=f\"Cargando {len(df)} f\u00f3rmulas maestras...\")\n        \n        # Parse formulas to tokens\n        valid_data = []\n        for _, row in df.iterrows():\n            try:\n                formula = row['formula']\n                # Re-parse to get clean tokens\n                tree = ExpressionTree.from_infix(formula)\n                if tree.is_valid:\n                    # Generate fresh data points for this formula to train robustly\n                    # We generate dynamic X to prevent overfitting to specific points\n                    # But we can also use fixed points?\n                    # Better: Generate random X, evaluate Y.\n                    \n                    # Generate X (Multi-var support)\n                    # We don't know if formula is 1D or ND from CSV easily without checking vars\n                    # But we can just assume 10 features and let the formula pick what it needs?\n                    # Yes, ExpressionTree handles x0..x9.\n                    \n                    x_val = np.random.uniform(-5, 5, (10, 10)) # 10 points, 10 feats\n                    y_val = tree.evaluate(x_val)\n                    \n                    if np.any(np.isnan(y_val)) or np.any(np.isinf(y_val)) or np.std(y_val) < 1e-6:\n                        continue\n                        \n                    valid_data.append({\n                        'tokens': tree.tokens,\n                        'tree': tree # Store tree to generate fresh data each epoch? Or pre-gen?\n                    })\n            except:\n                continue\n        \n        if not valid_data:\n             TRAINING_STATUS[\"running\"] = False\n             return \"No se pudieron parsear f\u00f3rmulas v\u00e1lidas del CSV.\", None\n             \n        # Training Setup\n        optimizer = torch.optim.AdamW(MODEL.parameters(), lr=1e-4)\n        ce_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n        VOCAB_SIZE = len(VOCABULARY)\n        SOS_ID = VOCAB_SIZE\n        \n        losses = []\n        MODEL.train()\n        \n        for epoch in range(int(epochs)):\n            # Shuffle\n            random.shuffle(valid_data)\n            \n            # Create Batches\n            epoch_loss = 0\n            count = 0\n            \n            for i in range(0, len(valid_data), int(batch_size)):\n                batch = valid_data[i:i+int(batch_size)]\n                \n                # Generate fresh X/Y for this batch (Data Augmentation on the fly)\n                x_list = []\n                y_list = []\n                token_lists = []\n                \n                for item in batch:\n                    # Generate random points\n                    x = np.random.uniform(-3, 3, (20, 10)) # 20 points\n                    y = item['tree'].evaluate(x)\n                    \n                    # Sanity check\n                    if np.any(np.isnan(y)) or np.max(np.abs(y)) > 1e4:\n                        continue\n                        \n                    x_list.append(x)\n                    y_list.append(y)\n                    token_lists.append([TOKEN_TO_ID.get(t, TOKEN_TO_ID['C']) for t in item['tokens']])\n                \n                if not x_list: continue\n                \n                x_list, y_list = normalize_batch(x_list, y_list)\n                \n                # Tensors\n                max_len = max(len(s) for s in token_lists)\n                dec_in = torch.full((len(x_list), max_len + 1), SOS_ID, dtype=torch.long).to(DEVICE)\n                tgt = torch.full((len(x_list), max_len + 1), -1, dtype=torch.long).to(DEVICE)\n                \n                for j, seq in enumerate(token_lists):\n                    dec_in[j, 1:len(seq)+1] = torch.tensor(seq, dtype=torch.long)\n                    tgt[j, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n                    \n                x_t = torch.tensor(np.array(x_list), dtype=torch.float32).to(DEVICE)\n                y_t = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n                \n                optimizer.zero_grad()\n                logits, _ = MODEL(x_t, y_t, dec_in)\n                loss = ce_loss(logits.view(-1, VOCAB_SIZE + 1), tgt.view(-1))\n                \n                loss.backward()\n                optimizer.step()\n                \n                epoch_loss += loss.item()\n                count += 1\n            \n            avg_loss = epoch_loss / max(1, count)\n            losses.append(avg_loss)\n            \n            progress((epoch + 1) / epochs, desc=f\"Epoca {epoch+1}/{epochs} | Loss: {avg_loss:.4f}\")\n            \n        save_model()\n        MODEL.eval()\n        TRAINING_STATUS[\"running\"] = False\n        \n        fig = create_loss_plot(losses, \"Offline Memory Training\")\n        \n        return f\"\"\"\n        <div style=\"background: #1a1a2e; padding: 20px; border-radius: 10px; border: 2px solid #a855f7;\">\n            <h2 style=\"color: #a855f7;\">Entrenamiento de Memoria Completado</h2>\n            <p style=\"color:white;\">F\u00f3rmulas aprendidas: {len(valid_data)}</p>\n            <p style=\"color:white;\">Loss Final: {losses[-1]:.4f}</p>\n        </div>\n        \"\"\", fig\n        \n    except Exception as e:\n        TRAINING_STATUS[\"running\"] = False\n        import traceback\n        traceback.print_exc()\n        return f\"Error: {str(e)}\", None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/ui/app_benchmark.py\n",
        "import gradio as gr\nfrom utils.benchmark_comparison import run_comparison_benchmark\nfrom ui.app_core import get_model, DEVICE\n\ndef get_benchmark_tab():\n    with gr.Tab(\"\ud83e\udd47 Benchmark (IQ Test)\"):\n        gr.Markdown(\"### Evaluar Inteligencia del Modelo (Comparativa)\")\n        gr.Markdown(\"Ejecuta una bater\u00eda de **10 problemas est\u00e1ndar** comparando diferentes m\u00e9todos de b\u00fasqueda.\")\n        \n        with gr.Row():\n            methods_chk = gr.CheckboxGroup(\n                choices=[\"beam\", \"mcts\", \"hybrid\"], \n                value=[\"hybrid\"], \n                label=\"M\u00e9todos a Evaluar\",\n                info=\"Selecciona uno o m\u00e1s m\u00e9todos para comparar.\"\n            )\n            timeout_slider = gr.Slider(\n                minimum=5, \n                maximum=60, \n                value=30, \n                step=5, \n                label=\"Timeout GP (s)\", \n                info=\"Tiempo m\u00e1ximo para Beta-GP por problema.\"\n            )\n        \n        run_btn = gr.Button(\"\ud83d\ude80 Iniciar Benchmark Comparativo\", variant=\"primary\")\n        \n        progress_bar = gr.HTML(\"\")\n        \n        # Area de resultados\n        summary_html = gr.HTML(\"Resultados aparecer\u00e1n aqu\u00ed...\")\n        \n        results_df = gr.Dataframe(\n            headers=[\"Problema\", \"Nivel\", \"M\u00e9todo\", \"Formula\", \"RMSE\", \"Tiempo\", \"Estado\"],\n            label=\"Resultados Detallados\",\n            interactive=False\n        )\n        \n        def run_bench(selected_methods, gp_timeout, progress=gr.Progress()):\n            model_obj, device_obj = get_model()\n            if not model_obj:\n                return \"<div>\u26a0\ufe0f Error: Modelo no cargado. Ve a la pesta\u00f1a 'Config' y carga un modelo.</div>\", None, []\n            \n            if not selected_methods:\n                return \"<div>\u26a0\ufe0f Error: Selecciona al menos un m\u00e9todo.</div>\", None, []\n                \n            progress(0, desc=\"Iniciando Benchmark...\")\n            \n            # Run comparison\n            try:\n                result_data = run_comparison_benchmark(\n                    model_obj, \n                    device_obj, \n                    methods=selected_methods,\n                    gp_timeout=gp_timeout,\n                    beam_width=50,\n                    progress_callback=lambda p, desc: progress(p, desc=desc)\n                )\n            except Exception as e:\n                import traceback\n                traceback.print_exc()\n                return f\"<div>\u274c Error en Benchmark: {e}</div>\", None, []\n            \n            results = result_data['results']\n            summary_dict = result_data['summary']\n            \n            # Format dataframe\n            rows = []\n            for r in results:\n                status_icon = \"\u2705\" if r['success'] else \"\u274c\"\n                rmse_val = f\"{r['rmse']:.5f}\" if r['rmse'] < 1e6 else \"> 10^6\"\n                rows.append([\n                    r['problem_name'],\n                    r['level'],\n                    r['method'].upper(),\n                    r['formula'],\n                    rmse_val,\n                    f\"{r['time']:.2f}s\",\n                    status_icon\n                ])\n            \n            # Generate HTML Summary\n            html_content = \"<div style='display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;'>\"\n            \n            # Determine winner if multiple methods\n            winner_method = None\n            if len(selected_methods) > 1:\n                winner_method = max(summary_dict.items(), key=lambda x: (x[1]['solved'], -x[1]['avg_rmse']))[0]\n            \n            for method, stats in summary_dict.items():\n                is_winner = (method == winner_method)\n                border_color = \"#4CAF50\" if is_winner else (\"#FF9800\" if stats['score'] > 50 else \"#F44336\")\n                bg_color = \"#1e1e2f\"\n                if is_winner:\n                    bg_color = \"#1b3a24\" # Dark green tint for winner\n                    \n                trophy = \"\ud83c\udfc6 GANADOR\" if is_winner else \"\"\n                \n                html_content += f\"\"\"\n                <div style=\"background: {bg_color}; padding: 15px; border-radius: 10px; border: 2px solid {border_color}; min-width: 200px; text-align: center;\">\n                    <h2 style=\"color: {border_color}; margin: 0 0 10px 0;\">{method.upper()} {trophy}</h2>\n                    <div style=\"font-size: 24px; font-weight: bold; margin-bottom: 5px;\">{stats['solved']} / {stats['total']}</div>\n                    <div style=\"color: #ccc; font-size: 14px;\">Resueltos</div>\n                    <hr style=\"border-color: #444; margin: 10px 0;\">\n                    <div style=\"font-size: 14px;\">Nota: <b>{stats['score']:.1f}%</b></div>\n                    <div style=\"font-size: 14px;\">Tiempo Avg: <b>{stats['avg_time']:.2f}s</b></div>\n                </div>\n                \"\"\"\n            html_content += \"</div>\"\n            \n            return html_content, rows\n            \n        run_btn.click(run_bench, inputs=[methods_chk, timeout_slider], outputs=[summary_html, results_df])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/ui/__init__.py\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/utils/optimize_constants.py\n",
        "\"\"\"\nConstant Optimization Module for AlphaSymbolic.\nUses scipy.optimize to find optimal values for 'C' placeholders.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom core.grammar import ExpressionTree\n\ndef optimize_constants(tree, x_data, y_data, method='L-BFGS-B', initial_guess=None):\n    \"\"\"\n    Given an ExpressionTree with 'C' placeholders, find optimal constant values.\n    \n    Args:\n        tree: ExpressionTree object\n        x_data: numpy array of x values\n        y_data: numpy array of target y values\n        method: optimization method ('L-BFGS-B', 'SLSQP', 'Nelder-Mead')\n        \n    Returns:\n        dict: mapping of path tuples to optimized constant values\n        float: final RMSE\n    \"\"\"\n    if not tree.is_valid:\n        return {}, float('inf')\n    \n    # Get positions of all constants\n    positions = tree.root.get_constant_positions()\n    n_constants = len(positions)\n    \n    if n_constants == 0:\n        # No constants to optimize, just evaluate\n        y_pred = tree.evaluate(x_data)\n        mse = np.mean((y_pred - y_data)**2)\n        return {}, np.sqrt(mse)\n    \n    def objective(params):\n        \"\"\"Objective function: RMSE given constant values.\"\"\"\n        # Build constants dict\n        constants = {tuple(pos): params[i] for i, pos in enumerate(positions)}\n        \n        # Evaluate\n        y_pred = tree.evaluate(x_data, constants=constants)\n        \n        # Handle invalid predictions\n        if np.any(np.isnan(y_pred)) or np.any(np.isinf(y_pred)):\n            return 1e10\n        \n        if not np.all(np.isfinite(y_pred)):\n            return 1e9\n        \n        # Clip huge values to prevent overflow in MSE\n        y_pred = np.clip(y_pred, -1e9, 1e9)\n        \n        mse = np.mean((y_pred - y_data)**2)\n        return mse\n    \n    \n    # Check if initial_guess matches n_constants\n    if initial_guess is not None:\n        if len(initial_guess) != n_constants:\n             # Fallback if mismatch\n             x0 = np.ones(n_constants)\n        else:\n             x0 = np.array(initial_guess)\n    else:\n        # Initial guess: all 1s\n        x0 = np.ones(n_constants)\n    \n    # Bounds: reasonable range for constants\n    bounds = [(-1e9, 1e9)] * n_constants\n    \n    try:\n        result = minimize(\n            objective,\n            x0,\n            method=method,\n            bounds=bounds if method in ['L-BFGS-B', 'SLSQP'] else None,\n            options={'maxiter': 1000, 'disp': False}\n        )\n        \n        # Build final constants dict\n        optimized_constants = {tuple(pos): result.x[i] for i, pos in enumerate(positions)}\n        final_rmse = np.sqrt(result.fun) if result.fun > 0 else 0.0\n        \n        return optimized_constants, final_rmse\n        \n    except Exception as e:\n        return {}, float('inf')\n\ndef convert_and_extract_constants(node, values=None):\n    \"\"\"\n    Recursively converts numeric nodes to 'C' and extracts their values.\n    Returns: list of initial values.\n    \"\"\"\n    if values is None:\n        values = []\n        \n    # Check if node is a number (and not a special constant like pi/e)\n    try:\n        val = float(node.value)\n        # It is a number. Convert to C.\n        node.value = 'C'\n        values.append(val)\n    except:\n        pass\n        \n    for child in node.children:\n        convert_and_extract_constants(child, values)\n        \n    return values\n\n\ndef substitute_constants(infix_str, constants_dict, positions):\n    \"\"\"\n    Replace 'C' in the infix string with optimized values.\n    Simple approach: replace each C with optimized value.\n    \"\"\"\n    # For proper substitution, we'd need to track positions properly\n    # This is a simplified version that replaces all C with the first constant\n    result = infix_str\n    for i, pos in enumerate(positions):\n        if tuple(pos) in constants_dict:\n            val = constants_dict[tuple(pos)]\n            # Format nicely\n            if abs(val - round(val)) < 1e-6:\n                val_str = str(int(round(val)))\n            else:\n                val_str = f\"{val:.4f}\"\n            # Replace first occurrence of C\n            result = result.replace('C', val_str, 1)\n    return result\n\n\n# Quick test\nif __name__ == \"__main__\":\n    # Test: C * x + C should be optimized to fit y = 2*x + 3\n    x_test = np.array([1, 2, 3, 4, 5], dtype=np.float64)\n    y_test = 2 * x_test + 3  # y = 2x + 3\n    \n    tokens = ['+', '*', 'C', 'x', 'C']  # C*x + C\n    tree = ExpressionTree(tokens)\n    \n    print(f\"Formula structure: {tree.get_infix()}\")\n    print(f\"Target: y = 2x + 3\")\n    \n    constants, rmse = optimize_constants(tree, x_test, y_test)\n    print(f\"Optimized constants: {constants}\")\n    print(f\"Final RMSE: {rmse:.6f}\")\n    \n    # Verify\n    y_pred = tree.evaluate(x_test, constants=constants)\n    print(f\"Predictions: {y_pred}\")\n    print(f\"Targets: {y_test}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/utils/detect_pattern.py\n",
        "\"\"\"\nTarget Pattern Detection for AlphaSymbolic.\nAnalyzes target Y values to detect patterns (polynomial, exponential, periodic, etc.)\nand suggests initial search biases.\n\"\"\"\nimport numpy as np\nfrom scipy import stats\nfrom scipy.fft import fft\nfrom core.grammar import ExpressionTree\n\ndef detect_pattern(x_values, y_values):\n    \"\"\"\n    Analyze (x, y) data to detect patterns.\n    Returns a dict with pattern type probabilities and suggested operators.\n    \"\"\"\n    x = np.array(x_values, dtype=np.float64)\n    y = np.array(y_values, dtype=np.float64)\n    \n    results = {\n        'type': 'unknown',\n        'confidence': 0.0,\n        'suggested_ops': [],\n        'details': {}\n    }\n    \n    if len(x) < 3:\n        return results\n\n    # Handle Multivariable Input (Skip 1D pattern checks)\n    if x.ndim > 1 and x.shape[1] > 1:\n        results['type'] = 'multivariable'\n        results['confidence'] = 1.0\n        results['suggested_ops'] = ['+', '-', '*', 'x', 'C']\n        results['details']['multivariable'] = {'num_vars': x.shape[1]}\n        return results\n    \n    scores = {}\n    \n    # 1. Check for linear pattern (y = ax + b)\n    if len(x) >= 2:\n        slope, intercept, r_value, _, _ = stats.linregress(x, y)\n        scores['linear'] = r_value ** 2\n        results['details']['linear'] = {\n            'slope': slope,\n            'intercept': intercept,\n            'r_squared': r_value ** 2\n        }\n    \n    # 2. Check for quadratic pattern (y = ax^2 + bx + c)\n    if len(x) >= 3:\n        try:\n            coeffs = np.polyfit(x, y, 2)\n            y_pred = np.polyval(coeffs, x)\n            ss_res = np.sum((y - y_pred) ** 2)\n            ss_tot = np.sum((y - np.mean(y)) ** 2)\n            r2 = 1 - ss_res / ss_tot if ss_tot > 0 else 0\n            scores['quadratic'] = r2\n            results['details']['quadratic'] = {\n                'coefficients': coeffs.tolist(),\n                'r_squared': r2\n            }\n        except:\n            pass\n    \n    # 3. Check for exponential pattern (y = a * e^(bx))\n    if np.all(y > 0):  # Exponential only for positive y\n        try:\n            log_y = np.log(y)\n            slope, intercept, r_value, _, _ = stats.linregress(x, log_y)\n            scores['exponential'] = r_value ** 2\n            results['details']['exponential'] = {\n                'a': np.exp(intercept),\n                'b': slope,\n                'r_squared': r_value ** 2\n            }\n        except:\n            pass\n    \n    # 4. Check for periodic/sinusoidal pattern\n    if len(y) >= 4:\n        try:\n            # Simple FFT analysis\n            y_centered = y - np.mean(y)\n            fft_vals = np.abs(fft(y_centered))\n            \n            # Check if there's a dominant frequency\n            if len(fft_vals) > 1:\n                max_idx = np.argmax(fft_vals[1:len(fft_vals)//2]) + 1\n                max_power = fft_vals[max_idx]\n                total_power = np.sum(fft_vals[1:len(fft_vals)//2])\n                \n                if total_power > 0:\n                    periodicity = max_power / total_power\n                    scores['periodic'] = periodicity\n                    results['details']['periodic'] = {\n                        'dominant_freq_idx': int(max_idx),\n                        'periodicity_score': periodicity\n                    }\n        except:\n            pass\n    \n    # 5. Check for power law (y = a * x^b)\n    if np.all(x > 0) and np.all(y > 0):\n        try:\n            log_x = np.log(x)\n            log_y = np.log(y)\n            slope, intercept, r_value, _, _ = stats.linregress(log_x, log_y)\n            scores['power'] = r_value ** 2\n            results['details']['power'] = {\n                'a': np.exp(intercept),\n                'b': slope,\n                'r_squared': r_value ** 2\n            }\n        except:\n            pass\n    \n    # 6. Check for factorial/gamma pattern (for integer-like x)\n    if np.all(x > 0) and np.all(x == np.floor(x)):\n        try:\n            from scipy.special import gamma\n            x_int = x.astype(int)\n            y_gamma = gamma(x_int + 1)  # gamma(n+1) = n!\n            \n            # Simple linear fit between y and gamma\n            if not np.any(np.isinf(y_gamma)):\n                slope, intercept, r_value, _, _ = stats.linregress(y_gamma, y)\n                scores['factorial'] = r_value ** 2\n                results['details']['factorial'] = {\n                    'r_squared': r_value ** 2\n                }\n        except:\n            pass\n    \n    # Determine best pattern\n    if scores:\n        best_pattern = max(scores.items(), key=lambda x: x[1])\n        results['type'] = best_pattern[0]\n        results['confidence'] = best_pattern[1]\n        \n        # Suggest operators based on pattern\n        op_suggestions = {\n            'linear': ['+', '-', '*', 'x', 'C'],\n            'quadratic': ['pow', '+', '*', 'x', 'C', '2'],\n            'exponential': ['exp', '*', '+', 'x', 'C'],\n            'periodic': ['sin', 'cos', '*', '+', 'x', 'C'],\n            'power': ['pow', '*', 'x', 'C'],\n            'factorial': ['gamma', '*', '+', 'x', 'C']\n        }\n        results['suggested_ops'] = op_suggestions.get(best_pattern[0], [])\n    \n    return results\n\n\ndef summarize_pattern(result):\n    \"\"\"Pretty-print pattern detection result.\"\"\"\n    print(f\"\\n=== Pattern Detection ===\")\n    print(f\"Detected Type: {result['type']} (confidence: {result['confidence']:.2%})\")\n    print(f\"Suggested Operators: {', '.join(result['suggested_ops'])}\")\n    \n    if result['type'] in result['details']:\n        print(f\"Details: {result['details'][result['type']]}\")\n\n\nif __name__ == \"__main__\":\n    # Test with different patterns\n    \n    # Linear: y = 2x + 3\n    print(\"\\n--- Test: Linear ---\")\n    x1 = np.linspace(0, 10, 20)\n    y1 = 2 * x1 + 3 + np.random.normal(0, 0.1, 20)\n    result1 = detect_pattern(x1, y1)\n    summarize_pattern(result1)\n    \n    # Quadratic: y = x^2 + 1\n    print(\"\\n--- Test: Quadratic ---\")\n    x2 = np.linspace(-5, 5, 20)\n    y2 = x2**2 + 1\n    result2 = detect_pattern(x2, y2)\n    summarize_pattern(result2)\n    \n    # Exponential: y = 2 * e^(0.5x)\n    print(\"\\n--- Test: Exponential ---\")\n    x3 = np.linspace(0, 5, 20)\n    y3 = 2 * np.exp(0.5 * x3)\n    result3 = detect_pattern(x3, y3)\n    summarize_pattern(result3)\n    \n    # Periodic: y = sin(x)\n    print(\"\\n--- Test: Periodic ---\")\n    x4 = np.linspace(0, 4*np.pi, 50)\n    y4 = np.sin(x4)\n    result4 = detect_pattern(x4, y4)\n    summarize_pattern(result4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/utils/benchmark_runner.py\n",
        "import torch\nimport numpy as np\nimport time\nimport traceback\nfrom search.mcts import MCTS\nfrom data.benchmark_data import BENCHMARK_SUITE, get_benchmark_data\nfrom utils.optimize_constants import optimize_constants\n\ndef run_benchmark_suite(model, device, progress_callback=None):\n    \"\"\"\n    Runs the full benchmark suite.\n    Args:\n        model: Loaded AlphaSymbolic model\n        device: Torch device\n        progress_callback: Function(float, string) to update UI\n        \n    Returns:\n        results: List of result dicts\n        summary: Dict with aggregated stats\n    \"\"\"\n    results = []\n    \n    # Configure MCTS for benchmark (balanced speed/accuracy)\n    # 500 simulations is decent for benchmarking\n    mcts = MCTS(model, device, max_simulations=500, batch_size=32)\n    \n    total = len(BENCHMARK_SUITE)\n    solved_count = 0\n    \n    for i, problem in enumerate(BENCHMARK_SUITE):\n        if progress_callback:\n            progress_callback(i / total, f\"Testing: {problem['name']}...\")\n            \n        x, y, _ = get_benchmark_data(problem['id'])\n        \n        start_time = time.time()\n        \n        # Run Search\n        try:\n            search_result = mcts.search(x, y)\n             # Determine success\n            # Success threshold: RMSE < 0.01 (or 1% relative error)\n            rmse = search_result['rmse']\n            is_solved = rmse < 0.05 # Looser threshold for general regression\n            \n            # Special check for exact integer symbolic match? No, RMSE is ground truth.\n            \n            elapsed = time.time() - start_time\n            \n            if is_solved:\n                solved_count += 1\n                status = \"\u2705 SOLVED\"\n            else:\n                status = \"\u274c FAILED\"\n                \n            results.append({\n                'id': problem['id'],\n                'name': problem['name'],\n                'level': problem['level'],\n                'rmse': rmse,\n                'time': elapsed,\n                'status': status,\n                'found_formula': search_result.get('formula', '???'),\n                'is_solved': is_solved\n            })\n            \n        except Exception as e:\n            print(f\"Error in benchmark {problem['name']}:\")\n            traceback.print_exc()\n            results.append({\n                'id': problem['id'],\n                'name': problem['name'],\n                'level': problem['level'],\n                'rmse': 1e9,\n                'time': 0,\n                'status': \"\u26a0\ufe0f ERROR\",\n                'found_formula': \"Error\",\n                'is_solved': False\n            })\n\n    # Summary\n    if progress_callback:\n        progress_callback(1.0, \"Done!\")\n        \n    score = (solved_count / total) * 100\n    summary = {\n        'total': total,\n        'solved': solved_count,\n        'score': score,\n        'avg_time': np.mean([r['time'] for r in results]) if results else 0\n    }\n    \n    return results, summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/utils/benchmark_comparison.py\n",
        "\"\"\"\nComparative Benchmark: Beam Search vs MCTS vs Alpha-GP Hybrid\nRuns all three search methods on the standard benchmark suite and compares performance.\n\"\"\"\nimport torch\nimport numpy as np\nimport time\nimport traceback\nfrom typing import List, Dict, Callable, Optional\n\nfrom search.mcts import MCTS\nfrom search.beam_search import BeamSearch\nfrom search.hybrid_search import hybrid_solve\nfrom data.benchmark_data import BENCHMARK_SUITE, get_benchmark_data\nfrom core.grammar import ExpressionTree\nfrom utils.optimize_constants import optimize_constants\n\n\ndef run_single_problem(\n    x: np.ndarray, \n    y: np.ndarray, \n    method: str, \n    model, \n    device,\n    timeout_sec: int = 30,\n    beam_width: int = 50\n) -> Dict:\n    \"\"\"\n    Runs a single search method on a single problem.\n    \n    Returns:\n        dict with keys: formula, rmse, time, success\n    \"\"\"\n    start_time = time.time()\n    \n    try:\n        if method == \"beam\":\n            searcher = BeamSearch(model, device, beam_width=beam_width)\n            # BeamSearch expects list-like input and returns a list of results sorted by RMSE\n            results_list = searcher.search(x.tolist(), y.tolist())\n            elapsed = time.time() - start_time\n            if results_list and len(results_list) > 0:\n                result = results_list[0]  # Best result (sorted by RMSE)\n                return {\n                    'formula': result.get('formula', 'N/A'),\n                    'rmse': result.get('rmse', 1e9),\n                    'time': elapsed,\n                    'success': result.get('rmse', 1e9) < 0.05\n                }\n            else:\n                return {'formula': 'No Result', 'rmse': 1e9, 'time': elapsed, 'success': False}\n            \n        elif method == \"mcts\":\n            mcts = MCTS(model, device, max_simulations=500, batch_size=32)\n            # MCTS expects list-like input \n            result = mcts.search(x.tolist(), y.tolist())\n            elapsed = time.time() - start_time\n            return {\n                'formula': result.get('formula', 'N/A'),\n                'rmse': result.get('rmse', 1e9),\n                'time': elapsed,\n                'success': result.get('rmse', 1e9) < 0.05\n            }\n            \n        elif method == \"hybrid\":\n            result = hybrid_solve(\n                model=model,\n                device=device,\n                x_values=x.tolist(),\n                y_values=y.tolist(),\n                beam_width=beam_width,\n                gp_timeout=timeout_sec\n            )\n            elapsed = time.time() - start_time\n            \n            if result['formula']:\n                # Evaluate RMSE for hybrid result\n                try:\n                    tree = ExpressionTree.from_infix(result['formula'])\n                    if tree.is_valid:\n                        preds = tree.evaluate(x)\n                        rmse = np.sqrt(np.mean((preds - y) ** 2))\n                    else:\n                        rmse = 1e9\n                except:\n                    rmse = 1e9\n            else:\n                rmse = 1e9\n                \n            return {\n                'formula': result.get('formula', 'N/A') or 'Failed',\n                'rmse': rmse,\n                'time': elapsed,\n                'success': rmse < 0.05\n            }\n        else:\n            return {'formula': 'Unknown Method', 'rmse': 1e9, 'time': 0, 'success': False}\n            \n    except Exception as e:\n        print(f\"[ERROR] Method {method} failed: {e}\")\n        traceback.print_exc()\n        return {'formula': 'Error', 'rmse': 1e9, 'time': time.time() - start_time, 'success': False}\n\n\ndef run_comparison_benchmark(\n    model, \n    device, \n    methods: List[str] = [\"beam\", \"mcts\", \"hybrid\"],\n    gp_timeout: int = 30,\n    beam_width: int = 50,\n    progress_callback: Optional[Callable] = None\n) -> Dict:\n    \"\"\"\n    Runs all methods on all benchmark problems.\n    \n    Returns:\n        Dict with 'results' (per-problem-per-method) and 'summary' (aggregated stats)\n    \"\"\"\n    results = []\n    method_stats = {m: {'solved': 0, 'total_time': 0, 'total_rmse': 0} for m in methods}\n    \n    total_steps = len(BENCHMARK_SUITE) * len(methods)\n    current_step = 0\n    \n    for problem in BENCHMARK_SUITE:\n        x, y, _ = get_benchmark_data(problem['id'])\n        \n        for method in methods:\n            current_step += 1\n            \n            if progress_callback:\n                progress_callback(\n                    current_step / total_steps, \n                    f\"[{method.upper()}] {problem['name']}...\"\n                )\n            \n            result = run_single_problem(x, y, method, model, device, gp_timeout, beam_width)\n            \n            results.append({\n                'problem_id': problem['id'],\n                'problem_name': problem['name'],\n                'level': problem['level'],\n                'method': method,\n                'formula': result['formula'],\n                'rmse': result['rmse'],\n                'time': result['time'],\n                'success': result['success']\n            })\n            \n            # Update stats\n            method_stats[method]['total_time'] += result['time']\n            method_stats[method]['total_rmse'] += result['rmse'] if result['rmse'] < 1e6 else 0\n            if result['success']:\n                method_stats[method]['solved'] += 1\n    \n    # Compute summary\n    num_problems = len(BENCHMARK_SUITE)\n    summary = {}\n    for method in methods:\n        stats = method_stats[method]\n        summary[method] = {\n            'solved': stats['solved'],\n            'total': num_problems,\n            'score': (stats['solved'] / num_problems) * 100,\n            'avg_time': stats['total_time'] / num_problems,\n            'avg_rmse': stats['total_rmse'] / num_problems\n        }\n    \n    if progress_callback:\n        progress_callback(1.0, \"Benchmark Complete!\")\n    \n    return {'results': results, 'summary': summary}\n\n\ndef format_comparison_table(results: List[Dict]) -> str:\n    \"\"\"\n    Formats the results as a human-readable table.\n    \"\"\"\n    # Group by problem\n    problems = {}\n    for r in results:\n        pid = r['problem_id']\n        if pid not in problems:\n            problems[pid] = {'name': r['problem_name'], 'level': r['level'], 'methods': {}}\n        problems[pid]['methods'][r['method']] = {\n            'rmse': r['rmse'],\n            'time': r['time'],\n            'success': r['success'],\n            'formula': r['formula']\n        }\n    \n    output = []\n    output.append(\"=\" * 100)\n    output.append(f\"{'Problem':<25} | {'Method':<8} | {'RMSE':<12} | {'Time':<8} | {'Status':<10} | Formula\")\n    output.append(\"=\" * 100)\n    \n    for pid, pdata in problems.items():\n        name = pdata['name'][:24]\n        for method, mdata in pdata['methods'].items():\n            rmse_str = f\"{mdata['rmse']:.6f}\" if mdata['rmse'] < 1e6 else \"FAILED\"\n            time_str = f\"{mdata['time']:.2f}s\"\n            status = \"[OK]\" if mdata['success'] else \"[FAIL]\"\n            formula = mdata['formula'][:40] if mdata['formula'] else \"N/A\"\n            output.append(f\"{name:<25} | {method:<8} | {rmse_str:<12} | {time_str:<8} | {status:<10} | {formula}\")\n        output.append(\"-\" * 100)\n    \n    return \"\\n\".join(output)\n\n\ndef print_summary(summary: Dict):\n    \"\"\"\n    Prints a formatted summary comparison.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"BENCHMARK SUMMARY - Method Comparison\")\n    print(\"=\" * 60)\n    print(f\"{'Method':<12} | {'Solved':<10} | {'Score':<10} | {'Avg Time':<10} | {'Avg RMSE':<12}\")\n    print(\"-\" * 60)\n    \n    for method, stats in summary.items():\n        solved_str = f\"{stats['solved']}/{stats['total']}\"\n        score_str = f\"{stats['score']:.1f}%\"\n        time_str = f\"{stats['avg_time']:.2f}s\"\n        rmse_str = f\"{stats['avg_rmse']:.6f}\"\n        print(f\"{method.upper():<12} | {solved_str:<10} | {score_str:<10} | {time_str:<10} | {rmse_str:<12}\")\n    \n    print(\"=\" * 60)\n    \n    # Determine winner\n    best_method = max(summary.items(), key=lambda x: (x[1]['solved'], -x[1]['avg_rmse']))\n    print(f\"\\n*** WINNER: {best_method[0].upper()} with {best_method[1]['solved']}/{best_method[1]['total']} problems solved! ***\")\n\n\nif __name__ == \"__main__\":\n    # Standalone test\n    import sys\n    sys.path.insert(0, '.')\n    \n    from ui.app_core import load_model, get_model\n    \n    print(\"Loading model...\")\n    load_model()\n    model, device = get_model()\n    \n    if model is None:\n        print(\"Error: No model loaded!\")\n        exit(1)\n    \n    print(\"Running comparison benchmark...\")\n    result = run_comparison_benchmark(\n        model, \n        device, \n        methods=[\"beam\", \"mcts\", \"hybrid\"],\n        gp_timeout=30,\n        beam_width=50\n    )\n    \n    print(format_comparison_table(result['results']))\n    print_summary(result['summary'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/utils/simplify.py\n",
        "\"\"\"\nAlgebraic Simplification Module for AlphaSymbolic.\nUses SymPy for symbolic math simplification.\n\"\"\"\nimport sympy as sp\nfrom core.grammar import Node, ExpressionTree, OPERATORS\n\n# SymPy symbols\nx_sym = sp.Symbol('x')\n# Multi-variable support (x0 through x9)\nx_syms = {f'x{i}': sp.Symbol(f'x{i}') for i in range(10)}\n\ndef tree_to_sympy(node):\n    \"\"\"Convert an ExpressionTree Node to a SymPy expression.\"\"\"\n    if node is None:\n        return sp.Integer(0)\n    \n    val = node.value\n    \n    # Terminals\n    if val == 'x':\n        return x_sym\n    if val in x_syms:\n        return x_syms[val]\n    if val == 'pi':\n        return sp.pi\n    if val == 'e':\n        return sp.E\n    if val == 'C':\n        # Keep C as symbol for now\n        return sp.Symbol('C')\n    \n    # Try numeric\n    try:\n        return sp.Float(float(val))\n    except:\n        pass\n    \n    # Operators\n    args = [tree_to_sympy(c) for c in node.children]\n    \n    if val == '+': return args[0] + args[1]\n    if val == '-': return args[0] - args[1]\n    if val == '*': return args[0] * args[1]\n    if val == '/': return args[0] / args[1]\n    if val == 'pow': return sp.Pow(args[0], args[1])\n    if val == 'mod': return sp.Mod(args[0], args[1])\n    if val == 'sin': return sp.sin(args[0])\n    if val == 'cos': return sp.cos(args[0])\n    if val == 'tan': return sp.tan(args[0])\n    if val == 'exp': return sp.exp(args[0])\n    if val == 'log': return sp.log(args[0])\n    if val == 'sqrt': return sp.sqrt(args[0])\n    if val == 'abs': return sp.Abs(args[0])\n    if val == 'floor': return sp.floor(args[0])\n    if val == 'ceil': return sp.ceiling(args[0])\n    if val == 'gamma': return sp.gamma(args[0])\n    if val == 'lgamma': return sp.loggamma(args[0])  # SymPy's log-gamma\n    if val == 'neg': return -args[0]\n    \n    return sp.Integer(0)\n\ndef sympy_to_infix(expr):\n    \"\"\"Convert SymPy expression back to a readable string.\"\"\"\n    return str(expr)\n\ndef simplify_tree(tree):\n    \"\"\"\n    Takes an ExpressionTree and returns a simplified infix string.\n    \"\"\"\n    if not tree.is_valid:\n        return \"Invalid\"\n    \n    original_infix = tree.get_infix()\n    \n    try:\n        sympy_expr = tree_to_sympy(tree.root)\n        simplified = sp.simplify(sympy_expr)\n        result_str = str(simplified)\n        \n        # Validate: reject results containing invalid SymPy artifacts\n        # zoo = complex infinity, nan, oo = infinity\n        invalid_terms = ['zoo', 'nan', 'I*']  # I* indicates complex numbers\n        for term in invalid_terms:\n            if term in result_str:\n                return original_infix  # Fall back to original\n        \n        return result_str\n    except Exception as e:\n        # If simplification fails, return original\n        return original_infix\n\ndef simplify_infix(infix_str):\n    \"\"\"\n    Takes an infix string and returns a simplified version.\n    \"\"\"\n    try:\n        expr = sp.sympify(infix_str)\n        simplified = sp.simplify(expr)\n        return str(simplified)\n    except:\n        return infix_str\n\n# Quick test\nif __name__ == \"__main__\":\n    from core.grammar import ExpressionTree\n    \n    # Test: x + 0 should simplify to x\n    tokens = ['+', 'x', '0']\n    tree = ExpressionTree(tokens)\n    print(f\"Original: {tree.get_infix()}\")\n    print(f\"Simplified: {simplify_tree(tree)}\")\n    \n    # Test: x * 1 should simplify to x\n    tokens2 = ['*', 'x', '1']\n    tree2 = ExpressionTree(tokens2)\n    print(f\"Original: {tree2.get_infix()}\")\n    print(f\"Simplified: {simplify_tree(tree2)}\")\n    \n    # Test: x - x should simplify to 0\n    tokens3 = ['-', 'x', 'x']\n    tree3 = ExpressionTree(tokens3)\n    print(f\"Original: {tree3.get_infix()}\")\n    print(f\"Simplified: {simplify_tree(tree3)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/utils/__init__.py\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/tools/run_benchmark_feynman.py\n",
        "\nimport torch\nimport numpy as np\nimport pandas as pd\nimport time\nfrom tabulate import tabulate\nfrom ui.app_core import get_model, load_model, MODEL_PRESETS\nfrom search.hybrid_search import hybrid_solve\nfrom data.expanded_benchmarks import load_expanded_feynman_subset, evaluate_projected_formula\nfrom core.grammar import ExpressionTree\n\ndef evaluate_dynamic(problem, x_val):\n    \"\"\"Wrapper to evaluate dynamic problems with fixed contexts.\"\"\"\n    return evaluate_projected_formula(\n        problem['original_formula'], \n        problem['target_var'], \n        x_val, \n        problem['fixed_context']\n    )\n\ndef run_benchmark():\n    print(\"\\n\" + \"=\"*80)\n    print(\"\ud83d\udd2c ALPHA SYMBOLIC: EXPANDED FEYNMAN BENCHMARK (LITE vs PRO)\")\n    print(\"=\"*80 + \"\\n\")\n    \n    # LOAD DATASETS\n    print(\"Loading Feynman Dataset (FULL)...\")\n    problems = load_expanded_feynman_subset(limit=None) \n    if not problems:\n        print(\"No problems loaded. Check data/benchmarks/FeynmanEquations.csv\")\n        return\n\n    presets_to_test = ['lite', 'pro']\n    all_results = []\n    \n    summary_comparison = []\n\n    for preset in presets_to_test:\n        print(f\"\\n>>> LOADING MODEL: {preset.upper()} <<<\")\n        try:\n            status, info = load_model(preset_name=preset)\n            print(f\"Status: {status} | Device: {info}\")\n            model, device = get_model()\n        except Exception as e:\n            print(f\"Failed to load {preset}: {e}\")\n            continue\n\n        preset_results = []\n        \n        # Iterate Problems\n        for i, problem in enumerate(problems):\n            print(f\"\\n[{preset.upper()}] Problem {i+1}/{len(problems)}: {problem['name']}\")\n            print(f\"Target: {problem['original_formula']}\")\n            print(f\"Desc: {problem['description']}\")\n            \n            # Generate Data\n            x_test = np.linspace(0.1, 5.0, 20)\n            y_test = evaluate_dynamic(problem, x_test)\n            \n            if np.any(np.isnan(y_test)) or np.any(np.isinf(y_test)):\n                print(\"Skipping due to numerical issues.\")\n                continue\n                \n            # Solve\n            start_time = time.time()\n            try:\n                solution = hybrid_solve(\n                    x_test, \n                    y_test, \n                    model, \n                    device, \n                    beam_width=50,\n                    gp_timeout=10,\n                    max_workers=6\n                )\n                \n                elapsed = time.time() - start_time\n                \n                if solution:\n                    pred_formula = solution.get('formula', \"N/A\")\n                    \n                    # Verify RMSE\n                    try:\n                        # For RMSE check we treat prediction as function of x\n                        # The ground truth y_test is already correct\n                        pred_tree = ExpressionTree.from_infix(pred_formula)\n                        y_pred = pred_tree.evaluate(x_test)\n                        real_rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n                        is_solved = real_rmse < 0.05 # Relaxed slightly for complex physics\n                        status_text = \"\u2705 SOLVED\" if is_solved else \"\u274c FAILED\"\n                    except:\n                        real_rmse = 999.0\n                        status_text = \"\u26a0\ufe0f ERROR\"\n                    \n                    print(f\"Result: {status_text} | RMSE: {real_rmse:.4f} | Time: {elapsed:.2f}s\")\n                    \n                    all_results.append({\n                        \"Model\": preset,\n                        \"ID\": problem['id'],\n                        \"Name\": problem['name'],\n                        \"Target\": problem['original_formula'],\n                        \"Prediction\": pred_formula,\n                        \"RMSE\": real_rmse,\n                        \"Time\": elapsed,\n                        \"Status\": status_text\n                    })\n                    \n                    preset_results.append({\n                        \"ID\": problem['id'],\n                        \"Status\": status_text,\n                        \"Time\": elapsed\n                    })\n\n                else:\n                    print(\"Result: No solution found.\")\n                    all_results.append({\"Model\": preset, \"ID\": problem['id'], \"Name\": problem['name'], \"Status\": \"NO_SOLUTION\", \"RMSE\": 999.0, \"Time\": elapsed})\n                    preset_results.append({\"ID\": problem['id'], \"Status\": \"NO_SOLUTION\", \"Time\": elapsed})\n                    \n            except Exception as e:\n                print(f\"Error executing solve: {e}\")\n                all_results.append({\"Model\": preset, \"ID\": problem['id'], \"Name\": problem['name'], \"Status\": \"CRASH\", \"RMSE\": 999.0, \"Time\": 0.0})\n                preset_results.append({\"ID\": problem['id'], \"Status\": \"CRASH\", \"Time\": 0.0})\n\n        summary_comparison.append({\"Model\": preset, \"Results\": preset_results})\n\n    # Final Comparative Report\n    print(\"\\n\" + \"=\"*80)\n    print(\"\ud83c\udfc6 FINAL COMPARISON REPORT (EXPANDED)\")\n    print(\"=\"*80)\n    \n    # Pivot results for side-by-side view\n    comparison_rows = []\n    \n    lite_map = {r['ID']: r for r in summary_comparison[0]['Results']} if len(summary_comparison) > 0 else {}\n    pro_map = {r['ID']: r for r in summary_comparison[1]['Results']} if len(summary_comparison) > 1 else {}\n    \n    for problem in problems:\n        pid = problem['id']\n        name = problem['name']\n        \n        l_res = lite_map.get(pid, {\"Status\": \"N/A\", \"Time\": 0.0})\n        p_res = pro_map.get(pid, {\"Status\": \"N/A\", \"Time\": 0.0})\n        \n        comparison_rows.append({\n            \"ID\": pid,\n            \"LITE Status\": l_res['Status'],\n            \"LITE Time\": f\"{l_res['Time']:.2f}s\",\n            \"PRO Status\": p_res['Status'],\n            \"PRO Time\": f\"{p_res['Time']:.2f}s\"\n        })\n        \n    df_compare = pd.DataFrame(comparison_rows)\n    print(tabulate(df_compare, headers=\"keys\", tablefmt=\"grid\", showindex=False))\n    \n    # Save CSV\n    pd.DataFrame(all_results).to_csv(\"feynman_expanded_results.csv\", index=False)\n    print(\"\\nDetailed results saved to 'feynman_expanded_results.csv'\")\n\nif __name__ == \"__main__\":\n    run_benchmark()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/run_gpu_console.py\n",
        "\nimport sys\nimport os\nimport torch\nimport numpy as np\nimport time\nfrom core.gpu import TensorGeneticEngine\n\n# Configuration matching C++ Globals\n# --- CONFIGURATION ---\nTARGETS = np.array([\n    1, 0, 0, 2, 10, 4, 40, 92, 352, 724, 2680, 14200, \n    73712, 365596, 2279184, 14772512, 95815104, 666090624, \n    4968057848, 39029188884, 314666222712, 2691008701644, \n    2423393768440, 227514171973736, 2207893435808352\n], dtype=np.float64)\n\n# Generate X_VALUES procedurally to match the pattern:\n# x0 = 1..25\n# x1 = x0 % 6\n# x2 = x0 % 2\nindices = np.arange(1, 26, dtype=np.float64)\nx1_vals = indices % 6\nx2_vals = indices % 2\nX_VALUES = np.column_stack((indices, x1_vals, x2_vals))\n\ndef console_mimic_callback(gen, best_rmse, best_rpn_tensor, best_consts_tensor, is_new_best, island_idx=-1):\n    \"\"\"\n    Mimics EXACTLY the C++ console output.\n    \"\"\"\n    \n    # 1. Decode Formula\n    # We need access to engine instance to decode? \n    # The callback doesn't have 'self'. We can assume external engine variable or pass it.\n    # But RPN decoding is simple if we have the method.\n    # We will use the global 'engine' instance defined below.\n    \n    formula_str = engine.rpn_to_infix(best_rpn_tensor, best_consts_tensor)\n    formula_size = engine.get_tree_size(best_rpn_tensor) \n\n    if is_new_best:\n        print(f\"\\n========================================\")\n        print(f\"New Global Best Found (Gen {gen}, Island {island_idx})\")\n        print(f\"Fitness: {best_rmse:.8f}\")\n        print(f\"Size: {formula_size}\")\n        print(f\"Formula: {formula_str}\")\n        print(\"Predictions vs Targets:\")\n        \n        # Show Predictions (Top 5 rows only to avoid spam? C++ showed all X_values)\n        # C++ showed all. Let's show all if small, or top 10.\n        # Recalculate predictions\n        try:\n            # We need to run evaluate on the best formula for single points\n            # Or just use the batch evaluator on CPU for display?\n            # Actually engine has 'rpn_to_infix', we can use ExpressionTree to eval?\n            # Or engine.evaluate_batch?\n            # engine.evaluate_batch expects a population.\n            # Let's use ExpressionTree for clean single-point eval if possible.\n            from core.grammar import ExpressionTree\n            tree = ExpressionTree.from_infix(formula_str)\n            \n            # Determine display targets\n            display_targets = TARGETS\n            if GpuGlobals.USE_LOG_TRANSFORMATION:\n                 # Parity with engine filtering if needed, but here we just trans for display\n                 # However, to be safe and match engine, we filter too\n                 mask = TARGETS > 1e-9\n                 display_targets = np.log(np.where(mask, TARGETS, 1.0)) # Safe log for display\n            \n            for i in range(len(X_VALUES)):\n                val = tree.evaluate(X_VALUES[i])\n                \n                # Ensure val is scalar\n                if isinstance(val, np.ndarray):\n                    val = val.item() if val.size == 1 else val[0]\n\n                target = display_targets[i] if i < len(display_targets) else float('nan')\n                diff = abs(val - target)\n                \n                # Format: x=(...): Pred=..., Target=..., Diff=... \n                # (Same as C++)\n                x_str = \",\".join([f\"{x:.1f}\" for x in X_VALUES[i]])\n                \n                print(f\"  x=({x_str}): Pred={val:12.4f}, Target={target:12.4f}, Diff={diff:12.4f}\")\n        except Exception as e:\n            print(f\"  (Error calculating detailed predictions for display: {e})\")\n        \n        print(\"========================================\")\n        sys.stdout.flush()\n        \n    else:\n        # Progress Report\n        if not hasattr(console_mimic_callback, \"last_time\"):\n             console_mimic_callback.last_time = start_time_global\n             console_mimic_callback.last_gen = 0\n        \n        current_time = time.time()\n        delta_t = current_time - console_mimic_callback.last_time\n        delta_g = gen - console_mimic_callback.last_gen\n        \n        instant_speed = (delta_g * engine.pop_size) / delta_t if delta_t > 0 else 0.0\n        \n        console_mimic_callback.last_time = current_time\n        console_mimic_callback.last_gen = gen\n\n        elapsed = time.time() - start_time_global\n        print(f\"\\n--- Gen {gen} (Elapsed: {elapsed:.2f}s) | Instant Speed: {instant_speed:,.0f} Evals/sec ---\")\n        print(f\"Overall Best Fitness: {best_rmse:.4e}\")\n        print(f\"Best Formula Size: {formula_size}\")\n        sys.stdout.flush()\n\nif __name__ == \"__main__\":\n    print(\"Starting Genetic Algorithm (GPU Mode)...\")\n    \n    if torch.cuda.is_available():\n        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    else:\n        print(\"WARNING: GPU NOT DETECTED. Running in CPU emulation mode (Slow).\")\n        \n\n    from core.gpu.config import GpuGlobals\n    \n    # User can override Globals here\n    GpuGlobals.POP_SIZE = 25000\n    GpuGlobals.NUM_ISLANDS = 20\n    GpuGlobals.PROGRESS_REPORT_INTERVAL = 100\n    GpuGlobals.USE_PARETO_SELECTION = False  # Disable NSGA-II for speed test\n    \n    # Engine will use Globals defaults for pop_size and n_islands\n    engine = TensorGeneticEngine(num_variables=3) # 3 variables as per new X_VALUES\n    \n    start_time_global = time.time()\n    \n    try:\n        # Run Infinite Loop (until Ctrl+C or solved)\n        # Timeout set to very high (1 hour)\n        print(\"Evaluating initial population...\")\n        \n        # SLICE INPUTS TO MATCH TARGETS (17)\n        # And ensure correct shape for num_variables=1\n        if engine.num_variables == 1:\n            x_input = X_VALUES[:len(TARGETS), 0]\n        else:\n            x_input = X_VALUES[:len(TARGETS)]\n        \n        seeds = []\n        if GpuGlobals.USE_INITIAL_FORMULA and GpuGlobals.INITIAL_FORMULA_STRING:\n            seeds.append(GpuGlobals.INITIAL_FORMULA_STRING)\n            print(f\"Info: Injecting initial formula: {GpuGlobals.INITIAL_FORMULA_STRING}\")\n\n        final_formula = engine.run(\n            x_input, \n            TARGETS, \n            seeds=seeds, \n            timeout_sec=3600, \n            callback=console_mimic_callback\n        )\n\n        \n        print(\"\\nSearch Finished.\")\n        if final_formula:\n            print(f\"Final Result: {final_formula}\")\n            \n    except KeyboardInterrupt:\n        print(\"\\nSearch interrupted by user.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/infinite_search.py\n",
        "import time\nimport numpy as np\nimport torch\nimport pandas as pd\nimport os\nimport random\nimport sys\n\n# Add project root to path\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\n\nfrom search.hybrid_search import hybrid_solve\nfrom ui.app_core import get_model\nfrom core.grammar import ExpressionTree\nfrom utils.optimize_constants import optimize_constants, substitute_constants, convert_and_extract_constants\n\n# --- CONFIGURATION ---\nX_FULL = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25], dtype=np.float64)\nY_FULL = np.array([1,0,0,2,10,4,40,92,352,724,2680,14200,73712,365596,2279184,14772512,95815104,666090624,4968057848,39029188884,314666222712,2691008701644,2423393768440,227514171973736,2207893435808352], dtype=np.float64)\n\n# Targets for Extrapolation\nX_TARGETS = np.array([26, 27], dtype=np.float64)\nY_TARGETS = np.array([22317699616364044, 234907967154122528], dtype=np.float64)\n\nCSV_FILE = \"top_formulas.csv\"\nPATTERN_FILE = \"pattern_memory.json\"\nTOP_K = 5\nMIN_SAMPLE_SIZE = 6 # > 5\n\nimport json\n\n# --- PATTERN MEMORY (\"La Biblioteca\") ---\ndef extract_structural_skeleton(formula_str):\n    \"\"\"\n    Parses formula and replaces all numeric constants with 'C'.\n    Returns the structural skeleton (infix).\n    \"\"\"\n    try:\n        from core.grammar import ExpressionTree, Node\n        tree = ExpressionTree.from_infix(formula_str)\n        if not tree.is_valid: return None\n        \n        def transform(node):\n            if not node: return\n            # If leaf is number, make it C\n            # How to detect number? \n            # In ExpressionTree, numbers are just values.\n            # Check if value is numeric string\n            try:\n                float(node.value)\n                node.value = 'C'\n            except:\n                pass # Operator or Variable\n            \n            for child in node.children:\n                transform(child)\n                \n        transform(tree.root)\n        return tree.root.to_infix()\n    except:\n        return None\n\ndef load_pattern_memory():\n    if os.path.exists(PATTERN_FILE):\n        try:\n            with open(PATTERN_FILE, 'r') as f:\n                return json.load(f)\n        except: return {}\n    return {}\n\ndef save_pattern_memory(memory):\n    try:\n        with open(PATTERN_FILE, 'w') as f:\n            json.dump(memory, f, indent=2)\n    except: pass\n\ndef update_pattern_memory(memory, formula_str):\n    skeleton = extract_structural_skeleton(formula_str)\n    if skeleton:\n        count = memory.get(skeleton, 0)\n        memory[skeleton] = count + 1\n        return True\n    return False\n\n\ndef load_or_create_top_list():\n    if os.path.exists(CSV_FILE):\n        try:\n            df = pd.read_csv(CSV_FILE)\n            return df.to_dict('records')\n        except Exception as e:\n            print(f\"Error loading CSV: {e}\")\n            return []\n    return []\n\ndef save_top_list(top_list):\n    df = pd.DataFrame(top_list)\n    # Sort by RMSLE (Global Fit) - Match main loop priority\n    # Use a safe sort that handles missing values if any\n    if 'rmsle_global' in df.columns:\n        df = df.sort_values(by='rmsle_global', ascending=True)\n    else:\n        df = df.sort_values(by='extrapolation_error', ascending=True)\n    df.to_csv(CSV_FILE, index=False)\n    print(f\"Saved Top {len(df)} to {CSV_FILE}\")\n    \n    # Auto-backup if in Colab\n    backup_to_drive()\n\ndef backup_to_drive():\n    \"\"\"\n    If running in Google Colab, copies the top list and pattern memory to Google Drive.\n    \"\"\"\n    try:\n        import shutil\n        if os.path.exists('/content/drive/MyDrive'):\n            drive_path = '/content/drive/MyDrive/AlphaSymbolic_Models'\n            os.makedirs(drive_path, exist_ok=True)\n            \n            # Files to backup\n            files = [CSV_FILE, PATTERN_FILE]\n            for f in files:\n                if os.path.exists(f):\n                    shutil.copy(f, os.path.join(drive_path, f))\n            # print(\"  [Backup] Synced to Google Drive.\")\n    except Exception as e:\n        # Silently fail if drive not mounted or other issues\n        pass\n\ndef main():\n    print(\"--- Infinite Formula Search Script ---\")\n    \n    # Check dependencies\n    # Pandas is required\n\n\n    # Load Model\n    print(\"Loading Model...\")\n    try:\n        MODEL, DEVICE = get_model()\n    except Exception as e:\n        print(f\"Failed to load model: {e}\")\n        return\n        \n    top_formulas = load_or_create_top_list()\n    pattern_memory = load_pattern_memory()\n    print(f\"Loaded Pattern Memory with {len(pattern_memory)} patterns.\")\n\n    \n    print(f\"Starting infinite search loop... (Press Ctrl+C to stop)\")\n    iteration = 0\n    \n    while True:\n        iteration += 1\n        \n        # 1. Random Sampling\n        # Ensure we pick at least MIN_SAMPLE_SIZE points\n        # USER REQUEST: Start from n > 4 (Indices >= 4, since X starts at 1)\n        valid_indices = np.arange(4, len(X_FULL)) # 4, 5, ... end\n        \n        # Adjust k if valid pool is small\n        pool_size = len(valid_indices)\n        k = random.randint(min(MIN_SAMPLE_SIZE, pool_size), pool_size)\n        \n        indices = np.sort(np.random.choice(valid_indices, k, replace=False))\n        \n        x_sample = X_FULL[indices]\n        y_sample = Y_FULL[indices]\n        \n        print(f\"[Iter {iteration}] Sampling {k} pts...\", end=\" \")\n        \n        # Prepare Seeds (Evolutionary Feedback)\n        # Prepare Seeds (Evolutionary Feedback)\n        extra_seeds = []\n        if top_formulas:\n            # User request: \"pick 3 samples from top 5\" (Updated from top 3)\n            candidates = top_formulas[:5]\n            candidate_formulas = [c['formula'] for c in candidates]\n            \n            if candidate_formulas:\n                # Sample 3 seeds with replacement \n                # (allows giving more compute to the very best if picked twice)\n                chosen = random.choices(candidate_formulas, k=3)\n                extra_seeds.extend(chosen)\n                print(f\"+ {len(chosen)} Seeds\")\n        \n        # Inject Patterns from Memory (The Library)\n        if pattern_memory:\n            # Pick top 3 most frequent patterns\n            # Sort by count desc\n            sorted_patterns = sorted(pattern_memory.items(), key=lambda x: x[1], reverse=True)\n            top_patterns = [p[0] for p in sorted_patterns[:3]]\n            if top_patterns:\n                extra_seeds.extend(top_patterns)\n                # print(f\"+ {len(top_patterns)} Architectures\")\n\n\n        # 2. Search\n        # 1.5 Flattening Transformation (The \"Feynman\" Trick)\n        # y_flat = log(y) - lgamma(x + 1)\n        # We use log1p for safety near 0, although y is usually large integerrs.\n        # But wait, user said \"log(target)\". \n        # Since we reconstruct with exp, we must be consistent.\n        # Shift y slightly to avoid log(0) if any y=0 exists (indices 1,2 are 0).\n        # We'll use a small epsilon.\n        epsilon = 1e-9\n        # y_sample indices corresponds to x_sample values.\n        # x_sample are values like 1, 2, ...\n        \n        # Calculate lgamma(n+1) which is log(n!)\n        from scipy.special import gammaln\n        factorial_term = gammaln(x_sample + 1)\n        \n        # Transform target\n        # Use abs(y) just in case, though they are positive counts usually.\n        y_sample_flat = np.log(np.abs(y_sample) + epsilon) - factorial_term\n        \n        # print first few to debug (in stdout)\n        if iteration == 1:\n            pass # print(f\"Flat Y: {y_sample_flat[:3]}\")\n\n        # 2. Search (on FLATTENED target)\n        try:\n            # We use a relatively small beam width for speed, relying on many iterations\n            result = hybrid_solve(\n                x_sample, y_sample_flat,  # PASS FLAT Y\n                MODEL, DEVICE, \n                beam_width=10, \n                gp_timeout=120, \n                max_workers=4, # Use 4 parallel workers (C++ Engine)\n                num_variables=1,\n                extra_seeds=extra_seeds\n            )\n        except Exception as e:\n            print(f\"Search failed: {e}\")\n            continue\n            \n        if not result or not result.get('formula'):\n            print(\"No formula found in this iteration.\")\n            continue\n            \n        residual_formula_str = result['formula']\n        # print(f\"Found residual candidate: {residual_formula_str}\")\n        \n        # 2.5 INTELLIGENT REFINEMENT (BFGS) on Residual\n        final_formula_str = residual_formula_str # Default if refinement fails\n        \n        try:\n            # Parse residual tree\n            tree = ExpressionTree.from_infix(residual_formula_str)\n            if tree.is_valid:\n                # 1. Convert hardcoded numbers to 'C' and get initial values\n                initial_values = convert_and_extract_constants(tree.root)\n                \n                # Refine on ALL data (1-27) but FLATTENED\n                x_all = np.concatenate((X_FULL, X_TARGETS))\n                y_all = np.concatenate((Y_FULL, Y_TARGETS))\n                \n                factorial_term_all = gammaln(x_all + 1)\n                y_all_flat = np.log(np.abs(y_all) + epsilon) - factorial_term_all\n                \n                if initial_values:\n                    # print(f\"Refining {len(initial_values)} constants on FLAT surface...\")\n                    \n                    # optimization expects C-tree\n                    constants_dict, rmse = optimize_constants(tree, x_all, y_all_flat, initial_guess=initial_values)\n                    \n                    if constants_dict:\n                         # 3. Substitute back into residual\n                         positions = tree.root.get_constant_positions()\n                         infix_with_Cs = tree.get_infix() \n                         refined_residual = substitute_constants(infix_with_Cs, constants_dict, positions)\n                         \n                         # print(f\"Refined Residual: {refined_residual}\")\n                         residual_formula_str = refined_residual\n                         \n        except Exception as e:\n            print(f\"Refinement failed: {e}\") \n        \n        # 3. RECONSTRUCTION & Transformation\n        # Formula = exp( Residual + lgamma(x+1) )\n        # We construct this string.\n        # Note: lgamma(x+1) is 'lgamma(x+1)' in our language (or similar).\n        # Our language has 'lgamma'. Input is 'x'.\n        # So we string concat: \"exp(\" + residual + \" + lgamma(x + 1))\"\n        \n        # We need to be careful about parens.\n        # FIX: lgamma in ExpressionTree adds +1 internally (lgamma(|x|+1)).\n        # So we use lgamma(x) to represent lgamma(x+1) mathematically.\n        full_formula_str = f\"exp({residual_formula_str} + lgamma(x0))\"\n        # print(f\"Reconstructed Full Formula: {full_formula_str}\")\n\n        # 4. Evaluate on FULL RANGE (History + Targets) w/ Reconstructed Formula\n        try:\n            tree = ExpressionTree.from_infix(full_formula_str)\n            if not tree.is_valid:\n                print(\"Invalid reconstructed tree.\")\n                # Fallback to evaluate residual directly? No, that's wrong scale.\n                continue\n            \n            # Combine all points for validation\n            x_all = np.concatenate((X_FULL, X_TARGETS))\n            y_all = np.concatenate((Y_FULL, Y_TARGETS))\n            \n            y_pred_all = tree.evaluate(x_all)\n            \n            # Calculate RMSLE on ORIGINAL SPACE\n            y_pred_safe = np.maximum(y_pred_all, 0) # Clip negative predictions\n            \n            # Validating log error\n            # Handle potential overflow in y_pred if it's huge?\n            # if y_pred is inf, log is inf.\n            \n            log_error = np.sqrt(np.mean((np.log1p(y_pred_safe) - np.log1p(y_all))**2))\n            \n            # Also calculate Extrapolation Absolute Error\n            pred_26 = y_pred_all[-2]\n            pred_27 = y_pred_all[-1]\n            extrap_error_sum = abs(pred_26 - Y_TARGETS[0]) + abs(pred_27 - Y_TARGETS[1])\n            \n            time_taken = result.get('time', 0)\n            print(f\"\\n[SUCCESS] Formula: {full_formula_str}\\n          RMSLE (1-27): {log_error:.6f} | Extrap Error: {extrap_error_sum:.2e} | Time: {time_taken:.2f}s\")\n            \n            # 5. Update Top List\n            entry = {\n                'formula': full_formula_str, # Store the FULL formula\n                'residual': residual_formula_str, # Store residual for curiosity\n                'rmsle_global': log_error, \n                'extrapolation_error': extrap_error_sum, \n                'pred_26': pred_26,\n                'pred_27': pred_27,\n                'sample_size': k,\n                'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\")\n            }\n            \n            # Add to list\n            top_formulas.append(entry)\n            \n            # Deduplicate by formula\n            unique_formulas = {d['formula']: d for d in top_formulas}\n            top_formulas = list(unique_formulas.values())\n            \n            # Sort by RMSLE\n            top_formulas.sort(key=lambda x: x.get('rmsle_global', float('inf')))\n            \n            # Keep Top 5\n            if len(top_formulas) > TOP_K:\n                top_formulas = top_formulas[:TOP_K]\n            \n            # Save\n            save_top_list(top_formulas)\n            \n            # Update Pattern Memory with the new winner\n            if update_pattern_memory(pattern_memory, full_formula_str):\n                 save_pattern_memory(pattern_memory)\n                 backup_to_drive() # Also backup when pattern memory changes\n\n            \n            current_best = top_formulas[0].get('rmsle_global', 999)\n            print(f\"Current Best RMSLE: {current_best:.6f}\")\n            \n        except Exception as e:\n            print(f\"Evaluation failed: {e}\")\n            continue\n\nif __name__ == \"__main__\":\n    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/generate_report.py\n",
        "import pandas as pd\nimport numpy as np\nimport sys\nimport os\n\n# Add project root to path\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\n\nfrom core.grammar import ExpressionTree\n\n# --- DATA ---\nX_FULL = np.arange(1, 28) # 1 to 27\nY_REAL = np.array([\n    1, 0, 0, 2, 10, 4, 40, 92, 352, 724, 2680, 14200, 73712, 365596, 2279184, \n    14772512, 95815104, 666090624, 4968057848, 39029188884, 314666222712, \n    2691008701644, 2423393768440, 227514171973736, 2207893435808352,\n    22317699616364044, 234907967154122528\n], dtype=np.float64)\n\nINPUT_CSV = \"top_formulas.csv\"\nOUTPUT_CSV = \"top_5_detailed_report.csv\"\n\ndef evaluate_formula(formula_str, x_vals):\n    try:\n        tree = ExpressionTree.from_infix(formula_str)\n        if not tree.is_valid:\n            print(f\"Invalid Formula: {formula_str}\")\n            return np.zeros_like(x_vals)\n        return tree.evaluate(x_vals) # Should handle array input\n    except Exception as e:\n        print(f\"Error evaluating {formula_str}: {e}\")\n        return np.zeros_like(x_vals)\n\ndef main():\n    print(f\"Generating report from {INPUT_CSV}...\")\n    \n    if not os.path.exists(INPUT_CSV):\n        print(\"Input file not found.\")\n        return\n\n    # Load Top 5\n    df_in = pd.read_csv(INPUT_CSV)\n    top_5 = df_in.head(5).copy()\n    \n    # Prepare List of Rows for new DataFrame\n    report_rows = []\n    \n    for idx, row in top_5.iterrows():\n        formula = row['formula']\n        print(f\"Processing #{idx+1}: {formula[:30]}...\")\n        \n        y_pred = evaluate_formula(formula, X_FULL)\n        \n        # Build Row Dictionary\n        new_row = {'Rank': idx + 1, 'Formula': formula}\n        \n        total_mape = 0\n        count = 0\n        \n        for i, x in enumerate(X_FULL):\n            real = Y_REAL[i]\n            pred = y_pred[i]\n            \n            # Handle division by zero if real is 0 (indexes 1 and 2 are 0)\n            if real == 0:\n                # If real is 0, error is absolute diff? Or undefined %?\n                # Usually we define error strictly. If pred is 0, error is 0%.\n                # Let's show abs diff for 0, or skip % calc.\n                # User asked for \"error porcentual\".\n                # If Real=0, Pred=0 -> 0%\n                # If Real=0, Pred=0.1 -> Infinite %.\n                # Let's put NaN or a placeholder for 0 values, or just show raw delta.\n                # Indices 1, 2 (i=1, i=2 since 0-indexed) are 0.\n                if abs(pred) < 1e-9:\n                    err_pct = 0.0\n                else:\n                    err_pct = np.nan # Undefined\n            else:\n                err_pct = abs((pred - real) / real) * 100\n                total_mape += err_pct\n                count += 1\n            \n            # Add columns\n            # new_row[f'X_{x}_Real'] = real\n            new_row[f'X_{x}_Pred'] = pred\n            new_row[f'X_{x}_Err%'] = err_pct\n        \n        new_row['Mean_Err%'] = total_mape / count if count > 0 else 0\n        report_rows.append(new_row)\n        \n    # Create DF\n    # To keep it organized, let's order columns: Rank, Formula, Mean_Err%, then X_1_Pred, X_1_Err%, etc.\n    df_out = pd.DataFrame(report_rows)\n    \n    # Calculate Mean Error Percentage across all points (excluding 0s)\n    \n    cols = ['Rank', 'Formula', 'Mean_Err%']\n    for x in X_FULL:\n        cols.append(f'X_{x}_Pred')\n        cols.append(f'X_{x}_Err%')\n        \n    df_out = df_out[cols]\n    \n    df_out.to_csv(OUTPUT_CSV, index=False)\n    print(f\"Report saved to {OUTPUT_CSV}\")\n\nif __name__ == \"__main__\":\n    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/app.py\n",
        "\"\"\"\nAlphaSymbolic - Gradio Web Interface\nWith GPU/CPU toggle and search method selection.\n\"\"\"\nimport gradio as gr\nimport torch\n\nfrom ui.app_core import load_model, get_device, get_device_info, set_device, get_training_errors, request_stop_training\nfrom ui.app_training import train_basic, train_curriculum, train_self_play, train_supervised, train_hybrid_feedback_loop, train_from_memory\nfrom ui.app_search import solve_formula, generate_example\nfrom ui.app_benchmark import get_benchmark_tab\nfrom ui.theme import get_theme, CUSTOM_CSS\nimport pandas as pd\nimport io\n\n\ndef toggle_device(use_gpu):\n    \"\"\"Toggle between GPU and CPU.\"\"\"\n    device_info = set_device(use_gpu)\n    color = \"#4ade80\" if \"CUDA\" in device_info else \"#fbbf24\" if \"MPS\" in device_info else \"#888\"\n    return f'<div style=\"padding: 10px; background: #0f0f23; border-radius: 8px; border-left: 3px solid {color};\"><span style=\"color: {color}; font-weight: bold;\">{device_info}</span></div>'\n\n\ndef create_app():\n    \"\"\"Create the Gradio app.\"\"\"\n    \n    custom_theme = get_theme()\n    \n    with gr.Blocks(title=\"AlphaSymbolic\") as demo:\n        \n        # Header\n        device_info = get_device_info()\n        device_color = \"#4ade80\" if \"CUDA\" in device_info else \"#fbbf24\" if \"MPS\" in device_info else \"#888\"\n        \n\n        def load_csv_data(file_obj):\n            \"\"\"Load CSV file to X/Y inputs.\"\"\"\n            if file_obj is None:\n                return None, None\n            \n            try:\n                # auto-detect separator\n                try:\n                    df = pd.read_csv(file_obj.name, sep=None, engine='python')\n                except:\n                    df = pd.read_csv(file_obj.name)\n                \n                if df.shape[1] < 2:\n                    return None, \"Error: El archivo debe tener al menos 2 columnas (X..., Y)\"\n                \n                # Assume last column is Y, rest are X\n                X = df.iloc[:, :-1].values\n                y = df.iloc[:, -1].values\n                \n                # Format X string\n                # If 1D: \"1, 2, 3\"\n                # If 2D: \"1 2; 3 4\"\n                if X.shape[1] == 1:\n                    x_str = \", \".join(map(str, X.flatten()))\n                else:\n                    # Multi-line format\n                    lines = [\" \".join(map(str, row)) for row in X]\n                    x_str = \"\\n\".join(lines)\n                \n                y_str = \", \".join(map(str, y.flatten()))\n                \n                return x_str, y_str\n            except Exception as e:\n                return None, f\"Error leyendo CSV: {str(e)}\"\n\n        # Header\n        device_info = get_device_info()\n        device_color = \"#22d3ee\" if \"CUDA\" in device_info else \"#fbbf24\"\n        gpu_short = device_info.replace('NVIDIA GeForce ', '').replace(' Laptop GPU', '').replace('CUDA (', '').replace(')', '')\n        \n        gr.HTML(f\"\"\"\n        <div style=\"display: flex; justify-content: space-between; align-items: center; padding: 20px 30px; background: linear-gradient(135deg, rgba(15, 23, 42, 0.9), rgba(30, 41, 59, 0.9)); border-radius: 16px; margin-bottom: 15px; border: 1px solid rgba(6, 182, 212, 0.2); box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);\">\n            <div>\n                <h1 style=\"margin: 0; font-size: 2.5rem; font-weight: 800; background: linear-gradient(135deg, #06b6d4, #8b5cf6); -webkit-background-clip: text; -webkit-text-fill-color: transparent; font-family: 'Orbitron', sans-serif; letter-spacing: 2px;\">\n                    \u03b1Symbolic\n                </h1>\n                <p style=\"margin: 5px 0 0 0; color: #64748b; font-size: 0.9rem;\">\n                    Deep Reinforcement Learning & Symbolic Regression\n                </p>\n            </div>\n            <div style=\"display: flex; align-items: center; gap: 15px;\">\n                <div style=\"text-align: right;\">\n                    <div style=\"background: {'rgba(34, 197, 94, 0.15)' if 'CUDA' in device_info else 'rgba(251, 191, 36, 0.15)'}; color: {'#22c55e' if 'CUDA' in device_info else '#fbbf24'}; padding: 8px 16px; border-radius: 25px; font-weight: 600; font-size: 0.85rem; border: 1px solid {'rgba(34, 197, 94, 0.3)' if 'CUDA' in device_info else 'rgba(251, 191, 36, 0.3)'};\">\n                        {'\u26a1 GPU' if 'CUDA' in device_info else '\ud83d\udcbb CPU'} | {gpu_short}\n                    </div>\n                </div>\n            </div>\n        </div>\n        \"\"\")\n        \n        # Model Selector - Compact inline\n        with gr.Row():\n            with gr.Column(scale=1):\n                model_selector = gr.Radio(choices=[\"lite\", \"pro\"], value=\"lite\", label=\"Modelo\", container=False)\n            with gr.Column(scale=4):\n                model_status = gr.HTML(value='<div style=\"padding: 8px 15px; background: rgba(34, 197, 94, 0.1); border-radius: 8px; color: #22c55e; font-size: 0.85rem; border: 1px solid rgba(34, 197, 94, 0.2);\">\u2713 Lite Model (Optimized) - Vocabulary 2.0</div>')\n        \n        def on_model_change(preset):\n            status, _ = load_model(preset_name=preset)\n            return f'<div style=\"padding: 8px 15px; background: rgba(34, 197, 94, 0.1); border-radius: 8px; color: #22c55e; font-size: 0.85rem; border: 1px solid rgba(34, 197, 94, 0.2);\">\u2713 {status}</div>'\n\n        model_selector.change(on_model_change, model_selector, model_status)\n        \n        with gr.Tabs():\n            # TAB 1: Search\n            with gr.Tab(\"\ud83d\udd0d Buscar Formula\"):\n                with gr.Row():\n                    # Column 1: Inputs + Config\n                    with gr.Column(scale=1, min_width=400):\n                        gr.Markdown(\"## Entrada\")\n                        x_input = gr.Textbox(label=\"Features (X)\", placeholder=\"1, 2, 3...\", lines=3)\n                        y_input = gr.Textbox(label=\"Target (Y)\", placeholder=\"2, 4, 6...\", lines=3)\n                        \n                        with gr.Accordion(\"\ud83d\udcc1 Cargar desde CSV\", open=False):\n                            file_upload = gr.File(label=\"Seleccionar archivo\", file_types=[\".csv\", \".txt\"], file_count=\"single\")\n                            file_upload.change(load_csv_data, inputs=[file_upload], outputs=[x_input, y_input])\n                        \n                        with gr.Row():\n                            gr.Button(\"Lineal\", size=\"sm\").click(lambda: generate_example(\"lineal\"), outputs=[x_input, y_input])\n                            gr.Button(\"Cuad\", size=\"sm\").click(lambda: generate_example(\"cuadratico\"), outputs=[x_input, y_input])\n                            gr.Button(\"Trig\", size=\"sm\").click(lambda: generate_example(\"trig\"), outputs=[x_input, y_input])\n                            gr.Button(\"Exp\", size=\"sm\").click(lambda: generate_example(\"exp\"), outputs=[x_input, y_input])\n                        \n                        gr.Markdown(\"---\")\n                        search_method = gr.Radio(\n                            choices=[\"Beam Search\", \"MCTS\", \"Alpha-GP Hybrid\"],\n                            value=\"Alpha-GP Hybrid\",\n                            label=\"Algoritmo de B\u00fasqueda\"\n                        )\n                        beam_slider = gr.Slider(5, 500, value=50, step=5, label=\"Intensidad (Beam Width)\")\n                        workers_slider = gr.Slider(1, 16, value=6, step=1, label=\"Workers (Paralelismo)\", info=\"Procesos para el motor GP\")\n                        solve_btn = gr.Button(\"\ud83d\ude80 BUSCAR F\u00d3RMULA\", variant=\"primary\", size=\"lg\", elem_classes=\"primary-btn\")\n                        \n                        with gr.Accordion(\"Tabla de Predicciones\", open=False):\n                            pred_html = gr.HTML(label=\"Predicciones\")\n                    \n                    # Column 2: Results + Visualization\n                    with gr.Column(scale=1, min_width=400):\n                        gr.Markdown(\"## Resultados\")\n                        result_html = gr.HTML(label=\"F\u00f3rmula Encontrada\")\n                        plot_output = gr.Plot(label=\"Visualizaci\u00f3n del Ajuste\")\n                        alt_html = gr.HTML(label=\"Alternativas\")\n                \n                raw_formula = gr.Textbox(visible=False)\n                solve_btn.click(solve_formula, [x_input, y_input, beam_slider, search_method, workers_slider], \n                               [result_html, plot_output, pred_html, alt_html, raw_formula])\n            \n            # TAB 2: Training\n            with gr.Tab(\"Entrenar Modelo\"):\n                # Training Control Panel - Compact Header\n                gr.HTML(f\"\"\"\n                <div style=\"display: flex; justify-content: space-between; align-items: center; padding: 15px 20px; background: linear-gradient(135deg, rgba(6, 182, 212, 0.1), rgba(139, 92, 246, 0.1)); border-radius: 12px; margin-bottom: 10px; border: 1px solid rgba(6, 182, 212, 0.3);\">\n                    <div style=\"display: flex; align-items: center; gap: 15px;\">\n                        <span style=\"font-size: 1.5rem;\"> </span>\n                        <div>\n                            <h3 style=\"margin: 0; color: #e2e8f0; font-size: 1.1rem;\">Centro de Entrenamiento</h3>\n                            <span style=\"color: #64748b; font-size: 0.8rem;\">Gestiona el aprendizaje del modelo</span>\n                        </div>\n                    </div>\n                    <div style=\"display: flex; align-items: center; gap: 20px;\">\n                        <div style=\"text-align: center;\">\n                            <span style=\"background: rgba(34, 197, 94, 0.2); color: #22c55e; padding: 4px 12px; border-radius: 20px; font-size: 0.75rem; font-weight: 600;\">\n                                {'\ud83d\udfe2 GPU' if torch.cuda.is_available() else '\ud83d\udfe1 CPU'}\n                            </span>\n                            <div style=\"color: {device_color}; font-size: 0.7rem; margin-top: 4px;\">{device_info.replace('NVIDIA GeForce ', '').replace(' Laptop GPU', '')}</div>\n                        </div>\n                    </div>\n                </div>\n                \"\"\")\n                \n                with gr.Row():\n                    use_gpu = gr.Checkbox(label=\"Usar GPU\", value=torch.cuda.is_available(), visible=False)\n                    device_display = gr.HTML(visible=False)\n                    use_gpu.change(toggle_device, [use_gpu], [device_display])\n                    stop_train_btn = gr.Button(\"Detener Todo\", variant=\"stop\", size=\"sm\", scale=1)\n                    delete_model_btn = gr.Button(\"Reset Pesos\", variant=\"secondary\", size=\"sm\", scale=1)\n                \n                stop_status = gr.HTML(visible=False)\n                delete_status = gr.HTML(visible=False)\n                \n                def delete_model_action():\n                    import os\n                    from ui.app_core import CURRENT_PRESET\n                    filename = f\"alpha_symbolic_model_{CURRENT_PRESET}.pth\"\n                    if os.path.exists(filename):\n                        os.remove(filename)\n                        return \"Modelo Eliminado\"\n                    return \"No existe modelo\"\n                \n                # Global Training Config\n                with gr.Row():\n                    reset_state_btn = gr.Button(\"\u26a0\ufe0f Reset Estado\", variant=\"secondary\", size=\"sm\")\n\n                def reset_training_state():\n                    from ui.app_training import TRAINING_STATUS\n                    TRAINING_STATUS[\"running\"] = False\n                    return \"Estado reseteado. Intenta entrenar de nuevo.\"\n\n                reset_state_btn.click(reset_training_state, outputs=[stop_status])\n                delete_model_btn.click(delete_model_action, outputs=[delete_status])\n                stop_train_btn.click(request_stop_training, outputs=[stop_status])\n                \n                with gr.Tabs():\n                    # Basic\n                    with gr.Tab(\"Basico\"):\n                        gr.HTML('<p style=\"color: #888;\">Entrenamiento rapido con datos sinteticos</p>')\n                        with gr.Row():\n                            with gr.Column():\n                                epochs_basic = gr.Slider(10, 500, value=100, step=10, label=\"Epocas\")\n                                batch_basic = gr.Slider(16, 128, value=32, step=16, label=\"Batch Size\")\n                                points_basic = gr.Slider(10, 100, value=20, step=10, label=\"Puntos por Formula\")\n                                train_basic_btn = gr.Button(\"Entrenar Basico\", variant=\"primary\")\n                            with gr.Column():\n                                result_basic = gr.HTML()\n                                plot_basic = gr.Plot()\n                        train_basic_btn.click(train_basic, [epochs_basic, batch_basic, points_basic], [result_basic, plot_basic])\n                    \n                    # Curriculum\n                    with gr.Tab(\"Curriculum\"):\n                        gr.HTML('''\n                        <div style=\"background: #0f0f23; padding: 15px; border-radius: 8px; margin-bottom: 15px;\">\n                            <p style=\"color: #00d4ff; margin: 0;\"><strong>Curriculum Learning</strong></p>\n                            <p style=\"color: #888; margin: 5px 0 0 0;\">Empieza con formulas simples y aumenta la dificultad.</p>\n                        </div>\n                        ''')\n                        with gr.Row():\n                            with gr.Column():\n                                epochs_curriculum = gr.Slider(50, 2000, value=200, step=50, label=\"Epocas\")\n                                batch_curriculum = gr.Slider(16, 128, value=64, step=16, label=\"Batch Size\")\n                                points_curriculum = gr.Slider(10, 100, value=20, step=10, label=\"Puntos por Formula\")\n                                train_curriculum_btn = gr.Button(\"Entrenar Curriculum\", variant=\"primary\")\n                            with gr.Column():\n                                result_curriculum = gr.HTML()\n                                plot_curriculum = gr.Plot()\n                        train_curriculum_btn.click(train_curriculum, [epochs_curriculum, batch_curriculum, points_curriculum], [result_curriculum, plot_curriculum])\n                    \n                    # Self-Play\n                    with gr.Tab(\"Self-Play\"):\n                        gr.HTML('''\n                        <div style=\"background: #0f0f23; padding: 15px; border-radius: 8px; margin-bottom: 15px; border-left: 3px solid #ff6b6b;\">\n                            <p style=\"color: #ff6b6b; margin: 0;\"><strong>AlphaZero Self-Play</strong></p>\n                            <p style=\"color: #888; margin: 5px 0 0 0;\">El modelo resuelve problemas y aprende de sus exitos.</p>\n                        </div>\n                        ''')\n                        with gr.Row():\n                            with gr.Column():\n                                iterations_sp = gr.Slider(10, 1000, value=100, step=10, label=\"Iteraciones\")\n                                problems_sp = gr.Slider(5, 200, value=10, step=5, label=\"Problemas/Iter\")\n                                points_sp = gr.Slider(10, 100, value=20, step=10, label=\"Puntos por Formula\")\n                                train_sp_btn = gr.Button(\"Iniciar Self-Play\", variant=\"primary\", elem_classes=\"primary-btn\")\n                            with gr.Column():\n                                result_sp = gr.HTML()\n                                plot_sp = gr.Plot()\n                        train_sp_btn.click(train_self_play, [iterations_sp, problems_sp, points_sp], [result_sp, plot_sp])\n                \n                    # Feedback Loop (Teacher-Student)\n                    with gr.Tab(\"Feedback Loop (Hybrid)\"):\n                        gr.HTML('''\n                        <div style=\"background: #0f0f23; padding: 15px; border-radius: 8px; margin-bottom: 15px; border-left: 3px solid #f1c40f;\">\n                            <p style=\"color: #f1c40f; margin: 0;\"><strong>Teacher-Student Feedback Loop</strong></p>\n                            <p style=\"color: #888; margin: 5px 0 0 0;\">El modelo (Estudiante) intenta resolver problemas. Si falla, el Alpha-GP (Maestro) interviene y a\u00f1ade la soluci\u00f3n al dataset.</p>\n                        </div>\n                        ''')\n                        with gr.Row():\n                            with gr.Column():\n                                iterations_fb = gr.Slider(5, 500, value=20, step=5, label=\"Ciclos\")\n                                problems_fb = gr.Slider(5, 50, value=10, step=5, label=\"Problemas Dif\u00edciles / Ciclo\")\n                                timeout_fb = gr.Slider(5, 30, value=10, step=5, label=\"Timeout Maestro (s)\")\n                                train_fb_btn = gr.Button(\"Iniciar Feedback Loop\", variant=\"primary\")\n                            with gr.Column():\n                                result_fb = gr.HTML()\n                                plot_fb = gr.Plot()\n                        train_fb_btn.click(train_hybrid_feedback_loop, [iterations_fb, problems_fb, timeout_fb, workers_slider], [result_fb, plot_fb])\n                \n                # --- PRE-TRAINING (Warmup) ---\n                with gr.Accordion(\"\ud83c\udf93 Escuela Primaria (Pre-Entrenamiento)\", open=False):\n                    gr.Markdown(\"Entrenamiento masivo supervisado de alta velocidad para aprender sintaxis basica. **Recomendado al inicio.**\")\n                    with gr.Row():\n                        with gr.Column():\n                            epochs_pre = gr.Slider(100, 10000, value=2000, step=100, label=\"Iteraciones R\u00e1pidas\")\n                            train_pre_btn = gr.Button(\"Iniciar Pre-Entrenamiento\", variant=\"primary\", elem_classes=\"primary-btn\")\n                        with gr.Column():\n                            result_pre = gr.HTML()\n                            plot_pre = gr.Plot()\n                    train_pre_btn.click(train_supervised, [epochs_pre], [result_pre, plot_pre])\n                \n                # --- MEMORY TRAINING (Offline RL) ---\n                with gr.Accordion(\"\ud83e\udde0 Entrenamiento de Memoria (Offline)\", open=False):\n                    gr.Markdown(\"Re-entrena el modelo usando las f\u00f3rmulas descubiertas y guardadas en `learned_formulas.csv`. Ideal para consolidar conocimientos.\")\n                    with gr.Row():\n                        with gr.Column():\n                            epochs_mem = gr.Slider(10, 500, value=50, step=10, label=\"Epocas de Repaso\")\n                            train_mem_btn = gr.Button(\"Iniciar Entrenamiento de Memoria\", variant=\"primary\")\n                        with gr.Column():\n                            result_mem = gr.HTML()\n                            plot_mem = gr.Plot()\n                    train_mem_btn.click(train_from_memory, [epochs_mem], [result_mem, plot_mem])\n\n                # --- HALL OF SHAME (Error Analysis) ---\n                with gr.Accordion(\"Hall of Shame (Analisis de Errores)\", open=False):\n                    gr.Markdown(\"Aqu\u00ed se muestran los problemas donde el modelo fall\u00f3 dr\u00e1sticamente hoy.\")\n                    error_table = gr.DataFrame(\n                        headers=[\"Time\", \"Target Formula\", \"Predicted\", \"Loss\", \"Stage\"],\n                        datatype=[\"str\", \"str\", \"str\", \"number\", \"str\"],\n                        interactive=False\n                    )\n                    refresh_errors_btn = gr.Button(\"Actualizar Errores\", size=\"sm\")\n                    \n                    def update_errors():\n                        errors = get_training_errors()\n                        # Reverse to show newest first\n                        data = [[\n                            e['time'], e['target'], e['predicted'], round(e['loss'], 2), e['stage']\n                        ] for e in reversed(errors)]\n                        return data\n                    \n                    refresh_errors_btn.click(update_errors, outputs=[error_table])\n            \n            # TAB 4: Benchmark\n            get_benchmark_tab()\n\n            # TAB 5: Info\n            with gr.Tab(\"Informacion\"):\n                device_info_current = get_device_info()\n                device_color_current = \"#4ade80\" if \"CUDA\" in device_info_current else \"#fbbf24\" if \"MPS\" in device_info_current else \"#888\"\n                \n                gr.HTML(f\"\"\"\n                <div style=\"background: #1a1a2e; padding: 30px; border-radius: 15px;\">\n                    <h2 style=\"color: #00d4ff;\">Que es AlphaSymbolic?</h2>\n                    <p style=\"color: #ccc; line-height: 1.8;\">\n                        Sistema de <strong style=\"color: #ff6b6b;\">regresion simbolica</strong> \n                        basado en <strong style=\"color: #00d4ff;\">Deep Learning</strong> y \n                        <strong style=\"color: #ffd93d;\">Monte Carlo Tree Search</strong>.\n                    </p>\n                    \n                    <h3 style=\"color: #00d4ff; margin-top: 30px;\">Dispositivo Actual</h3>\n                    <p style=\"color: {device_color_current}; font-size: 20px;\">{device_info_current}</p>\n                    \n                    <h3 style=\"color: #00d4ff; margin-top: 30px;\">Metodos de Busqueda</h3>\n                    <ul style=\"color: #ccc;\">\n                        <li><strong>Beam Search:</strong> Explora multiples candidatos en paralelo (rapido)</li>\n                        <li><strong>MCTS:</strong> Monte Carlo Tree Search (mas preciso, lento)</li>\n                        <li><strong>Alpha-GP Hybrid:</strong> Fusiona Neural Search con Algoritmo Genetico GPU (Extremo)</li>\n                    </ul>\n                    \n                    <h3 style=\"color: #00d4ff; margin-top: 30px;\">Operadores</h3>\n                    <div style=\"display: flex; flex-wrap: wrap; gap: 10px; margin: 15px 0;\">\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #00d4ff;\">+</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #00d4ff;\">-</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #00d4ff;\">*</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #00d4ff;\">/</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #ff6b6b;\">sin</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #ff6b6b;\">cos</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #ffd93d;\">exp</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #ffd93d;\">log</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #4ade80;\">pow</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #4ade80;\">sqrt</span>\n                    </div>\n                </div>\n                \"\"\")\n        \n        gr.HTML(\"\"\"\n        <div style=\"text-align: center; padding: 20px; color: #666; margin-top: 30px;\">\n            <p>Powered by PyTorch - SymPy - Scipy - Gradio</p>\n        </div>\n        \"\"\")\n    \n    return demo\n\n\n\n\n# --- Global Initialization for Hot Reloading ---\n# IMPORTANT: For Windows Multiprocessing, we must protect entry point.\n# However, Gradio needs 'demo' to be available for 'gradio app.py'.\n# The issue is 'gradio app.py' imports this file, and multiprocessing spawns new processes that import it again.\n\nif __name__ == \"__main__\":\n    # If run directly (python app.py)\n    print(\"Iniciando AlphaSymbolic (Global Init - Direct Execution)...\")\n    status_init, device_info_init = load_model() \n    print(f\"   {status_init} | {device_info_init}\")\n    demo = create_app()\n    print(\"Abriendo navegador...\")\n    from ui.theme import CUSTOM_CSS, get_theme\n    demo.launch(share=True, inbrowser=True, theme=get_theme(), css=CUSTOM_CSS)\nelse:\n    # If imported by 'gradio app.py' or multiprocessing workers\n    # We only want to load the model if it's the Main Process (Gradio Server)\n    # But multiprocessing workers import this too.\n    # We can try to detect if we are a worker or the server.\n    \n    # Simple fix for Gradio Reload:\n    # define demo globally but lazy load model?\n    # No, let's keep it simple.\n    \n    print(\"AlphaSymbolic Module Imported.\")\n    # Attempt to load model only if not in a worker process?\n    # Actually, for 'gradio app.py', this 'else' block runs.\n    # We need 'demo' to be defined here.\n    \n    try:\n        status_init, device_info_init = load_model() \n        print(f\"   {status_init} | {device_info_init}\")\n    except Exception:\n        pass # Might fail in workers, that's fine\n\n    demo = create_app()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/data/benchmarks/FeynmanEquations.csv\n",
        "\ufeffFilename,Number,Output,Formula,# variables,v1_name,v1_low,v1_high,v2_name,v2_low,v2_high,v3_name,v3_low,v3_high,v4_name,v4_low,v4_high,v5_name,v5_low,v5_high,v6_name,v6_low,v6_high,v7_name,v7_low,v7_high,v8_name,v8_low,v8_high,v9_name,v9_low,v9_high,v10_name,v10_low,v10_high\nI.6.2a,1,f,exp(-theta**2/2)/sqrt(2*pi),1,theta,1,3,,,,,,,,,,,,,,,,,,,,,,,,,,,\nI.6.2,2,f,exp(-(theta/sigma)**2/2)/(sqrt(2*pi)*sigma),2,sigma,1,3,theta,1,3,,,,,,,,,,,,,,,,,,,,,,,,\nI.6.2b,3,f,exp(-((theta-theta1)/sigma)**2/2)/(sqrt(2*pi)*sigma),3,sigma,1,3,theta,1,3,theta1,1,3,,,,,,,,,,,,,,,,,,,,,\nI.8.14,4,d,sqrt((x2-x1)**2+(y2-y1)**2),4,x1,1,5,x2,1,5,y1,1,5,y2,1,5,,,,,,,,,,,,,,,,,,\nI.9.18,5,F,G*m1*m2/((x2-x1)**2+(y2-y1)**2+(z2-z1)**2),9,m1,1,2,m2,1,2,G,1,2,x1,3,4,x2,1,2,y1,3,4,y2,1,2,z1,3,4,z2,1,2,,,\nI.10.7,6,m,m_0/sqrt(1-v**2/c**2),3,m_0,1,5,v,1,2,c,3,10,,,,,,,,,,,,,,,,,,,,,\nI.11.19,7,A,x1*y1+x2*y2+x3*y3,6,x1,1,5,x2,1,5,x3,1,5,y1,1,5,y2,1,5,y3,1,5,,,,,,,,,,,,\nI.12.1,8,F,mu*Nn,2,mu,1,5,Nn,1,5,,,,,,,,,,,,,,,,,,,,,,,,\nI.12.2,10,F,q1*q2*r/(4*pi*epsilon*r**3),4,q1,1,5,q2,1,5,epsilon,1,5,r,1,5,,,,,,,,,,,,,,,,,,\nI.12.4,11,Ef,q1*r/(4*pi*epsilon*r**3),3,q1,1,5,epsilon,1,5,r,1,5,,,,,,,,,,,,,,,,,,,,,\nI.12.5,12,F,q2*Ef,2,q2,1,5,Ef,1,5,,,,,,,,,,,,,,,,,,,,,,,,\nI.12.11,13,F,q*(Ef+B*v*sin(theta)),5,q,1,5,Ef,1,5,B,1,5,v,1,5,theta,1,5,,,,,,,,,,,,,,,\nI.13.4,9,K,1/2*m*(v**2+u**2+w**2),4,m,1,5,v,1,5,u,1,5,w,1,5,,,,,,,,,,,,,,,,,,\nI.13.12,14,U,G*m1*m2*(1/r2-1/r1),5,m1,1,5,m2,1,5,r1,1,5,r2,1,5,G,1,5,,,,,,,,,,,,,,,\nI.14.3,15,U,m*g*z,3,m,1,5,g,1,5,z,1,5,,,,,,,,,,,,,,,,,,,,,\nI.14.4,16,U,1/2*k_spring*x**2,2,k_spring,1,5,x,1,5,,,,,,,,,,,,,,,,,,,,,,,,\nI.15.3x,17,x1,(x-u*t)/sqrt(1-u**2/c**2),4,x,5,10,u,1,2,c,3,20,t,1,2,,,,,,,,,,,,,,,,,,\nI.15.3t,18,t1,(t-u*x/c**2)/sqrt(1-u**2/c**2),4,x,1,5,c,3,10,u,1,2,t,1,5,,,,,,,,,,,,,,,,,,\nI.15.1,19,p,m_0*v/sqrt(1-v**2/c**2),3,m_0,1,5,v,1,2,c,3,10,,,,,,,,,,,,,,,,,,,,,\nI.16.6,20,v1,(u+v)/(1+u*v/c**2),3,c,1,5,v,1,5,u,1,5,,,,,,,,,,,,,,,,,,,,,\nI.18.4,21,r,(m1*r1+m2*r2)/(m1+m2),4,m1,1,5,m2,1,5,r1,1,5,r2,1,5,,,,,,,,,,,,,,,,,,\nI.18.12,22,tau,r*F*sin(theta),2,r,1,5,F,1,5,theta,0,5,,,,,,,,,,,,,,,,,,,,,\nI.18.14,23,L,m*r*v*sin(theta),3,m,1,5,r,1,5,v,1,5,theta,1,5,,,,,,,,,,,,,,,,,,\nI.24.6,24,E_n,1/2*m*(omega**2+omega_0**2)*1/2*x**2,4,m,1,3,omega,1,3,omega_0,1,3,x,1,3,,,,,,,,,,,,,,,,,,\nI.25.13,25,Volt,q/C,2,q,1,5,C,1,5,,,,,,,,,,,,,,,,,,,,,,,,\nI.26.2,26,theta1,arcsin(n*sin(theta2)),2,n,0,1,theta2,1,5,,,,,,,,,,,,,,,,,,,,,,,,\nI.27.6,27,foc,1/(1/d1+n/d2),3,d1,1,5,d2,1,5,n,1,5,,,,,,,,,,,,,,,,,,,,,\nI.29.4,28,k,omega/c,2,omega,1,10,c,1,10,,,,,,,,,,,,,,,,,,,,,,,,\nI.29.16,29,x,sqrt(x1**2+x2**2-2*x1*x2*cos(theta1-theta2)),4,x1,1,5,x2,1,5,theta1,1,5,theta2,1,5,,,,,,,,,,,,,,,,,,\nI.30.3,30,Int,Int_0*sin(n*theta/2)**2/sin(theta/2)**2,3,Int_0,1,5,theta,1,5,n,1,5,,,,,,,,,,,,,,,,,,,,,\nI.30.5,31,theta,arcsin(lambd/(n*d)),3,lambd,1,2,d,2,5,n,1,5,,,,,,,,,,,,,,,,,,,,,\nI.32.5,32,Pwr,q**2*a**2/(6*pi*epsilon*c**3),4,q,1,5,a,1,5,epsilon,1,5,c,1,5,,,,,,,,,,,,,,,,,,\nI.32.17,33,Pwr,(1/2*epsilon*c*Ef**2)*(8*pi*r**2/3)*(omega**4/(omega**2-omega_0**2)**2),6,epsilon,1,2,c,1,2,Ef,1,2,r,1,2,omega,1,2,omega_0,3,5,,,,,,,,,,,,\nI.34.8,34,omega,q*v*B/p,4,q,1,5,v,1,5,B,1,5,p,1,5,,,,,,,,,,,,,,,,,,\nI.34.1,35,omega,omega_0/(1-v/c),3,c,3,10,v,1,2,omega_0,1,5,,,,,,,,,,,,,,,,,,,,,\nI.34.14,36,omega,(1+v/c)/sqrt(1-v**2/c**2)*omega_0,3,c,3,10,v,1,2,omega_0,1,5,,,,,,,,,,,,,,,,,,,,,\nI.34.27,37,E_n,(h/(2*pi))*omega,2,omega,1,5,h,1,5,,,,,,,,,,,,,,,,,,,,,,,,\nI.37.4,38,Int,I1+I2+2*sqrt(I1*I2)*cos(delta),3,I1,1,5,I2,1,5,delta,1,5,,,,,,,,,,,,,,,,,,,,,\nI.38.12,39,r,4*pi*epsilon*(h/(2*pi))**2/(m*q**2),3,m,1,5,q,1,5,h,1,5,epsilon,1,5,,,,,,,,,,,,,,,,,,\nI.39.1,40,E_n,3/2*pr*V,2,pr,1,5,V,1,5,,,,,,,,,,,,,,,,,,,,,,,,\nI.39.11,41,E_n,1/(gamma-1)*pr*V,3,gamma,2,5,pr,1,5,V,1,5,,,,,,,,,,,,,,,,,,,,,\nI.39.22,42,pr,n*kb*T/V,4,n,1,5,T,1,5,V,1,5,kb,1,5,,,,,,,,,,,,,,,,,,\nI.40.1,43,n,n_0*exp(-m*g*x/(kb*T)),6,n_0,1,5,m,1,5,x,1,5,T,1,5,g,1,5,kb,1,5,,,,,,,,,,,,\nI.41.16,44,L_rad,h/(2*pi)*omega**3/(pi**2*c**2*(exp((h/(2*pi))*omega/(kb*T))-1)),5,omega,1,5,T,1,5,h,1,5,kb,1,5,c,1,5,,,,,,,,,,,,,,,\nI.43.16,45,v,mu_drift*q*Volt/d,4,mu_drift,1,5,q,1,5,Volt,1,5,d,1,5,,,,,,,,,,,,,,,,,,\nI.43.31,46,D,mob*kb*T,3,mob,1,5,T,1,5,kb,1,5,,,,,,,,,,,,,,,,,,,,,\nI.43.43,47,kappa,1/(gamma-1)*kb*v/A,4,gamma,2,5,kb,1,5,A,1,5,v,1,5,,,,,,,,,,,,,,,,,,\nI.44.4,48,E_n,n*kb*T*ln(V2/V1),5,n,1,5,kb,1,5,T,1,5,V1,1,5,V2,1,5,,,,,,,,,,,,,,,\nI.47.23,49,c,sqrt(gamma*pr/rho),3,gamma,1,5,pr,1,5,rho,1,5,,,,,,,,,,,,,,,,,,,,,\nI.48.2,50,E_n,m*c**2/sqrt(1-v**2/c**2),3,m,1,5,v,1,2,c,3,10,,,,,,,,,,,,,,,,,,,,,\nI.50.26,51,x,x1*(cos(omega*t)+alpha*cos(omega*t)**2),4,x1,1,3,omega,1,3,t,1,3,alpha,1,3,,,,,,,,,,,,,,,,,,\nII.2.42,52,Pwr,kappa*(T2-T1)*A/d,5,kappa,1,5,T1,1,5,T2,1,5,A,1,5,d,1,5,,,,,,,,,,,,,,,\nII.3.24,53,flux,Pwr/(4*pi*r**2),2,Pwr,1,5,r,1,5,,,,,,,,,,,,,,,,,,,,,,,,\nII.4.23,54,Volt,q/(4*pi*epsilon*r),3,q,1,5,epsilon,1,5,r,1,5,,,,,,,,,,,,,,,,,,,,,\nII.6.11,55,Volt,1/(4*pi*epsilon)*p_d*cos(theta)/r**2,4,epsilon,1,3,p_d,1,3,theta,1,3,r,1,3,,,,,,,,,,,,,,,,,,\nII.6.15a,56,Ef,p_d/(4*pi*epsilon)*3*z/r**5*sqrt(x**2+y**2),6,epsilon,1,3,p_d,1,3,r,1,3,x,1,3,y,1,3,z,1,3,,,,,,,,,,,,\nII.6.15b,57,Ef,p_d/(4*pi*epsilon)*3*cos(theta)*sin(theta)/r**3,4,epsilon,1,3,p_d,1,3,theta,1,3,r,1,3,,,,,,,,,,,,,,,,,,\nII.8.7,58,E_n,3/5*q**2/(4*pi*epsilon*d),3,q,1,5,epsilon,1,5,d,1,5,,,,,,,,,,,,,,,,,,,,,\nII.8.31,59,E_den,epsilon*Ef**2/2,2,epsilon,1,5,Ef,1,5,,,,,,,,,,,,,,,,,,,,,,,,\nII.10.9,60,Ef,sigma_den/epsilon*1/(1+chi),3,sigma_den,1,5,epsilon,1,5,chi,1,5,,,,,,,,,,,,,,,,,,,,,\nII.11.3,61,x,q*Ef/(m*(omega_0**2-omega**2)),5,q,1,3,Ef,1,3,m,1,3,omega_0,3,5,omega,1,2,,,,,,,,,,,,,,,\nII.11.17,62,n,n_0*(1+p_d*Ef*cos(theta)/(kb*T)),6,n_0,1,3,kb,1,3,T,1,3,theta,1,3,p_d,1,3,Ef,1,3,,,,,,,,,,,,\nII.11.20,63,Pol,n_rho*p_d**2*Ef/(3*kb*T),5,n_rho,1,5,p_d,1,5,Ef,1,5,kb,1,5,T,1,5,,,,,,,,,,,,,,,\nII.11.27,64,Pol,n*alpha/(1-(n*alpha/3))*epsilon*Ef,4,n,0,1,alpha,0,1,epsilon,1,2,Ef,1,2,,,,,,,,,,,,,,,,,,\nII.11.28,65,theta,1+n*alpha/(1-(n*alpha/3)),2,n,0,1,alpha,0,1,,,,,,,,,,,,,,,,,,,,,,,,\nII.13.17,66,B,1/(4*pi*epsilon*c**2)*2*I/r,4,epsilon,1,5,c,1,5,I,1,5,r,1,5,,,,,,,,,,,,,,,,,,\nII.13.23,67,rho_c,rho_c_0/sqrt(1-v**2/c**2),3,rho_c_0,1,5,v,1,2,c,3,10,,,,,,,,,,,,,,,,,,,,,\nII.13.34,68,j,rho_c_0*v/sqrt(1-v**2/c**2),3,rho_c_0,1,5,v,1,2,c,3,10,,,,,,,,,,,,,,,,,,,,,\nII.15.4,69,E_n,-mom*B*cos(theta),3,mom,1,5,B,1,5,theta,1,5,,,,,,,,,,,,,,,,,,,,,\nII.15.5,70,E_n,-p_d*Ef*cos(theta),3,p_d,1,5,Ef,1,5,theta,1,5,,,,,,,,,,,,,,,,,,,,,\nII.21.32,71,Volt,q/(4*pi*epsilon*r*(1-v/c)),5,q,1,5,epsilon,1,5,r,1,5,v,1,2,c,3,10,,,,,,,,,,,,,,,\nII.24.17,72,k,sqrt(omega**2/c**2-pi**2/d**2),3,omega,4,6,c,1,2,d,2,4,,,,,,,,,,,,,,,,,,,,,\nII.27.16,73,flux,epsilon*c*Ef**2,3,epsilon,1,5,c,1,5,Ef,1,5,,,,,,,,,,,,,,,,,,,,,\nII.27.18,74,E_den,epsilon*Ef**2,2,epsilon,1,5,Ef,1,5,,,,,,,,,,,,,,,,,,,,,,,,\nII.34.2a,75,I,q*v/(2*pi*r),3,q,1,5,v,1,5,r,1,5,,,,,,,,,,,,,,,,,,,,,\nII.34.2,76,mom,q*v*r/2,3,q,1,5,v,1,5,r,1,5,,,,,,,,,,,,,,,,,,,,,\nII.34.11,77,omega,g_*q*B/(2*m),4,g_,1,5,q,1,5,B,1,5,m,1,5,,,,,,,,,,,,,,,,,,\nII.34.29a,78,mom,q*h/(4*pi*m),3,q,1,5,h,1,5,m,1,5,,,,,,,,,,,,,,,,,,,,,\nII.34.29b,79,E_n,g_*mom*B*Jz/(h/(2*pi)),5,g_,1,5,h,1,5,Jz,1,5,mom,1,5,B,1,5,,,,,,,,,,,,,,,\nII.35.18,80,n,n_0/(exp(mom*B/(kb*T))+exp(-mom*B/(kb*T))),5,n_0,1,3,kb,1,3,T,1,3,mom,1,3,B,1,3,,,,,,,,,,,,,,,\nII.35.21,81,M,n_rho*mom*tanh(mom*B/(kb*T)),5,n_rho,1,5,mom,1,5,B,1,5,kb,1,5,T,1,5,,,,,,,,,,,,,,,\nII.36.38,82,f,mom*H/(kb*T)+(mom*alpha)/(epsilon*c**2*kb*T)*M,8,mom,1,3,H,1,3,kb,1,3,T,1,3,alpha,1,3,epsilon,1,3,c,1,3,M,1,3,,,,,,\nII.37.1,83,E_n,mom*(1+chi)*B,6,mom,1,5,B,1,5,chi,1,5,,,,,,,,,,,,,,,,,,,,,\nII.38.3,84,F,Y*A*x/d,4,Y,1,5,A,1,5,d,1,5,x,1,5,,,,,,,,,,,,,,,,,,\nII.38.14,85,mu_S,Y/(2*(1+sigma)),2,Y,1,5,sigma,1,5,,,,,,,,,,,,,,,,,,,,,,,,\nIII.4.32,86,n,1/(exp((h/(2*pi))*omega/(kb*T))-1),4,h,1,5,omega,1,5,kb,1,5,T,1,5,,,,,,,,,,,,,,,,,,\nIII.4.33,87,E_n,(h/(2*pi))*omega/(exp((h/(2*pi))*omega/(kb*T))-1),4,h,1,5,omega,1,5,kb,1,5,T,1,5,,,,,,,,,,,,,,,,,,\nIII.7.38,88,omega,2*mom*B/(h/(2*pi)),3,mom,1,5,B,1,5,h,1,5,,,,,,,,,,,,,,,,,,,,,\nIII.8.54,89,prob,sin(E_n*t/(h/(2*pi)))**2,3,E_n,1,2,t,1,2,h,1,4,,,,,,,,,,,,,,,,,,,,,\nIII.9.52,90,prob,(p_d*Ef*t/(h/(2*pi)))*sin((omega-omega_0)*t/2)**2/((omega-omega_0)*t/2)**2,6,p_d,1,3,Ef,1,3,t,1,3,h,1,3,omega,1,5,omega_0,1,5,,,,,,,,,,,,\nIII.10.19,91,E_n,mom*sqrt(Bx**2+By**2+Bz**2),3,mom,1,5,Bx,1,5,By,1,5,Bz,1,5,,,,,,,,,,,,,,,,,,\nIII.12.43,92,L,n*(h/(2*pi)),2,n,1,5,h,1,5,,,,,,,,,,,,,,,,,,,,,,,,\nIII.13.18,93,v,2*E_n*d**2*k/(h/(2*pi)),4,E_n,1,5,d,1,5,k,1,5,h,1,5,,,,,,,,,,,,,,,,,,\nIII.14.14,94,I,I_0*(exp(q*Volt/(kb*T))-1),5,I_0,1,5,q,1,2,Volt,1,2,kb,1,2,T,1,2,,,,,,,,,,,,,,,\nIII.15.12,95,E_n,2*U*(1-cos(k*d)),3,U,1,5,k,1,5,d,1,5,,,,,,,,,,,,,,,,,,,,,\nIII.15.14,96,m,(h/(2*pi))**2/(2*E_n*d**2),3,h,1,5,E_n,1,5,d,1,5,,,,,,,,,,,,,,,,,,,,,\nIII.15.27,97,k,2*pi*alpha/(n*d),3,alpha,1,5,n,1,5,d,1,5,,,,,,,,,,,,,,,,,,,,,\nIII.17.37,98,f,beta*(1+alpha*cos(theta)),3,beta,1,5,alpha,1,5,theta,1,5,,,,,,,,,,,,,,,,,,,,,\nIII.19.51,99,E_n,-m*q**4/(2*(4*pi*epsilon)**2*(h/(2*pi))**2)*(1/n**2),4,m,1,5,q,1,5,h,1,5,n,1,5,epsilon,1,5,,,,,,,,,,,,,,,\nIII.21.20,100,j,-rho_c_0*q*A_vec/m,4,rho_c_0,1,5,q,1,5,A_vec,1,5,m,1,5,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/data/benchmarks/BonusEquations.csv\n",
        "\ufeffFilename,Number,Name,Eqn. No.,Output,Formula,# variables,v1_name,v1_low,v1_high,v2_name,v2_low,v2_high,v3_name,v3_low,v3_high,v4_name,v4_low,v4_high,v5_name,v5_low,v5_high,v6_name,v6_low,v6_high,v7_name,v7_low,v7_high,v8_name,v8_low,v8_high,v9_name,v9_low,v9_high,v10_name,v10_low,v10_high\ntest_1,1,Rutherford scattering,1,A,(Z_1*Z_2*alpha*hbar*c/(4*E_n*sin(theta/2)**2))**2,7,Z_1,1,2,Z_2,1,2,alpha,1,2,hbar,1,2,c,1,2,E_n,1,3,theta,1,3,,,,,,,,,\ntest_2,2,3.55 Goldstein,2,k,m*k_G/L**2*(1+sqrt(1+2*E_n*L**2/(m*k_G**2))*cos(theta1-theta2)),6,m,1,3,k_G,1,3,L,1,3,E_n,1,3,theta1,0,6,theta2,0,6,,,,,,,,,,,,\ntest_3,3,3.64 Goldstein,3,r,d*(1-alpha**2)/(1+alpha*cos(theta1-theta2)),4,d,1,3,alpha,2,4,theta1,4,5,theta2,4,5,,,,,,,,,,,,,,,,,,\ntest_4,4,3.16 Goldstein,4,v,sqrt(2/m*(E_n-U-L**2/(2*m*r**2))),5,m,1,3,E_n,8,12,U,1,3,L,1,3,r,1,3,,,,,,,,,,,,,,,\ntest_5,5,3.74 Goldstein,5,t,2*pi*d**(3/2)/sqrt(G*(m1+m2)),4,d,1,3,G,1,3,m1,1,3,m2,1,3,,,,,,,,,,,,,,,,,,\ntest_6,6,3.99 Goldstein,6,alpha,sqrt(1+2*epsilon**2*E_n*L**2/(m*(Z_1*Z_2*q**2)**2)),7,epsilon,1,3,L,1,3,m,1,3,Z_1,1,3,Z_2,1,3,q,1,3,E_n,1,3,,,,,,,,,\ntest_7,7,Friedman Equation,7,H_G,sqrt(8*pi*G*rho/3-alpha*c**2/d**2),5,G,1,3,rho,1,3,alpha,1,2,c,1,2,d,1,3,,,,,,,,,,,,,,,\ntest_8,8,Compton Scattering,8,K,E_n/(1+E_n/(m*c**2)*(1-cos(theta))),4,E_n,1,3,m,1,3,c,1,3,theta,1,3,,,,,,,,,,,,,,,,,,\ntest_9,9,Gravitational wave ratiated power,9,Pwr,-32/5*G**4/c**5*(m1*m2)**2*(m1+m2)/r**5,5,G,1,2,c,1,2,m1,1,5,m2,1,5,r,1,2,,,,,,,,,,,,,,,\ntest_10,10,Relativistic aberation,10,theta1,arccos((cos(theta2)-v/c)/(1-v/c*cos(theta2))),3,c,4,6,v,1,3,theta2,1,3,,,,,,,,,,,,,,,,,,,,,\ntest_11,11,N-slit diffraction,11,I,I_0*(sin(alpha/2)*sin(n*delta/2)/(alpha/2*sin(delta/2)))**2,4,I_0,1,3,alpha,1,3,delta,1,3,n,1,2,,,,,,,,,,,,,,,,,,\ntest_12,12,2.11 Jackson,12,F,q/(4*pi*epsilon*y**2)*(4*pi*epsilon*Volt*d-q*d*y**3/(y**2-d**2)**2),4,q,1,5,y,1,3,Volt,1,5,d,4,6,epsilon,1,5,,,,,,,,,,,,,,,\ntest_13,13,3.45 Jackson,13,Volt,1/(4*pi*epsilon)*q/sqrt(r**2+d**2-2*r*d*cos(alpha)),4,q,1,5,r,1,3,d,4,6,alpha,0,6,epsilon,1,5,,,,,,,,,,,,,,,\ntest_14,14,4.60' Jackson,14,Volt,Ef*cos(theta)*(-r+d**3/r**2*(alpha-1)/(alpha+2)),5,Ef,1,5,theta,0,6,r,1,5,d,1,5,alpha,1,5,,,,,,,,,,,,,,,\ntest_15,15,11.38 Jackson,15,omega_0,sqrt(1-v**2/c**2)*omega/(1+v/c*cos(theta)),4,c,5,20,v,1,3,omega,1,5,theta,0,6,,,,,,,,,,,,,,,,,,\ntest_16,16,8.56 Goldstein,16,E_n,sqrt((p-q*A_vec)**2*c**2+m**2*c**4)+q*Volt,6,m,1,5,c,1,5,p,1,5,q,1,5,A_vec,1,5,Volt,1,5,,,,,,,,,,,,\ntest_17,17,12.80' Goldstein,17,E_n,1/(2*m)*(p**2+m**2*omega**2*x**2*(1+alpha*x/y)),6,m,1,5,omega,1,5,p,1,5,y,1,5,x,1,5,alpha,1,5,,,,,,,,,,,,\ntest_18,18,15.2.1 Weinberg,18,rho_0,3/(8*pi*G)*(c**2*k_f/r**2+H_G**2),4,G,1,5,k_f,1,5,r,1,5,H_G,1,5,c,1,5,,,,,,,,,,,,,,,\ntest_19,19,15.2.2 Weinberg,19,pr,-1/(8*pi*G)*(c**4*k_f/r**2+H_G**2*c**2*(1-2*alpha)),5,G,1,5,k_f,1,5,r,1,5,H_G,1,5,alpha,1,5,c,1,5,,,,,,,,,,,,\ntest_20,20,Klein-Nishina (13.132 Schwarz),20,A,1/(4*pi)*alpha**2*h**2/(m**2*c**2)*(omega_0/omega)**2*(omega_0/omega+omega/omega_0-sin(beta)**2),7,omega,1,5,omega_0,1,5,alpha,1,5,h,1,5,m,1,5,c,1,5,beta,0,6,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/top_formulas.csv\n",
        "formula,residual,rmsle_global,extrapolation_error,pred_26,pred_27,sample_size,timestamp\nexp((log(log((38.4335 * (x0 * log(log((x0 - (x0 / (log(x0) - 2.1513))))))))) - x0) + lgamma(x)),(log(log((38.4335 * (x0 * log(log((x0 - (x0 / (log(x0) - 2.1513))))))))) - x0),0.446213921297748,1.384836563676553e+17,9118205752846800.0,1.0962380464998448e+17,18,2026-01-10 01:01:51\nexp(((g(x0) / (((e(((6.5993 * x0) / (x0 * (24 - x0)))) - 2.2322) / ((x0 - 5.5466) / 8.2059)) + x0)) - x0) + lgamma(x)),((g(x0) / (((e(((6.5993 * x0) / (x0 * (24 - x0)))) - 2.2322) / ((x0 - 5.5466) / 8.2059)) + x0)) - x0),0.49214994109680077,7161996148432400.0,2.3612133893656412e+16,2.4077552902526256e+17,15,2026-01-10 02:08:04\nexp((log((x0 / log(((0.2404 - (x0 + x0)) + 12.0234)))) - x0) + lgamma(x)),(log((x0 / log(((0.2404 - (x0 + x0)) + 12.0234)))) - x0),0.49586141956618673,9.458651551890949e+16,1.4548596099178496e+16,1.480905551523986e+17,21,2026-01-10 02:16:06\nexp((0.1527 + ((((1.0016 ^ x0) / (((x0 - 1.1483) / (((x0 ^ -3.1587) ^ -0.3481) - x0)) - (x0 + (x0 + x0)))) - x0) + 1.6047)) + lgamma(x)),(0.1527 + ((((1.0016 ^ x0) / (((x0 - 1.1483) / (((x0 ^ -3.1587) ^ -0.3481) - x0)) - (x0 + (x0 + x0)))) - x0) + 1.6047)),0.5438126039885908,1.2831730371202491e+17,1.178547406114365e+16,1.1712288899731802e+17,17,2026-01-10 01:55:57\nexp((log(((x0 - 3.0000) / log(((((26.6821 / x0) / (22.9188 - x0)) ^ x0) + 13.3840)))) - x0) + lgamma(x)),(log(((x0 - 3.0000) / log(((((26.6821 / x0) / (22.9188 - x0)) ^ x0) + 13.3840)))) - x0),0.6612535694422839,4.960786910565984e+16,1.8268803826830184e+16,1.8934899383799654e+17,9,2026-01-10 02:06:03\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/pattern_memory.json\n",
        "{\n  \"e(((C + ((C / (C * (((x0 + x0) - g((C - x0))) * C))) - x0)) + g(x0)))\": 1,\n  \"e(((((((((C + (x0 / (x0 - C))) / C) + C) / (C - x0)) + x0) / x0) - (x0 / C)) + g(x0)))\": 1,\n  \"e(((C - x0) + g(x0)))\": 1,\n  \"e(((C - ((log(x0) / ((log(((x0 ^ log((x0 * x0))) * x0)) - ((C / (C - x0)) + x0)) * (C - x0))) + x0)) + g(x0)))\": 1,\n  \"e(((((x0 + (C / x0)) * C) / ((x0 / (((C - (x0 + ((((x0 * C) / (x0 - C)) / (x0 + x0)) - C))) - C) * (((((C - (x0 + x0)) + C) / (x0 * x0)) - C) + x0))) - C)) + g(x0)))\": 1,\n  \"e(((log((C - (x0 / (x0 * (C - x0))))) - x0) + g(x0)))\": 1,\n  \"e(((log(log((C * (x0 * log(log((x0 - (x0 / (log(x0) - C))))))))) - x0) + g(x0)))\": 1,\n  \"e((((C - x0) - (x0 / ((x0 + (x0 - C)) + (x0 - (x0 ^ (((C - x0) / (C + x0)) + log((log((x0 + C)) - x0)))))))) + g(x0)))\": 1,\n  \"e(((C + ((((C ^ x0) / (((x0 - C) / (((x0 ^ C) ^ C) - x0)) - (x0 + (x0 + x0)))) - x0) + C)) + g(x0)))\": 1,\n  \"e((((((x0 - (((C - x0) / x0) / (C + ((x0 - C) / C)))) + x0) / x0) - x0) + g(x0)))\": 1,\n  \"e(((C - ((C ^ x0) + x0)) + g(x0)))\": 1,\n  \"e((((C * (C / (C - x0))) - ((C / ((x0 - (x0 / C)) * ((C - (x0 / C)) + C))) + x0)) + g(x0)))\": 1,\n  \"e((((((x0 + x0) + (((x0 - C) / (((((x0 * C) / (x0 / (C * x0))) + C) / x0) - (x0 + ((x0 - C) / ((C / x0) - C))))) / x0)) / (x0 * C)) - x0) + g(x0)))\": 1,\n  \"e(((log(((x0 - C) / log(((((C / x0) / (C - x0)) ^ x0) + C)))) - x0) + g(x0)))\": 1,\n  \"e((((g(x0) / (((e(((C * x0) / (x0 * (C - x0)))) - C) / ((x0 - C) / C)) + x0)) - x0) + g(x0)))\": 1,\n  \"e(((((C + x0) / ((C / (g((C - (x0 - C))) - (x0 + x0))) + x0)) - (x0 - C)) + g(x0)))\": 1,\n  \"e((((x0 / ((x0 + C) * ((C * (C / x0)) - (x0 + ((C * (C / x0)) * C))))) + (log((x0 * (C / (C + x0)))) - x0)) + g(x0)))\": 1,\n  \"e(((((neg(g(x0)) / (C - (C + x0))) - x0) + (C ^ (C - ((x0 * C) ^ (log((C - (x0 ^ C))) - C))))) + g(x0)))\": 1,\n  \"e(((log((x0 / log(((C - (x0 + x0)) + C)))) - x0) + g(x0)))\": 1,\n  \"e(((log(((log(((C - ((((x0 / C) / C) + C) - C)) * C)) - (x0 + ((C / ((log(x0) / x0) - ((x0 / C) + C))) / ((((x0 ^ log(x0)) - x0) / (neg(x0) / (C - x0))) - x0)))) / C)) - x0) + g(x0)))\": 1,\n  \"e((((C - ((x0 / C) / C)) - (x0 - ((C / e((C - (x0 / C)))) / (((C - ((x0 - (x0 / C)) - C)) / C) + ((((x0 / C) - ((x0 / C) - (C + x0))) / (C + ((C - (x0 - C)) / C))) + x0))))) + g(x0)))\": 1\n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run AlphaSymbolic\n",
        "# The binaries are in ../Code/build/\n",
        "%cd AlphaSymbolic\n",
        "!python app.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Backup Learned Formulas to Drive (Run manually or keep active)\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "DRIVE_PATH = '/content/drive/MyDrive/AlphaSymbolic_Models'\n",
        "FILES_TO_BACKUP = [\n",
        "    ('AlphaSymbolic/top_formulas.csv', 'top_formulas.csv'),\n",
        "    ('AlphaSymbolic/pattern_memory.json', 'pattern_memory.json'),\n",
        "    ('AlphaSymbolic/top_5_detailed_report.csv', 'top_5_detailed_report.csv'),\n",
        "    ('AlphaSymbolic/results/learned_formulas.csv', 'learned_formulas.csv')\n",
        "]\n",
        "\n",
        "os.makedirs(DRIVE_PATH, exist_ok=True)\n",
        "\n",
        "for src, name in FILES_TO_BACKUP:\n",
        "    dst = os.path.join(DRIVE_PATH, name)\n",
        "    try:\n",
        "        if os.path.exists(src):\n",
        "            shutil.copy(src, dst)\n",
        "            print(f\"\u2705 Backup successful: {name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Backup failed for {name}: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}