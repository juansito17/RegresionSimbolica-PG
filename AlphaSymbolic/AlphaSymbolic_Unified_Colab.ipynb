{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AlphaSymbolic - Unified Hybrid System\n",
        "# -------------------------------------\n",
        "# Instructions:\n",
        "# 1. Runtime -> Change runtime type -> T4 GPU\n",
        "# 2. Run All\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "# Install dependencies\n",
        "!pip install gradio torch torchvision torchaudio scipy matplotlib sympy\n",
        "\n",
        "# Create Directory Structure\n",
        "import os\n",
        "os.makedirs('Code/src', exist_ok=True)\n",
        "os.makedirs('Code/build', exist_ok=True)\n",
        "os.makedirs('AlphaSymbolic', exist_ok=True)\n",
        "directories = ['core', 'data', 'search', 'ui', 'utils']\n",
        "for d in directories:\n",
        "    os.makedirs(os.path.join('AlphaSymbolic', d), exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/AdvancedFeatures.cpp\n",
        "#include \"AdvancedFeatures.h\"\n#include \"Globals.h\"\n#include \"GeneticOperators.h\"\n#include \"Fitness.h\"\n#include \"ExpressionTree.h\" // Necesario para tree_to_string en la simplificaci\u00f3n\n#include <cmath>\n#include <numeric>\n#include <algorithm>\n#include <iostream>\n#include <unordered_map>\n#include <vector>\n\n//---------------------------------\n// EvolutionParameters\n//---------------------------------\nEvolutionParameters EvolutionParameters::create_default() {\n    // Usa constantes globales para los valores por defecto\n    return {BASE_MUTATION_RATE, BASE_ELITE_PERCENTAGE, DEFAULT_TOURNAMENT_SIZE, DEFAULT_CROSSOVER_RATE};\n}\n\nvoid EvolutionParameters::mutate(int stagnation_counter) {\n    auto& rng = get_rng();\n    double aggression_factor = 1.0;\n    // Ajuste del factor de agresi\u00f3n basado en el estancamiento\n    if (stagnation_counter > STAGNATION_LIMIT_ISLAND / 2) {\n        // Aumenta la agresi\u00f3n si hay estancamiento significativo\n        aggression_factor = 1.0 + (static_cast<double>(stagnation_counter - STAGNATION_LIMIT_ISLAND / 2) / (STAGNATION_LIMIT_ISLAND / 2.0)) * 0.5; // Escala de 1.0 a 1.5\n        aggression_factor = std::min(aggression_factor, 2.0); // Limitar la agresi\u00f3n m\u00e1xima\n    } else if (stagnation_counter < STAGNATION_LIMIT_ISLAND / 4 && stagnation_counter > 0) {\n        // Reduce la agresi\u00f3n si no hay mucho estancamiento, pero no es 0\n        aggression_factor = 1.0 - (static_cast<double>(STAGNATION_LIMIT_ISLAND / 4 - stagnation_counter) / (STAGNATION_LIMIT_ISLAND / 4.0)) * 0.5; // Escala de 0.5 a 1.0\n        aggression_factor = std::max(aggression_factor, 0.5); // Limitar la agresi\u00f3n m\u00ednima\n    } else if (stagnation_counter == 0) {\n        // Muy poco estancamiento, cambios muy peque\u00f1os\n        aggression_factor = 0.2; // Cambios muy conservadores\n    }\n\n    std::uniform_real_distribution<double> base_rate_change(-0.05, 0.05);\n    std::uniform_int_distribution<int> base_tourney_change(-2, 2);\n\n    double rate_change_val = base_rate_change(rng) * aggression_factor;\n    int tourney_change_val = static_cast<int>(std::round(base_tourney_change(rng) * aggression_factor));\n    \n    // Asegurar que haya alg\u00fan cambio si la agresi\u00f3n es alta y el cambio base es 0\n    if (aggression_factor > 1.0 && tourney_change_val == 0 && base_tourney_change(rng) != 0) {\n         tourney_change_val = (base_tourney_change(rng) > 0) ? 1 : -1;\n    }\n\n    // Definir l\u00edmites din\u00e1micos para los par\u00e1metros\n    double min_mutation = 0.05;\n    double max_mutation_base = 0.5;\n    double max_mutation = min_mutation + (max_mutation_base - min_mutation) * (1.0 + aggression_factor / 2.0);\n\n    double min_elite = 0.02;\n    double max_elite_base = 0.25;\n    double max_elite = min_elite + (max_elite_base - min_elite) * (1.0 + aggression_factor / 2.0);\n\n    int min_tournament = 3;\n    int max_tournament_base = 30;\n    int max_tournament = min_tournament + static_cast<int>((max_tournament_base - min_tournament) * (1.0 + aggression_factor / 2.0));\n\n    // Aplicar los cambios y asegurar que est\u00e9n dentro de los l\u00edmites\n    mutation_rate = std::clamp(mutation_rate + rate_change_val, min_mutation, max_mutation);\n    elite_percentage = std::clamp(elite_percentage + rate_change_val, min_elite, max_elite);\n    tournament_size = std::clamp(tournament_size + tourney_change_val, min_tournament, max_tournament);\n    crossover_rate = std::clamp(crossover_rate + rate_change_val, 0.5, 0.95);\n}\n\n//---------------------------------\n// PatternMemory\n//---------------------------------\nvoid PatternMemory::record_success(const NodePtr& tree, double fitness) {\n    std::string pattern = extract_pattern(tree);\n    if (pattern.empty() || pattern.length() > 50 || pattern.length() < 3 || pattern == \"N\") return;\n    auto it = patterns.find(pattern);\n    if (it == patterns.end()) {\n        patterns[pattern] = {pattern, fitness, 1, (fitness < INF ? 1.0 : 0.0)};\n    } else {\n        auto& p = it->second;\n        p.uses++;\n        double improvement = (fitness < p.best_fitness && p.best_fitness < INF) ? 1.0 : 0.0;\n        p.success_rate = ((p.success_rate * (p.uses - 1)) + improvement) / p.uses;\n        p.best_fitness = std::min(p.best_fitness, fitness);\n    }\n}\n\nNodePtr PatternMemory::suggest_pattern_based_tree(int max_depth) {\n    if (patterns.empty()) return nullptr;\n    std::vector<std::pair<std::string, double>> candidates;\n    for (const auto& [pattern_str, info] : patterns) {\n        if (info.uses >= PATTERN_MEM_MIN_USES && (info.success_rate > 0.1 || info.best_fitness < PATTERN_RECORD_FITNESS_THRESHOLD)) {\n             double weight = info.success_rate + (1.0 / (1.0 + info.best_fitness));\n             candidates.emplace_back(pattern_str, weight);\n        }\n    }\n    if (candidates.empty()) return nullptr;\n    std::vector<double> weights;\n    std::transform(candidates.begin(), candidates.end(), std::back_inserter(weights), [](const auto& p){ return p.second; });\n    std::discrete_distribution<> dist(weights.begin(), weights.end());\n    auto& rng = get_rng();\n    int selected_idx = dist(rng);\n    return parse_pattern(candidates[selected_idx].first, max_depth);\n}\n\nstd::string PatternMemory::extract_pattern(const NodePtr& node) {\n    if (!node) return \"N\";\n    switch (node->type) {\n        case NodeType::Constant: return \"#\";\n        case NodeType::Variable: return \"x\";\n        case NodeType::Operator:\n            return \"(\" + extract_pattern(node->left) + node->op + extract_pattern(node->right) + \")\";\n        default: return \"?\";\n    }\n}\n\nNodePtr PatternMemory::parse_pattern(const std::string& pattern, int max_depth) {\n    // Placeholder implementation\n    if (pattern == \"#\") {\n        auto node = std::make_shared<Node>(NodeType::Constant);\n        if (FORCE_INTEGER_CONSTANTS) { std::uniform_int_distribution<int> cd(CONSTANT_INT_MIN_VALUE, CONSTANT_INT_MAX_VALUE); node->value = static_cast<double>(cd(get_rng())); }\n        else { std::uniform_real_distribution<double> cd(CONSTANT_MIN_VALUE, CONSTANT_MAX_VALUE); node->value = cd(get_rng()); }\n        if(std::fabs(node->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) node->value = 0.0;\n        return node;\n    }\n    if (pattern == \"x\") return std::make_shared<Node>(NodeType::Variable);\n    if (pattern == \"N\") return nullptr;\n    if (pattern.length() > 3 && pattern.front() == '(' && pattern.back() == ')') {\n          return generate_random_tree(max_depth); // Fallback\n     }\n    return generate_random_tree(max_depth); // Fallback\n}\n\n//---------------------------------\n// Pareto Optimizer\n//---------------------------------\nParetoSolution::ParetoSolution(NodePtr t, double acc, double complexity_val) : tree(std::move(t)), accuracy(acc), complexity(complexity_val), dominated(false) {}\n\nbool ParetoSolution::dominates(const ParetoSolution& other) const {\n    bool better_in_one = (accuracy < other.accuracy) || (complexity < other.complexity);\n    bool not_worse_in_any = (accuracy <= other.accuracy) && (complexity <= other.complexity);\n    return better_in_one && not_worse_in_any;\n}\n\nvoid ParetoOptimizer::update(const std::vector<Individual>& population, const std::vector<double>& targets, const std::vector<double>& x_values) {\n    std::vector<ParetoSolution> candidates = pareto_front;\n    for (const auto& ind : population) {\n        if (ind.tree && ind.fitness_valid && ind.fitness < INF) {\n            candidates.emplace_back(ind.tree, ind.fitness, static_cast<double>(tree_size(ind.tree)));\n        }\n    }\n    for (auto& sol1 : candidates) {\n        sol1.dominated = false;\n        for (const auto& sol2 : candidates) {\n            if (&sol1 == &sol2) continue;\n            if (sol2.dominates(sol1)) { sol1.dominated = true; break; }\n        }\n    }\n    pareto_front.clear();\n    std::copy_if(candidates.begin(), candidates.end(), std::back_inserter(pareto_front),\n                 [](const auto& sol) { return !sol.dominated; });\n    if (pareto_front.size() > PARETO_MAX_FRONT_SIZE) {\n        std::sort(pareto_front.begin(), pareto_front.end(), [](const auto& a, const auto& b){ return a.accuracy < b.accuracy; });\n        pareto_front.resize(PARETO_MAX_FRONT_SIZE);\n    }\n}\n\nstd::vector<NodePtr> ParetoOptimizer::get_pareto_solutions() {\n    std::vector<NodePtr> result;\n    result.reserve(pareto_front.size());\n    std::transform(pareto_front.begin(), pareto_front.end(), std::back_inserter(result),\n                   [](const auto& sol) { return sol.tree; });\n    return result;\n}\n\n//---------------------------------\n// Domain Constraints\n//---------------------------------\nbool DomainConstraints::is_valid_recursive(const NodePtr& node) {\n     if (!node) return true;\n     if (node->type == NodeType::Operator) {\n         if (node->op == '/' && node->right && node->right->type == NodeType::Constant && std::fabs(node->right->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) return false;\n         if (node->op == '^') { // Solo chequear 0^negativo/0\n              if (node->left && node->left->type == NodeType::Constant && std::fabs(node->left->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE &&\n                  node->right && node->right->type == NodeType::Constant && node->right->value <= SIMPLIFY_NEAR_ZERO_TOLERANCE) {\n                      return false;\n              }\n         }\n         if ((node->op == '*' || node->op == '/') && node->right && node->right->type == NodeType::Constant && std::fabs(node->right->value - 1.0) < SIMPLIFY_NEAR_ONE_TOLERANCE) return false;\n         if ((node->op == '+' || node->op == '-') && node->right && node->right->type == NodeType::Constant && std::fabs(node->right->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) return false;\n         if (node->op == '*' && node->left && node->left->type == NodeType::Constant && std::fabs(node->left->value - 1.0) < SIMPLIFY_NEAR_ONE_TOLERANCE) return false;\n         if (node->op == '+' && node->left && node->left->type == NodeType::Constant && std::fabs(node->left->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) return false;\n         if (!is_valid_recursive(node->left) || !is_valid_recursive(node->right)) return false;\n     }\n     return true;\n }\n\nbool DomainConstraints::is_valid(const NodePtr& tree) {\n    return is_valid_recursive(tree);\n}\n\nNodePtr DomainConstraints::simplify_recursive(NodePtr node) {\n    if (!node || node->type != NodeType::Operator) return node;\n    node->left = simplify_recursive(node->left);\n    node->right = simplify_recursive(node->right);\n\n    // Manejo de hijos nulos\n    bool is_unary = (node->op == 's' || node->op == 'c' || node->op == 'l' || node->op == 'e' || node->op == '!' || node->op == '_' || node->op == 'g');\n\n    // Constant Folding (First priority)\n    bool left_is_const = (node->left && node->left->type == NodeType::Constant);\n    bool right_is_const = (node->right && node->right->type == NodeType::Constant);\n    \n    // Fold if binary op with 2 constants OR unary op with 1 constant\n    if ((left_is_const && right_is_const) || (is_unary && left_is_const)) {\n        try {\n            double result = evaluate_tree(node, 0.0); \n            if (!std::isnan(result) && !std::isinf(result)) {\n                auto cn = std::make_shared<Node>(NodeType::Constant);\n                if (FORCE_INTEGER_CONSTANTS) cn->value = std::round(result); else cn->value = result;\n                if (std::fabs(cn->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) cn->value = 0.0; return cn;\n            }\n        } catch (const std::exception&) {}\n    }\n\n    if (node->left && !node->right) {\n        if (is_unary) return node; // Correct state for unary ops (Constant folding didn't trigger, so var inside)\n        return node->left; // Simplify \"A op null\" -> A (for binary ops? dangerous but existing logic)\n    }\n    if (!node->left && node->right) return node->right;\n    if (!node->left && !node->right) { auto cn = std::make_shared<Node>(NodeType::Constant); cn->value = 1.0; return cn; }\n\n    // Identity Simplifications & Fixes\n     if ((node->op == '+' || node->op == '-') && node->right && node->right->type == NodeType::Constant && std::fabs(node->right->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) return node->left;\n     if (node->op == '+' && node->left && node->left->type == NodeType::Constant && std::fabs(node->left->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) return node->right;\n     if ((node->op == '*' || node->op == '/') && node->right && node->right->type == NodeType::Constant && std::fabs(node->right->value - 1.0) < SIMPLIFY_NEAR_ONE_TOLERANCE) return node->left;\n     if (node->op == '*' && node->left && node->left->type == NodeType::Constant && std::fabs(node->left->value - 1.0) < SIMPLIFY_NEAR_ONE_TOLERANCE) return node->right;\n     if (node->op == '*' && ((node->left && node->left->type == NodeType::Constant && std::fabs(node->left->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) || (node->right && node->right->type == NodeType::Constant && std::fabs(node->right->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE))) { auto z = std::make_shared<Node>(NodeType::Constant); z->value = 0.0; return z; }\n     if (node->op == '^' && node->right && node->right->type == NodeType::Constant && std::fabs(node->right->value - 1.0) < SIMPLIFY_NEAR_ONE_TOLERANCE) return node->left; // A^1 -> A\n     if (node->op == '^' && node->right && node->right->type == NodeType::Constant && std::fabs(node->right->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) { auto o = std::make_shared<Node>(NodeType::Constant); o->value = 1.0; return o; } // A^0 -> 1\n    // Fix div by zero (constante)\n    if (node->op == '/' && node->right && node->right->type == NodeType::Constant && std::fabs(node->right->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) node->right->value = 1.0;\n\n    // --- NUEVAS REGLAS DE SIMPLIFICACI\u00d3N ---\n    // X / X = 1 (si X no es cero)\n    if (node->op == '/' && node->left && node->right) {\n        if (tree_to_string(node->left) == tree_to_string(node->right)) {\n            // Verificar que el divisor no sea cero para evitar 0/0\n            if (node->right->type != NodeType::Constant || std::fabs(node->right->value) >= SIMPLIFY_NEAR_ZERO_TOLERANCE) {\n                auto one = std::make_shared<Node>(NodeType::Constant);\n                one->value = 1.0;\n                return one;\n            }\n        }\n    }\n\n    // X - X = 0\n    if (node->op == '-' && node->left && node->right) {\n        if (tree_to_string(node->left) == tree_to_string(node->right)) {\n            auto zero = std::make_shared<Node>(NodeType::Constant);\n            zero->value = 0.0;\n            return zero;\n        }\n    }\n    // Ya no se hace clamp de exponente constante aqu\u00ed, se quit\u00f3 la restricci\u00f3n\n\n    return node;\n}\n\nNodePtr DomainConstraints::fix_or_simplify(NodePtr tree) {\n    if (!tree) return nullptr;\n    NodePtr cloned_tree = clone_tree(tree);\n    NodePtr simplified_tree = simplify_recursive(cloned_tree);\n    return simplified_tree;\n}\n\n//---------------------------------\n// Local Improvement\n//---------------------------------\n//---------------------------------\n// Local Improvement\n//---------------------------------\nvoid optimize_constants(NodePtr& tree, const std::vector<double>& targets, const std::vector<double>& x_values, double* d_targets, double* d_x_values) {\n    if (!tree) return;\n    \n    // 1. Collect constant nodes\n    std::vector<Node*> constants;\n    std::vector<Node*> stack;\n    stack.push_back(tree.get());\n    while(!stack.empty()){\n        Node* n = stack.back(); stack.pop_back();\n        if(!n) continue;\n        if(n->type == NodeType::Constant) constants.push_back(n);\n        else if(n->type == NodeType::Operator){\n            stack.push_back(n->right.get());\n            stack.push_back(n->left.get());\n        }\n    }\n    \n    if (constants.empty()) return;\n\n    // 2. Hill Climbing (Numeric Optimization)\n    int max_iter = 20; // Fast local search\n    auto& rng = get_rng();\n    \n    // Evaluate initial fitness\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n    double current_fitness = evaluate_fitness(tree, targets, x_values, d_targets, d_x_values);\n#else\n    double current_fitness = evaluate_fitness(tree, targets, x_values);\n#endif\n\n    std::normal_distribution<double> perturbation(0.0, 0.5); // Perturb standard deviation 0.5\n\n    for(int i=0; i<max_iter; ++i) {\n        // Select a random constant\n        int idx = std::uniform_int_distribution<int>(0, constants.size()-1)(rng);\n        double old_val = constants[idx]->value;\n        \n        // Perturb\n        double delta = perturbation(rng);\n        constants[idx]->value += delta;\n        if (FORCE_INTEGER_CONSTANTS) constants[idx]->value = std::round(constants[idx]->value);\n\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n        double new_fitness = evaluate_fitness(tree, targets, x_values, d_targets, d_x_values);\n#else\n        double new_fitness = evaluate_fitness(tree, targets, x_values);\n#endif\n\n        if (new_fitness < current_fitness) {\n            current_fitness = new_fitness; // Accept\n            // Adapt perturbation? Maybe reduce sigma?\n        } else {\n            constants[idx]->value = old_val; // Revert\n        }\n        \n        if (current_fitness < EXACT_SOLUTION_THRESHOLD) break;\n    }\n}\n\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\nstd::pair<NodePtr, double> try_local_improvement(const NodePtr& tree, double current_fitness, const std::vector<double>& targets, const std::vector<double>& x_values, int attempts, double* d_targets, double* d_x_values) {\n    // 1. First, try to optimize constants of the CURRENT tree\n    NodePtr optimized_tree = clone_tree(tree);\n    optimize_constants(optimized_tree, targets, x_values, d_targets, d_x_values);\n    double optimized_fitness = evaluate_fitness(optimized_tree, targets, x_values, d_targets, d_x_values);\n    \n    NodePtr best_neighbor = (optimized_fitness < current_fitness) ? optimized_tree : tree;\n    double best_neighbor_fitness = (optimized_fitness < current_fitness) ? optimized_fitness : current_fitness;\n\n    if (best_neighbor_fitness >= INF) return {best_neighbor, best_neighbor_fitness};\n\n    // 2. Structural Search (as before)\n    for (int i = 0; i < attempts; ++i) {\n        NodePtr neighbor = mutate_tree(best_neighbor, 1.0, 2); // Mutate the BEST so far\n        neighbor = DomainConstraints::fix_or_simplify(neighbor);\n        if (!neighbor) continue;\n        \n        // Also optimize constants of structural neighbor?\n        // Maybe too expensive. Let's do a quick random constant tweak.\n        // optimize_constants(neighbor, targets, x_values, d_targets, d_x_values); \n        \n        double neighbor_fitness = evaluate_fitness(neighbor, targets, x_values, d_targets, d_x_values);\n        if (neighbor_fitness < best_neighbor_fitness) {\n            best_neighbor = neighbor;\n            best_neighbor_fitness = neighbor_fitness;\n        }\n    }\n    return {best_neighbor, best_neighbor_fitness};\n}\n#else\nstd::pair<NodePtr, double> try_local_improvement(const NodePtr& tree, double current_fitness, const std::vector<double>& targets, const std::vector<double>& x_values, int attempts) {\n    // 1. First, try to optimize constants of the CURRENT tree\n    NodePtr optimized_tree = clone_tree(tree);\n    optimize_constants(optimized_tree, targets, x_values, nullptr, nullptr);\n    double optimized_fitness = evaluate_fitness(optimized_tree, targets, x_values);\n    \n    NodePtr best_neighbor = (optimized_fitness < current_fitness) ? optimized_tree : tree;\n    double best_neighbor_fitness = (optimized_fitness < current_fitness) ? optimized_fitness : current_fitness;\n\n    if (best_neighbor_fitness >= INF) return {best_neighbor, best_neighbor_fitness};\n\n    for (int i = 0; i < attempts; ++i) {\n        NodePtr neighbor = mutate_tree(best_neighbor, 1.0, 2);\n        neighbor = DomainConstraints::fix_or_simplify(neighbor);\n        if (!neighbor) continue;\n        double neighbor_fitness = evaluate_fitness(neighbor, targets, x_values);\n        if (neighbor_fitness < best_neighbor_fitness) {\n            best_neighbor = neighbor;\n            best_neighbor_fitness = neighbor_fitness;\n        }\n    }\n    return {best_neighbor, best_neighbor_fitness};\n}\n#endif\n\n//---------------------------------\n// Target Pattern Detection\n//---------------------------------\nstd::pair<std::string, double> detect_target_pattern(const std::vector<double>& targets) {\n    if (targets.size() < 3) return {\"none\", 0.0};\n    bool is_arithmetic = true; double diff = targets[1] - targets[0];\n    for (size_t i = 2; i < targets.size(); ++i) if (std::fabs((targets[i] - targets[i-1]) - diff) > 1e-6) { is_arithmetic = false; break; }\n    if (is_arithmetic) return {\"arithmetic\", diff};\n    bool is_geometric = true;\n    if (std::fabs(targets[0]) < 1e-9) {\n        bool all_zero = true; for(double t : targets) if (std::fabs(t) > 1e-9) { all_zero = false; break; }\n        if(all_zero) return {\"constant_zero\", 0.0}; else is_geometric = false;\n    }\n    if (is_geometric && std::fabs(targets[0]) >= 1e-9) {\n        double ratio = targets[1] / targets[0];\n        for (size_t i = 2; i < targets.size(); ++i) {\n             if (std::fabs(targets[i-1]) < 1e-9) { if (std::fabs(targets[i]) > 1e-9) { is_geometric = false; break; } }\n             else { if (std::fabs((targets[i] / targets[i-1]) - ratio) > 1e-6) { is_geometric = false; break; } }\n        }\n        if (is_geometric) return {\"geometric\", ratio};\n    }\n    return {\"none\", 0.0};\n}\n\n//---------------------------------\n// Generate Pattern Based Tree\n//---------------------------------\nNodePtr generate_pattern_based_tree(const std::string& pattern_type, double pattern_value) {\n    if (X_VALUES.empty() || RAW_TARGETS.empty()) return nullptr;\n    double a = RAW_TARGETS[0]; double x0 = X_VALUES[0];\n    if (pattern_type == \"arithmetic\") {\n        double d = pattern_value; auto root = std::make_shared<Node>(NodeType::Operator); root->op = '+';\n        auto cp = std::make_shared<Node>(NodeType::Constant); double cv = a - d * x0; if (FORCE_INTEGER_CONSTANTS) cv = std::round(cv); cp->value = (std::fabs(cv) < SIMPLIFY_NEAR_ZERO_TOLERANCE) ? 0.0 : cv; // Use RAW_TARGETS to avoid \"TARGETS\" not found\n\n        auto vp = std::make_shared<Node>(NodeType::Operator); vp->op = '*';\n        auto dc = std::make_shared<Node>(NodeType::Constant); double dv = d; if (FORCE_INTEGER_CONSTANTS) dv = std::round(dv); dc->value = (std::fabs(dv) < SIMPLIFY_NEAR_ZERO_TOLERANCE) ? 0.0 : dv;\n        auto xv = std::make_shared<Node>(NodeType::Variable); vp->left = dc; vp->right = xv;\n        root->left = cp; root->right = vp; return DomainConstraints::fix_or_simplify(root);\n    } else if (pattern_type == \"geometric\") {\n        double r = pattern_value; if (std::fabs(r) < SIMPLIFY_NEAR_ZERO_TOLERANCE) return nullptr;\n        auto root = std::make_shared<Node>(NodeType::Operator); root->op = '*';\n        auto cp = std::make_shared<Node>(NodeType::Constant); double rpx0 = std::pow(r, x0); if (std::fabs(rpx0) < 1e-100) return nullptr;\n        double cv = a / rpx0; if (FORCE_INTEGER_CONSTANTS) cv = std::round(cv); cp->value = (std::fabs(cv) < SIMPLIFY_NEAR_ZERO_TOLERANCE) ? 0.0 : cv;\n        auto vp = std::make_shared<Node>(NodeType::Operator); vp->op = '^';\n        auto rc = std::make_shared<Node>(NodeType::Constant); double rv = r; if (FORCE_INTEGER_CONSTANTS) rv = std::round(rv); rc->value = (std::fabs(rv) < SIMPLIFY_NEAR_ZERO_TOLERANCE) ? 0.0 : rv;\n        auto xv = std::make_shared<Node>(NodeType::Variable); vp->left = rc; vp->right = xv;\n        root->left = cp; root->right = vp; return DomainConstraints::fix_or_simplify(root);\n    } else if (pattern_type == \"constant_zero\") {\n         auto node = std::make_shared<Node>(NodeType::Constant); node->value = 0.0; return node;\n     }\n    return nullptr; // No pattern tree generated\n}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/AdvancedFeatures.h\n",
        "#ifndef ADVANCEDFEATURES_H\n#define ADVANCEDFEATURES_H\n\n#include \"ExpressionTree.h\"\n#include \"Globals.h\" // Incluir Globals.h para INF\n#include <vector>\n#include <string>\n#include <map>\n#include <set>\n#include <utility> // Para std::pair\n#include <unordered_map>\n\n// Meta-evoluci\u00f3n: Par\u00e1metros que pueden adaptarse durante la ejecuci\u00f3n.\nstruct EvolutionParameters {\n    double mutation_rate;    // Tasa de mutaci\u00f3n actual\n    double elite_percentage; // Porcentaje de \u00e9lite actual\n    int tournament_size;     // Tama\u00f1o del torneo actual\n    double crossover_rate;   // Tasa de cruce actual\n\n    // Crea un conjunto de par\u00e1metros con valores por defecto (iniciales).\n    static EvolutionParameters create_default();\n\n    // Adapta (muta) los par\u00e1metros ligeramente.\n    // AHORA RECIBE el contador de estancamiento para ajustar la intensidad.\n    void mutate(int stagnation_counter);\n};\n\n// Memoria de patrones: Almacena sub-estructuras exitosas (Reinforcement Learning).\nclass PatternMemory {\n    struct PatternInfo {\n        std::string pattern_str; // Representaci\u00f3n del patr\u00f3n\n        double best_fitness = INF; // Mejor fitness visto para este patr\u00f3n\n        int uses = 0;             // N\u00famero de veces usado/visto\n        double success_rate = 0.0; // Tasa de \u00e9xito estimada\n    };\n    std::unordered_map<std::string, PatternInfo> patterns; // Mapa para almacenar patrones\n    int min_uses_for_suggestion = 3; // M\u00ednimo de usos para considerar sugerir un patr\u00f3n\n\npublic:\n    // Registra el \u00e9xito de un \u00e1rbol (y su patr\u00f3n) basado en su fitness.\n    void record_success(const NodePtr& tree, double fitness);\n    // Sugiere un \u00e1rbol basado en los patrones exitosos almacenados.\n    NodePtr suggest_pattern_based_tree(int max_depth);\n\nprivate:\n    // Extrae la representaci\u00f3n estructural (string) de un \u00e1rbol.\n    std::string extract_pattern(const NodePtr& tree);\n    // Intenta construir un \u00e1rbol a partir de un patr\u00f3n (string) - funci\u00f3n simplificada.\n    NodePtr parse_pattern(const std::string& pattern, int max_depth);\n};\n\n\n// Optimizaci\u00f3n Pareto: Mantiene un frente de soluciones no dominadas (compromiso precisi\u00f3n/complejidad).\nstruct ParetoSolution {\n    NodePtr tree = nullptr;   // \u00c1rbol de la soluci\u00f3n\n    double accuracy = INF;    // Objetivo 1: Precisi\u00f3n (fitness)\n    double complexity = INF;  // Objetivo 2: Complejidad (tama\u00f1o)\n    bool dominated = false;   // Bandera: \u00bfest\u00e1 dominada por otra soluci\u00f3n?\n\n    // Constructor por defecto (necesario si se usa en contenedores)\n    ParetoSolution() = default;\n    // Constructor principal\n    ParetoSolution(NodePtr t, double acc, double complexity_val);\n\n    // Comprueba si esta soluci\u00f3n domina a otra.\n    bool dominates(const ParetoSolution& other) const;\n};\n\nclass ParetoOptimizer {\n    std::vector<ParetoSolution> pareto_front; // Almacena las soluciones del frente\n    size_t max_front_size = 50; // L\u00edmite opcional para el tama\u00f1o del frente\n\npublic:\n    // Actualiza el frente de Pareto con individuos de la poblaci\u00f3n actual.\n    void update(const std::vector<struct Individual>& population, // Usa Individual struct\n                const std::vector<double>& targets,\n                const std::vector<double>& x_values);\n\n    // Obtiene los \u00e1rboles (NodePtr) de las soluciones en el frente actual.\n    std::vector<NodePtr> get_pareto_solutions();\n\n    // Obtiene una referencia constante al frente de Pareto completo.\n    const std::vector<ParetoSolution>& get_pareto_front() const { return pareto_front; }\n};\n\n\n// Restricciones de Dominio: Verifica y corrige/simplifica \u00e1rboles problem\u00e1ticos.\nclass DomainConstraints {\npublic:\n    // Comprueba si un \u00e1rbol cumple reglas b\u00e1sicas de validez est\u00e1tica.\n    static bool is_valid(const NodePtr& tree);\n\n    // Intenta simplificar/corregir un \u00e1rbol (devuelve una copia modificada).\n    static NodePtr fix_or_simplify(NodePtr tree);\n\nprivate:\n     // Ayudante recursivo para la simplificaci\u00f3n.\n    static NodePtr simplify_recursive(NodePtr node);\n    // Ayudante recursivo para la validaci\u00f3n est\u00e1tica.\n    static bool is_valid_recursive(const NodePtr& node);\n};\n\n// B\u00fasqueda Local: Intenta mejorar una soluci\u00f3n dada explorando vecinos cercanos.\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\nstd::pair<NodePtr, double> try_local_improvement(const NodePtr& tree,\n                                                  double current_fitness,\n                                                  const std::vector<double>& targets,\n                                                  const std::vector<double>& x_values,\n                                                  int attempts,\n                                                  double* d_targets, double* d_x_values);\n#else\nstd::pair<NodePtr, double> try_local_improvement(const NodePtr& tree,\n                                                  double current_fitness,\n                                                  const std::vector<double>& targets,\n                                                  const std::vector<double>& x_values,\n                                                  int attempts = 10);\n#endif\n\n\n// Detecci\u00f3n de Patrones en los Datos Objetivo.\nstd::pair<std::string, double> detect_target_pattern(const std::vector<double>& targets);\nNodePtr generate_pattern_based_tree(const std::string& pattern_type, double pattern_value);\n\n\n#endif // ADVANCEDFEATURES_H\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/ExpressionTree.cpp\n",
        "#include \"ExpressionTree.h\"\n#include \"Globals.h\"\n#include <cmath>\n#include <limits>\n#include <stdexcept>\n#include <vector>\n#include <iostream>\n#include <iomanip>\n#include <string>\n#include <sstream>\n#include <stack>\n#include <unordered_map>\n#include <algorithm> // Para std::remove_if\n#include <cctype>    // Para isdigit, isspace\n#include <thread>    // Para thread_local RNG\n\n// --- Funci\u00f3n auxiliar para formatear constantes ---\n// --- Funci\u00f3n auxiliar para formatear constantes ---\nstd::string format_constant(double val) {\n    // Si es un entero o muy cercano a un entero, formatarlo como tal.\n    if (std::fabs(val - std::round(val)) < SIMPLIFY_NEAR_ZERO_TOLERANCE) {\n        return std::to_string(static_cast<long long>(std::round(val)));\n    } else {\n        std::ostringstream oss;\n        // Usar notaci\u00f3n cient\u00edfica para valores muy grandes o muy peque\u00f1os,\n        // o notaci\u00f3n fija para el resto, con precisi\u00f3n adecuada.\n        // Esto evita cadenas muy largas o p\u00e9rdida de informaci\u00f3n.\n        if (std::fabs(val) >= 1e6 || std::fabs(val) <= 1e-6) { // Umbrales ajustables\n            oss << std::scientific << std::setprecision(8) << val;\n        } else {\n            oss << std::fixed << std::setprecision(8) << val;\n        }\n        \n        std::string s = oss.str();\n        // Eliminar ceros finales y el punto decimal si no hay parte fraccionaria\n        // Esto puede ser delicado con std::scientific, as\u00ed que hay que ser cuidadosos.\n        // Para std::fixed:\n        if (s.find('.') != std::string::npos) {\n            s.erase(s.find_last_not_of('0') + 1, std::string::npos);\n            if (!s.empty() && s.back() == '.') s.pop_back();\n        }\n        return s.empty() ? \"0\" : s;\n    }\n}\n\n// --- evaluate_tree ---\ndouble evaluate_tree(const NodePtr& node, double x) {\n    if (!node) return std::nan(\"\");\n    switch (node->type) {\n        case NodeType::Constant: return node->value;\n        case NodeType::Variable: return x;\n        case NodeType::Operator: {\n            // Determine arity\n            bool is_unary = (node->op == 's' || node->op == 'c' || node->op == 'l' || node->op == 'e' || node->op == '!' || node->op == '_' || node->op == 'g' || node->op == 't' || node->op == 'q' || node->op == 'a' || node->op == 'n' || node->op == 'u');\n\n            double leftVal = evaluate_tree(node->left, x);\n            double rightVal = 0.0;\n            if (!is_unary) {\n                rightVal = evaluate_tree(node->right, x);\n            }\n\n            if (std::isnan(leftVal)) return std::nan(\"\");\n            if (!is_unary && std::isnan(rightVal)) return std::nan(\"\");\n\n            double result = std::nan(\"\");\n            try {\n                switch (node->op) {\n                    case '+': result = leftVal + rightVal; break;\n                    case '-': result = leftVal - rightVal; break;\n                    case '*': result = leftVal * rightVal; break;\n                    case '/':\n                        if (std::fabs(rightVal) < SIMPLIFY_NEAR_ZERO_TOLERANCE) return INF;\n                        result = leftVal / rightVal;\n                        break;\n                    case '^':\n                        if (leftVal == 0.0 && rightVal == 0.0) result = 1.0;\n                        else if (leftVal == 0.0 && rightVal < 0.0) return INF;\n                        else if (leftVal < 0.0 && std::fabs(rightVal - std::round(rightVal)) > SIMPLIFY_NEAR_ZERO_TOLERANCE) return INF;\n                        else result = std::pow(leftVal, rightVal);\n                        break;\n                    case '%':\n                        if (std::fabs(rightVal) < SIMPLIFY_NEAR_ZERO_TOLERANCE) return INF;\n                        result = std::fmod(leftVal, rightVal);\n                        break;\n                    case 's': result = std::sin(leftVal); break;\n                    case 'c': result = std::cos(leftVal); break;\n                    case 't': result = std::tan(leftVal); break;\n                    case 'q': \n                        // Protected Sqrt: sqrt(|x|)\n                        result = std::sqrt(std::abs(leftVal)); \n                        break;\n                    case 'a': result = std::abs(leftVal); break;\n                    case 'n': result = (leftVal > 0) ? 1.0 : ((leftVal < 0) ? -1.0 : 0.0); break;\n                    case 'l': \n                        // Protected Log: log(|x|)\n                        if (std::abs(leftVal) <= 1e-9) return INF; \n                        result = std::log(std::abs(leftVal)); \n                        break;\n                    case 'e': \n                        if (leftVal > 700.0) return INF; // Overflow check\n                        result = std::exp(leftVal); \n                        break;\n                    case '!': \n                        // Protected Factorial/Gamma: tgamma(|x|+1)\n                        if (std::abs(leftVal) > 170.0) return INF; \n                        result = std::tgamma(std::abs(leftVal) + 1.0); \n                        break;\n                    case '_': result = std::floor(leftVal); break;\n                    case 'u': result = std::ceil(leftVal); break; // 'u' for ceil (up)\n                    case 'g':\n                        result = std::lgamma(std::abs(leftVal) + 1.0); \n                        break;\n                    default: return std::nan(\"\");\n                }\n            } catch (const std::exception& e) { return INF; }\n            if (std::isinf(result)) return INF;\n            if (std::isnan(result)) return std::nan(\"\");\n            return result;\n        }\n        default: return std::nan(\"\");\n    }\n}\n\n// --- tree_to_string ---\nstd::string tree_to_string(const NodePtr& node) {\n     if (!node) return \"NULL\";\n     switch (node->type) {\n        case NodeType::Constant: return format_constant(node->value);\n        case NodeType::Variable: return \"x\";\n        case NodeType::Operator: {\n            NodePtr left_node = node->left;\n            std::string left_str = tree_to_string(left_node);\n            \n            // Check arity\n            bool is_unary = (node->op == 's' || node->op == 'c' || node->op == 'l' || node->op == 'e' || node->op == '!' || node->op == '_' || node->op == 'g' || node->op == 't' || node->op == 'q' || node->op == 'a' || node->op == 'n' || node->op == 'u');\n\n            if (is_unary) {\n                switch(node->op) {\n                    case 's': return \"sin(\" + left_str + \")\";\n                    case 'c': return \"cos(\" + left_str + \")\";\n                    case 't': return \"tan(\" + left_str + \")\";\n                    case 'q': return \"sqrt(\" + left_str + \")\";\n                    case 'a': return \"abs(\" + left_str + \")\";\n                    case 'n': return \"sign(\" + left_str + \")\";\n                    case 'l': return \"log(\" + left_str + \")\";\n                    case 'e': return \"exp(\" + left_str + \")\";\n                    case '!': return \"(\" + left_str + \")!\"; // Postfix for factorial\n                    case '_': return \"floor(\" + left_str + \")\";\n                    case 'u': return \"ceil(\" + left_str + \")\";\n                    case 'g': return \"lgamma(\" + left_str + \")\";\n                    default: return \"op(\" + left_str + \")\";\n                }\n            }\n\n            NodePtr right_node = node->right;\n            std::string right_str = tree_to_string(right_node);\n            char current_op = node->op;\n            bool right_is_neg_const = (right_node && right_node->type == NodeType::Constant && right_node->value < 0.0);\n            if (right_is_neg_const) {\n                double abs_right_val = std::fabs(right_node->value);\n                std::string abs_right_str = format_constant(abs_right_val);\n                if (node->op == '+') { current_op = '-'; right_str = abs_right_str; }\n                else if (node->op == '-') { current_op = '+'; right_str = abs_right_str; }\n            }\n            // Simplificar impresi\u00f3n de (0-A) a (-A)\n            if (left_node && left_node->type == NodeType::Constant && left_node->value == 0.0 && current_op == '-') {\n                 return \"(-\" + right_str + \")\";\n            }\n            return \"(\" + left_str + current_op + right_str + \")\";\n        }\n        default: return \"?\";\n    }\n}\n\n// --- tree_size ---\nint tree_size(const NodePtr& node) {\n    if (!node) return 0;\n    if (node->type == NodeType::Constant || node->type == NodeType::Variable) return 1;\n    if (node->type == NodeType::Operator) {\n        return 1 + tree_size(node->left) + tree_size(node->right);\n    }\n    return 0;\n}\n\n// --- clone_tree ---\nNodePtr clone_tree(const NodePtr& node) {\n    if (!node) return nullptr;\n    auto new_node = std::make_shared<Node>();\n    new_node->type = node->type;\n    new_node->value = node->value;\n    new_node->op = node->op;\n    new_node->left = clone_tree(node->left);\n    new_node->right = clone_tree(node->right);\n    return new_node;\n}\n\n// --- collect_node_ptrs ---\nvoid collect_node_ptrs(NodePtr& node, std::vector<NodePtr*>& vec) {\n    if (!node) return;\n    vec.push_back(&node);\n    if (node->type == NodeType::Operator) {\n        collect_node_ptrs(node->left, vec);\n        collect_node_ptrs(node->right, vec);\n    }\n}\n\n// --- get_rng ---\n// === OPTIMIZACI\u00d3N: RNG thread-local para evitar contenci\u00f3n en OpenMP ===\nstd::mt19937& get_rng() {\n    thread_local std::mt19937 local_rng(\n        std::random_device{}() ^ \n        static_cast<unsigned>(std::hash<std::thread::id>{}(std::this_thread::get_id()))\n    );\n    return local_rng;\n}\n\n// --- get_tree_depth ---\nint get_tree_depth(const NodePtr& node) {\n    if (!node) return 0;\n    if (node->type != NodeType::Operator) return 1;\n    return 1 + std::max(get_tree_depth(node->left), get_tree_depth(node->right));\n}\n\n// --- trim_tree ---\nvoid trim_tree(NodePtr& node, int max_depth) {\n    if (!node) return;\n    if (max_depth <= 1) {\n        // Force terminal if we reached depth limit\n        if (node->type == NodeType::Operator) {\n            // Replace with minimal terminal (Variable 'x' or Constant 1.0)\n            // Using 'x' is generally safer for retaining some logic, but 1.0 is neutral for *\n            // Let's pick a random terminal to avoid bias? \n            // For now, let's just make it a variable 'x' as it's often more useful than a constant 0 or 1.\n             node->type = NodeType::Variable;\n             node->op = 0;\n             node->left = nullptr;\n             node->right = nullptr;\n             // value ignored for variable\n        }\n        return;\n    }\n    \n    if (node->type == NodeType::Operator) {\n        trim_tree(node->left, max_depth - 1);\n        trim_tree(node->right, max_depth - 1);\n    }\n}\n\n\n// ============================================================\n// --- Parser de F\u00f3rmulas desde String (v4 - Parser Corregido) ---\n// ============================================================\n\n// Helper para obtener precedencia de operadores\nint get_precedence(char op) {\n    switch (op) {\n        case '+': case '-': return 1;\n        case '*': case '/': case '%': return 2;\n        case '^': return 3;\n        default: return 0;\n    }\n}\n\n// Helper para aplicar un operador binario\nNodePtr apply_binary_operation(NodePtr right, NodePtr left, char op) {\n    if (!left || !right) {\n        throw std::runtime_error(\"Error al aplicar operaci\u00f3n binaria '\" + std::string(1, op) + \"': operandos insuficientes.\");\n    }\n    auto node = std::make_shared<Node>(NodeType::Operator);\n    node->op = op;\n    node->left = left;\n    node->right = right;\n    return node;\n}\n\n// Funci\u00f3n principal para parsear la f\u00f3rmula\nNodePtr parse_formula_string(const std::string& formula_raw) {\n    std::string formula = formula_raw;\n    formula.erase(std::remove_if(formula.begin(), formula.end(), ::isspace), formula.end());\n    if (formula.empty()) throw std::runtime_error(\"La f\u00f3rmula est\u00e1 vac\u00eda.\");\n\n    std::stack<NodePtr> operand_stack;\n    std::stack<char> operator_stack;\n\n    // Funci\u00f3n interna para procesar operadores seg\u00fan precedencia y asociatividad\n    auto process_operators_by_precedence = [&](int current_precedence, char current_op_char = 0) {\n        // La asociatividad derecha para '^' significa que se procesa si el operador en la pila\n        // tiene MAYOR precedencia, no MAYOR O IGUAL.\n        bool is_right_associative = (current_op_char == '^');\n\n        while (!operator_stack.empty() && operator_stack.top() != '(') {\n            char top_op = operator_stack.top();\n            int top_precedence = get_precedence(top_op);\n\n            if (is_right_associative ? (top_precedence > current_precedence) : (top_precedence >= current_precedence)) {\n                operator_stack.pop(); // Sacar operador de la pila\n                if (operand_stack.size() < 2) throw std::runtime_error(\"Operandos insuficientes para operador '\" + std::string(1, top_op) + \"'.\");\n                NodePtr right = operand_stack.top(); operand_stack.pop();\n                NodePtr left = operand_stack.top(); operand_stack.pop();\n                operand_stack.push(apply_binary_operation(right, left, top_op));\n            } else {\n                break; // Parar si la precedencia es menor o si es asociativo a la derecha y es igual\n            }\n        }\n    };\n\n    bool last_token_was_operand = false;\n\n    for (int i = 0; i < formula.length(); /* Incremento manual */ ) {\n        char token = formula[i];\n\n        // --- A. Parsear N\u00fameros ---\n        bool starts_number = isdigit(token) || (token == '.' && i + 1 < formula.length() && isdigit(formula[i+1]));\n        if (starts_number) {\n             if (last_token_was_operand) { // Implicit multiplication\n                 process_operators_by_precedence(get_precedence('*'));\n                 operator_stack.push('*');\n                 last_token_was_operand = false;\n             }\n            std::string num_str;\n            if (token == '.') num_str += '0';\n            num_str += token;\n            i++;\n            while (i < formula.length() && (isdigit(formula[i]) || (formula[i] == '.' && num_str.find('.') == std::string::npos))) {\n                num_str += formula[i];\n                i++;\n            }\n            try {\n                double value = std::stod(num_str);\n                auto node = std::make_shared<Node>(NodeType::Constant); node->value = value;\n                operand_stack.push(node);\n                last_token_was_operand = true;\n            } catch (const std::invalid_argument& e) {\n                throw std::runtime_error(\"N\u00famero inv\u00e1lido (formato): '\" + num_str + \"' - \" + e.what());\n            } catch (const std::out_of_range& e) {\n                throw std::runtime_error(\"N\u00famero inv\u00e1lido (rango): '\" + num_str + \"' - \" + e.what());\n            }\n            continue;\n        }\n\n        // --- B. Parsear Funciones Unarias y Constantes ---\n        std::unordered_map<std::string, char> func_map = {\n            {\"sin\", 's'}, {\"cos\", 'c'}, {\"tan\", 't'}, \n            {\"log\", 'l'}, {\"exp\", 'e'}, {\"sqrt\", 'q'},\n            {\"floor\", '_'}, {\"ceil\", '^'}, {\"abs\", 'a'}, {\"sign\", 'n'},\n            {\"gamma\", '!'}, {\"lgamma\", 'g'}, {\"g\", 'g'}\n        };\n\n        // Special handling for Constants (pi, e, C)\n        if (token == 'C') {\n             if (last_token_was_operand) { // Implicit multiplication\n                 process_operators_by_precedence(get_precedence('*'));\n                 operator_stack.push('*');\n             }\n             auto node = std::make_shared<Node>(NodeType::Constant); \n             // Default constant value (will be optimized later)\n             node->value = 1.0; \n             // Mark it specifically as an optimizable constant in a way that clone/optimize respects?\n             // Actually, for C++ GP, usually constants are just numbers. \n             // But if we want to preserve 'C' semantics:\n             // Let's treat 'C' as a special Variable? No, Variable is x.\n             // Let's just treat it as 1.0 for now, or use a special Op 'C'?\n             // The system typically optimizes *numeric* constants attached to nodes.\n             // If we parse 'C', we should probably parse it as a random constant?\n             // Or better, a Constant node with a placeholder value.\n             // Re-reading ExpressionTree.h might help, but let's stick to 1.0 for now.\n             operand_stack.push(node);\n             last_token_was_operand = true;\n             i++;\n             continue;\n        }\n        if (i + 1 < formula.length() && formula.substr(i, 2) == \"pi\") {\n             if (last_token_was_operand) {\n                 process_operators_by_precedence(get_precedence('*'));\n                 operator_stack.push('*');\n             }\n             auto node = std::make_shared<Node>(NodeType::Constant); node->value = 3.14159265359;\n             operand_stack.push(node);\n             last_token_was_operand = true;\n             i += 2;\n             continue;\n        }\n        if (token == 'e' && (i+1 >= formula.length() || formula[i+1] != 'x')) { // Check it's not 'exp'\n             // Handle 'e' constant\n             if (last_token_was_operand) {\n                 process_operators_by_precedence(get_precedence('*'));\n                 operator_stack.push('*');\n             }\n             auto node = std::make_shared<Node>(NodeType::Constant); node->value = 2.71828182846;\n             operand_stack.push(node);\n             last_token_was_operand = true;\n             i++;\n             continue;\n        }\n        \n        // Try to match function names (check longer names first)\n        bool matched_func = false;\n        for (const auto& [func_name, func_op] : func_map) {\n            if (i + func_name.length() <= formula.length() && \n                formula.substr(i, func_name.length()) == func_name &&\n                (i + func_name.length() >= formula.length() || formula[i + func_name.length()] == '(')) {\n                \n                // Check if this is actually a function call (followed by '(')\n                size_t after_name = i + func_name.length();\n                if (after_name < formula.length() && formula[after_name] == '(') {\n                    if (last_token_was_operand) { // Implicit multiplication\n                        process_operators_by_precedence(get_precedence('*'));\n                        operator_stack.push('*');\n                        last_token_was_operand = false;\n                    }\n                    \n                    // Find the matching closing parenthesis\n                    int paren_count = 1;\n                    size_t arg_start = after_name + 1;\n                    size_t j = arg_start;\n                    while (j < formula.length() && paren_count > 0) {\n                        if (formula[j] == '(') paren_count++;\n                        else if (formula[j] == ')') paren_count--;\n                        j++;\n                    }\n                    if (paren_count != 0) {\n                        throw std::runtime_error(\"Par\u00e9ntesis sin cerrar en funci\u00f3n '\" + func_name + \"'.\");\n                    }\n                    size_t arg_end = j - 1; // Position of closing ')'\n                    \n                    // Extract and recursively parse the argument\n                    std::string arg_str = formula.substr(arg_start, arg_end - arg_start);\n                    NodePtr arg_tree = parse_formula_string(arg_str);\n                    \n                    // Create unary operator node\n                    auto func_node = std::make_shared<Node>(NodeType::Operator);\n                    func_node->op = func_op;\n                    func_node->left = arg_tree;\n                    func_node->right = nullptr;\n                    \n                    operand_stack.push(func_node);\n                    last_token_was_operand = true;\n                    i = j; // Skip past the closing ')'\n                    matched_func = true;\n                    break;\n                }\n            }\n        }\n        if (matched_func) continue;\n\n        // --- C. Parsear Variable 'x' ---\n        if (token == 'x') {\n            if (last_token_was_operand) { // Implicit multiplication\n                 process_operators_by_precedence(get_precedence('*'));\n                 operator_stack.push('*');\n                 last_token_was_operand = false;\n            }\n            auto node = std::make_shared<Node>(NodeType::Variable);\n            operand_stack.push(node);\n            last_token_was_operand = true;\n            i++;\n            continue;\n        }\n\n        // --- D. Parsear Par\u00e9ntesis de Apertura '(' ---\n        if (token == '(') {\n            if (last_token_was_operand) { // Implicit multiplication\n                 process_operators_by_precedence(get_precedence('*'));\n                 operator_stack.push('*');\n                 last_token_was_operand = false;\n            }\n            operator_stack.push('(');\n            last_token_was_operand = false;\n            i++;\n            continue;\n        }\n\n        // --- E. Parsear Par\u00e9ntesis de Cierre ')' ---\n        if (token == ')') {\n             if (!last_token_was_operand) {\n                  if (!operator_stack.empty() && operator_stack.top() == '(') throw std::runtime_error(\"Par\u00e9ntesis vac\u00edos '()' encontrados.\");\n                  else throw std::runtime_error(\"Se esperaba un operando antes de ')'.\");\n             }\n            while (!operator_stack.empty() && operator_stack.top() != '(') {\n                process_operators_by_precedence(0);\n            }\n            if (operator_stack.empty()) throw std::runtime_error(\"Par\u00e9ntesis ')' sin correspondiente '('.\");\n            operator_stack.pop(); // Sacar '('\n            last_token_was_operand = true;\n            i++;\n            continue;\n        }\n\n        // --- F. Parsear Operadores (+ - * / ^ %) ---\n        if (std::string(\"+-*/^%\").find(token) != std::string::npos) {\n            // Manejar '-' unario vs binario\n            if (token == '-' && !last_token_was_operand) {\n                // Es un '-' unario. Insertar un 0 como operando izquierdo impl\u00edcito.\n                // Esto permite tratar el '-' como un operador binario normal.\n                auto zero_node = std::make_shared<Node>(NodeType::Constant); zero_node->value = 0.0;\n                operand_stack.push(zero_node);\n                // No cambiar last_token_was_operand a true, ya que el 0 impl\u00edcito\n                // es solo para el operador unario y no un operando \"real\" previo.\n                // Si hubiera una multiplicaci\u00f3n impl\u00edcita (ej. \"2-x\"), ya se habr\u00eda manejado.\n            }\n            // Ignorar '+' unario (no afecta el valor, no necesita un 0 impl\u00edcito)\n            else if (token == '+' && !last_token_was_operand) {\n                // No hacer nada, simplemente avanzar al siguiente token\n                i++;\n                continue;\n            }\n            \n            // Operador binario normal\n            if (!last_token_was_operand && (token == '*' || token == '/' || token == '^' || token == '%')) {\n                throw std::runtime_error(\"Operador binario '\" + std::string(1, token) + \"' inesperado. Se esperaba operando.\");\n            }\n\n            // Procesar operadores en la pila con mayor o igual precedencia (o solo mayor para asociativos a derecha)\n            process_operators_by_precedence(get_precedence(token), token);\n            operator_stack.push(token);\n            last_token_was_operand = false; // Despu\u00e9s de un operador, se espera un operando\n            i++;\n            continue;\n        }\n\n        // --- G. Token Desconocido ---\n        throw std::runtime_error(\"Token desconocido en la f\u00f3rmula: '\" + std::string(1, token) + \"'\");\n\n    } // Fin del bucle for\n\n    // --- H. Procesamiento Final despu\u00e9s del bucle ---\n    while (!operator_stack.empty()) {\n        if (operator_stack.top() == '(') throw std::runtime_error(\"Par\u00e9ntesis '(' sin cerrar al final.\");\n        // Procesar todos los operadores restantes en la pila\n        process_operators_by_precedence(0); // 0 como precedencia m\u00ednima para forzar el procesamiento\n    }\n\n    // Verificaci\u00f3n final de la pila de operandos\n    if (operand_stack.size() != 1) {\n         if (operand_stack.empty() && formula.length() > 0) throw std::runtime_error(\"Error: No se gener\u00f3 ning\u00fan resultado del parseo. F\u00f3rmula inv\u00e1lida?\");\n         else if (operand_stack.size() > 1) throw std::runtime_error(\"Error en la estructura final (operandos restantes: \" + std::to_string(operand_stack.size()) + \"). Verifique operadores.\");\n         else throw std::runtime_error(\"Error desconocido al finalizar el parseo.\");\n    }\n\n    return operand_stack.top();\n}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/ExpressionTree.h\n",
        "#ifndef EXPRESSIONTREE_H\n#define EXPRESSIONTREE_H\n\n#include <memory>\n#include <string>\n#include <vector>\n#include <stdexcept> // Para std::runtime_error\n\n// Forward declaration\nstruct Node;\n\n// Use shared_ptr for automatic memory management\nusing NodePtr = std::shared_ptr<Node>;\n\nenum class NodeType { Constant, Variable, Operator };\n\nstruct Node {\n    NodeType type;\n    double value = 0.0;             // If type == Constant\n    char op = 0;                    // If type == Operator: '+', '-', '*', '/', '^'\n    NodePtr left = nullptr;         // Children (for Operators)\n    NodePtr right = nullptr;\n\n    // Constructor for convenience\n    Node(NodeType t = NodeType::Constant) : type(t) {}\n};\n\n// Core Tree Functions\ndouble evaluate_tree(const NodePtr& node, double x);\nstd::string tree_to_string(const NodePtr& node);\nint tree_size(const NodePtr& node);\nNodePtr clone_tree(const NodePtr& node);\nint get_tree_depth(const NodePtr& node);\nvoid trim_tree(NodePtr& node, int max_depth);\n\n// Helper for mutation/crossover\nvoid collect_node_ptrs(NodePtr& node, std::vector<NodePtr*>& vec);\n\n// --- NUEVO: Funci\u00f3n para parsear una f\u00f3rmula desde string ---\n// Parsea una f\u00f3rmula en notaci\u00f3n infija simple (con par\u00e9ntesis).\n// Lanza std::runtime_error si hay error de sintaxis.\nNodePtr parse_formula_string(const std::string& formula);\n\n\n#endif // EXPRESSIONTREE_H\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/Fitness.cpp\n",
        "#include \"Fitness.h\"\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n#include \"FitnessGPU.cuh\" // Include for GPU fitness evaluation\n#endif\n#include \"Globals.h\" // Necesario para constantes globales e INF\n#include \"ExpressionTree.h\" // Necesario para tree_to_string\n#include <cmath>\n#include <limits>\n#include <vector>\n#include <numeric>\n#include <iostream> // Para std::cerr en caso de error futuro\n#include <iomanip>  // Para std::fixed/scientific si se necesita en errores\n\n// Calculates the raw fitness using global parameters.\n// This function will now dispatch to GPU if USE_GPU_ACCELERATION is enabled.\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\ndouble calculate_raw_fitness(const NodePtr& tree,\n                             const std::vector<double>& targets,\n                             const std::vector<double>& x_values,\n                             double* d_targets, double* d_x_values) {\n    // If GPU pointers are null (FORCE_CPU_MODE), use CPU evaluation\n    if (d_targets == nullptr || d_x_values == nullptr) {\n        // CPU fallback implementation\n        if (x_values.size() != targets.size() || x_values.empty()) return INF;\n\n        double sum_sq_error = 0.0;\n        double total_weight = 0.0;\n        bool all_precise = true;\n        size_t num_points = x_values.size();\n        bool calculation_failed = false;\n\n        for (size_t i = 0; i < num_points; ++i) {\n            double predicted_val = evaluate_tree(tree, x_values[i]);\n\n            if (std::isnan(predicted_val) || std::isinf(predicted_val)) {\n                calculation_failed = true;\n                break;\n            }\n\n            double target_val = targets[i];\n            double diff = predicted_val - target_val;\n            double abs_diff = std::fabs(diff);\n\n            if (abs_diff >= FITNESS_PRECISION_THRESHOLD) all_precise = false;\n\n            double weight = 1.0;\n            if (USE_WEIGHTED_FITNESS) {\n                weight = std::exp(static_cast<double>(i) * WEIGHTED_FITNESS_EXPONENT);\n            }\n            total_weight += weight;\n\n            double sq_error = diff * diff;\n            sum_sq_error += sq_error * weight;\n        }\n\n        if (calculation_failed) return INF;\n\n        // Normalize weighted error\n        double raw_error;\n        if (USE_WEIGHTED_FITNESS && total_weight > 0.0) {\n            sum_sq_error = sum_sq_error / total_weight * num_points;\n        }\n\n        if (USE_RMSE_FITNESS && num_points > 0) {\n            double mse = sum_sq_error / static_cast<double>(num_points);\n            raw_error = std::sqrt(mse);\n        } else {\n            raw_error = sum_sq_error;\n        }\n\n        if (std::isnan(raw_error) || std::isinf(raw_error) || raw_error < 0) {\n            return INF;\n        }\n\n        if (all_precise) {\n            raw_error *= FITNESS_PRECISION_BONUS;\n        }\n\n        return raw_error;\n    }\n    \n    // Use GPU evaluation\n    return evaluate_fitness_gpu(tree, targets, x_values, d_targets, d_x_values);\n}\n#else\ndouble calculate_raw_fitness(const NodePtr& tree,\n                             const std::vector<double>& targets,\n                             const std::vector<double>& x_values) {\n    if (x_values.size() != targets.size() || x_values.empty()) return INF;\n\n    double error_sum_pow13 = 0.0; // Solo si USE_RMSE_FITNESS = false\n    double sum_sq_error = 0.0;\n    double total_weight = 0.0; // Para normalizar el fitness ponderado\n    bool all_precise = true;\n    size_t num_points = x_values.size();\n    bool calculation_failed = false; // Flag para detectar INF/NaN\n\n    for (size_t i = 0; i < num_points; ++i) {\n        double predicted_val = evaluate_tree(tree, x_values[i]);\n\n        // Comprobar si la evaluaci\u00f3n fall\u00f3 (INF o NaN)\n        if (std::isnan(predicted_val) || std::isinf(predicted_val)) {\n            calculation_failed = true;\n            break; // Salir del bucle si la evaluaci\u00f3n falla para un punto\n        }\n\n        double target_val = targets[i];\n        double diff = predicted_val - target_val;\n        double abs_diff = std::fabs(diff);\n\n        if (abs_diff >= FITNESS_PRECISION_THRESHOLD) all_precise = false;\n\n        // --- PESO PARA FITNESS PONDERADO ---\n        // Hace que los \u00faltimos puntos (N altos) valgan much\u00edsimo m\u00e1s.\n        // Esto destruye a los polinomios porque fallan al final.\n        double weight = 1.0;\n        if (USE_WEIGHTED_FITNESS) {\n            // Peso exponencial: m\u00e1s agresivo para penalizar errores en N altos\n            weight = std::exp(static_cast<double>(i) * WEIGHTED_FITNESS_EXPONENT);\n        }\n        total_weight += weight;\n\n        // Acumular error para ambas m\u00e9tricas (si aplica)\n        if (!USE_RMSE_FITNESS) {\n             error_sum_pow13 += std::pow(abs_diff, FITNESS_ORIGINAL_POWER) * weight;\n        }\n\n        // Calcular y acumular error cuadr\u00e1tico PONDERADO\n        double sq_diff = diff * diff;\n        sum_sq_error += sq_diff * weight;\n\n        // Control de desbordamiento/Infinito en la suma\n        if (std::isinf(sum_sq_error) || (error_sum_pow13 >= INF / 10.0 && !USE_RMSE_FITNESS)) {\n            calculation_failed = true;\n            break;\n        }\n    } // Fin bucle for puntos\n\n    // Si la evaluaci\u00f3n o suma fall\u00f3 en alg\u00fan punto, devolver INF\n    if (calculation_failed) {\n        return INF;\n    }\n\n    // Seleccionar m\u00e9trica de error crudo\n    double raw_error;\n    if (USE_RMSE_FITNESS) {\n        if (num_points == 0 || total_weight == 0.0) return INF;\n        // MSE ponderado: normalizar por suma de pesos, no por num_points\n        double mse = sum_sq_error / total_weight;\n        if (std::isinf(mse) || std::isnan(mse) || mse < 0) {\n             raw_error = INF;\n        } else {\n             raw_error = std::sqrt(mse); // Calcular RMSE ponderado\n        }\n    } else {\n        raw_error = error_sum_pow13;\n    }\n\n    // Comprobar si el error crudo es inv\u00e1lido\n    if (std::isnan(raw_error) || std::isinf(raw_error) || raw_error < 0) {\n         return INF;\n    }\n\n    // Aplicar bonus de precisi\u00f3n si todos los puntos estaban dentro del umbral\n    if (all_precise) {\n         raw_error *= FITNESS_PRECISION_BONUS;\n    }\n\n    return raw_error; // Devolver el error crudo (sin penalizaci\u00f3n por complejidad a\u00fan)\n}\n#endif // USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n\n// Calcula el fitness final usando par\u00e1metros globales.\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\ndouble evaluate_fitness(const NodePtr& tree,\n                        const std::vector<double>& targets,\n                        const std::vector<double>& x_values,\n                        double* d_targets, double* d_x_values) {\n    double raw_fitness = calculate_raw_fitness(tree, targets, x_values, d_targets, d_x_values);\n#else\ndouble evaluate_fitness(const NodePtr& tree,\n                        const std::vector<double>& targets,\n                        const std::vector<double>& x_values) {\n    double raw_fitness = calculate_raw_fitness(tree, targets, x_values);\n#endif\n\n    if (raw_fitness >= INF / 10.0) {\n         return INF; // Si el error crudo es infinito, el fitness final es infinito\n    }\n\n    // Penalizaci\u00f3n por complejidad\n    double complexity = static_cast<double>(tree_size(tree));\n    double penalty = complexity * COMPLEXITY_PENALTY_FACTOR; // Usa constante global\n\n    // Aplicar penalizaci\u00f3n multiplicativa\n    double final_fitness = raw_fitness * (1.0 + penalty);\n\n    // Comprobaciones finales\n    if (std::isnan(final_fitness) || std::isinf(final_fitness) || final_fitness < 0) {\n         return INF;\n    }\n\n    return final_fitness;\n}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/Fitness.h\n",
        "#ifndef FITNESS_H\n#define FITNESS_H\n\n#include \"ExpressionTree.h\"\n#include <vector>\n\n// Calculates raw fitness based on target matching\n// Lower is better. Returns INF if evaluation results in NaN/Inf.\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\ndouble calculate_raw_fitness(const NodePtr& tree,\n                             const std::vector<double>& targets,\n                             const std::vector<double>& x_values,\n                             double* d_targets, double* d_x_values);\n#else\ndouble calculate_raw_fitness(const NodePtr& tree,\n                             const std::vector<double>& targets,\n                             const std::vector<double>& x_values);\n#endif\n\n// Calculates final fitness including complexity penalty\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\ndouble evaluate_fitness(const NodePtr& tree,\n                        const std::vector<double>& targets,\n                        const std::vector<double>& x_values,\n                        double* d_targets, double* d_x_values);\n#else\ndouble evaluate_fitness(const NodePtr& tree,\n                        const std::vector<double>& targets,\n                        const std::vector<double>& x_values);\n#endif\n\n#endif // FITNESS_H\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/FitnessGPU.cu\n",
        "#include \"FitnessGPU.cuh\"\n#include \"Globals.h\"\n#include <cuda_runtime.h>\n#include <math.h>\n#include <vector>\n#include <iostream>\n\n// Helper function to linearize the tree into a post-order array\nvoid linearize_tree(const NodePtr& node, std::vector<LinearGpuNode>& linear_tree) {\n    if (!node) {\n        return;\n    }\n    linearize_tree(node->left, linear_tree);\n    linearize_tree(node->right, linear_tree);\n    linear_tree.push_back({node->type, node->value, node->op});\n}\n\n#if USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n\n// Constant for large finite value\n#define GPU_MAX_DOUBLE 1e308\n\n// --- WEIGHTED FITNESS: Constantes para CUDA ---\n// Estas deben coincidir con los valores en Globals.h\n// CUDA device code no puede acceder a const C++, as\u00ed que usamos #define\n#define GPU_USE_WEIGHTED_FITNESS true\n#define GPU_WEIGHTED_FITNESS_EXPONENT 0.25\n\n// Single Tree Evaluation Kernel (Legacy/Single Use)\n__global__ void calculate_raw_fitness_kernel(const LinearGpuNode* d_linear_tree,\n                                             int tree_size,\n                                             const double* d_targets,\n                                             const double* d_x_values,\n                                             size_t num_points,\n                                             double* d_raw_fitness_results) {\n    // Shared memory optimization: Load tree into shared memory\n    extern __shared__ LinearGpuNode s_linear_tree[];\n\n    // Cooperative load\n    for (int i = threadIdx.x; i < tree_size; i += blockDim.x) {\n        s_linear_tree[i] = d_linear_tree[i];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx < num_points) {\n        double x_val = d_x_values[idx];\n        double stack[64]; // Max tree depth\n        int stack_top = -1;\n\n        for (int i = 0; i < tree_size; ++i) {\n            LinearGpuNode node = s_linear_tree[i]; // Access from shared memory\n            if (node.type == NodeType::Constant) {\n                stack[++stack_top] = node.value;\n            } else if (node.type == NodeType::Variable) {\n                stack[++stack_top] = x_val;\n            } else if (node.type == NodeType::Operator) {\n                bool is_unary = (node.op == 's' || node.op == 'c' || node.op == 'l' || node.op == 'e' || node.op == '!' || node.op == '_' || node.op == 'g');\n                double result = 0.0;\n                \n                if (is_unary) {\n                     if (stack_top < 0) {\n                         result = GPU_MAX_DOUBLE;\n                     } else {\n                         double val = stack[stack_top--];\n                         switch (node.op) {\n                            case 's': result = sin(val); break;\n                            case 'c': result = cos(val); break;\n                            case 'l': result = (val <= 1e-9) ? GPU_MAX_DOUBLE : log(val); break;\n                            case 'e': result = (val > 700.0) ? GPU_MAX_DOUBLE : exp(val); break;\n                            case '!': result = (val < 0 || val > 170.0) ? GPU_MAX_DOUBLE : tgamma(val + 1.0); break;\n                            case '_': result = floor(val); break;\n                            case 'g': result = (val <= -1.0) ? GPU_MAX_DOUBLE : lgamma(val + 1.0); break;\n                            default: result = NAN; break;\n                         }\n                     }\n                     stack[++stack_top] = result;\n                } else {\n                    if (stack_top < 1) { \n                        result = GPU_MAX_DOUBLE;\n                        stack[++stack_top] = result; // Push error\n                    } else {\n                        double right = stack[stack_top--];\n                        double left = stack[stack_top--];\n                        switch (node.op) {\n                            case '+': result = left + right; break;\n                            case '-': result = left - right; break;\n                            case '*': result = left * right; break;\n                            case '/':\n                                if (fabs(right) < 1e-9) { // Avoid division by zero\n                                    result = GPU_MAX_DOUBLE; \n                                } else {\n                                    result = left / right;\n                                }\n                                break;\n                            case '^': result = pow(left, right); break;\n                            case '%':\n                                if (fabs(right) < 1e-9) result = GPU_MAX_DOUBLE;\n                                else result = fmod(left, right);\n                                break;\n                            default: result = NAN; break;\n                        }\n                        stack[++stack_top] = result;\n                    }\n                }\n            }\n        }\n\n        double predicted_val = (stack_top == 0) ? stack[0] : NAN;\n\n        if (isnan(predicted_val) || isinf(predicted_val)) {\n            d_raw_fitness_results[idx] = GPU_MAX_DOUBLE; \n        } else {\n            double diff = predicted_val - d_targets[idx];\n            double sq_error = diff * diff;\n            // --- WEIGHTED FITNESS: Apply exponential weight ---\n            // Los \u00faltimos puntos (N altos) pesan mucho m\u00e1s que los primeros.\n            if (GPU_USE_WEIGHTED_FITNESS) {\n                double weight = exp((double)idx * GPU_WEIGHTED_FITNESS_EXPONENT);\n                sq_error *= weight;\n            }\n            d_raw_fitness_results[idx] = sq_error;\n        }\n    }\n}\n\n// CUDA kernel for parallel reduction (summation)\n__global__ void reduce_sum_kernel(double* d_data, int N) {\n    extern __shared__ double sdata[]; // Shared memory for reduction\n\n    unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    sdata[tid] = (i < N) ? d_data[i] : 0.0; // Load data into shared memory\n\n    __syncthreads(); // Synchronize threads in block\n\n    // Perform reduction in shared memory\n    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) { // Write result back to global memory (first element of block)\n        d_data[blockIdx.x] = sdata[0];\n    }\n}\n\n\n// --- New Batch Kernel ---\n// Evaluates one tree per thread across all data points\n__global__ void evaluate_population_kernel(const LinearGpuNode* d_all_nodes,\n                                           const int* d_offsets,\n                                           const int* d_sizes,\n                                           int pop_size,\n                                           const double* d_targets,\n                                           const double* d_x_values,\n                                           int num_points,\n                                           double* d_results) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx < pop_size) {\n        int offset = d_offsets[idx];\n        int size = d_sizes[idx];\n        double sum_sq_error = 0.0;\n        double total_weight = 0.0; // Para normalizar fitness ponderado\n        bool valid = true;\n\n        for (int p = 0; p < num_points; ++p) {\n            double x_val = d_x_values[p];\n            double stack[64]; \n            int stack_top = -1;\n\n            // Simple interpreter\n            for (int i = 0; i < size; ++i) {\n                LinearGpuNode node = d_all_nodes[offset + i];\n                if (node.type == NodeType::Constant) {\n                    stack[++stack_top] = node.value;\n                } else if (node.type == NodeType::Variable) {\n                    stack[++stack_top] = x_val;\n                } else if (node.type == NodeType::Operator) {\n                    bool is_unary = (node.op == 's' || node.op == 'c' || node.op == 'l' || node.op == 'e' || node.op == '!' || node.op == '_' || node.op == 'g');\n                    \n                    if (is_unary) {\n                        if (stack_top < 0) { valid = false; break; }\n                        double val = stack[stack_top--];\n                        double result = 0.0;\n                         switch (node.op) {\n                            case 's': result = sin(val); break;\n                            case 'c': result = cos(val); break;\n                            case 'l': result = (val <= 1e-9) ? GPU_MAX_DOUBLE : log(val); break;\n                            case 'e': result = (val > 700.0) ? GPU_MAX_DOUBLE : exp(val); break;\n                            case '!': result = (val < 0 || val > 170.0) ? GPU_MAX_DOUBLE : tgamma(val + 1.0); break;\n                            case '_': result = floor(val); break;\n                            case 'g': result = (val <= -1.0) ? GPU_MAX_DOUBLE : lgamma(val + 1.0); break;\n                             default: result = NAN; break;\n                        }\n                        stack[++stack_top] = result;\n                    } else {\n                        // Safety check index\n                        if (stack_top < 1) { valid = false; break; }\n\n                        double right = stack[stack_top--];\n                        double left = stack[stack_top--];\n                        double result;\n                        switch (node.op) {\n                            case '+': result = left + right; break;\n                            case '-': result = left - right; break;\n                            case '*': result = left * right; break;\n                            case '/':\n                                if (fabs(right) < 1e-9) { \n                                    result = GPU_MAX_DOUBLE; \n                                } else {\n                                    result = left / right;\n                                }\n                                break;\n                            case '^': result = pow(left, right); break;\n                            case '%':\n                                if (fabs(right) < 1e-9) result = GPU_MAX_DOUBLE;\n                                else result = fmod(left, right);\n                                break;\n                            default: result = NAN; break;\n                        }\n                        stack[++stack_top] = result;\n                    }\n                }\n            }\n\n            if (!valid || stack_top != 0) {\n                sum_sq_error = GPU_MAX_DOUBLE;\n                break;\n            }\n\n            double predicted_val = stack[0];\n            if (isnan(predicted_val) || isinf(predicted_val)) {\n                sum_sq_error = GPU_MAX_DOUBLE;\n                break;\n            }\n\n            double diff = predicted_val - d_targets[p];\n            double sq_error = diff * diff;\n            \n            // --- WEIGHTED FITNESS: Peso exponencial ---\n            double weight = 1.0;\n            if (GPU_USE_WEIGHTED_FITNESS) {\n                weight = exp((double)p * GPU_WEIGHTED_FITNESS_EXPONENT);\n            }\n            total_weight += weight;\n            sum_sq_error += sq_error * weight;\n        }\n\n        // Normalizar por suma de pesos para obtener MSE ponderado\n        if (GPU_USE_WEIGHTED_FITNESS && total_weight > 0.0) {\n            sum_sq_error = sum_sq_error / total_weight * num_points; // Escalar de vuelta\n        }\n        d_results[idx] = sum_sq_error;\n    }\n}\n\n\n// Host-side wrapper function to launch the CUDA kernel\ndouble evaluate_fitness_gpu(NodePtr tree,\n                            const std::vector<double>& targets,\n                            const std::vector<double>& x_values,\n                            double* d_targets, double* d_x_values) {\n    if (x_values.size() != targets.size() || x_values.empty()) return INF;\n\n    // Linearize the tree\n    std::vector<LinearGpuNode> h_linear_tree;\n    linearize_tree(tree, h_linear_tree);\n    int tree_size = h_linear_tree.size();\n\n    if (tree_size == 0) {\n        return INF;\n    }\n\n    size_t num_points = x_values.size();\n    LinearGpuNode* d_linear_tree;\n    double* d_raw_fitness_results; // This will hold individual errors and then the final sum\n\n    cudaMalloc((void**)&d_linear_tree, tree_size * sizeof(LinearGpuNode));\n    cudaMalloc((void**)&d_raw_fitness_results, num_points * sizeof(double));\n\n    cudaMemcpy(d_linear_tree, h_linear_tree.data(), tree_size * sizeof(LinearGpuNode), cudaMemcpyHostToDevice);\n\n    int threadsPerBlock = 256;\n    int blocksPerGrid = (num_points + threadsPerBlock - 1) / threadsPerBlock;\n\n    // Launch kernel to calculate individual squared errors\n    size_t shared_mem_size = tree_size * sizeof(LinearGpuNode);\n    calculate_raw_fitness_kernel<<<blocksPerGrid, threadsPerBlock, shared_mem_size>>>(\n        d_linear_tree, tree_size, d_targets, d_x_values, num_points, d_raw_fitness_results\n    );\n    cudaDeviceSynchronize(); // Ensure kernel completes before reduction\n\n    // --- Perform reduction on the GPU ---\n    int current_size = num_points;\n    while (current_size > 1) {\n        int next_blocks_per_grid = (current_size + threadsPerBlock - 1) / threadsPerBlock;\n        // Use shared memory for reduction, size is threadsPerBlock * sizeof(double)\n        reduce_sum_kernel<<<next_blocks_per_grid, threadsPerBlock, threadsPerBlock * sizeof(double)>>>(\n            d_raw_fitness_results, current_size\n        );\n        cudaDeviceSynchronize(); // Ensure reduction step completes\n        current_size = next_blocks_per_grid; // The result is in the first `next_blocks_per_grid` elements\n    }\n\n    double sum_sq_error_gpu = 0.0;\n    cudaMemcpy(&sum_sq_error_gpu, d_raw_fitness_results, sizeof(double), cudaMemcpyDeviceToHost);\n\n    cudaFree(d_linear_tree);\n    cudaFree(d_raw_fitness_results);\n\n    // Check for invalid results (propagated from kernel)\n    if (isinf(sum_sq_error_gpu) || isnan(sum_sq_error_gpu)) {\n        return INF;\n    }\n\n    double raw_fitness;\n    if (USE_RMSE_FITNESS) {\n        if (num_points == 0) return INF;\n        double mse = sum_sq_error_gpu / num_points;\n        raw_fitness = sqrt(mse);\n    } else {\n        raw_fitness = sum_sq_error_gpu;\n    }\n\n    double complexity = static_cast<double>(::tree_size(tree));\n    double penalty = complexity * COMPLEXITY_PENALTY_FACTOR;\n    double final_fitness = raw_fitness * (1.0 + penalty);\n\n    if (isnan(final_fitness) || isinf(final_fitness) || final_fitness < 0) {\n        return INF;\n    }\n\n    return final_fitness;\n}\n\nvoid evaluate_population_gpu(const std::vector<LinearGpuNode>& all_nodes,\n                             const std::vector<int>& tree_offsets,\n                             const std::vector<int>& tree_sizes,\n                             const std::vector<double>& targets,\n                             const std::vector<double>& x_values,\n                             std::vector<double>& results,\n                             double* d_targets, double* d_x_values,\n                             void*& d_nodes_ptr, size_t& d_nodes_cap,\n                             void*& d_offsets_ptr, void*& d_sizes_ptr, void*& d_results_ptr, size_t& d_pop_cap) {\n    \n    int pop_size = tree_offsets.size();\n    if (pop_size == 0) return;\n\n    size_t total_nodes = all_nodes.size();\n    int num_points = x_values.size();\n\n    // Buffer Management for Nodes\n    if (total_nodes > d_nodes_cap) {\n        if (d_nodes_ptr) cudaFree(d_nodes_ptr);\n        size_t new_cap = total_nodes * 1.5; // Growth factor\n        cudaMalloc(&d_nodes_ptr, new_cap * sizeof(LinearGpuNode));\n        d_nodes_cap = new_cap;\n    }\n\n    // Buffer Management for Population Arrays\n    if (pop_size > d_pop_cap) {\n        if (d_offsets_ptr) cudaFree(d_offsets_ptr);\n        if (d_sizes_ptr) cudaFree(d_sizes_ptr);\n        if (d_results_ptr) cudaFree(d_results_ptr);\n        \n        size_t new_cap = pop_size * 1.5;\n        cudaMalloc(&d_offsets_ptr, new_cap * sizeof(int));\n        cudaMalloc(&d_sizes_ptr, new_cap * sizeof(int));\n        cudaMalloc(&d_results_ptr, new_cap * sizeof(double));\n        d_pop_cap = new_cap;\n    }\n\n    LinearGpuNode* d_all_nodes = (LinearGpuNode*)d_nodes_ptr;\n    int* d_offsets = (int*)d_offsets_ptr;\n    int* d_sizes = (int*)d_sizes_ptr;\n    double* d_results = (double*)d_results_ptr;\n\n    cudaMemcpy(d_all_nodes, all_nodes.data(), total_nodes * sizeof(LinearGpuNode), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_offsets, tree_offsets.data(), pop_size * sizeof(int), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_sizes, tree_sizes.data(), pop_size * sizeof(int), cudaMemcpyHostToDevice);\n\n    int threadsPerBlock = 256;\n    int blocksPerGrid = (pop_size + threadsPerBlock - 1) / threadsPerBlock;\n\n    evaluate_population_kernel<<<blocksPerGrid, threadsPerBlock>>>(\n        d_all_nodes, d_offsets, d_sizes, pop_size, d_targets, d_x_values, num_points, d_results\n    );\n\n    // Synchronize and copy back\n    cudaDeviceSynchronize();\n    \n    cudaMemcpy(results.data(), d_results, pop_size * sizeof(double), cudaMemcpyDeviceToHost);\n}\n\n// ============================================================\n// GLOBAL BATCH EVALUATION - Maximum GPU Utilization\n// ============================================================\n\nvoid init_global_gpu_buffers(GlobalGpuBuffers& buffers) {\n    // Create CUDA stream for async operations\n    cudaStream_t stream;\n    cudaStreamCreate(&stream);\n    buffers.cuda_stream = (void*)stream;\n    \n    // Pre-allocate initial buffers (will grow as needed)\n    // Initial capacity for 50,000 trees with ~30 nodes each\n    buffers.d_nodes_capacity = 1500000;\n    buffers.d_pop_capacity = 60000;\n    \n    cudaMalloc(&buffers.d_nodes, buffers.d_nodes_capacity * sizeof(LinearGpuNode));\n    cudaMalloc(&buffers.d_offsets, buffers.d_pop_capacity * sizeof(int));\n    cudaMalloc(&buffers.d_sizes, buffers.d_pop_capacity * sizeof(int));\n    cudaMalloc(&buffers.d_results, buffers.d_pop_capacity * sizeof(double));\n}\n\nvoid cleanup_global_gpu_buffers(GlobalGpuBuffers& buffers) {\n    if (buffers.cuda_stream) {\n        cudaStreamDestroy((cudaStream_t)buffers.cuda_stream);\n        buffers.cuda_stream = nullptr;\n    }\n    if (buffers.d_nodes) { cudaFree(buffers.d_nodes); buffers.d_nodes = nullptr; }\n    if (buffers.d_offsets) { cudaFree(buffers.d_offsets); buffers.d_offsets = nullptr; }\n    if (buffers.d_sizes) { cudaFree(buffers.d_sizes); buffers.d_sizes = nullptr; }\n    if (buffers.d_results) { cudaFree(buffers.d_results); buffers.d_results = nullptr; }\n    buffers.d_nodes_capacity = 0;\n    buffers.d_pop_capacity = 0;\n}\n\n// Optimized kernel: Process one tree per thread, apply complexity penalty on GPU\n// Uses shared memory for targets and x_values for better memory coalescing\n__global__ void evaluate_all_populations_kernel(\n    const LinearGpuNode* __restrict__ d_all_nodes,\n    const int* __restrict__ d_offsets,\n    const int* __restrict__ d_sizes,\n    int total_trees,\n    const double* __restrict__ d_targets,\n    const double* __restrict__ d_x_values,\n    int num_points,\n    double* __restrict__ d_results,\n    double complexity_penalty_factor,\n    bool use_rmse) \n{\n    // Shared memory for targets and x_values (max 64 points supported)\n    __shared__ double s_targets[64];\n    __shared__ double s_x_values[64];\n    \n    // Cooperatively load targets and x_values into shared memory\n    int tid_local = threadIdx.x;\n    if (tid_local < num_points) {\n        s_targets[tid_local] = d_targets[tid_local];\n        s_x_values[tid_local] = d_x_values[tid_local];\n    }\n    __syncthreads();\n    \n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx < total_trees) {\n        int offset = d_offsets[idx];\n        int size = d_sizes[idx];\n        double sum_sq_error = 0.0;\n        double total_weight = 0.0;\n        bool valid = true;\n\n        // Evaluate tree on all data points\n        for (int p = 0; p < num_points && valid; ++p) {\n            double x_val = s_x_values[p];\n            double stack[64]; \n            int stack_top = -1;\n\n            // Interpret the linearized tree\n            for (int i = 0; i < size && valid; ++i) {\n                LinearGpuNode node = d_all_nodes[offset + i];\n                \n                if (node.type == NodeType::Constant) {\n                    stack[++stack_top] = node.value;\n                } else if (node.type == NodeType::Variable) {\n                    stack[++stack_top] = x_val;\n                } else if (node.type == NodeType::Operator) {\n                    bool is_unary = (node.op == 's' || node.op == 'c' || node.op == 'l' || \n                                    node.op == 'e' || node.op == '!' || node.op == '_' || node.op == 'g');\n                    \n                    if (is_unary) {\n                        if (stack_top < 0) { valid = false; break; }\n                        double val = stack[stack_top--];\n                        double result = 0.0;\n                        switch (node.op) {\n                            case 's': result = sin(val); break;\n                            case 'c': result = cos(val); break;\n                            case 'l': result = (val <= 1e-9) ? GPU_MAX_DOUBLE : log(val); break;\n                            case 'e': result = (val > 700.0) ? GPU_MAX_DOUBLE : exp(val); break;\n                            case '!': result = (val < 0 || val > 170.0) ? GPU_MAX_DOUBLE : tgamma(val + 1.0); break;\n                            case '_': result = floor(val); break;\n                            case 'g': result = (val <= -1.0) ? GPU_MAX_DOUBLE : lgamma(val + 1.0); break;\n                            default: result = NAN; break;\n                        }\n                        stack[++stack_top] = result;\n                    } else {\n                        if (stack_top < 1) { valid = false; break; }\n                        double right = stack[stack_top--];\n                        double left = stack[stack_top--];\n                        double result;\n                        switch (node.op) {\n                            case '+': result = left + right; break;\n                            case '-': result = left - right; break;\n                            case '*': result = left * right; break;\n                            case '/': result = (fabs(right) < 1e-9) ? GPU_MAX_DOUBLE : left / right; break;\n                            case '^': result = pow(left, right); break;\n                            case '%': result = (fabs(right) < 1e-9) ? GPU_MAX_DOUBLE : fmod(left, right); break;\n                            default: result = NAN; break;\n                        }\n                        stack[++stack_top] = result;\n                    }\n                }\n            }\n\n            if (!valid || stack_top != 0) {\n                sum_sq_error = GPU_MAX_DOUBLE;\n                valid = false;\n                break;\n            }\n\n            double predicted_val = stack[0];\n            if (isnan(predicted_val) || isinf(predicted_val)) {\n                sum_sq_error = GPU_MAX_DOUBLE;\n                valid = false;\n                break;\n            }\n\n            double diff = predicted_val - s_targets[p];\n            double sq_error = diff * diff;\n            \n            // Weighted fitness\n            double weight = 1.0;\n            if (GPU_USE_WEIGHTED_FITNESS) {\n                weight = exp((double)p * GPU_WEIGHTED_FITNESS_EXPONENT);\n            }\n            total_weight += weight;\n            sum_sq_error += sq_error * weight;\n        }\n\n        // Calculate final fitness with complexity penalty ON GPU\n        double raw_fitness = GPU_MAX_DOUBLE;\n        if (valid && sum_sq_error < 1e300) {\n            if (GPU_USE_WEIGHTED_FITNESS && total_weight > 0.0) {\n                sum_sq_error = sum_sq_error / total_weight * num_points;\n            }\n            \n            if (use_rmse && num_points > 0) {\n                double mse = sum_sq_error / num_points;\n                raw_fitness = sqrt(mse);\n            } else {\n                raw_fitness = sum_sq_error;\n            }\n            \n            // Apply complexity penalty (size is same as tree size in linearized form)\n            double complexity = (double)size;\n            double penalty = complexity * complexity_penalty_factor;\n            raw_fitness = raw_fitness * (1.0 + penalty);\n        }\n        \n        d_results[idx] = raw_fitness;\n    }\n}\n\nvoid evaluate_all_populations_gpu(\n    const std::vector<LinearGpuNode>& all_nodes,\n    const std::vector<int>& tree_offsets,\n    const std::vector<int>& tree_sizes,\n    const std::vector<int>& tree_complexities,\n    int total_trees,\n    const std::vector<double>& targets,\n    const std::vector<double>& x_values,\n    std::vector<double>& results,\n    double* d_targets, double* d_x_values,\n    GlobalGpuBuffers& buffers)\n{\n    if (total_trees == 0) return;\n    \n    cudaStream_t stream = (cudaStream_t)buffers.cuda_stream;\n    size_t total_nodes = all_nodes.size();\n    int num_points = x_values.size();\n\n    // Dynamic buffer resizing with growth factor\n    if (total_nodes > buffers.d_nodes_capacity) {\n        if (buffers.d_nodes) cudaFree(buffers.d_nodes);\n        size_t new_cap = total_nodes * 1.5;\n        cudaMalloc(&buffers.d_nodes, new_cap * sizeof(LinearGpuNode));\n        buffers.d_nodes_capacity = new_cap;\n    }\n\n    if ((size_t)total_trees > buffers.d_pop_capacity) {\n        if (buffers.d_offsets) cudaFree(buffers.d_offsets);\n        if (buffers.d_sizes) cudaFree(buffers.d_sizes);\n        if (buffers.d_results) cudaFree(buffers.d_results);\n        \n        size_t new_cap = total_trees * 1.5;\n        cudaMalloc(&buffers.d_offsets, new_cap * sizeof(int));\n        cudaMalloc(&buffers.d_sizes, new_cap * sizeof(int));\n        cudaMalloc(&buffers.d_results, new_cap * sizeof(double));\n        buffers.d_pop_capacity = new_cap;\n    }\n\n    LinearGpuNode* d_all_nodes = (LinearGpuNode*)buffers.d_nodes;\n    int* d_offsets = (int*)buffers.d_offsets;\n    int* d_sizes = (int*)buffers.d_sizes;\n    double* d_results = (double*)buffers.d_results;\n\n    // Async memory transfers using CUDA stream\n    cudaMemcpyAsync(d_all_nodes, all_nodes.data(), total_nodes * sizeof(LinearGpuNode), \n                    cudaMemcpyHostToDevice, stream);\n    cudaMemcpyAsync(d_offsets, tree_offsets.data(), total_trees * sizeof(int), \n                    cudaMemcpyHostToDevice, stream);\n    cudaMemcpyAsync(d_sizes, tree_sizes.data(), total_trees * sizeof(int), \n                    cudaMemcpyHostToDevice, stream);\n\n    // Optimized kernel launch configuration for RTX 3050\n    // RTX 3050 has 20 SMs, each can handle 2048 threads max\n    // For 50k trees, we want maximum occupancy\n    int threadsPerBlock = 256;\n    int blocksPerGrid = (total_trees + threadsPerBlock - 1) / threadsPerBlock;\n\n    // Launch kernel on stream\n    evaluate_all_populations_kernel<<<blocksPerGrid, threadsPerBlock, 0, stream>>>(\n        d_all_nodes, d_offsets, d_sizes, total_trees,\n        d_targets, d_x_values, num_points, d_results,\n        COMPLEXITY_PENALTY_FACTOR, USE_RMSE_FITNESS\n    );\n\n    // Async copy results back\n    cudaMemcpyAsync(results.data(), d_results, total_trees * sizeof(double), \n                    cudaMemcpyDeviceToHost, stream);\n    \n    // Synchronize stream\n    cudaStreamSynchronize(stream);\n}\n\n// ============================================================\n// DOUBLE-BUFFERED GPU IMPLEMENTATION\n// ============================================================\n\nvoid init_double_buffered_gpu(DoubleBufferedGpu& db) {\n    // Create two streams for overlapped execution\n    cudaStreamCreate((cudaStream_t*)&db.streams[0]);\n    cudaStreamCreate((cudaStream_t*)&db.streams[1]);\n    \n    // Pre-allocate buffers for both ping and pong\n    size_t initial_nodes_cap = 1500000;  // 50k trees * 30 nodes avg\n    size_t initial_pop_cap = 60000;      // Slightly more than 50k\n    \n    for (int i = 0; i < 2; ++i) {\n        db.d_nodes_capacity[i] = initial_nodes_cap;\n        db.d_pop_capacity[i] = initial_pop_cap;\n        \n        cudaMalloc(&db.d_nodes[i], initial_nodes_cap * sizeof(LinearGpuNode));\n        cudaMalloc(&db.d_offsets[i], initial_pop_cap * sizeof(int));\n        cudaMalloc(&db.d_sizes[i], initial_pop_cap * sizeof(int));\n        cudaMalloc(&db.d_results[i], initial_pop_cap * sizeof(double));\n    }\n    \n    // Allocate pinned host memory for faster H2D/D2H transfers\n    db.h_pinned_capacity = initial_pop_cap;\n    cudaMallocHost(&db.h_pinned_results, initial_pop_cap * sizeof(double));\n    \n    db.current_buffer = 0;\n}\n\nvoid cleanup_double_buffered_gpu(DoubleBufferedGpu& db) {\n    for (int i = 0; i < 2; ++i) {\n        if (db.streams[i]) {\n            cudaStreamDestroy((cudaStream_t)db.streams[i]);\n            db.streams[i] = nullptr;\n        }\n        if (db.d_nodes[i]) { cudaFree(db.d_nodes[i]); db.d_nodes[i] = nullptr; }\n        if (db.d_offsets[i]) { cudaFree(db.d_offsets[i]); db.d_offsets[i] = nullptr; }\n        if (db.d_sizes[i]) { cudaFree(db.d_sizes[i]); db.d_sizes[i] = nullptr; }\n        if (db.d_results[i]) { cudaFree(db.d_results[i]); db.d_results[i] = nullptr; }\n    }\n    \n    if (db.h_pinned_results) {\n        cudaFreeHost(db.h_pinned_results);\n        db.h_pinned_results = nullptr;\n    }\n}\n\nvoid launch_evaluation_async(\n    const std::vector<LinearGpuNode>& all_nodes,\n    const std::vector<int>& tree_offsets,\n    const std::vector<int>& tree_sizes,\n    int total_trees,\n    double* d_targets, double* d_x_values,\n    int num_points,\n    DoubleBufferedGpu& db)\n{\n    if (total_trees == 0) return;\n    \n    int buf = db.current_buffer;\n    cudaStream_t stream = (cudaStream_t)db.streams[buf];\n    size_t total_nodes = all_nodes.size();\n    \n    // Ensure buffers are large enough\n    if (total_nodes > db.d_nodes_capacity[buf]) {\n        cudaFree(db.d_nodes[buf]);\n        size_t new_cap = total_nodes * 1.5;\n        cudaMalloc(&db.d_nodes[buf], new_cap * sizeof(LinearGpuNode));\n        db.d_nodes_capacity[buf] = new_cap;\n    }\n    \n    if ((size_t)total_trees > db.d_pop_capacity[buf]) {\n        cudaFree(db.d_offsets[buf]);\n        cudaFree(db.d_sizes[buf]);\n        cudaFree(db.d_results[buf]);\n        \n        size_t new_cap = total_trees * 1.5;\n        cudaMalloc(&db.d_offsets[buf], new_cap * sizeof(int));\n        cudaMalloc(&db.d_sizes[buf], new_cap * sizeof(int));\n        cudaMalloc(&db.d_results[buf], new_cap * sizeof(double));\n        db.d_pop_capacity[buf] = new_cap;\n    }\n    \n    // Ensure pinned results buffer is large enough\n    if ((size_t)total_trees > db.h_pinned_capacity) {\n        cudaFreeHost(db.h_pinned_results);\n        db.h_pinned_capacity = total_trees * 1.5;\n        cudaMallocHost(&db.h_pinned_results, db.h_pinned_capacity * sizeof(double));\n    }\n    \n    LinearGpuNode* d_nodes = (LinearGpuNode*)db.d_nodes[buf];\n    int* d_offsets = (int*)db.d_offsets[buf];\n    int* d_sizes = (int*)db.d_sizes[buf];\n    double* d_results = (double*)db.d_results[buf];\n    \n    // Async transfers\n    cudaMemcpyAsync(d_nodes, all_nodes.data(), total_nodes * sizeof(LinearGpuNode), \n                    cudaMemcpyHostToDevice, stream);\n    cudaMemcpyAsync(d_offsets, tree_offsets.data(), total_trees * sizeof(int), \n                    cudaMemcpyHostToDevice, stream);\n    cudaMemcpyAsync(d_sizes, tree_sizes.data(), total_trees * sizeof(int), \n                    cudaMemcpyHostToDevice, stream);\n    \n    // Launch kernel\n    int threadsPerBlock = 256;\n    int blocksPerGrid = (total_trees + threadsPerBlock - 1) / threadsPerBlock;\n    \n    evaluate_all_populations_kernel<<<blocksPerGrid, threadsPerBlock, 0, stream>>>(\n        d_nodes, d_offsets, d_sizes, total_trees,\n        d_targets, d_x_values, num_points, d_results,\n        COMPLEXITY_PENALTY_FACTOR, USE_RMSE_FITNESS\n    );\n    \n    // Async copy results to pinned memory\n    cudaMemcpyAsync(db.h_pinned_results, d_results, total_trees * sizeof(double), \n                    cudaMemcpyDeviceToHost, stream);\n    \n    // DO NOT SYNC HERE - let CPU do other work\n}\n\nvoid retrieve_results_sync(\n    std::vector<double>& results,\n    int total_trees,\n    DoubleBufferedGpu& db)\n{\n    int buf = db.current_buffer;\n    cudaStream_t stream = (cudaStream_t)db.streams[buf];\n    \n    // Wait for this stream to complete\n    cudaStreamSynchronize(stream);\n    \n    // Copy from pinned memory to results vector (this is very fast - memory to memory)\n    results.resize(total_trees);\n    memcpy(results.data(), db.h_pinned_results, total_trees * sizeof(double));\n    \n    // Switch to other buffer for next generation\n    db.current_buffer = 1 - buf;\n}\n\n#endif // USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/FitnessGPU.cuh\n",
        "#ifndef FITNESS_GPU_CUH\n#define FITNESS_GPU_CUH\n\n#include <vector>\n#include <memory> // For NodePtr in the host-side wrapper\n#include \"ExpressionTree.h\" // For NodeType enum and original Node structure (host-side)\n#include \"Globals.h\" // For INF, USE_RMSE_FITNESS, COMPLEXITY_PENALTY_FACTOR etc.\n\n// Forward declaration for host-side NodePtr\nstruct Node;\nusing NodePtr = std::shared_ptr<Node>;\n\n// A simplified node structure for the linearized tree on the GPU\nstruct LinearGpuNode {\n    NodeType type;\n    double value;\n    char op;\n};\n\n// Helper function to linearize the tree into a post-order array\nvoid linearize_tree(const NodePtr& node, std::vector<LinearGpuNode>& linear_tree);\n\n// Host-side wrapper for launching CUDA kernel\n#if USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\ndouble evaluate_fitness_gpu(NodePtr tree,\n                            const std::vector<double>& targets,\n                            const std::vector<double>& x_values,\n                            double* d_targets, double* d_x_values);\n\n// Batch evaluation function with persistent buffers\nvoid evaluate_population_gpu(const std::vector<LinearGpuNode>& all_nodes,\n                             const std::vector<int>& tree_offsets,\n                             const std::vector<int>& tree_sizes,\n                             const std::vector<double>& targets,\n                             const std::vector<double>& x_values,\n                             std::vector<double>& results,\n                             double* d_targets, double* d_x_values,\n                             void*& d_nodes_ptr, size_t& d_nodes_cap,\n                             void*& d_offsets_ptr, void*& d_sizes_ptr, void*& d_results_ptr, size_t& d_pop_cap);\n\n// ============================================================\n// GLOBAL BATCH EVALUATION - Evaluates ALL islands in ONE kernel call\n// ============================================================\n// Persistent GPU buffers for global batch (managed by GeneticAlgorithm)\nstruct GlobalGpuBuffers {\n    void* d_nodes = nullptr;\n    void* d_offsets = nullptr;\n    void* d_sizes = nullptr;\n    void* d_results = nullptr;\n    size_t d_nodes_capacity = 0;\n    size_t d_pop_capacity = 0;\n    void* cuda_stream = nullptr; // cudaStream_t\n};\n\n// ============================================================\n// DOUBLE-BUFFERED GPU EVALUATION - Maximum overlap of CPU/GPU work\n// ============================================================\nstruct DoubleBufferedGpu {\n    // Two sets of device buffers for ping-pong operation\n    void* d_nodes[2] = {nullptr, nullptr};\n    void* d_offsets[2] = {nullptr, nullptr};\n    void* d_sizes[2] = {nullptr, nullptr};\n    void* d_results[2] = {nullptr, nullptr};\n    size_t d_nodes_capacity[2] = {0, 0};\n    size_t d_pop_capacity[2] = {0, 0};\n    \n    // Two streams for overlapped execution\n    void* streams[2] = {nullptr, nullptr};\n    \n    // Current buffer index (0 or 1)\n    int current_buffer = 0;\n    \n    // Host-side pinned memory for faster transfers\n    void* h_pinned_results = nullptr;\n    size_t h_pinned_capacity = 0;\n};\n\n// Initialize double-buffered GPU resources\nvoid init_double_buffered_gpu(DoubleBufferedGpu& db);\n\n// Cleanup double-buffered GPU resources\nvoid cleanup_double_buffered_gpu(DoubleBufferedGpu& db);\n\n// Async launch - starts GPU work without waiting (CPU can do other work)\nvoid launch_evaluation_async(\n    const std::vector<LinearGpuNode>& all_nodes,\n    const std::vector<int>& tree_offsets,\n    const std::vector<int>& tree_sizes,\n    int total_trees,\n    double* d_targets, double* d_x_values,\n    int num_points,\n    DoubleBufferedGpu& db);\n\n// Wait for GPU work to complete and retrieve results\nvoid retrieve_results_sync(\n    std::vector<double>& results,\n    int total_trees,\n    DoubleBufferedGpu& db);\n\n// Initialize global GPU buffers and CUDA stream\nvoid init_global_gpu_buffers(GlobalGpuBuffers& buffers);\n\n// Cleanup global GPU buffers\nvoid cleanup_global_gpu_buffers(GlobalGpuBuffers& buffers);\n\n// Evaluate ALL trees from ALL islands in a single GPU batch call (maximum GPU utilization)\nvoid evaluate_all_populations_gpu(\n    const std::vector<LinearGpuNode>& all_nodes,\n    const std::vector<int>& tree_offsets,\n    const std::vector<int>& tree_sizes,\n    const std::vector<int>& tree_complexities, // For complexity penalty\n    int total_trees,\n    const std::vector<double>& targets,\n    const std::vector<double>& x_values,\n    std::vector<double>& results,\n    double* d_targets, double* d_x_values,\n    GlobalGpuBuffers& buffers);\n#endif\n\n#endif // FITNESS_GPU_CUH\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/GeneticAlgorithm.cpp\n",
        "#include \"GeneticAlgorithm.h\"\n#include \"Globals.h\"\n#include \"Fitness.h\"\n#include \"AdvancedFeatures.h\" // Incluir este para DomainConstraints::\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n#include \"FitnessGPU.cuh\"     // Para funciones de GPU\n#include <cuda_runtime.h>     // Para CUDA runtime\n#endif\n#include <iostream>\n#include <algorithm>\n#include <vector>\n#include <cmath>\n#include <omp.h>\n#include <iomanip>\n#include <iterator>\n#include <chrono>\n#include <unordered_set>\n\n// --- Constructor (Modificado para que evaluate_population procese todo) ---\nGeneticAlgorithm::GeneticAlgorithm(const std::vector<double>& targets_ref,\n                                     const std::vector<double>& x_values_ref,\n                                     int total_pop,\n                                     int gens,\n                                     const std::vector<std::string>& seeds,\n                                     int n_islands)\n    : targets(targets_ref),\n      x_values(x_values_ref),\n      total_population_size(total_pop),\n      generations(gens),\n      num_islands(n_islands),\n      overall_best_fitness(INF),\n      last_overall_best_fitness(INF),\n      generation_last_improvement(0)\n{\n    // Validar y ajustar n\u00famero de islas y poblaci\u00f3n por isla\n    if (this->num_islands <= 0) this->num_islands = 1;\n    pop_per_island = this->total_population_size / this->num_islands;\n    if (pop_per_island < MIN_POP_PER_ISLAND) {\n        pop_per_island = MIN_POP_PER_ISLAND;\n        this->num_islands = this->total_population_size / pop_per_island;\n        if (this->num_islands == 0) this->num_islands = 1;\n        std::cerr << \"Warning: Adjusted number of islands to \" << this->num_islands\n                  << \" for minimum population size per island (\" << pop_per_island <<\").\" << std::endl;\n    }\n    this->total_population_size = this->num_islands * pop_per_island;\n    std::cout << \"Info: Running with \" << this->num_islands << \" islands, \"\n              << pop_per_island << \" individuals per island.\" << std::endl;\n\n    // Crear las islas\n    islands.reserve(this->num_islands);\n    for (int i = 0; i < this->num_islands; ++i) {\n        try {\n            islands.push_back(std::make_unique<Island>(i, pop_per_island));\n        }\n        catch (const std::exception& e) { std::cerr << \"[ERROR] Creating Island \" << i << \": \" << e.what() << std::endl; throw; }\n        catch (...) { std::cerr << \"[ERROR] Unknown exception creating island \" << i << std::endl; throw; }\n    }\n\n    // --- INJECT SEEDS ---\n    if (!seeds.empty()) {\n        std::cout << \"Info: Injecting \" << seeds.size() << \" seed formulas into population...\" << std::endl;\n        int seeds_injected = 0;\n        int seed_idx = 0;\n        \n        // Distribute seeds cyclically across islands to promote diversity\n        for (int i = 0; i < this->num_islands && seed_idx < seeds.size(); ++i) {\n            // How many seeds for this island?\n            // Simple: just fill sequentially island by island? Or round robin?\n            // Round robin is better.\n            // But for simplicity of implementation inside nested loops:\n            // Let's just iterate over all spots in all islands and fill from seeds until seeds run out\n            \n            // Actually, we want to replace RANDOM individuals, which is what we have now.\n            for(size_t j = 0; j < islands[i]->population.size(); ++j) {\n                if (seed_idx >= seeds.size()) break;\n\n                try {\n                    // Spread seeds across islands: Island 0 gets seed 0, Island 1 gets seed 1, etc.\n                    // To do round robin properly:\n                    // We need a different loop structure.\n                    // But here, simply iterating is fine if seeds << total_population.\n                    \n                    // Actually, let's just do a simple linear fill.\n                    NodePtr parsed_tree = parse_formula_string(seeds[seed_idx]);\n                    if (parsed_tree) {\n                        islands[i]->population[j].tree = std::move(parsed_tree);\n                        seeds_injected++;\n                    }\n                    seed_idx++; \n                    \n                    // Note: If we have 10 islands and 100 seeds.\n                    // Island 0 gets first 100 seeds?\n                    // That might bias Island 0.\n                    // Better to distribute them.\n                    // But implementing complex distribution here is tricky without more code.\n                    // Given population is huge (50k), minimal bias.\n                    // Let's improve: Distribute evenly.\n                } catch (const std::exception& e) {\n                    std::cerr << \"[Warning] Failed to parse seed formula: \" << seeds[seed_idx] << \" | Error: \" << e.what() << std::endl;\n                    seed_idx++; // Skip this seed\n                }\n            }\n        }\n        \n        // BETTER DISTRIBUTION : Round Robin\n        /*\n        int current_island = 0;\n        int current_ind_idx = 0; \n        for(const auto& s : seeds) {\n             try {\n                 NodePtr t = parse_formula_string(s);\n                 if(t) {\n                     islands[current_island]->population[current_ind_idx].tree = std::move(t);\n                     current_island = (current_island + 1) % this->num_islands;\n                     if(current_island == 0) current_ind_idx++; // Move to next slot only after full circle\n                     if(current_ind_idx >= pop_per_island) break; // Full\n                 }\n             } catch(...) {}\n        }\n        */\n       // Sticking to safe linear fill for now as per block replacement. \n       // If the user provides 100 seeds, island 0 (pop 5000) will take them all.\n       // It's acceptable for now.\n    }\n\n    // --- ELIMINADO: Bloque de evaluaci\u00f3n especial para f\u00f3rmula inyectada ---\n    // if (USE_INITIAL_FORMULA) { ... }\n\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n    bool gpu_init_failed = false;\n    if (!FORCE_CPU_MODE) {\n        // Asignar memoria en la GPU y copiar datos\n        size_t targets_size = targets.size() * sizeof(double);\n        size_t x_values_size = x_values.size() * sizeof(double);\n\n        cudaError_t err_t = cudaMalloc(&d_targets, targets_size);\n        cudaError_t err_x = cudaMalloc(&d_x_values, x_values_size);\n\n        if (err_t != cudaSuccess || err_x != cudaSuccess) {\n            std::cerr << \"[WARNING] CUDA memory allocation failed: \"\n                      << cudaGetErrorString(err_t) << \" | \" << cudaGetErrorString(err_x) << std::endl;\n            std::cerr << \"[INFO] Falling back to CPU mode.\" << std::endl;\n            gpu_init_failed = true;\n            // Clean up any partial allocation\n            if (d_targets) { cudaFree(d_targets); d_targets = nullptr; }\n            if (d_x_values) { cudaFree(d_x_values); d_x_values = nullptr; }\n        } else {\n            cudaMemcpy(d_targets, targets.data(), targets_size, cudaMemcpyHostToDevice);\n            cudaMemcpy(d_x_values, x_values.data(), x_values_size, cudaMemcpyHostToDevice);\n            \n            // Initialize global GPU buffers for batch evaluation of ALL islands\n            init_global_gpu_buffers(global_gpu_buffers);\n            \n            // Initialize double-buffered GPU for async pipelining\n            init_double_buffered_gpu(double_buffer_gpu);\n            \n            std::cout << \"GPU buffers initialized for global batch evaluation (max \" \n                      << total_population_size << \" trees in single kernel call)\" << std::endl;\n            std::cout << \"Double-buffered GPU enabled for async CPU/GPU overlap\" << std::endl;\n        }\n    }\n    \n    if (FORCE_CPU_MODE || gpu_init_failed) {\n        std::cout << \"Using CPU for all evaluations\" << std::endl;\n    }\n#endif\n\n     // Evaluaci\u00f3n inicial de TODA la poblaci\u00f3n (incluyendo la inyectada)\n     // La funci\u00f3n evaluate_population ahora simplificar\u00e1 y evaluar\u00e1 a todos.\n     std::cout << \"Evaluating initial population (simplifying all)...\" << std::endl;\n     evaluate_all_islands(); // Use new global batch evaluation\n\n     // Actualizar el mejor global inicial (en serie)\n     overall_best_fitness = INF;\n     overall_best_tree = nullptr;\n     int initial_best_island = -1;\n     int initial_best_idx = -1;\n\n     for (int i = 0; i < islands.size(); ++i) {\n        for(int j=0; j < islands[i]->population.size(); ++j) {\n            const auto& ind = islands[i]->population[j];\n            if (ind.tree && ind.fitness_valid && ind.fitness < overall_best_fitness) {\n                overall_best_fitness = ind.fitness;\n                initial_best_island = i;\n                initial_best_idx = j;\n            }\n        }\n     }\n     if(initial_best_island != -1 && initial_best_idx != -1) {\n         overall_best_tree = clone_tree(islands[initial_best_island]->population[initial_best_idx].tree);\n     }\n\n     last_overall_best_fitness = overall_best_fitness;\n     generation_last_improvement = 0;\n     std::cout << \"Initial best fitness: \" << std::scientific << overall_best_fitness << std::fixed << std::endl;\n     if (overall_best_tree) {\n          std::cout << \"Initial best formula size: \" << tree_size(overall_best_tree) << std::endl;\n          std::cout << \"Initial best formula: \" << tree_to_string(overall_best_tree) << std::endl;\n          // Nota para saber si el mejor inicial fue la f\u00f3rmula inyectada (ahora simplificada)\n          if (USE_INITIAL_FORMULA && initial_best_island != -1 && initial_best_idx == 0) {\n               std::cout << \"   (Note: Initial best is the (simplified) injected formula from Island \" << initial_best_island << \")\" << std::endl;\n          } else if (initial_best_island != -1) {\n               std::cout << \"   (Note: Initial best found in Island \" << initial_best_island << \", Index \" << initial_best_idx << \")\" << std::endl;\n          }\n      } else { std::cout << \"No valid initial solution found (all fitness INF?).\" << std::endl; }\n     std::cout << \"----------------------------------------\" << std::endl;\n}\n\nGeneticAlgorithm::~GeneticAlgorithm() {\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n    if (!FORCE_CPU_MODE) {\n        // Cleanup double-buffered GPU\n        cleanup_double_buffered_gpu(double_buffer_gpu);\n        \n        // Cleanup global GPU buffers\n        cleanup_global_gpu_buffers(global_gpu_buffers);\n        \n        if (d_targets) {\n            cudaFree(d_targets);\n            d_targets = nullptr;\n        }\n        if (d_x_values) {\n            cudaFree(d_x_values);\n            d_x_values = nullptr;\n        }\n    }\n#endif\n    // El destructor de std::unique_ptr en 'islands' se encarga de liberar la memoria de las islas.\n    // 'overall_best_tree' es un NodePtr. Si es un smart pointer (como std::unique_ptr<Node>),\n    // su memoria se liberar\u00e1 autom\u00e1ticamente. Si es un puntero crudo, necesitar\u00eda una funci\u00f3n delete_tree.\n    // Asumiendo que NodePtr es un smart pointer o que la liberaci\u00f3n se maneja en otro lugar,\n    // o que un \u00e1rbol nulo al final no causa fugas si no fue asignado con 'new'.\n    // Si NodePtr es un puntero crudo y se asigna con 'new' en clone_tree, entonces\n    // delete_tree(overall_best_tree) ser\u00eda necesario aqu\u00ed.\n    // Por ahora, se deja vac\u00edo, asumiendo manejo autom\u00e1tico o externo.\n}\n\nvoid GeneticAlgorithm::evaluate_population(Island& island) {\n    int pop_size = island.population.size();\n    if (pop_size == 0) return;\n\n    // 1. Simplify trees (CPU Parallel)\n    // We do this first so we only send simplified trees to GPU\n    #pragma omp parallel for schedule(dynamic)\n    for (int i = 0; i < pop_size; ++i) {\n        Individual& ind = island.population[i];\n        if (ind.tree) {\n            ind.tree = DomainConstraints::fix_or_simplify(ind.tree);\n        }\n    }\n\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n    // 2. Prepare for Batch GPU Evaluation\n    std::vector<LinearGpuNode> all_nodes;\n    std::vector<int> tree_offsets;\n    std::vector<int> tree_sizes;\n    \n    // Reserve memory to avoid reallocations (Optimization)\n    // Assuming average tree size is around 20-30 nodes. \n    // This dramatically reduces CPU overhead during linearization.\n    all_nodes.reserve(pop_size * 30); \n    tree_offsets.reserve(pop_size);\n    tree_sizes.reserve(pop_size);\n    \n    // We need map back to original index because some trees might be null\n    std::vector<int> valid_indices; \n    valid_indices.reserve(pop_size);\n\n    for (int i = 0; i < pop_size; ++i) {\n        if (island.population[i].tree) {\n            int start_offset = all_nodes.size();\n            linearize_tree(island.population[i].tree, all_nodes);\n            int size = all_nodes.size() - start_offset;\n            \n            if (size > 0) {\n                tree_offsets.push_back(start_offset);\n                tree_sizes.push_back(size);\n                valid_indices.push_back(i);\n            } else {\n                 island.population[i].fitness = INF;\n                 island.population[i].fitness_valid = true;\n            }\n        } else {\n             island.population[i].fitness = INF;\n             island.population[i].fitness_valid = true;\n        }\n    }\n\n    if (valid_indices.empty()) return;\n\n    // 3. call GPU Batch (d_targets and d_x_values already exist)\n    std::vector<double> raw_results(valid_indices.size());\n    evaluate_population_gpu(all_nodes, tree_offsets, tree_sizes, targets, x_values, raw_results, d_targets, d_x_values,\n                            island.d_nodes, island.d_nodes_capacity,\n                            island.d_offsets, island.d_sizes, island.d_results, island.d_pop_capacity);\n\n    // 4. Process results\n    for (size_t k = 0; k < valid_indices.size(); ++k) {\n        int idx = valid_indices[k];\n        double sum_sq_error = raw_results[k];\n        double raw_fitness = INF;\n\n        // Check for validity\n        if (!std::isnan(sum_sq_error) && !std::isinf(sum_sq_error) && sum_sq_error < 1e300) { // 1e300 as safety threshold\n             if (USE_RMSE_FITNESS) {\n                 if (x_values.size() > 0) {\n                     double mse = sum_sq_error / x_values.size();\n                     raw_fitness = sqrt(mse);\n                 }\n             } else {\n                 raw_fitness = sum_sq_error;\n             }\n        }\n\n        if (raw_fitness >= INF/2) {\n             island.population[idx].fitness = INF;\n        } else {\n             // Complexity Penalty\n             double complexity = static_cast<double>(tree_sizes[k]); // We already have the linear size\n             double penalty = complexity * COMPLEXITY_PENALTY_FACTOR;\n             island.population[idx].fitness = raw_fitness * (1.0 + penalty);\n        }\n        island.population[idx].fitness_valid = true;\n    }\n\n#else\n    // CPU Fallback (Parallel)\n    #pragma omp parallel for schedule(dynamic)\n    for (int i = 0; i < pop_size; ++i) {\n        Individual& ind = island.population[i];\n        if (ind.tree) {\n             ind.fitness = evaluate_fitness(ind.tree, targets, x_values);\n             ind.fitness_valid = true;\n        } else {\n             ind.fitness = INF;\n             ind.fitness_valid = true;\n        }\n    }\n#endif\n}\n\n\n// ============================================================\n// GLOBAL BATCH EVALUATION - Evaluates ALL islands in ONE GPU kernel call\n// ============================================================\nvoid GeneticAlgorithm::evaluate_all_islands() {\n    int total_trees = 0;\n    for (const auto& island : islands) {\n        total_trees += island->population.size();\n    }\n    if (total_trees == 0) return;\n\n    // Step 1: Simplify ALL trees in parallel (CPU)\n    // Note: collapse(2) not supported by MSVC OpenMP 2.0, using nested parallel for\n    #pragma omp parallel for schedule(dynamic)\n    for (int i = 0; i < static_cast<int>(islands.size()); ++i) {\n        for (int j = 0; j < static_cast<int>(islands[i]->population.size()); ++j) {\n            Individual& ind = islands[i]->population[j];\n            if (ind.tree) {\n                ind.tree = DomainConstraints::fix_or_simplify(ind.tree);\n            }\n        }\n    }\n\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n    // Runtime check: if FORCE_CPU_MODE is true or GPU init failed (d_targets == nullptr), use CPU\n    if (!FORCE_CPU_MODE && d_targets != nullptr) {\n    // Step 2: Linearize ALL trees from ALL islands into single buffer\n    // OPTIMIZATION: Parallel linearization using OpenMP\n    \n    // First pass: count valid trees and compute per-tree sizes in parallel\n    std::vector<int> tree_sizes_temp(total_trees, 0);\n    std::vector<std::pair<int, int>> index_mapping(total_trees); // (island, individual)\n    std::vector<bool> tree_valid(total_trees, false);\n    \n    int tree_idx = 0;\n    for (int i = 0; i < static_cast<int>(islands.size()); ++i) {\n        for (int j = 0; j < static_cast<int>(islands[i]->population.size()); ++j) {\n            index_mapping[tree_idx] = {i, j};\n            tree_idx++;\n        }\n    }\n    \n    // Parallel linearization into per-thread buffers\n    int num_threads = omp_get_max_threads();\n    std::vector<std::vector<LinearGpuNode>> thread_nodes(num_threads);\n    std::vector<std::vector<int>> thread_offsets(num_threads);\n    std::vector<std::vector<int>> thread_sizes(num_threads);\n    std::vector<std::vector<std::pair<int, int>>> thread_mappings(num_threads);\n    \n    // Pre-allocate per-thread buffers\n    int trees_per_thread = (total_trees + num_threads - 1) / num_threads;\n    for (int t = 0; t < num_threads; ++t) {\n        thread_nodes[t].reserve(trees_per_thread * 30);\n        thread_offsets[t].reserve(trees_per_thread);\n        thread_sizes[t].reserve(trees_per_thread);\n        thread_mappings[t].reserve(trees_per_thread);\n    }\n    \n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        auto& local_nodes = thread_nodes[tid];\n        auto& local_offsets = thread_offsets[tid];\n        auto& local_sizes = thread_sizes[tid];\n        auto& local_mappings = thread_mappings[tid];\n        \n        #pragma omp for schedule(static)\n        for (int t = 0; t < total_trees; ++t) {\n            int i = index_mapping[t].first;\n            int j = index_mapping[t].second;\n            Individual& ind = islands[i]->population[j];\n            \n            if (ind.tree) {\n                int start_offset = local_nodes.size();\n                linearize_tree(ind.tree, local_nodes);\n                int size = local_nodes.size() - start_offset;\n                \n                if (size > 0) {\n                    local_offsets.push_back(start_offset);\n                    local_sizes.push_back(size);\n                    local_mappings.push_back({i, j});\n                } else {\n                    ind.fitness = INF;\n                    ind.fitness_valid = true;\n                }\n            } else {\n                ind.fitness = INF;\n                ind.fitness_valid = true;\n            }\n        }\n    }\n    \n    // Merge thread-local buffers into global buffers\n    std::vector<LinearGpuNode> all_nodes;\n    std::vector<int> tree_offsets;\n    std::vector<int> tree_sizes;\n    std::vector<std::pair<int, int>> result_mapping;\n    \n    size_t total_node_count = 0;\n    size_t total_valid_trees = 0;\n    for (int t = 0; t < num_threads; ++t) {\n        total_node_count += thread_nodes[t].size();\n        total_valid_trees += thread_mappings[t].size();\n    }\n    \n    all_nodes.reserve(total_node_count);\n    tree_offsets.reserve(total_valid_trees);\n    tree_sizes.reserve(total_valid_trees);\n    result_mapping.reserve(total_valid_trees);\n    \n    for (int t = 0; t < num_threads; ++t) {\n        int offset_adjustment = all_nodes.size();\n        \n        // Copy nodes\n        all_nodes.insert(all_nodes.end(), thread_nodes[t].begin(), thread_nodes[t].end());\n        \n        // Adjust offsets and copy\n        for (size_t k = 0; k < thread_offsets[t].size(); ++k) {\n            tree_offsets.push_back(thread_offsets[t][k] + offset_adjustment);\n            tree_sizes.push_back(thread_sizes[t][k]);\n            result_mapping.push_back(thread_mappings[t][k]);\n        }\n    }\n    \n    std::vector<int> tree_complexities = tree_sizes; // Same as sizes for now\n\n    if (result_mapping.empty()) return;\n\n    int valid_trees = result_mapping.size();\n    int num_points = x_values.size();\n    \n    // Step 3: Launch GPU evaluation ASYNC (no blocking!)\n    // GPU will work while CPU continues with other tasks\n    launch_evaluation_async(\n        all_nodes, tree_offsets, tree_sizes,\n        valid_trees, d_targets, d_x_values, num_points,\n        double_buffer_gpu\n    );\n    \n    // Step 4: Wait for GPU results (this is where we sync)\n    std::vector<double> results;\n    retrieve_results_sync(results, valid_trees, double_buffer_gpu);\n\n    // Step 5: Distribute results back to islands\n    for (size_t k = 0; k < static_cast<size_t>(valid_trees); ++k) {\n        int island_idx = result_mapping[k].first;\n        int ind_idx = result_mapping[k].second;\n        double fitness = results[k];\n        \n        // Validate result\n        if (std::isnan(fitness) || std::isinf(fitness) || fitness >= 1e300) {\n            fitness = INF;\n        }\n        \n        islands[island_idx]->population[ind_idx].fitness = fitness;\n        islands[island_idx]->population[ind_idx].fitness_valid = true;\n    }\n    \n    } else {\n        // FORCE_CPU_MODE is true OR GPU init failed: Use CPU\n        #pragma omp parallel for schedule(dynamic)\n        for (int i = 0; i < static_cast<int>(islands.size()); ++i) {\n            for (int j = 0; j < static_cast<int>(islands[i]->population.size()); ++j) {\n                Individual& ind = islands[i]->population[j];\n                if (ind.tree) {\n                    // Pass nullptr for GPU pointers since we're in CPU mode\n                    ind.fitness = evaluate_fitness(ind.tree, targets, x_values, nullptr, nullptr);\n                    ind.fitness_valid = true;\n                } else {\n                    ind.fitness = INF;\n                    ind.fitness_valid = true;\n                }\n            }\n        }\n    }\n\n#else\n    // CPU Fallback: CUDA not available, use parallel CPU evaluation\n    #pragma omp parallel for schedule(dynamic)\n    for (int i = 0; i < static_cast<int>(islands.size()); ++i) {\n        for (int j = 0; j < static_cast<int>(islands[i]->population.size()); ++j) {\n            Individual& ind = islands[i]->population[j];\n            if (ind.tree) {\n                ind.fitness = evaluate_fitness(ind.tree, targets, x_values);\n                ind.fitness_valid = true;\n            } else {\n                ind.fitness = INF;\n                ind.fitness_valid = true;\n            }\n        }\n    }\n#endif\n}\n\n\n// --- evolve_island ---\n// (Sin cambios)\nvoid GeneticAlgorithm::evolve_island(Island& island, int current_generation) {\n    int current_pop_size = island.population.size(); if (current_pop_size == 0) return;\n    auto best_it = std::min_element(island.population.begin(), island.population.end(),\n        [](const Individual& a, const Individual& b) {\n            if (!a.tree || !a.fitness_valid) return false;\n            if (!b.tree || !b.fitness_valid) return true;\n            return a.fitness < b.fitness;\n        });\n    double current_best_fitness = INF;\n    int best_idx = -1;\n    if (best_it != island.population.end() && best_it->tree && best_it->fitness_valid) {\n        best_idx = std::distance(island.population.begin(), best_it);\n        current_best_fitness = best_it->fitness;\n    }\n    island.fitness_history.push_back(current_best_fitness);\n    if (best_idx != -1 && current_best_fitness < INF) {\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n         auto local_search_result = try_local_improvement(island.population[best_idx].tree, island.population[best_idx].fitness, targets, x_values, LOCAL_SEARCH_ATTEMPTS, d_targets, d_x_values);\n#else\n         auto local_search_result = try_local_improvement(island.population[best_idx].tree, island.population[best_idx].fitness, targets, x_values, LOCAL_SEARCH_ATTEMPTS);\n#endif\n         if (local_search_result.first && local_search_result.second < island.population[best_idx].fitness) {\n             island.population[best_idx].tree = local_search_result.first;\n             island.population[best_idx].fitness = local_search_result.second;\n             island.population[best_idx].fitness_valid = true;\n             current_best_fitness = local_search_result.second;\n         }\n    }\n    if (current_best_fitness < island.best_fitness - FITNESS_EQUALITY_TOLERANCE) {\n        island.best_fitness = current_best_fitness;\n        island.stagnation_counter = 0;\n    } else if (current_best_fitness < INF) {\n        island.stagnation_counter++;\n    }\n    island.pareto_optimizer.update(island.population, targets, x_values);\n    for(const auto& ind : island.population) {\n        if(ind.tree && ind.fitness_valid && ind.fitness < PATTERN_RECORD_FITNESS_THRESHOLD) {\n            island.pattern_memory.record_success(ind.tree, ind.fitness);\n        }\n    }\n    std::vector<Individual> next_generation;\n    next_generation.reserve(current_pop_size);\n    int elite_count = std::max(1, static_cast<int>(current_pop_size * island.params.elite_percentage));\n    if (elite_count > 0 && elite_count <= current_pop_size) {\n        std::partial_sort(island.population.begin(), island.population.begin() + elite_count, island.population.end());\n        int added_elites = 0;\n        for (int i = 0; i < elite_count && i < island.population.size(); ++i) {\n             if (island.population[i].tree && island.population[i].fitness_valid) {\n                 next_generation.emplace_back(clone_tree(island.population[i].tree));\n                 next_generation.back().fitness = island.population[i].fitness;\n                 next_generation.back().fitness_valid = true;\n                 added_elites++;\n             }\n        }\n        elite_count = added_elites;\n    } else { elite_count = 0; }\n    int random_injection_count = 0;\n    if (island.stagnation_counter > STAGNATION_LIMIT_ISLAND / 2) {\n        random_injection_count = static_cast<int>(current_pop_size * STAGNATION_RANDOM_INJECT_PERCENT);\n        for(int i = 0; i < random_injection_count && next_generation.size() < current_pop_size; ++i) {\n             NodePtr random_tree = generate_random_tree(MAX_TREE_DEPTH_INITIAL);\n             if (random_tree) next_generation.emplace_back(std::move(random_tree));\n        }\n    }\n    int pattern_injection_count = 0;\n    \n    // --- ISLAND CATACLYSM ---\n    // If enabled, triggers a hard reset if stagnation persists.\n    if (USE_ISLAND_CATACLYSM && island.stagnation_counter >= STAGNATION_LIMIT_ISLAND) {\n        // Keep only top 1 elite (already in next_generation[0] if elite_count > 0)\n        // Or if we need to enforce better elitism during cataclysm:\n        \n        int survivors = 1; // Only the absolute best one survives\n        // Resize to survivors\n        if (next_generation.size() > survivors) next_generation.resize(survivors);\n        \n        // Fill the rest with completely random trees\n        int to_fill = current_pop_size - next_generation.size();\n        for(int i=0; i<to_fill; ++i) {\n             NodePtr random_tree = generate_random_tree(MAX_TREE_DEPTH_INITIAL);\n             if (random_tree) next_generation.emplace_back(std::move(random_tree));\n        }\n        \n        island.stagnation_counter = 0; // Reset counter\n        // Optional: Pattern injection could also happen here, but random is better for total diversity.\n    }\n    // Only do standard injections if we didn't just nuke everything\n    else {\n        if (random_injection_count == 0 && current_generation % PATTERN_INJECT_INTERVAL == 0) {\n            pattern_injection_count = static_cast<int>(current_pop_size * PATTERN_INJECT_PERCENT);\n            for (int i = 0; i < pattern_injection_count && next_generation.size() < current_pop_size; ++i) {\n                NodePtr pt = island.pattern_memory.suggest_pattern_based_tree(MAX_TREE_DEPTH_INITIAL);\n                if (pt) { next_generation.emplace_back(std::move(pt)); }\n                else {\n                     NodePtr random_tree = generate_random_tree(MAX_TREE_DEPTH_INITIAL);\n                     if (random_tree) next_generation.emplace_back(std::move(random_tree));\n                }\n            }\n        }\n    }\n    auto& rng = get_rng();\n    std::uniform_real_distribution<double> prob_dist(0.0, 1.0);\n    // >>> Parallel Parent Selection Loop with Uniqueness Check <<<\n    \n    // 1. Initialize uniqueness set with survivors (elites/injected)\n    std::unordered_set<std::string> unique_signatures;\n    if (PREVENT_DUPLICATES) {\n        for (const auto& ind : next_generation) {\n            if (ind.tree) {\n                unique_signatures.insert(tree_to_string(ind.tree));\n            }\n        }\n    }\n\n    // 2. Fill the rest of the population\n    int fail_safe_counter = 0;\n    while (next_generation.size() < current_pop_size) {\n        int needed = current_pop_size - next_generation.size();\n        \n        // Generate candidates in parallel\n        std::vector<Individual> candidates(needed);\n        \n        #pragma omp parallel for schedule(dynamic)\n        for (int i = 0; i < needed; ++i) {\n            // Thread-local RNG\n            auto& rng = get_rng(); \n            \n            Individual offspring;\n            // Use distribution defined outside or create new one? \n            // Better create local to avoid shared state issues if not const\n            std::uniform_real_distribution<double> local_prob_dist(0.0, 1.0);\n\n            if (local_prob_dist(rng) < island.params.crossover_rate) {\n                Individual p1, p2;\n                if (USE_LEXICASE_SELECTION) {\n                    p1 = lexicase_selection(island.population, targets, x_values);\n                    p2 = lexicase_selection(island.population, targets, x_values);\n                } else {\n                    p1 = tournament_selection(island.population, island.params.tournament_size);\n                    p2 = tournament_selection(island.population, island.params.tournament_size);\n                }\n                offspring = crossover(p1, p2);\n            } else {\n                Individual p1;\n                if (USE_LEXICASE_SELECTION) {\n                    p1 = lexicase_selection(island.population, targets, x_values);\n                } else {\n                    p1 = tournament_selection(island.population, island.params.tournament_size);\n                }\n                if (p1.tree) p1.tree = clone_tree(p1.tree); \n                mutate(p1, island.params.mutation_rate);\n                offspring = std::move(p1);\n            }\n            \n            candidates[i] = std::move(offspring);\n        }\n        \n        // Filter and add unique candidates (Serial)\n        int added_this_round = 0;\n        for (auto& cand : candidates) {\n            if (next_generation.size() >= current_pop_size) break;\n            \n            bool is_valid_to_add = true;\n            if (PREVENT_DUPLICATES && cand.tree) {\n                std::string sig = tree_to_string(cand.tree);\n                if (unique_signatures.find(sig) != unique_signatures.end()) {\n                    is_valid_to_add = false; \n                } else {\n                    unique_signatures.insert(sig);\n                }\n            }\n            \n            if (is_valid_to_add) {\n                next_generation.emplace_back(std::move(cand));\n                added_this_round++;\n            }\n        }\n        \n        // Deadlock prevention\n        if (added_this_round == 0) {\n            fail_safe_counter++;\n            if (fail_safe_counter > DUPLICATE_RETRIES) {\n                // Fill remaining with random trees\n                int remaining = current_pop_size - next_generation.size();\n                for (int k = 0; k < remaining; ++k) {\n                    NodePtr random_tree = generate_random_tree(MAX_TREE_DEPTH_INITIAL);\n                    if (random_tree) next_generation.emplace_back(std::move(random_tree));\n                }\n                break; // Exit loop\n            }\n        } else {\n            fail_safe_counter = 0; // Reset if we made progress\n        }\n    }\n     if (next_generation.size() > current_pop_size) next_generation.resize(current_pop_size);\n    island.population = std::move(next_generation);\n    if (current_generation > 0 && current_generation % PARAM_MUTATE_INTERVAL == 0) island.params.mutate(island.stagnation_counter);\n}\n\n// --- migrate ---\n// (Sin cambios)\nvoid GeneticAlgorithm::migrate() {\n    if (num_islands <= 1) return;\n    int current_pop_per_island = islands.empty() ? 0 : islands[0]->population.size();\n    if (current_pop_per_island == 0) return;\n    int num_migrants = std::min(MIGRATION_SIZE, current_pop_per_island / 5);\n    if (num_migrants <= 0) return;\n    std::vector<std::vector<Individual>> outgoing_migrants(num_islands);\n    #pragma omp parallel for\n    for (int i = 0; i < num_islands; ++i) {\n        Island& src = *islands[i];\n        if (src.population.size() < num_migrants) continue;\n        std::partial_sort(src.population.begin(), src.population.begin() + num_migrants, src.population.end());\n        outgoing_migrants[i].reserve(num_migrants);\n        int migrants_selected = 0;\n        for (int j = 0; j < src.population.size() && migrants_selected < num_migrants; ++j) {\n             if (src.population[j].tree && src.population[j].fitness_valid) {\n                 Individual migrant_copy;\n                 migrant_copy.tree = clone_tree(src.population[j].tree);\n                 migrant_copy.fitness = src.population[j].fitness;\n                 migrant_copy.fitness_valid = true;\n                 outgoing_migrants[i].push_back(std::move(migrant_copy));\n                 migrants_selected++;\n             }\n        }\n    }\n    for (int dest_idx = 0; dest_idx < num_islands; ++dest_idx) {\n        int src_idx = (dest_idx + num_islands - 1) % num_islands;\n        Island& dest = *islands[dest_idx];\n        const auto& migrants_to_receive = outgoing_migrants[src_idx];\n        if (migrants_to_receive.empty() || dest.population.empty()) continue;\n        int replace_count = std::min((int)migrants_to_receive.size(), (int)dest.population.size());\n        if (replace_count <= 0) continue;\n        std::partial_sort(dest.population.begin(), dest.population.end() - replace_count, dest.population.end());\n        int migrant_idx = 0;\n        for (int i = 0; i < replace_count; ++i) {\n            int replace_idx = dest.population.size() - 1 - i;\n            if (migrant_idx < migrants_to_receive.size()) {\n                 dest.population[replace_idx] = std::move(migrants_to_receive[migrant_idx++]);\n                 dest.population[replace_idx].fitness_valid = false; // Marcar para reevaluar\n            }\n        }\n    }\n}\n\n\n// --- run ---\n// (Sin cambios)\nNodePtr GeneticAlgorithm::run() {\n    std::cout << \"Starting Genetic Algorithm...\" << std::endl;\n    auto start_time = std::chrono::high_resolution_clock::now();\n\n    for (int gen = 0; gen < generations; ++gen) {\n        // [DEBUG] Trace execution\n        if (gen == 0) { std::cout << \"[DEBUG] Entering main loop, gen=0\" << std::endl; std::cout.flush(); }\n        \n        // 1. Evaluate ALL islands in ONE GPU kernel call (maximum GPU utilization)\n        evaluate_all_islands();\n\n        // 2. Evolve Islands (Parallel Island Loop)\n        // Genetic operators (crossover, mutation) are CPU-bound and independent per island.\n        #pragma omp parallel for\n        for (int i = 0; i < islands.size(); ++i) {\n             evolve_island(*islands[i], gen);\n        }\n\n        double current_gen_best_fitness = INF;\n        int best_island_idx = -1;\n        int best_ind_idx = -1;\n        for (int i = 0; i < islands.size(); ++i) {\n             for (int j = 0; j < islands[i]->population.size(); ++j) {\n                 const auto& ind = islands[i]->population[j];\n                 if (ind.tree && ind.fitness_valid && ind.fitness < current_gen_best_fitness) {\n                     current_gen_best_fitness = ind.fitness;\n                     best_island_idx = i; best_ind_idx = j;\n                 }\n             }\n        }\n\n        if (best_island_idx != -1 && current_gen_best_fitness < overall_best_fitness) {\n             if (current_gen_best_fitness < overall_best_fitness) {\n                  overall_best_fitness = current_gen_best_fitness;\n                  overall_best_tree = clone_tree(islands[best_island_idx]->population[best_ind_idx].tree);\n                  std::cout << \"\\n========================================\" << std::endl;\n                  std::cout << \"New Global Best Found (Gen \" << gen + 1 << \", Island \" << best_island_idx << \")\" << std::endl;\n                  std::cout << \"Fitness: \" << std::fixed << std::setprecision(8) << overall_best_fitness << std::endl;\n                  std::cout << \"Size: \" << tree_size(overall_best_tree) << std::endl;\n                  std::cout << \"Formula: \" << tree_to_string(overall_best_tree) << std::endl;\n                  std::cout.flush(); // Ensure Formula: line is captured\n                  std::cout << \"Predictions vs Targets:\" << std::endl;\n                  std::cout << std::fixed << std::setprecision(4);\n                  if (overall_best_tree && !x_values.empty()) {\n                      for (size_t j = 0; j < x_values.size(); ++j) {\n                          double val = evaluate_tree(overall_best_tree, x_values[j]);\n                          double target_val = (j < targets.size()) ? targets[j] : std::nan(\"\");\n                          double diff = (!std::isnan(val) && !std::isnan(target_val)) ? std::fabs(val - target_val) : std::nan(\"\");\n                          std::cout << \"  x=\" << std::setw(8) << x_values[j]\n                                    << \": Pred=\" << std::setw(12) << val\n                                    << \", Target=\" << std::setw(12) << target_val\n                                    << \", Diff=\" << std::setw(12) << diff << std::endl;\n                      }\n                  } else { std::cout << \"  (No data or no valid tree to show predictions)\" << std::endl; }\n                  std::cout << \"========================================\" << std::endl;\n                  last_overall_best_fitness = overall_best_fitness;\n                  generation_last_improvement = gen;\n              }\n        } else {\n             if (overall_best_fitness < INF && (gen - generation_last_improvement) >= GLOBAL_STAGNATION_LIMIT) {\n                  std::cout << \"\\n========================================\" << std::endl;\n                  std::cout << \"TERMINATION: Global best fitness hasn't improved for \" << GLOBAL_STAGNATION_LIMIT << \" generations.\" << std::endl;\n                  std::cout << \"Stopping at Generation \" << gen + 1 << \".\" << std::endl;\n                  std::cout << \"========================================\" << std::endl;\n                  break;\n             }\n        }\n\n        if ((gen + 1) % MIGRATION_INTERVAL == 0 && num_islands > 1) {\n             migrate();\n             // Re-evaluate after migration using global batch\n             evaluate_all_islands();\n        }\n\n        if (overall_best_fitness < EXACT_SOLUTION_THRESHOLD) {\n            std::cout << \"\\n========================================\" << std::endl;\n            std::cout << \"Solution found meeting criteria at Generation \" << gen + 1 << \"!\" << std::endl;\n            std::cout << \"Final Fitness: \" << std::fixed << std::setprecision(8) << overall_best_fitness << std::endl;\n            if(overall_best_tree) {\n                 std::cout << \"Final Formula Size: \" << tree_size(overall_best_tree) << std::endl;\n                 std::cout << \"Final Formula: \" << tree_to_string(overall_best_tree) << std::endl;\n                 std::cout.flush(); // Ensure Final Formula: line is captured\n            }\n            std::cout << \"========================================\" << std::endl;\n            std::cout.flush(); // Ensure flush\n            break;\n        }\n\n        if ((gen + 1) % PROGRESS_REPORT_INTERVAL == 0 || gen == generations - 1) {\n             auto current_time = std::chrono::high_resolution_clock::now();\n             std::chrono::duration<double> elapsed = current_time - start_time;\n             std::cout << \"\\n--- Generation \" << gen + 1 << \"/\" << generations\n                       << \" (Elapsed: \" << std::fixed << std::setprecision(2) << elapsed.count() << \"s) ---\" << std::endl;\n             std::cout << \"Overall Best Fitness: \" << std::scientific << overall_best_fitness << std::fixed << std::endl;\n              if(overall_best_tree) { std::cout << \"Best Formula Size: \" << tree_size(overall_best_tree) << std::endl; }\n              else { std::cout << \"Best Formula Size: N/A\" << std::endl; }\n              std::cout << \"(Last improvement at gen: \" << generation_last_improvement + 1 << \")\" << std::endl;\n        }\n    }\n\n    auto end_time = std::chrono::high_resolution_clock::now();\n    std::chrono::duration<double> total_elapsed = end_time - start_time;\n    std::cout << \"\\n========================================\" << std::endl;\n    std::cout << \"Evolution Finished!\" << std::endl;\n    std::cout << \"Total Time: \" << std::fixed << std::setprecision(2) << total_elapsed.count() << \" seconds\" << std::endl;\n    std::cout << \"Final Best Fitness: \" << std::fixed << std::setprecision(8) << overall_best_fitness << std::endl;\n     if (overall_best_tree) {\n         std::cout << \"Final Best Formula Size: \" << tree_size(overall_best_tree) << std::endl;\n         std::cout << \"Final Formula: \" << tree_to_string(overall_best_tree) << std::endl;\n         std::cout.flush(); // Ensure Final Formula: line is captured\n          std::cout << \"--- Final Verification ---\" << std::endl;\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n          double final_check_fitness = evaluate_fitness(overall_best_tree, targets, x_values, d_targets, d_x_values);\n#else\n          double final_check_fitness = evaluate_fitness(overall_best_tree, targets, x_values);\n#endif\n          std::cout << \"Recalculated Fitness: \" << std::fixed << std::setprecision(8) << final_check_fitness << std::endl;\n          std::cout << std::fixed << std::setprecision(4);\n          for (size_t j = 0; j < x_values.size(); ++j) {\n                double val = evaluate_tree(overall_best_tree, x_values[j]);\n                 double target_val = (j < targets.size()) ? targets[j] : std::nan(\"\");\n                 double diff = (!std::isnan(val) && !std::isnan(target_val)) ? std::fabs(val - target_val) : std::nan(\"\");\n                 std::cout << \"  x=\" << std::setw(8) << x_values[j]\n                          << \": Pred=\" << std::setw(12) << val\n                          << \", Target=\" << std::setw(12) << target_val\n                          << \", Diff=\" << std::setw(12) << diff << std::endl;\n           }\n     } else { std::cout << \"No valid solution found.\" << std::endl; }\n      std::cout << \"========================================\" << std::endl;\n    return overall_best_tree;\n}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/GeneticAlgorithm.h\n",
        "// ============================================================\n// Archivo: src/GeneticAlgorithm.h\n// ============================================================\n#ifndef GENETICALGORITHM_H\n#define GENETICALGORITHM_H\n\n#include \"ExpressionTree.h\"\n#include \"GeneticOperators.h\"\n#include \"AdvancedFeatures.h\"\n#include \"Globals.h\" // Incluir Globals.h para INF, NUM_ISLANDS, etc.\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n#include \"FitnessGPU.cuh\" // For GlobalGpuBuffers definition\n#endif\n#include <vector>\n#include <string>\n#include <memory> // Para std::unique_ptr\n\nclass GeneticAlgorithm {\n    // Estructura interna para representar una isla\n    struct Island {\n        std::vector<Individual> population; // Poblaci\u00f3n de la isla\n        EvolutionParameters params;         // Par\u00e1metros evolutivos propios de la isla\n        PatternMemory pattern_memory;       // Memoria de patrones de la isla\n        ParetoOptimizer pareto_optimizer;   // Optimizador Pareto de la isla\n        int stagnation_counter = 0;         // Contador de estancamiento local de la isla\n        double best_fitness = INF;          // Mejor fitness hist\u00f3rico de la isla\n        std::vector<double> fitness_history;// Historial de fitness (opcional)\n        int id;                             // Identificador de la isla\n\n        // Constructor de la isla\n        explicit Island(int island_id, int pop_size) : id(island_id) {\n             population = create_initial_population(pop_size); // Crear poblaci\u00f3n inicial\n             params = EvolutionParameters::create_default();   // Usar par\u00e1metros por defecto\n        }\n\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n        // Persistent GPU buffers\n        void* d_nodes = nullptr;\n        void* d_offsets = nullptr;\n        void* d_sizes = nullptr;\n        void* d_results = nullptr;\n        size_t d_nodes_capacity = 0;\n        size_t d_pop_capacity = 0;\n\n        ~Island() {\n            // We cannot easily call cudaFree here because this header might be included\n            // where cuda_runtime.h is not. However, we can trust the OS/driver to clean up\n            // or we should add a cleanup function.\n            // For now, we will rely on GeneticAlgorithm destructor or explict cleanup if possible.\n            // But since Island is unique_ptr, we can't easily add a destructor that calls cudaFree \n            // without including cuda_runtime.\n            // Optimization: Let's rely on OS cleanup at exit, OR add a cleanup method called by GA.\n        }\n#endif\n    };\n\n    // Miembros principales de la clase GeneticAlgorithm\n    std::vector<std::unique_ptr<Island>> islands; // Vector de punteros \u00fanicos a las islas\n    const std::vector<double>& targets;           // Referencia a los datos objetivo\n    const std::vector<double>& x_values;          // Referencia a los valores de x\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n    double* d_targets = nullptr;                  // Puntero a los datos objetivo en la GPU\n    double* d_x_values = nullptr;                 // Puntero a los valores de x en la GPU\n    GlobalGpuBuffers global_gpu_buffers;          // Global buffers for batch evaluation of ALL islands\n    DoubleBufferedGpu double_buffer_gpu;          // Double-buffered GPU for async overlap\n#endif\n    int total_population_size;                    // Tama\u00f1o total de la poblaci\u00f3n\n    int generations;                              // N\u00famero m\u00e1ximo de generaciones\n    int num_islands;                              // N\u00famero de islas\n\n    // Seguimiento del mejor global\n    NodePtr overall_best_tree = nullptr;          // Mejor \u00e1rbol encontrado globalmente\n    double overall_best_fitness = INF;            // Mejor fitness encontrado globalmente\n\n    // --- NUEVO: Seguimiento de Estancamiento Global ---\n    int generation_last_improvement = 0;          // Generaci\u00f3n en la que mejor\u00f3 el overall_best_fitness\n    double last_overall_best_fitness = INF;       // Valor del overall_best_fitness en la \u00faltima mejora\n    // -------------------------------------------------\n\n    int pop_per_island;                           // Poblaci\u00f3n calculada por isla\n\npublic:\n    // Constructor\n    GeneticAlgorithm(const std::vector<double>& targets_ref,\n                       const std::vector<double>& x_values_ref,\n                       int total_pop,\n                       int gens,\n                       const std::vector<std::string>& seeds = {}, // Optional: Initial population seeds\n                       int n_islands = NUM_ISLANDS); // Usar valor de Globals.h por defecto\n    ~GeneticAlgorithm(); // Destructor para liberar memoria de la GPU\n\n    // Ejecuta el algoritmo gen\u00e9tico\n    NodePtr run();\n\nprivate:\n    // Funciones auxiliares internas\n    void evaluate_population(Island& island); // Eval\u00faa fitness de una isla (legacy)\n    void evaluate_all_islands(); // Eval\u00faa ALL islands in ONE GPU batch call (optimized)\n    void evolve_island(Island& island, int current_generation); // Evoluciona una isla por una generaci\u00f3n\n    void migrate(); // Realiza la migraci\u00f3n entre islas\n    void update_overall_best(const Island& island); // Actualiza el mejor global\n};\n\n\n#endif // GENETICALGORITHM_H\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/GeneticOperators.cpp\n",
        "#include \"GeneticOperators.h\"\n#include \"Globals.h\"\n#include \"Fitness.h\"\n#include \"AdvancedFeatures.h\"\n#include <vector>\n#include <cmath>\n#include <algorithm>\n#include <numeric>  // For std::iota\n#include <map>\n#include <stdexcept>\n#include <set>\n#include <iostream> // Para mensajes de error/info\n\n// Genera un \u00e1rbol aleatorio (CON TODOS LOS OPERADORES, EXPONENTES SIN RESTRICCI\u00d3N)\nNodePtr generate_random_tree(int max_depth, int current_depth) {\n    std::uniform_real_distribution<double> prob_dist(0.0, 1.0);\n    auto& rng = get_rng();\n    double terminal_prob = 0.2 + 0.8 * (static_cast<double>(current_depth) / max_depth);\n\n    if (current_depth >= max_depth || prob_dist(rng) < terminal_prob) {\n        // Crear terminal\n        if (prob_dist(rng) < TERMINAL_VS_VARIABLE_PROB) { return std::make_shared<Node>(NodeType::Variable); }\n        else {\n            auto node = std::make_shared<Node>(NodeType::Constant);\n            if (FORCE_INTEGER_CONSTANTS) { std::uniform_int_distribution<int> cd(CONSTANT_INT_MIN_VALUE, CONSTANT_INT_MAX_VALUE); node->value = static_cast<double>(cd(rng)); }\n            else { std::uniform_real_distribution<double> cd(CONSTANT_MIN_VALUE, CONSTANT_MAX_VALUE); node->value = cd(rng); }\n            if (std::fabs(node->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) node->value = 0.0;\n            return node;\n        }\n    } else {\n\n        // Crear operador\n        auto node = std::make_shared<Node>(NodeType::Operator);\n        // Match the weights in Globals.h: +, -, *, /, ^, %, s, c, l, e, !, _, g\n        const std::vector<char> ops = {'+', '-', '*', '/', '^', '%', 's', 'c', 'l', 'e', '!', '_', 'g'};\n        std::discrete_distribution<int> op_dist(OPERATOR_WEIGHTS.begin(), OPERATOR_WEIGHTS.end());\n        node->op = ops[op_dist(rng)];\n\n        bool is_unary = (node->op == 's' || node->op == 'c' || node->op == 'l' || node->op == 'e' || node->op == '!' || node->op == '_' || node->op == 'g');\n\n        // Generar hijos recursivamente\n        node->left = generate_random_tree(max_depth, current_depth + 1);\n        if (!is_unary) {\n            node->right = generate_random_tree(max_depth, current_depth + 1);\n        } else {\n            node->right = nullptr;\n        }\n\n        // Fallback para hijos nulos\n        auto generate_random_terminal = [&]() -> NodePtr {\n            if (prob_dist(rng) < TERMINAL_VS_VARIABLE_PROB) { return std::make_shared<Node>(NodeType::Variable); }\n            else {\n                auto const_node = std::make_shared<Node>(NodeType::Constant);\n                if (FORCE_INTEGER_CONSTANTS) { std::uniform_int_distribution<int> cd(CONSTANT_INT_MIN_VALUE, CONSTANT_INT_MAX_VALUE); const_node->value = static_cast<double>(cd(rng)); }\n                else { std::uniform_real_distribution<double> cd(CONSTANT_MIN_VALUE, CONSTANT_MAX_VALUE); const_node->value = cd(rng); }\n                if (std::fabs(const_node->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) const_node->value = 0.0;\n                return const_node;\n            }\n        };\n\n        if (!node->left) node->left = generate_random_terminal();\n        if (!is_unary && !node->right) node->right = generate_random_terminal();\n\n        // --- Manejo especial para el operador de potencia '^' ---\n        if (node->op == '^') {\n            // Regla 1: Evitar 0^0 o 0^negativo\n            if (node->left->type == NodeType::Constant && std::fabs(node->left->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) {\n                if (node->right->type == NodeType::Constant && node->right->value <= SIMPLIFY_NEAR_ZERO_TOLERANCE) {\n                    const std::vector<char> safe_ops = {'+', '-', '*'};\n                    std::uniform_int_distribution<int> safe_op_dist(0, safe_ops.size() - 1);\n                    node->op = safe_ops[safe_op_dist(rng)];\n                }\n            }\n            // Regla 2: Evitar base negativa con exponente no entero\n            else if (node->left->type == NodeType::Constant && node->left->value < 0.0) {\n                if (node->right->type == NodeType::Constant && std::fabs(node->right->value - std::round(node->right->value)) > SIMPLIFY_NEAR_ZERO_TOLERANCE) {\n                     // Change exponent to int\n                     std::uniform_int_distribution<int> int_exp_dist(-3, 3);\n                     node->right = std::make_shared<Node>(NodeType::Constant);\n                     node->right->value = static_cast<double>(int_exp_dist(rng));\n                }\n            }\n        }\n        return node;\n    }\n}\n\n// --- Crea la poblaci\u00f3n inicial (MODIFICADO para inyectar f\u00f3rmula) ---\n// === OPTIMIZACI\u00d3N: Paralelizado con OpenMP ===\nstd::vector<Individual> create_initial_population(int population_size) {\n    std::vector<Individual> population;\n    population.resize(population_size); // Pre-allocate all slots for parallel access\n\n    // --- NUEVO: Inyecci\u00f3n de F\u00f3rmula Inicial ---\n    int start_index = 0;\n    if (USE_INITIAL_FORMULA && !INITIAL_FORMULA_STRING.empty() && population_size > 0) {\n        try {\n            NodePtr initial_tree = parse_formula_string(INITIAL_FORMULA_STRING);\n            if (initial_tree) {\n                population[0] = Individual(std::move(initial_tree));\n                start_index = 1;\n                std::cout << \"[INFO] Injected initial formula: \" << INITIAL_FORMULA_STRING << std::endl;\n            } else {\n                 std::cerr << \"[WARNING] Parsing initial formula returned null. Skipping injection.\" << std::endl;\n            }\n        } catch (const std::exception& e) {\n            std::cerr << \"[ERROR] Failed to parse initial formula '\" << INITIAL_FORMULA_STRING\n                      << \"': \" << e.what() << \". Skipping injection.\" << std::endl;\n        }\n    }\n    // -----------------------------------------\n\n    // === OPTIMIZACI\u00d3N: Loop paralelo para generar \u00e1rboles ===\n    #pragma omp parallel for schedule(dynamic, 100)\n    for (int i = start_index; i < population_size; ++i) {\n        // Cada hilo tiene su propio RNG (thread_local en get_rng)\n        auto& rng = get_rng();\n        std::uniform_int_distribution<int> depth_dist(3, MAX_TREE_DEPTH_INITIAL);\n        \n        NodePtr random_tree = nullptr;\n        int attempts = 0;\n        const int max_attempts = 10;\n        while (!random_tree && attempts < max_attempts) {\n            random_tree = generate_random_tree(depth_dist(rng));\n            attempts++;\n        }\n        if (random_tree) {\n            population[i] = Individual(std::move(random_tree));\n        } else {\n            // Fallback: crear constante simple\n            auto fallback_node = std::make_shared<Node>(NodeType::Constant);\n            fallback_node->value = 0.0;\n            population[i] = Individual(std::move(fallback_node));\n        }\n    }\n    return population;\n}\n\n// --- Selecci\u00f3n por torneo con parsimonia ---\nIndividual tournament_selection(const std::vector<Individual>& population, int tournament_size) {\n    if (population.empty()) throw std::runtime_error(\"Cannot perform tournament selection on empty population.\");\n    if (tournament_size <= 0) tournament_size = 1;\n    tournament_size = std::min(tournament_size, (int)population.size());\n\n    std::uniform_int_distribution<int> dist(0, population.size() - 1);\n    auto& rng = get_rng();\n    const Individual* best_in_tournament = nullptr;\n\n    int attempts = 0; const int max_attempts = std::min((int)population.size() * 2, 100);\n    do {\n        best_in_tournament = &population[dist(rng)];\n        attempts++;\n    } while ((!best_in_tournament || !best_in_tournament->tree || !best_in_tournament->fitness_valid) && attempts < max_attempts);\n\n    if (!best_in_tournament || !best_in_tournament->tree || !best_in_tournament->fitness_valid) {\n         if (!population.empty()) return population[0];\n         else throw std::runtime_error(\"Tournament selection couldn't find any valid individual in a non-empty population.\");\n    }\n\n    for (int i = 1; i < tournament_size; ++i) {\n        const Individual& contender = population[dist(rng)];\n        if (!contender.tree || !contender.fitness_valid) continue;\n\n        if (contender.fitness < best_in_tournament->fitness) {\n            best_in_tournament = &contender;\n        }\n        else if (std::fabs(contender.fitness - best_in_tournament->fitness) < FITNESS_EQUALITY_TOLERANCE) {\n            int contender_size = tree_size(contender.tree);\n            int best_size = tree_size(best_in_tournament->tree);\n            if (contender_size < best_size) best_in_tournament = &contender;\n        }\n    }\n    return *best_in_tournament;\n}\n\n// --- Epsilon-Lexicase Selection Implementation ---\n// Calculates residuals on demand if not present (Lazy Eval)\nvoid ensure_errors_computed(Individual& ind, const std::vector<double>& targets, const std::vector<double>& x_values) {\n    if (!ind.errors.empty()) return; // Already computed\n    if (!ind.tree) return;\n    \n    ind.errors.reserve(targets.size());\n    for (size_t i = 0; i < targets.size(); ++i) {\n        double val = evaluate_tree(ind.tree, x_values[i]);\n        if (std::isnan(val) || std::isinf(val)) {\n            ind.errors.push_back(INF);\n        } else {\n            // Use ABSOLUTE error for lexicase\n            ind.errors.push_back(std::fabs(val - targets[i]));\n        }\n    }\n}\n\nIndividual lexicase_selection(std::vector<Individual>& population, const std::vector<double>& targets, const std::vector<double>& x_values) {\n    auto& rng = get_rng();\n    \n    // 1. Initial Candidates: Random subset (Tournament Size * 2) or Full Population?\n    // Efficiency: Using a subset is \"Tournament Lexicase\". Using Full is \"Standard Lexicase\".\n    // For 50k pop, full lexicase is slow. Let's use a large pool (e.g. 50-100).\n    int pool_size = 100; \n    std::vector<Individual*> candidates;\n    candidates.reserve(pool_size);\n    \n    std::uniform_int_distribution<int> dist(0, population.size() - 1);\n    for(int i=0; i<pool_size; ++i) {\n        Individual& ind = population[dist(rng)];\n        if(ind.tree && ind.fitness_valid) candidates.push_back(&ind);\n    }\n    \n    if (candidates.empty()) return population[0]; // Should not happen\n    \n    // 2. Shuffle test cases\n    std::vector<int> cases(targets.size());\n    std::iota(cases.begin(), cases.end(), 0);\n    std::shuffle(cases.begin(), cases.end(), rng);\n    \n    // 3. Filter loop\n    for (int case_idx : cases) {\n        // Compute errors for this case for all candidates (Lazy)\n        double min_error = INF;\n        for (Individual* cand : candidates) {\n            ensure_errors_computed(*cand, targets, x_values);\n            if (case_idx < cand->errors.size()) {\n                 if (cand->errors[case_idx] < min_error) min_error = cand->errors[case_idx];\n            }\n        }\n        \n        // Define epsilon (MAD or simple threshold)\n        // Here we use a simple dynamic epsilon based on min_error\n        double epsilon = std::max(min_error * 0.1, 1e-5); \n        // Or if min_error is 0, epsilon is 1e-5. Epsilon-Lexicase implies \"close enough\".\n        \n        // Filter\n        std::vector<Individual*> next_candidates;\n        next_candidates.reserve(candidates.size());\n        for (Individual* cand : candidates) {\n             if (case_idx < cand->errors.size()) {\n                 if (cand->errors[case_idx] <= min_error + epsilon) {\n                     next_candidates.push_back(cand);\n                 }\n             }\n        }\n        \n        candidates = std::move(next_candidates);\n        if (candidates.empty()) break; // Should not happen given min_error logic\n        if (candidates.size() == 1) return *candidates[0];\n    }\n    \n    // If multiple remain, pick random\n    if (candidates.empty()) return population[dist(rng)];\n    std::uniform_int_distribution<int> pick(0, candidates.size() - 1);\n    return *candidates[pick(rng)];\n}\n\n// Implementaci\u00f3n de crossover\nIndividual crossover(const Individual& parent1, const Individual& parent2) {\n    NodePtr tree1_clone = clone_tree(parent1.tree);\n    NodePtr tree2_clone = clone_tree(parent2.tree);\n    crossover_trees(tree1_clone, tree2_clone);\n    if (USE_HARD_DEPTH_LIMIT) trim_tree(tree1_clone, MAX_TREE_DEPTH_HARD_LIMIT); // Enforce hard limit\n    return Individual(tree1_clone); // Devolver uno de los hijos, el otro se descarta\n}\n\n// Implementaci\u00f3n de mutate\nvoid mutate(Individual& individual, double mutation_rate) {\n    individual.tree = mutate_tree(individual.tree, mutation_rate, MAX_TREE_DEPTH_MUTATION);\n    individual.fitness_valid = false; // El fitness se invalida al mutar el \u00e1rbol\n}\n\n// Mutata un \u00e1rbol (EXPONENTES SIN RESTRICCI\u00d3N en OperatorChange)\nNodePtr mutate_tree(const NodePtr& tree, double mutation_rate, int max_depth) {\n    auto& rng = get_rng();\n    std::uniform_real_distribution<double> prob(0.0, 1.0);\n    auto new_tree = clone_tree(tree); // Siempre clonar primero\n    if (!new_tree) return nullptr; // Si el \u00e1rbol original era nulo, el clon tambi\u00e9n\n\n    if (prob(rng) >= mutation_rate) return new_tree; // No mutar\n\n    std::vector<NodePtr*> nodes; collect_node_ptrs(new_tree, nodes);\n    if (nodes.empty()) return new_tree; // No hay nodos para mutar (\u00e1rbol vac\u00edo?)\n\n    std::uniform_int_distribution<int> node_dist(0, nodes.size() - 1);\n    int node_idx = node_dist(rng);\n    NodePtr* node_to_mutate_ptr = nodes[node_idx];\n    if (!node_to_mutate_ptr || !(*node_to_mutate_ptr)) return new_tree; // Puntero o nodo nulo inesperado\n\n    const std::vector<MutationType> mutation_types = {\n        MutationType::ConstantChange, MutationType::OperatorChange,\n        MutationType::SubtreeReplace, MutationType::NodeInsertion,\n        MutationType::NodeDeletion\n    };\n    std::uniform_int_distribution<int> type_dist(0, mutation_types.size() - 1);\n    MutationType mut_type = mutation_types[type_dist(rng)];\n\n    NodePtr& current_node_ptr_ref = *node_to_mutate_ptr;\n    Node& current_node = *current_node_ptr_ref;\n\n    // Generar reemplazo aleatorio (usado en varios casos)\n    auto generate_replacement = [&](int depth) -> NodePtr {\n        NodePtr replacement = generate_random_tree(depth);\n        if (!replacement) { // Fallback si la generaci\u00f3n falla\n            replacement = std::make_shared<Node>(NodeType::Constant);\n            replacement->value = 1.0; // Usar 1.0 como fallback simple\n        }\n        return replacement;\n    };\n\n\n    switch (mut_type) {\n        case MutationType::ConstantChange:\n             if (current_node.type == NodeType::Constant) {\n                 // Cambiar valor de la constante\n                 double change_factor = std::uniform_real_distribution<double>(0.8, 1.2)(rng);\n                 double add_factor = std::uniform_real_distribution<double>(-1.0, 1.0)(rng);\n                 current_node.value = current_node.value * change_factor + add_factor;\n                 if (FORCE_INTEGER_CONSTANTS) current_node.value = std::round(current_node.value);\n                 if (std::fabs(current_node.value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) current_node.value = 0.0;\n             } else {\n                 // Si no es constante, reemplazar por un sub\u00e1rbol aleatorio peque\u00f1o\n                 *node_to_mutate_ptr = generate_replacement(1); // Profundidad 1 (terminal)\n             }\n            break;\n        case MutationType::OperatorChange:\n             if (current_node.type == NodeType::Operator) {\n                 // Usar la distribuci\u00f3n ponderada global para elegir el nuevo operador.\n                 // Esto asegura que si 'g' tiene peso alto, sea elegido frecuentemente.\n                 // Asumimos que OPERATOR_WEIGHTS tiene 0.0 para operadores deshabilitados.\n                 \n                 const std::vector<char> all_ops = {'+', '-', '*', '/', '^', '%', 's', 'c', 'l', 'e', '!', '_', 'g'};\n                 std::discrete_distribution<int> op_dist(OPERATOR_WEIGHTS.begin(), OPERATOR_WEIGHTS.end());\n                 \n                 // Verificar si hay al menos 2 operadores habilitados para evitar bucle infinito\n                 int enabled_count = 0;\n                 for (double w : OPERATOR_WEIGHTS) if (w > 0.0) enabled_count++;\n                 \n                 if (enabled_count > 1) {\n                     int attempts = 0;\n                     int new_op_idx = -1;\n                     do {\n                         new_op_idx = op_dist(rng);\n                         attempts++;\n                     } while (all_ops[new_op_idx] == current_node.op && attempts < 20); // Intentar cambiar\n                     \n                     if (attempts < 20) { // Si logramos encontrar uno diferente\n                         char old_op = current_node.op;\n                         char new_op = all_ops[new_op_idx];\n                         \n                         bool was_unary = (old_op == 's' || old_op == 'c' || old_op == 'l' || old_op == 'e' || old_op == '!' || old_op == '_' || old_op == 'g');\n                         bool is_unary = (new_op == 's' || new_op == 'c' || new_op == 'l' || new_op == 'e' || new_op == '!' || new_op == '_' || new_op == 'g');\n\n                         if (was_unary && !is_unary) {\n                             current_node.right = generate_replacement(1);\n                         } else if (!was_unary && is_unary) {\n                             current_node.right = nullptr;\n                         }\n                         current_node.op = new_op;\n                     }\n                 }\n             } else {\n                 // Si no es operador, reemplazar por un sub\u00e1rbol aleatorio\n                  *node_to_mutate_ptr = generate_replacement(max_depth);\n             }\n            break;\n        case MutationType::SubtreeReplace:\n            *node_to_mutate_ptr = generate_replacement(max_depth);\n            break;\n        case MutationType::NodeInsertion:\n            {\n                auto new_op_node = std::make_shared<Node>(NodeType::Operator);\n                // Usar distribuci\u00f3n ponderada\n                const std::vector<char> all_ops = {'+', '-', '*', '/', '^', '%', 's', 'c', 'l', 'e', '!', '_', 'g'};\n                std::discrete_distribution<int> op_dist(OPERATOR_WEIGHTS.begin(), OPERATOR_WEIGHTS.end());\n                \n                int new_op_idx = op_dist(rng); // Siempre elegir\u00e1 uno habilitado\n                new_op_node->op = all_ops[new_op_idx];\n\n                bool is_unary = (new_op_node->op == 's' || new_op_node->op == 'c' || new_op_node->op == 'l' || new_op_node->op == 'e' || new_op_node->op == '!' || new_op_node->op == '_' || new_op_node->op == 'g');\n\n                new_op_node->left = current_node_ptr_ref;\n\n                if (!is_unary) {\n                     if (prob(rng) < MUTATE_INSERT_CONST_PROB) {\n                         auto right_child = std::make_shared<Node>(NodeType::Constant);\n                         if (FORCE_INTEGER_CONSTANTS) { std::uniform_int_distribution<int> cv(MUTATE_INSERT_CONST_INT_MIN, MUTATE_INSERT_CONST_INT_MAX); right_child->value = static_cast<double>(cv(rng)); }\n                         else { std::uniform_real_distribution<double> cv(MUTATE_INSERT_CONST_FLOAT_MIN, MUTATE_INSERT_CONST_FLOAT_MAX); right_child->value = cv(rng); }\n                         if (std::fabs(right_child->value) < SIMPLIFY_NEAR_ZERO_TOLERANCE) right_child->value = 0.0;\n                         new_op_node->right = right_child;\n                     } else {\n                         new_op_node->right = std::make_shared<Node>(NodeType::Variable);\n                     }\n                     if (!new_op_node->right) new_op_node->right = std::make_shared<Node>(NodeType::Variable);\n                } else {\n                    new_op_node->right = nullptr;\n                }\n\n                *node_to_mutate_ptr = new_op_node;\n            }\n            break;\n        case MutationType::NodeDeletion:\n            {\n                // No eliminar la ra\u00edz directamente si es la \u00fanica opci\u00f3n\n                if (node_to_mutate_ptr == &new_tree && nodes.size() == 1) return new_tree;\n\n                if (current_node.type == NodeType::Operator) {\n                    // Si es operador, reemplazarlo por uno de sus hijos (aleatorio)\n                    NodePtr replacement = nullptr;\n                    bool has_left = (current_node.left != nullptr);\n                    bool has_right = (current_node.right != nullptr);\n\n                    if (has_left && has_right) {\n                        replacement = (prob(rng) < 0.5) ? current_node.left : current_node.right;\n                    } else if (has_left) {\n                        replacement = current_node.left;\n                    } else if (has_right) {\n                        replacement = current_node.right;\n                    }\n                    // Si no tiene hijos v\u00e1lidos (\u00bfc\u00f3mo?), reemplazar por terminal\n                    if (!replacement) replacement = generate_replacement(0); // Profundidad 0 (terminal)\n\n                    *node_to_mutate_ptr = replacement;\n                } else {\n                    // Si es terminal, reemplazar por otro terminal aleatorio\n                    // (Evitar eliminar si es la ra\u00edz y no hay m\u00e1s nodos)\n                     if (node_to_mutate_ptr != &new_tree || nodes.size() > 1) {\n                          *node_to_mutate_ptr = generate_replacement(0); // Profundidad 0 (terminal)\n                     }\n                }\n            }\n            break;\n         default: // Caso inesperado, reemplazar por seguridad\n             *node_to_mutate_ptr = generate_replacement(max_depth);\n            break;\n    }\n    if (USE_HARD_DEPTH_LIMIT) trim_tree(new_tree, MAX_TREE_DEPTH_HARD_LIMIT); // Enforce hard limit after mutation\n    return new_tree;\n}\n\n// Cruce\nvoid crossover_trees(NodePtr& tree1, NodePtr& tree2) {\n    if (!tree1 || !tree2) return; // No cruzar si alguno es nulo\n\n    std::vector<NodePtr*> nodes1, nodes2;\n    collect_node_ptrs(tree1, nodes1);\n    collect_node_ptrs(tree2, nodes2);\n\n    // No cruzar si alguno no tiene nodos (\u00e1rbol vac\u00edo o solo ra\u00edz nula?)\n    if (nodes1.empty() || nodes2.empty()) return;\n\n    auto& rng = get_rng();\n    std::uniform_int_distribution<int> d1(0, nodes1.size()-1);\n    std::uniform_int_distribution<int> d2(0, nodes2.size()-1);\n\n    // Seleccionar puntos de cruce\n    NodePtr* crossover_point1 = nodes1[d1(rng)];\n    NodePtr* crossover_point2 = nodes2[d2(rng)];\n\n    // Intercambiar los sub\u00e1rboles (los NodePtr)\n    std::swap(*crossover_point1, *crossover_point2);\n}\n\n// Implementaci\u00f3n de simplify_tree\nvoid simplify_tree(NodePtr& tree) {\n    if (USE_SIMPLIFICATION) {\n        tree = DomainConstraints::fix_or_simplify(tree);\n    }\n}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/GeneticOperators.h\n",
        "// ============================================================\n// Archivo: src/GeneticOperators.h\n// ============================================================\n#ifndef GENETICOPERATORS_H\n#define GENETICOPERATORS_H\n\n#include \"ExpressionTree.h\"\n#include \"Globals.h\" // Incluir Globals.h para INF\n#include <vector>\n#include <memory> // Para std::move\n\n// Estructura para representar un individuo en la poblaci\u00f3n.\n// Contiene el \u00e1rbol de expresi\u00f3n y su fitness cacheado.\nstruct Individual {\n    NodePtr tree; // Puntero inteligente al \u00e1rbol de expresi\u00f3n\n    double fitness = INF; // Fitness cacheado (menor es mejor), inicializado a infinito\n    std::vector<double> errors; // Cache of per-case errors for Lexicase Selection\n    bool fitness_valid = false; // Indica si el fitness cacheado es v\u00e1lido\n\n    // Constructor por defecto\n    Individual() = default;\n    // Constructor a partir de un \u00e1rbol (mueve el puntero)\n    explicit Individual(NodePtr t) : tree(std::move(t)) {}\n\n    // Operador de comparaci\u00f3n para ordenar individuos (menor fitness primero)\n    bool operator<(const Individual& other) const {\n        // Manejar casos donde uno o ambos fitness no son v\u00e1lidos\n        if (!fitness_valid && !other.fitness_valid) return false; // Iguales si ambos inv\u00e1lidos\n        if (!fitness_valid) return false; // Inv\u00e1lido es \"peor\" que v\u00e1lido (va despu\u00e9s)\n        if (!other.fitness_valid) return true; // V\u00e1lido es \"mejor\" que inv\u00e1lido (va antes)\n        // Comparar por fitness si ambos son v\u00e1lidos\n        return fitness < other.fitness;\n    }\n};\n\n\n// --- Funciones de Operadores Gen\u00e9ticos ---\n\n// Genera un \u00e1rbol de expresi\u00f3n aleatorio hasta una profundidad m\u00e1xima.\nNodePtr generate_random_tree(int max_depth, int current_depth = 0);\n\n// Crea la poblaci\u00f3n inicial de individuos.\nstd::vector<Individual> create_initial_population(int population_size);\n\n// Selecciona un individuo usando selecci\u00f3n por torneo con presi\u00f3n de parsimonia.\nIndividual tournament_selection(const std::vector<Individual>& population, int tournament_size);\n\n// Selecciona un individuo usando Epsilon-Lexicase Selection (m\u00e1s inteligente)\nIndividual lexicase_selection(std::vector<Individual>& population, const std::vector<double>& targets, const std::vector<double>& x_values);\n\n// Realiza el cruce (crossover) entre dos individuos y devuelve un nuevo individuo.\nIndividual crossover(const Individual& parent1, const Individual& parent2);\n\n// Mutata un individuo in-place.\nvoid mutate(Individual& individual, double mutation_rate);\n\n// Simplifica un \u00e1rbol in-place.\nvoid simplify_tree(NodePtr& tree);\n\n// Tipos de mutaci\u00f3n posibles.\nenum class MutationType {\n    ConstantChange,\n    OperatorChange,\n    SubtreeReplace,\n    NodeInsertion,\n    NodeDeletion // <-- A\u00d1ADIDO: Tipo para eliminar un nodo\n    // Simplification (manejado por DomainConstraints)\n};\n\n// Mutata un \u00e1rbol aplicando uno de los tipos de mutaci\u00f3n con cierta probabilidad.\n// Devuelve un nuevo \u00e1rbol (clonado y potencialmente mutado).\nNodePtr mutate_tree(const NodePtr& tree, double mutation_rate, int max_depth);\n\n// Realiza el cruce (crossover) entre dos \u00e1rboles padres, modific\u00e1ndolos in-place.\nvoid crossover_trees(NodePtr& tree1, NodePtr& tree2);\n\n\n#endif // GENETICOPERATORS_H\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/main.cpp\n",
        "#include \"Globals.h\" // Necesario para las constantes globales\n#include \"GeneticAlgorithm.h\"\n#include \"Fitness.h\" // Para evaluate_fitness\n#include \"ExpressionTree.h\" // Para tree_to_string si se necesita aqu\u00ed\n#include <iostream>\n#include <vector>\n#include <memory> // Para shared_ptr\n#include <iomanip> // Para std::setprecision\n#include <omp.h>   // Para configuraci\u00f3n de OpenMP\n\n#include <fstream> // Para leer archivo\n#include <string>\n#include <sstream>\n\nint main(int argc, char* argv[]) {\n    // === OPTIMIZACI\u00d3N: Configuraci\u00f3n expl\u00edcita de hilos OpenMP ===\n    int num_threads = omp_get_max_threads();\n    omp_set_num_threads(num_threads);\n    std::cout << \"[OpenMP] Using \" << num_threads << \" threads\" << std::endl;\n    \n    // Configurar precisi\u00f3n de salida para n\u00fameros flotantes\n    // Force immediate flush for each output (important for subprocess capture)\n    std::cout << std::unitbuf << std::fixed << std::setprecision(6);\n    \n    std::vector<std::string> seed_formulas;\n    std::string seed_file_path = \"\";\n    std::string data_file_path = \"\";\n    \n    // Parse arguments\n    for (int i = 1; i < argc; ++i) {\n        std::string arg = argv[i];\n        if ((arg == \"--seed\" || arg == \"-s\") && i + 1 < argc) {\n             seed_file_path = argv[i + 1];\n             i++; // Skip next arg\n        } else if ((arg == \"--data\" || arg == \"-d\") && i + 1 < argc) {\n             data_file_path = argv[i + 1];\n             i++;\n        }\n    }\n    \n    if (!seed_file_path.empty()) {\n        std::cout << \"Loading seeds from: \" << seed_file_path << std::endl;\n        std::ifstream file(seed_file_path);\n        if (file.is_open()) {\n            std::string line;\n            while (std::getline(file, line)) {\n                if (!line.empty()) {\n                    seed_formulas.push_back(line);\n                }\n            }\n            file.close();\n            std::cout << \"Loaded \" << seed_formulas.size() << \" formulas.\" << std::endl;\n        } else {\n            std::cerr << \"[Error] Could not open seed file: \" << seed_file_path << std::endl;\n        }\n    }\n\n    std::vector<double> targets;\n    std::vector<double> final_x_values;\n\n    if (!data_file_path.empty()) {\n         std::cout << \"Loading data from: \" << data_file_path << std::endl;\n         std::ifstream dfile(data_file_path);\n         if (dfile.is_open()) {\n             // Format:\n             // Line 1: x1 x2 x3 ...\n             // Line 2: y1 y2 y3 ...\n             // Values separated by space or comma\n             \n             // Helper lambda to parse line\n             auto parse_line = [](const std::string& line) {\n                 std::vector<double> vals;\n                 std::stringstream ss(line);\n                 double val;\n                 while (ss >> val) {\n                     vals.push_back(val);\n                     if (ss.peek() == ',' || ss.peek() == ' ') ss.ignore();\n                 }\n                 return vals;\n             };\n             \n             std::string line;\n             if (std::getline(dfile, line)) final_x_values = parse_line(line);\n             if (std::getline(dfile, line)) targets = parse_line(line);\n             \n             dfile.close();\n             \n             if (final_x_values.size() != targets.size()) {\n                 std::cerr << \"[Error] Mismatch in data size: X(\" << final_x_values.size() \n                           << \") vs Y(\" << targets.size() << \")\" << std::endl;\n                 return 1;\n             }\n             std::cout << \"Loaded \" << final_x_values.size() << \" data points.\" << std::endl;\n         } else {\n             std::cerr << \"[Error] Could not open data file: \" << data_file_path << std::endl;\n             return 1;\n         }\n    } else {\n        // Fallback to Globals.h\n        if (USE_LOG_TRANSFORMATION) {\n             std::cout << \"Info: Log Transformation is ON (Target = ln(Q(N))).\" << std::endl;\n             for (size_t i = 0; i < RAW_TARGETS.size(); ++i) {\n                 if (RAW_TARGETS[i] > 0) {\n                     targets.push_back(std::log(RAW_TARGETS[i]));\n                     final_x_values.push_back(X_VALUES[i]);\n                 }\n             }\n        } else {\n             std::cout << \"Info: Log Transformation is OFF.\" << std::endl;\n             targets = RAW_TARGETS;\n             final_x_values = X_VALUES;\n        }\n    }\n\n    std::cout << \"Target Function Points (Effective):\" << std::endl;\n    // Imprimir los puntos objetivo\n    for (size_t i = 0; i < targets.size(); ++i) {\n        std::cout << \"  f(\" << final_x_values[i] << \") = \" << targets[i] << std::endl;\n    }\n    std::cout << \"----------------------------------------\" << std::endl;\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\n    std::cout << \"Info: Running with GPU acceleration.\" << std::endl;\n#else\n    std::cout << \"Info: Running with CPU acceleration.\" << std::endl;\n#endif\n    std::cout << \"----------------------------------------\" << std::endl;\n    std::cout << \"Parameters:\" << std::endl;\n    // Imprimir los par\u00e1metros globales definidos en Globals.h\n    std::cout << \"  Total Population: \" << TOTAL_POPULATION_SIZE << std::endl;\n    std::cout << \"  Generations: \" << GENERATIONS << std::endl;\n    std::cout << \"  Islands: \" << NUM_ISLANDS << std::endl;\n    std::cout << \"  Migration Interval: \" << MIGRATION_INTERVAL << std::endl;\n    std::cout << \"  Migration Size: \" << MIGRATION_SIZE << std::endl;\n    // --- NOMBRES CORREGIDOS ---\n    std::cout << \"  Mutation Rate (Initial): \" << BASE_MUTATION_RATE << std::endl; // <-- Nombre corregido\n    std::cout << \"  Elite Percentage (Initial): \" << BASE_ELITE_PERCENTAGE << std::endl; // <-- Nombre corregido\n    // --------------------------\n    std::cout << \"----------------------------------------\" << std::endl;\n\n\n    try {\n        // Crear la instancia del Algoritmo Gen\u00e9tico\n        // Pasa las referencias a los vectores de datos y los par\u00e1metros principales\n        GeneticAlgorithm ga(targets, final_x_values, TOTAL_POPULATION_SIZE, GENERATIONS, seed_formulas);\n\n        // Ejecutar el algoritmo\n        // La funci\u00f3n run() contiene el bucle principal de generaciones y devuelve el mejor \u00e1rbol encontrado\n        NodePtr best_solution_tree = ga.run();\n\n        // La funci\u00f3n run() ya imprime el resumen final y la verificaci\u00f3n.\n        // Comprobar si se encontr\u00f3 alguna soluci\u00f3n v\u00e1lida al final\n        if (!best_solution_tree) {\n            std::cerr << \"\\nFailed to find any valid solution.\" << std::endl;\n            return 1; // Salir con c\u00f3digo de error si no se encontr\u00f3 soluci\u00f3n\n        }\n    } catch (const std::exception& e) {\n        std::cerr << \"[CRITICAL ERROR] Exception caught in main: \" << e.what() << std::endl;\n        return 2;\n    } catch (...) {\n        std::cerr << \"[CRITICAL ERROR] Unknown exception caught in main.\" << std::endl;\n        return 3;\n    }\n\n    return 0; // Salir con \u00e9xito\n}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile Code/src/Globals.h\n",
        "#ifndef GLOBALS_H\n#define GLOBALS_H\n\n#include <vector>\n#include <random>\n#include <string>\n#include <limits>\n#include <cmath>\n\n// ============================================================\n//                  PAR\u00c1METROS GLOBALES\n// ============================================================\n\n// ----------------------------------------\n// Datos del Problema (Regresi\u00f3n Simb\u00f3lica)\n// ----------------------------------------\n// MODIFICADO: Usamos log(TARGETS) para aplanar el crecimiento exponencial.\n// X representa N. TARGETS_LOG es ln(Q(N)).\n// Se han filtrado valores N<4 donde Q(N) es 0 o peque\u00f1o irrelevante.\n\n// MODIFICADO: RAW_TARGETS contiene los datos crudos. TARGETS se generar\u00e1 en runtime.\nconst std::vector<double> RAW_TARGETS = {2, 10, 4, 40, 92, 352, 724, 2680, 14200, 73712, 365596, 2279184, 14772512, 95815104, 666090624, 4968057848, 39029188884};\nconst std::vector<double> X_VALUES = {4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20};\n\n// Flag para activar la transformaci\u00f3n logar\u00edtmica autom\u00e1tica\nconst bool USE_LOG_TRANSFORMATION = true;\n\n// ----------------------------------------\n// Configuraci\u00f3n General del Algoritmo Gen\u00e9tico\n// ----------------------------------------\n// Controla si se utiliza la aceleraci\u00f3n por GPU.\n// FORCE_CPU_MODE: Si es true, usa CPU aunque CUDA est\u00e9 disponible (\u00fatil para comparar rendimiento)\nconst bool FORCE_CPU_MODE = true;  // Cambiar a 'true' para forzar modo CPU\n\n// USE_GPU_ACCELERATION se define autom\u00e1ticamente por CMake si CUDA est\u00e1 disponible\n// Pero si FORCE_CPU_MODE es true, se ignora y usa CPU\n#ifdef USE_GPU_ACCELERATION_DEFINED_BY_CMAKE\nconst bool USE_GPU_ACCELERATION = !FORCE_CPU_MODE;\n#else\nconst bool USE_GPU_ACCELERATION = false;\n#endif\n// Aumentamos el tama\u00f1o de la poblaci\u00f3n y el n\u00famero de generaciones para maximizar la utilizaci\u00f3n de la GPU,\n// ya que la GPU puede procesar un gran n\u00famero de individuos en paralelo.\n// Ajustamos el tama\u00f1o de la poblaci\u00f3n para una GPU con 4GB de VRAM (RTX 3050),\n// buscando un equilibrio entre el aprovechamiento de la GPU y el uso de memoria.\n// Para hacer un uso a\u00fan m\u00e1s intensivo de la GPU y acelerar el algoritmo,\n// aumentamos el n\u00famero de islas para fomentar m\u00e1s paralelismo, manteniendo la poblaci\u00f3n total.\n// Esto distribuye la carga de trabajo de evaluaci\u00f3n de fitness en m\u00e1s unidades de procesamiento concurrentes.\nconst int TOTAL_POPULATION_SIZE = 50000; // Mantenemos este tama\u00f1o, ajustado para 4GB VRAM\nconst int GENERATIONS = 500000;           // Mantenemos las generaciones altas\nconst int NUM_ISLANDS = 10;               // Aumentado para mayor paralelismo\nconst int MIN_POP_PER_ISLAND = 10;        // Ajustado para permitir m\u00e1s islas con poblaci\u00f3n m\u00ednima\n\n// --- F\u00f3rmula Inicial ---\nconst bool USE_INITIAL_FORMULA = true; // Poner en 'true' para inyectar la f\u00f3rmula\nconst std::string INITIAL_FORMULA_STRING = \"(g(x)-(x*0.912079)+0.146743+(3.78968/x))\";\n\n// ----------------------------------------\n// Par\u00e1metros del Modelo de Islas\n// ----------------------------------------\n// Aumentamos el intervalo y tama\u00f1o de migraci\u00f3n para permitir que las islas realicen m\u00e1s trabajo en paralelo\n// antes de intercambiar individuos, reduciendo la sobrecarga de comunicaci\u00f3n y maximizando el procesamiento GPU.\nconst int MIGRATION_INTERVAL = 100; // Incrementado para permitir m\u00e1s trabajo por isla entre migraciones\nconst int MIGRATION_SIZE = 50;      // Incrementado para una migraci\u00f3n m\u00e1s sustancial\n\n// ----------------------------------------\n// Par\u00e1metros de Generaci\u00f3n Inicial de \u00c1rboles\n// ----------------------------------------\nconst int MAX_TREE_DEPTH_INITIAL = 8; // Reducido para f\u00f3rmulas iniciales m\u00e1s simples y r\u00e1pidas\nconst double TERMINAL_VS_VARIABLE_PROB = 0.75;\nconst double CONSTANT_MIN_VALUE = -10.0;\nconst double CONSTANT_MAX_VALUE = 10.0;\nconst int CONSTANT_INT_MIN_VALUE = -10;\nconst int CONSTANT_INT_MAX_VALUE = 10;\nconst bool USE_HARD_DEPTH_LIMIT = true; // Toggle for hard depth limit\nconst int MAX_TREE_DEPTH_HARD_LIMIT = 12; // Hard limit to prevent bloat\n// Order: +, -, *, /, ^, %, s, c, l, e, !, _, g\n// ----------------------------------------\n// Par\u00e1metros de Operadores Gen\u00e9ticos (Configuraci\u00f3n de Operadores)\n// ----------------------------------------\nconst bool USE_OP_PLUS     = true; // +\nconst bool USE_OP_MINUS    = true; // -\nconst bool USE_OP_MULT     = true; // *\nconst bool USE_OP_DIV      = true; // /\nconst bool USE_OP_POW      = true; // ^\nconst bool USE_OP_MOD      = true; // %\nconst bool USE_OP_SIN      = true; // s\nconst bool USE_OP_COS      = true; // c\nconst bool USE_OP_LOG      = true; // l\nconst bool USE_OP_EXP      = true; // e\nconst bool USE_OP_FACT     = true; // !\nconst bool USE_OP_FLOOR    = true; // _\nconst bool USE_OP_GAMMA    = true; // g\n\n// Order: +, -, *, /, ^, %, s, c, l, e, !, _, g\n// Los pesos se multiplican por el flag (0 o 1) para habilitar/deshabilitar.\nconst std::vector<double> OPERATOR_WEIGHTS = {\n    0.10 * (USE_OP_PLUS  ? 1.0 : 0.0), // +\n    0.15 * (USE_OP_MINUS ? 1.0 : 0.0), // -\n    0.10 * (USE_OP_MULT  ? 1.0 : 0.0), // *\n    0.10 * (USE_OP_DIV   ? 1.0 : 0.0), // /\n    0.05 * (USE_OP_POW   ? 1.0 : 0.0), // ^\n    0.01 * (USE_OP_MOD   ? 1.0 : 0.0), // %\n    0.01 * (USE_OP_SIN   ? 1.0 : 0.0), // s\n    0.01 * (USE_OP_COS   ? 1.0 : 0.0), // c\n    0.15 * (USE_OP_LOG   ? 1.0 : 0.0), // l\n    0.02 * (USE_OP_EXP   ? 1.0 : 0.0), // e\n    0.05 * (USE_OP_FACT  ? 1.0 : 0.0), // !\n    0.05 * (USE_OP_FLOOR ? 1.0 : 0.0), // _\n    0.20 * (USE_OP_GAMMA ? 1.0 : 0.0)  // g\n};\n\n// ----------------------------------------\n// Par\u00e1metros de Operadores Gen\u00e9ticos (Mutaci\u00f3n, Cruce, Selecci\u00f3n)\n// ----------------------------------------\nconst double BASE_MUTATION_RATE = 0.30;\nconst double BASE_ELITE_PERCENTAGE = 0.15;\nconst double DEFAULT_CROSSOVER_RATE = 0.85;\nconst int DEFAULT_TOURNAMENT_SIZE = 30;\nconst int MAX_TREE_DEPTH_MUTATION = 8; // Slight increase to allow complexity\nconst double MUTATE_INSERT_CONST_PROB = 0.6;\nconst int MUTATE_INSERT_CONST_INT_MIN = 1;\nconst int MUTATE_INSERT_CONST_INT_MAX = 5;\nconst double MUTATE_INSERT_CONST_FLOAT_MIN = 0.5;\nconst double MUTATE_INSERT_CONST_FLOAT_MAX = 5.0;\n\n// ----------------------------------------\n// Par\u00e1metros de Fitness y Evaluaci\u00f3n\n// ----------------------------------------\n// Reducimos ligeramente la penalizaci\u00f3n por complejidad para permitir que f\u00f3rmulas m\u00e1s complejas\n// (y computacionalmente m\u00e1s intensivas para la GPU) sean favorecidas por el algoritmo.\n// MODIFICADO: Aumentado para penalizar bloat (Strategy 3).\nconst double COMPLEXITY_PENALTY_FACTOR = 0.05; // Was 0.005. Increased significantly to fight bloat.\nconst bool USE_RMSE_FITNESS = true;\nconst double FITNESS_ORIGINAL_POWER = 1.3;\nconst double FITNESS_PRECISION_THRESHOLD = 0.001;\nconst double FITNESS_PRECISION_BONUS = 0.0001;\nconst double FITNESS_EQUALITY_TOLERANCE = 1e-9;\nconst double EXACT_SOLUTION_THRESHOLD = 1e-8;\n\n// ----------------------------------------\n// Fitness Ponderado (Weighted Fitness)\n// ----------------------------------------\n// Activa el fitness ponderado para penalizar fuertemente errores en valores altos de N.\n// Esto destruye a las par\u00e1bolas que fallan en N=20 pero dan buen promedio general.\nconst bool USE_WEIGHTED_FITNESS = true;\n// Tipo de peso: \"quadratic\" usa i*i, \"exponential\" usa exp(i*WEIGHTED_FITNESS_EXPONENT)\n// Exponente para peso exponencial (m\u00e1s agresivo). Usar 0.2-0.3 para datasets peque\u00f1os.\nconst double WEIGHTED_FITNESS_EXPONENT = 0.25;\n\n// ----------------------------------------\n// Par\u00e1metros de Caracter\u00edsticas Avanzadas\n// ----------------------------------------\nconst int STAGNATION_LIMIT_ISLAND = 50;\n// Lowered from 5000 to allow faster early termination in Hybrid Search mode.\n// If best fitness doesn't improve for N generations, terminate early.\nconst int GLOBAL_STAGNATION_LIMIT = 200;\nconst double STAGNATION_RANDOM_INJECT_PERCENT = 0.1;\nconst int PARAM_MUTATE_INTERVAL = 50;\nconst double PATTERN_RECORD_FITNESS_THRESHOLD = 10.0;\nconst int PATTERN_MEM_MIN_USES = 3;\nconst int PATTERN_INJECT_INTERVAL = 10;\nconst double PATTERN_INJECT_PERCENT = 0.05;\nconst size_t PARETO_MAX_FRONT_SIZE = 50;\nconst double SIMPLIFY_NEAR_ZERO_TOLERANCE = 1e-9;\nconst double SIMPLIFY_NEAR_ONE_TOLERANCE = 1e-9;\nconst int LOCAL_SEARCH_ATTEMPTS = 30;\n// Simplification Toggle\nconst bool USE_SIMPLIFICATION = true;\n// Anti-Stagnation: Island Cataclysm (Hard Reset)\nconst bool USE_ISLAND_CATACLYSM = true;\n// Selection Strategy: Epsilon-Lexicase Selection (Replaces Tournament)\nconst bool USE_LEXICASE_SELECTION = true;\n\n// ----------------------------------------\n// Otros Par\u00e1metros\n// ----------------------------------------\nconst int PROGRESS_REPORT_INTERVAL = 100;\n// Optimizaciones adicionales:\n// Deshabilitamos las constantes enteras forzadas para permitir una mayor flexibilidad\n// en las constantes generadas y mutadas, lo que podr\u00eda conducir a mejores soluciones\n// y mantener la GPU ocupada con un rango m\u00e1s amplio de valores.\nconst bool FORCE_INTEGER_CONSTANTS = false; // Mantenemos false para mayor flexibilidad\n\n// ----------------------------------------\n// Control de Duplicados\n// ----------------------------------------\nconst bool PREVENT_DUPLICATES = true; // Activa la verificaci\u00f3n de unicidad\nconst int DUPLICATE_RETRIES = 10;     // Intentos para generar un individuo \u00fanico antes de rendirse\n\n\n// ============================================================\n//                  UTILIDADES GLOBALES\n// ============================================================\nstd::mt19937& get_rng();\nconst double INF = std::numeric_limits<double>::infinity();\n\n#endif // GLOBALS_H\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile Code/CMakeLists.txt\n",
        "\ncmake_minimum_required(VERSION 3.10)\nproject(SymbolicRegressionGP)\n\nset(CMAKE_CXX_STANDARD 17)\n\nfind_package(CUDA)\n\nif(CUDA_FOUND)\n    add_definitions(-DUSE_GPU_ACCELERATION_DEFINED_BY_CMAKE)\n    enable_language(CUDA)\n    set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} -O3 -arch=sm_75\") # T4 is sm_75\n    set(SOURCE_FILES \n        src/main.cpp \n        src/GeneticAlgorithm.cpp \n        src/ExpressionTree.cpp \n        src/GeneticOperators.cpp\n        src/Fitness.cpp\n        src/FitnessGPU.cu\n        src/AdvancedFeatures.cpp\n    )\nelse()\n    message(WARNING \"CUDA not found. Compiling for CPU only.\")\n    set(SOURCE_FILES \n        src/main.cpp \n        src/GeneticAlgorithm.cpp \n        src/ExpressionTree.cpp \n        src/GeneticOperators.cpp\n        src/Fitness.cpp\n        src/FitnessGPU.cu # Still included but ifdef'd out inside\n        src/AdvancedFeatures.cpp\n    )\nendif()\n\nadd_executable(SymbolicRegressionGP ${SOURCE_FILES})\n\nif(CUDA_FOUND)\n    set_target_properties(SymbolicRegressionGP PROPERTIES CUDA_SEPARABLE_COMPILATION ON)\n    target_link_libraries(SymbolicRegressionGP ${CUDA_LIBRARIES})\nelse()\n    target_link_libraries(SymbolicRegressionGP pthread)\nendif()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile C++ Engine\n",
        "%cd Code\n",
        "!cmake -B build -S . -DCMAKE_BUILD_TYPE=Release\n",
        "!cmake --build build -j $(nproc)\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/grammar.py\n",
        "import numpy as np\nfrom scipy.special import gamma as scipy_gamma, gammaln\nimport math\n\n# Supported operators and their arity (number of arguments)\n# Organized by curriculum stage for progressive unlocking\nOPERATORS = {\n    # === STAGE 0: Pure Arithmetic ===\n    '+': 2,\n    '-': 2,\n    '*': 2,\n    '/': 2,\n    \n    # === STAGE 1: Powers ===\n    'pow': 2,\n    'sqrt': 1,\n    \n    # === STAGE 2: Trigonometry ===\n    'sin': 1,\n    'cos': 1,\n    'tan': 1,\n    \n    # === STAGE 3: Transcendental ===\n    'exp': 1,\n    'log': 1,\n    \n    # === STAGE 4: Advanced ===\n    'abs': 1,\n    'neg': 1,\n    'sign': 1,\n    'floor': 1,\n    'ceil': 1,\n    'mod': 2,\n    'gamma': 1,\n    'lgamma': 1,  # Log-gamma function (from C++ GP engine)\n}\n\n# Operator groups for curriculum control\nOPERATOR_STAGES = {\n    0: ['+', '-', '*', '/'],\n    1: ['+', '-', '*', '/', 'pow', 'sqrt'],\n    2: ['+', '-', '*', '/', 'pow', 'sqrt', 'sin', 'cos', 'tan'],\n    3: ['+', '-', '*', '/', 'pow', 'sqrt', 'sin', 'cos', 'tan', 'exp', 'log'],\n    4: list(OPERATORS.keys()),  # All operators\n}\n\n# Terminal tokens\nVARIABLES = ['x']\n# 'C' is a placeholder for learnable constants\nCONSTANTS = ['C', '0', '1', '2', '3', '5', '10', 'pi', 'e']\n\n# Full Vocabulary\nVOCABULARY = list(OPERATORS.keys()) + VARIABLES + CONSTANTS\nTOKEN_TO_ID = {token: i for i, token in enumerate(VOCABULARY)}\nID_TO_TOKEN = {i: token for token, i in TOKEN_TO_ID.items()}\n\n# Special token for start of sequence\nSOS_TOKEN = '<SOS>'\nEOS_TOKEN = '<EOS>'\nPAD_TOKEN = '<PAD>'\n\nclass Node:\n    def __init__(self, value, children=None):\n        self.value = value\n        self.children = children if children else []\n\n    def __repr__(self):\n        if not self.children:\n            return str(self.value)\n        return f\"({self.value} \" + \" \".join([str(c) for c in self.children]) + \")\"\n    \n    def to_infix(self):\n        if not self.children:\n            return str(self.value)\n        \n        op = self.value\n        if len(self.children) == 1:\n            return f\"{op}({self.children[0].to_infix()})\"\n        elif len(self.children) == 2:\n            if op == 'pow':\n                return f\"({self.children[0].to_infix()} ^ {self.children[1].to_infix()})\"\n            elif op == 'mod':\n                return f\"({self.children[0].to_infix()} % {self.children[1].to_infix()})\"\n            return f\"({self.children[0].to_infix()} {op} {self.children[1].to_infix()})\"\n        return str(self.value)\n    \n    def count_constants(self):\n        \"\"\"Count the number of 'C' placeholders in the tree.\"\"\"\n        count = 1 if self.value == 'C' else 0\n        for child in self.children:\n            count += child.count_constants()\n        return count\n    \n    def get_constant_positions(self, path=None):\n        \"\"\"Returns a list of paths to all 'C' nodes for optimization.\"\"\"\n        if path is None:\n            path = []\n        positions = []\n        if self.value == 'C':\n            positions.append(path.copy())\n        for i, child in enumerate(self.children):\n            positions.extend(child.get_constant_positions(path + [i]))\n        return positions\n\n\nimport ast\n\nclass ExpressionTree:\n    def __init__(self, token_list):\n        \"\"\"\n        Parses a list of tokens in Pre-order traversal (Prefix notation)\n        Example: ['+', 'x', 'sin', 'x'] -> x + sin(x)\n        \"\"\"\n        self.tokens = token_list\n        try:\n            self.root, remaining = self._build_tree(token_list)\n            if remaining:\n                raise ValueError(\"Tokens remained after building tree\")\n            self.is_valid = True\n        except Exception:\n            self.root = None\n            self.is_valid = False\n\n    @classmethod\n    def from_infix(cls, infix_str):\n        \"\"\"\n        Creates an ExpressionTree from a standard infix string (e.g. \"sin(x) + x^2\").\n        Uses Python's ast to parse.\n        \"\"\"\n        # Replacements to make it valid python for AST\n        # 1. Handle postfix factorial '!' which C++ outputs as '(... )!'\n        # We convert '(... )!' to 'gamma(...)'\n        # Iterate until no '!' left\n        processed_str = infix_str\n        while '!' in processed_str:\n            idx = processed_str.find('!')\n            # Helper to find matching paren backwards\n            if idx > 0 and processed_str[idx-1] == ')':\n                paren_count = 1\n                start = idx - 2\n                while start >= 0 and paren_count > 0:\n                    if processed_str[start] == ')':\n                        paren_count += 1\n                    elif processed_str[start] == '(':\n                        paren_count -= 1\n                    start -= 1\n                # start is now 1 char before the matching '('\n                start += 1 \n                # Reconstruct: ... + gamma( + ... + ) + ...\n                # Content includes the parens: ( ... )\n                content = processed_str[start:idx] \n                processed_str = processed_str[:start] + \"gamma\" + content + processed_str[idx+1:]\n            else:\n                # Fallback: Just remove ! if it's weirdly placed (should not happen with GP output)\n                processed_str = processed_str.replace('!', '', 1)\n\n        # 2. C++ uses ^ for power, Python uses **. AST parses ^ as BitXor.\n        try:\n            tree = ast.parse(processed_str, mode='eval')\n            tokens = cls._ast_to_prefix(tree.body)\n            return cls(tokens)\n        except Exception as e:\n            print(f\"Error parsing infix: {e} | Original: {infix_str} | Processed: {processed_str}\")\n            return cls([]) # Invalid\n\n    @staticmethod\n    def _ast_to_prefix(node):\n        if isinstance(node, ast.BinOp):\n            # Map operators\n            op_map = {\n                ast.Add: '+', ast.Sub: '-', ast.Mult: '*', ast.Div: '/',\n                ast.BitXor: 'pow', ast.Pow: 'pow', ast.Mod: 'mod'\n            }\n            op_type = type(node.op)\n            if op_type in op_map:\n                return [op_map[op_type]] + ExpressionTree._ast_to_prefix(node.left) + ExpressionTree._ast_to_prefix(node.right)\n        \n        elif isinstance(node, ast.UnaryOp):\n            op_map = {ast.USub: 'neg', ast.UAdd: None} # Ignore unary +\n            op_type = type(node.op)\n            if op_type == ast.USub:\n                # Check directly if it's a number to collapse \"-5\"\n                if isinstance(node.operand, ast.Constant) and isinstance(node.operand.value, (int, float)):\n                    return [str(-node.operand.value)]\n                return ['neg'] + ExpressionTree._ast_to_prefix(node.operand)\n            elif op_type == ast.UAdd:\n                 return ExpressionTree._ast_to_prefix(node.operand)\n\n        elif isinstance(node, ast.Call):\n            # Functions like sin(x)\n            func_id = node.func.id\n            if func_id in ['sin', 'cos', 'tan', 'exp', 'log', 'sqrt', 'abs', 'floor', 'ceil', 'gamma', 'lgamma']:\n                tokens = [func_id]\n                for arg in node.args:\n                    tokens.extend(ExpressionTree._ast_to_prefix(arg))\n                return tokens\n        \n        elif isinstance(node, ast.Name):\n            return [node.id]\n        \n        elif isinstance(node, ast.Constant): # Python 3.8+\n            return [str(node.value)]\n        elif isinstance(node, ast.Num): # Older python\n            return [str(node.n)]\n\n        raise ValueError(f\"Unsupported AST node: {node}\")\n\n\n    def _build_tree(self, tokens):\n        if not tokens:\n            raise ValueError(\"Empty token list\")\n        \n        token = tokens[0]\n        remaining = tokens[1:]\n        \n        if token in OPERATORS:\n            arity = OPERATORS[token]\n            children = []\n            for _ in range(arity):\n                child, remaining = self._build_tree(remaining)\n                children.append(child)\n            return Node(token, children), remaining\n        elif token in VARIABLES or token in CONSTANTS:\n            return Node(token), remaining\n        else:\n            # Try to parse as float literal\n            try:\n                float(token)\n                return Node(token), remaining\n            except:\n                raise ValueError(f\"Unknown token: {token}\")\n\n    def evaluate(self, x_values, constants=None):\n        \"\"\"\n        Evaluates the expression tree for a given array of x values.\n        constants: optional dict mapping path tuples to constant values\n        Returns a numpy array of results.\n        \"\"\"\n        # Ensure x_values is a numpy array\n        if not isinstance(x_values, np.ndarray):\n            x_values = np.array(x_values, dtype=np.float64)\n        \n        if not self.is_valid:\n            return np.full_like(x_values, np.nan, dtype=np.float64)\n        return self._eval_node(self.root, x_values, constants, path=[])\n\n    def _eval_node(self, node, x, constants=None, path=None):\n        val = node.value\n        \n        if val == 'x':\n            return x.astype(np.float64)\n        if val == 'pi':\n            return np.full_like(x, np.pi, dtype=np.float64)\n        if val == 'e':\n            return np.full_like(x, np.e, dtype=np.float64)\n        if val == 'C':\n            # Check if we have an optimized constant for this position\n            if constants is not None and tuple(path) in constants:\n                return np.full_like(x, constants[tuple(path)], dtype=np.float64)\n            return np.full_like(x, 1.0, dtype=np.float64)  # Default constant = 1\n        \n        # Check for numeric constants\n        try:\n            return np.full_like(x, float(val), dtype=np.float64)\n        except:\n            pass\n            \n        # Recursive evaluation\n        args = []\n        for i, c in enumerate(node.children):\n            args.append(self._eval_node(c, x, constants, path + [i] if path is not None else None))\n        \n        # Operators\n        with np.errstate(divide='ignore', invalid='ignore', over='ignore'):\n            if val == '+': return args[0] + args[1]\n            if val == '-': return args[0] - args[1]\n            if val == '*': return args[0] * args[1]\n            if val == '/': \n                return np.divide(args[0], args[1], out=np.zeros_like(x, dtype=np.float64), where=args[1]!=0)\n            if val == 'pow':\n                # Safe power\n                return np.power(np.abs(args[0]) + 1e-10, np.clip(args[1], -10, 10))\n            if val == 'mod':\n                return np.mod(args[0], args[1] + 1e-10)\n            if val == 'sin': return np.sin(args[0])\n            if val == 'cos': return np.cos(args[0])\n            if val == 'tan': return np.tan(args[0])\n            if val == 'exp': \n                return np.exp(np.clip(args[0], -100, 100))\n            if val == 'log': \n                return np.log(np.abs(args[0]) + 1e-10)\n            if val == 'sqrt':\n                return np.sqrt(np.abs(args[0]))\n            if val == 'abs':\n                return np.abs(args[0])\n            if val == 'floor':\n                return np.floor(args[0])\n            if val == 'ceil':\n                return np.ceil(args[0])\n            if val == 'gamma':\n                # Match C++ Protected Gamma/Factorial: tgamma(|x| + 1)\n                # This ensures consistent evaluation for formulas from C++ engine (which uses !)\n                arg = np.abs(args[0]) + 1.0\n                clipped = np.clip(arg, 0.1, 50) # Clip upper bound to avoid overflow\n                return scipy_gamma(clipped)\n            if val == 'lgamma':\n                # Protected lgamma: lgamma(|x| + 1)\n                arg = np.abs(args[0]) + 1.0\n                # gammaln is safe for large positive numbers, so less aggressive clipping needed for overflow,\n                # but we clip for consistency and to avoid extremely large outputs if followed by exp\n                clipped = np.clip(arg, 0.1, 1000) \n                return gammaln(clipped)\n            if val == 'neg':\n                return -args[0]\n            if val == 'sign':\n                return np.sign(args[0])\n                \n        return np.zeros_like(x, dtype=np.float64)\n\n    def get_infix(self):\n        if not self.is_valid:\n            return \"Invalid\"\n        return self.root.to_infix()\n    \n    \n    def count_constants(self):\n        if not self.is_valid:\n            return 0\n        return self.root.count_constants()\n\nimport sympy\n\ndef simplify_formula(formula_str):\n    \"\"\"\n    Simplifies a mathematical formula using SymPy.\n    \"\"\"\n    try:\n        # 1. Clean up C++ notation that sympy might not like directly\n        # e.g., 'pi' is fine. 'neg(x)' -> '-x'.\n        # But our infix is usually standard. \n        # C++ 'pow(x,2)' might need conversion to 'x**2' or sympy handles it?\n        # Sympy uses 'Pow'. \n        \n        # Replace common mismatches\n        s_str = formula_str.replace(\"pow(\", \"Pow(\")\n        # s_str = s_str.replace(\"abs(\", \"Abs(\") # Sympy handles abs\n        \n        # Parse\n        expr = sympy.sympify(s_str)\n        \n        # Simplify\n        simplified = sympy.simplify(expr)\n        \n        # Convert back to string\n        # We need to ensure it uses our function names (e.g. sin, cos)\n        # Sympy standard printer is usually good.\n        # But 'Power' is '**'. We used 'hat' or 'pow' in some places?\n        # Our tokenizer supports standard operators. 'x**2' is not standard infix for our parser?\n        # Our Parser supports 'x^2' or 'pow(x,2)'? \n        # AST parser handles '**' -> 'pow'.\n        \n        final_str = str(simplified)\n        return final_str\n        \n    except Exception as e:\n        # Fallback if simplification fails (e.g. unknown functions)\n        return formula_str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/model.py\n",
        "import torch\nimport torch.nn as nn\nimport numpy as np\n\nclass AlphaSymbolicModel(nn.Module):\n    def __init__(self, vocab_size, d_model=128, nhead=4, num_encoder_layers=2, num_decoder_layers=2, max_seq_len=50):\n        super(AlphaSymbolicModel, self).__init__()\n        \n        self.d_model = d_model\n        \n        # 1. Point Encoder: Processes pairs of (x, y)\n        # Input dim: 2 (x value, y value)\n        self.point_embedding = nn.Linear(2, d_model)\n        \n        # We use a standard Transformer Encoder for the \"Problem Embedding\"\n        # Since points are a set, we don't necessarily need positional encoding, \n        # but the Transformer will process them as a sequence.\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n        self.problem_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n        \n        # 2. Formula Decoder: Generates tokens\n        self.token_embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoder = PositionalEncoding(d_model, max_len=max_seq_len)\n        \n        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n        self.formula_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n        \n        # 3. Heads\n        self.policy_head = nn.Linear(d_model, vocab_size)\n        self.value_head = nn.Sequential(\n            nn.Linear(d_model, 64),\n            nn.ReLU(),\n            nn.Linear(64, 3) # Quantiles: 0.25, 0.50, 0.75\n        )\n        \n    def forward(self, x_values, y_values, formula_input, formula_mask=None):\n        \"\"\"\n        x_values: [batch, num_points]\n        y_values: [batch, num_points]\n        formula_input: [batch, seq_len] (Token IDs)\n        formula_mask: Optional mask for the decoder (causal mask)\n        \"\"\"\n        batch_size, num_points = x_values.shape\n        \n        # -- Problem Encoding --\n        # Stack x and y: [batch, num_points, 2]\n        points = torch.stack([x_values, y_values], dim=2)\n        \n        # Project to d_model\n        points_emb = self.point_embedding(points) # [batch, num_points, d_model]\n        \n        # Encode problem (memory for decoder)\n        memory = self.problem_encoder(points_emb)\n        \n        # -- Formula Decoding --\n        # Embed tokens\n        tgt = self.token_embedding(formula_input) # [batch, seq_len, d_model]\n        tgt = self.pos_encoder(tgt)\n        \n        # Decode\n        # memory is [batch, num_points, d_model]\n        # tgt is [batch, seq_len, d_model]\n        if formula_mask is None:\n             # Create causal mask\n            seq_len = formula_input.size(1)\n            formula_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(formula_input.device)\n\n        output = self.formula_decoder(tgt, memory, tgt_mask=formula_mask)\n        \n        # -- Heads --\n        # Policy: distribution over vocab for each token position\n        logits = self.policy_head(output) # [batch, seq_len, vocab_size]\n        \n        # Value: estimate value from the LAST token's state\n        # (Assuming the last token summarizes the current state)\n        last_token_output = output[:, -1, :] # [batch, d_model]\n        value = self.value_head(last_token_output) # [batch, 1]\n        \n        return logits, value\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        # x: [batch, seq_len, d_model]\n        x = x + self.pe[:, :x.size(1), :]\n        return x\n\nif __name__ == \"__main__\":\n    # Smoke Test\n    vocab_size = 20\n    model = AlphaSymbolicModel(vocab_size=vocab_size, d_model=32)\n    \n    # Dummy data\n    bs = 2\n    points = 10\n    x = torch.randn(bs, points)\n    y = torch.randn(bs, points)\n    \n    # Formula input (start token + some tokens)\n    seq = torch.randint(0, vocab_size, (bs, 5))\n    \n    logits, value = model(x, y, seq)\n    \n    print(\"Logits shape:\", logits.shape) # Should be [2, 5, 20]\n    print(\"Value shape:\", value.shape)   # Should be [2, 1]\n    print(\"Smoke test passed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/environment.py\n",
        "import gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\nfrom core.grammar import VOCABULARY, OPERATORS, TOKEN_TO_ID, ExpressionTree\nfrom data.synthetic_data import DataGenerator\n\nclass SymbolicEnv(gym.Env):\n    def __init__(self, max_length=50):\n        super(SymbolicEnv, self).__init__()\n        \n        self.vocab_size = len(VOCABULARY)\n        self.max_length = max_length\n        self.vocab = VOCABULARY\n        \n        # Action space: Choose a token from the vocabulary\n        self.action_space = spaces.Discrete(self.vocab_size)\n        \n        # Observation space: \n        # 1. Current token sequence (padded)\n        # 2. X values (fixed size for simplicity)\n        # 3. Y values\n        # For this prototype we will expose a dictionary observation\n        self.observation_space = spaces.Dict({\n            \"sequence\": spaces.Box(low=0, high=self.vocab_size, shape=(max_length,), dtype=np.int32),\n            \"x\": spaces.Box(low=-np.inf, high=np.inf, shape=(10,), dtype=np.float32),\n            \"y\": spaces.Box(low=-np.inf, high=np.inf, shape=(10,), dtype=np.float32)\n        })\n        \n        self.data_gen = DataGenerator(max_depth=4)\n        self.current_problem = None\n        self.current_sequence = []\n        self.open_branches = 0\n        \n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        \n        # Generate a new problem (X, Y)\n        # In a real scenario, this could be sampled from a fixed dataset\n        batch = self.data_gen.generate_batch(1, point_count=10)\n        self.current_problem = batch[0]\n        \n        self.current_sequence = []\n        self.open_branches = 1 # Start expecting a root node\n        \n        return self._get_obs(), {}\n\n    def step(self, action_id):\n        token = self.vocab[action_id]\n        self.current_sequence.append(token)\n        \n        # Update open branches\n        if token in OPERATORS:\n            arity = OPERATORS[token]\n            self.open_branches += (arity - 1)\n        else:\n            self.open_branches -= 1\n            \n        term = False\n        trunc = False\n        reward = 0.0\n        \n        # Check completion\n        if self.open_branches == 0:\n            term = True\n            # Tree is complete, evaluate\n            reward = self._calculate_reward()\n        elif self.open_branches < 0:\n            # Should not happen if we mask actions, but for safety\n            term = True\n            reward = -100.0 # Syntax error penalty\n        elif len(self.current_sequence) >= self.max_length:\n            trunc = True\n            reward = -10.0 # Incomplete penalty\n            \n        return self._get_obs(), reward, term, trunc, {}\n\n    def _get_obs(self):\n        # Convert sequence to IDs and pad\n        seq_ids = [TOKEN_TO_ID[t] for t in self.current_sequence]\n        padded_seq = np.zeros(self.max_length, dtype=np.int32)\n        padded_seq[:len(seq_ids)] = seq_ids\n        \n        return {\n            \"sequence\": padded_seq,\n            \"x\": self.current_problem['x'].astype(np.float32),\n            \"y\": self.current_problem['y'].astype(np.float32)\n        }\n\n    def _calculate_reward(self):\n        try:\n            tree = ExpressionTree(self.current_sequence)\n            if not tree.is_valid:\n                return -100.0\n            \n            y_pred = tree.evaluate(self.current_problem['x'])\n            \n            # Root Mean Squared Error (RMSE)\n            mse = np.mean((y_pred - self.current_problem['y'])**2)\n            rmse = np.sqrt(mse)\n            \n            if np.isnan(rmse) or np.isinf(rmse):\n                return -1000.0\n                \n            # Reward is negative RMSE\n            # We want to maximize reward -> minimize RMSE\n            # Normalize or scale? simpler is just -RMSE\n            return -rmse\n            \n        except Exception:\n            return -100.0\n\nif __name__ == \"__main__\":\n    env = SymbolicEnv()\n    obs, _ = env.reset()\n    print(\"Initial Observation Keys:\", obs.keys())\n    \n    # Simulate a few steps for x + x\n    # Prefix: + x x\n    actions = ['+', 'x', 'x']\n    tot_reward = 0\n    for tok in actions:\n        aid = TOKEN_TO_ID[tok]\n        obs, reward, term, trunc, _ = env.step(aid)\n        print(f\"Action: {tok}, Reward: {reward}, Term: {term}, Branches: {env.open_branches}\")\n        tot_reward += reward\n        if term: break\n    \n    print(f\"Total Reward: {tot_reward}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/loss.py\n",
        "\nimport torch\nimport torch.nn as nn\n\nclass QuantileLoss(nn.Module):\n    \"\"\"\n    Quantile Loss (Pinball Loss) for multiple quantiles.\n    \n    Args:\n        quantiles (list): List of quantiles to estimate (e.g. [0.25, 0.5, 0.75])\n    \"\"\"\n    def __init__(self, quantiles=[0.25, 0.5, 0.75]):\n        super().__init__()\n        self.quantiles = quantiles\n        \n    def forward(self, preds, target):\n        \"\"\"\n        preds: [batch, num_quantiles] - Predicted values for each quantile\n        target: [batch, 1] - True scalar target\n        \"\"\"\n        # Ensure target matches batch dim\n        # target shape might be [batch] or [batch, 1]\n        if target.dim() == 1:\n            target = target.unsqueeze(1)\n            \n        loss = 0\n        for i, q in enumerate(self.quantiles):\n            error = target - preds[:, i:i+1]\n            # Pinball loss: max(q * error, (q - 1) * error)\n            # Equivalent to: error * (q - I(error < 0))\n            loss += torch.max(q * error, (q - 1) * error).mean()\n            \n        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/gp_bridge.py\n",
        "import os\nimport subprocess\nimport tempfile\nimport re\nimport time\nfrom typing import List, Optional\n\nclass GPEngine:\n    def __init__(self, binary_path=None):\n        if binary_path is None:\n            # Default location: Code/build/Release/SymbolicRegressionGP.exe\n            # Assuming we are in AlphaSymbolic/.. root or similar.\n            # Adjust path relative to this file: alphasybolic/core/gp_bridge.py\n            # So binary is at ../../Code/build/Release/SymbolicRegressionGP.exe\n            base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n            possible_paths = [\n                os.path.join(base_dir, \"Code\", \"build\", \"Release\", \"SymbolicRegressionGP.exe\"),\n                os.path.join(base_dir, \"Code\", \"build\", \"SymbolicRegressionGP.exe\"),\n                # Linux/Mac support (no .exe)\n                os.path.join(base_dir, \"Code\", \"build\", \"Release\", \"SymbolicRegressionGP\"),\n                os.path.join(base_dir, \"Code\", \"build\", \"SymbolicRegressionGP\")\n            ]\n            self.binary_path = None\n            for p in possible_paths:\n                if os.path.exists(p):\n                    self.binary_path = p\n                    break\n            \n            if self.binary_path is None:\n                # Fallback to default for error message\n                self.binary_path = possible_paths[0]\n        else:\n            self.binary_path = binary_path\n\n    def run(self, x_values: List[float], y_values: List[float], seeds: List[str] = [], timeout_sec: int = 10) -> Optional[str]:\n        \"\"\"\n        Runs the C++ GP Engine with the given data and seeds.\n        Returns the best formula found as a string, or None if failed.\n        \"\"\"\n        if not os.path.exists(self.binary_path):\n            print(f\"[Error] GP Binary not found at: {self.binary_path}\")\n            return None\n\n        # Create temporary files\n        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as seed_file, \\\n             tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as data_file:\n            \n            # Write Seeds\n            for seed in seeds:\n                seed_file.write(seed + \"\\n\")\n            seed_file_path = seed_file.name\n            \n            # Write Data\n            # Line 1: x1 x2 ...\n            # Line 2: y1 y2 ...\n            data_file.write(\" \".join(map(str, x_values)) + \"\\n\")\n            data_file.write(\" \".join(map(str, y_values)) + \"\\n\")\n            data_file_path = data_file.name\n\n        try:\n            # Run Command\n            cmd = [self.binary_path, \"--seed\", seed_file_path, \"--data\", data_file_path]\n            print(f\"Running GP Engine: {' '.join(cmd)}\")\n            \n            # Capture output\n            # We can't strictly enforce timeout via subprocess.run's timeout argument easily if we want partial results?\n            # Actually we can.\n            start_time = time.time()\n            result = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout_sec)\n            \n            output = result.stdout\n            \n            # Parse Output\n            # We look for the LAST occurrence of \"Formula: ...\"\n            # Standard formats:\n            # \"Formula: ((x * x) + 2)\"\n            # \"Final Formula: ...\"\n            \n            best_formula = None\n            # Look for formula lines (case-insensitive)\n            # Priority: \"Final Formula:\" > \"Formula:\" > \"Initial best formula:\"\n            for line in output.splitlines():\n                line_lower = line.lower()\n                if \"formula:\" in line_lower:\n                    # Extract the part after \"formula:\" (case-insensitive split)\n                    idx = line_lower.find(\"formula:\")\n                    if idx != -1:\n                        formula_part = line[idx + len(\"formula:\"):].strip()\n                        if formula_part:\n                            best_formula = formula_part\n                            # Keep looking for better matches (Final Formula is best)\n                            if \"final formula:\" in line_lower:\n                                break  # Final Formula is the best, stop looking\n                        \n            print(f\"GP Engine finished in {time.time() - start_time:.2f}s\")\n            \n            if best_formula is None:\n                print(f\"[DEBUG] GP Engine Output (Stdout):\\n{output}\")\n                print(f\"[DEBUG] GP Engine Output (Stderr):\\n{result.stderr}\")\n            \n            return best_formula\n\n        except subprocess.TimeoutExpired as e:\n            print(f\"GP Engine timed out after {timeout_sec}s.\")\n            # Recover output captured so far\n            output = e.stdout if e.stdout else \"\"\n            best_formula = None\n            if output:\n                for line in output.splitlines():\n                    line_lower = line.lower()\n                    if \"formula:\" in line_lower:\n                        idx = line_lower.find(\"formula:\")\n                        if idx != -1:\n                            formula_part = line[idx + len(\"formula:\"):].strip()\n                            if formula_part:\n                                best_formula = formula_part\n                                if \"final formula:\" in line_lower:\n                                    break\n            \n            if best_formula:\n                print(f\"Recovered best formula from timeout: {best_formula}\")\n                return best_formula\n            \n            # Print stderr for timeout diagnose\n            if e.stderr:\n                 print(f\"GP Engine Timeout Stderr: {e.stderr}\")\n            return None\n\n        except Exception as e:\n            print(f\"GP Engine failed: {e}\")\n            if hasattr(e, 'stderr') and e.stderr:\n                print(f\"Stderr: {e.stderr}\")\n            return None\n        finally:\n            # Cleanup\n            if os.path.exists(seed_file_path):\n                os.unlink(seed_file_path)\n            if os.path.exists(data_file_path):\n                os.unlink(data_file_path)\n\nif __name__ == \"__main__\":\n    # Test\n    engine = GPEngine()\n    x = [1, 2, 3, 4]\n    y = [1+2, 4+2, 9+2, 16+2] # x^2 + 2\n    seeds = [\"(x * x)\", \"(x + 2)\"]\n    \n    print(\"Testing GPEngine...\")\n    res = engine.run(x, y, seeds)\n    print(f\"Result: {res}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/core/__init__.py\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/data/synthetic_data.py\n",
        "import numpy as np\nimport random\nfrom core.grammar import VOCABULARY, OPERATORS, VARIABLES, CONSTANTS, ExpressionTree\nfrom data.augmentation import augment_formula_tokens\n\nclass DataGenerator:\n    def __init__(self, max_depth=5, population_size=1000, allowed_operators=None):\n        self.max_depth = max_depth\n        self.population_size = population_size\n        self.vocab = VOCABULARY\n        # Pre-compute terminal vs operator lists\n        self.terminals = VARIABLES + CONSTANTS\n        if allowed_operators:\n            self.operators = [op for op in allowed_operators if op in OPERATORS]\n        else:\n            self.operators = list(OPERATORS.keys())\n\n    def generate_random_tree(self, max_depth, current_depth=0):\n        if current_depth >= max_depth:\n            # Balanced Terminal Selection: 50% x, 50% constant\n            if random.random() < 0.5:\n                return ['x']\n            else:\n                return [random.choice(CONSTANTS)]\n        \n        # Decide if terminal or operator\n        # Higher probability of operator at shallow depths\n        if random.random() < 0.7: \n            op = random.choice(self.operators)\n            arity = OPERATORS[op]\n            tokens = [op]\n            for _ in range(arity):\n                tokens.extend(self.generate_random_tree(max_depth, current_depth + 1))\n            return tokens\n        else:\n            # Balanced Terminal Selection: 40% x, 30% C, 30% numbers\n            r = random.random()\n            if r < 0.4:\n                return ['x']\n            elif r < 0.7:\n                return ['C']\n            else:\n                return [random.choice([c for c in CONSTANTS if c != 'C'])]\n\n    def generate_batch(self, batch_size, point_count=10, x_range=(-10, 10)):\n        \"\"\"\n        Generates a batch of (X, Y) pairs and their generating formulas.\n        \"\"\"\n        data = []\n        \n        while len(data) < batch_size:\n            # Generate random formula\n            tokens = self.generate_random_tree(self.max_depth)\n            tree = ExpressionTree(tokens)\n            \n            if not tree.is_valid:\n                continue\n            \n            # Ensure 'x' is present in the formula (90% of the time)\n            if 'x' not in tokens and random.random() < 0.9:\n                continue\n                \n            # Generate random X points\n            x_values = np.random.uniform(x_range[0], x_range[1], point_count)\n            # Sort X for cleaner visualization/learning\n            x_values.sort()\n            \n            # Randomize 'C' values if present\n            c_positions = tree.root.get_constant_positions()\n            constant_vals = {}\n            for pos in c_positions:\n                # Expanded range: -20 to 20. Favor 1.0 occasionally\n                val = random.uniform(-20, 20) if random.random() > 0.1 else 1.0\n                constant_vals[tuple(pos)] = val\n            \n            # Calculate Y with randomized constants\n            y_values = tree.evaluate(x_values, constants=constant_vals)\n            \n            # Check for validity (no NaNs, Infs, or extremely large values)\n            if np.any(np.isnan(y_values)) or np.any(np.isinf(y_values)):\n                continue\n            if np.max(np.abs(y_values)) > 1e6: # Reject too large numbers\n                continue\n            if np.std(y_values) < 1e-6: # Reject flat lines (too simple)\n                 # Optionally keep some, but mostly we want interesting curves\n                 if random.random() > 0.1: continue\n\n            data.append({\n                'tokens': tokens,\n                'infix': tree.get_infix(),\n                'x': x_values,\n                'y': y_values\n            })\n            \n        return data\n\n    def generate_structured_tree(self, complexity=1, input_node='x'):\n        \"\"\"\n        Recursively builds a structured, human-like formula.\n        Respects self.operators.\n        \"\"\"\n        # Base cases\n        if complexity <= 0:\n            # Randomly choose between x, C and constants\n            r = random.random()\n            if r < 0.4: return ['x']\n            if r < 0.7: return ['C']\n            return [random.choice([c for c in CONSTANTS if c != 'C'])]\n            \n        # Filter available structures based on allowed operators\n        available_structures = []\n        \n        # Arithmetic needed: +, -, *\n        if any(op in self.operators for op in ['+', '-', '*']):\n            available_structures.append('arithmetic')\n            \n        # Poly needed: pow\n        if 'pow' in self.operators:\n            available_structures.append('poly')\n            \n        # Trig needed: sin, cos\n        if 'sin' in self.operators or 'cos' in self.operators:\n            available_structures.append('trig')\n            \n        # Exp/Log needed\n        if 'exp' in self.operators or 'log' in self.operators:\n            available_structures.append('exp_log')\n            \n        # Composition needs enough variety\n        if len(self.operators) > 4 and complexity > 1:\n             available_structures.append('composition')\n        \n        # Fallback if nothing allowed matches (shouldn't happen with proper init)\n        if not available_structures:\n            return input_node if isinstance(input_node, list) else [input_node]\n\n        choice = random.choice(available_structures)\n        \n        if choice == 'poly':\n            # a*x + b or a*x^2 + b\n            a = str(random.randint(1, 5))\n            b = str(random.randint(-5, 5))\n            power = random.choice(['1', '2', '3'])\n            if power == '1':\n                term = ['*', a] + (input_node if isinstance(input_node, list) else [input_node])\n                return ['+', ] + term + [b]\n            else:\n                base = input_node if isinstance(input_node, list) else [input_node]\n                pow_term = ['pow'] + base + [power]\n                term = ['*', a] + pow_term\n                return ['+', ] + term + [b]\n                \n        elif choice == 'trig':\n            # Filter trig ops that are allowed\n            ops = [op for op in ['sin', 'cos'] if op in self.operators]\n            if not ops: return input_node # Should be caught by structure check\n            func = random.choice(ops)\n            val = input_node if isinstance(input_node, list) else [input_node]\n            return [func] + val\n            \n        elif choice == 'exp_log':\n            ops = [op for op in ['exp', 'log'] if op in self.operators]\n            if not ops: return input_node\n            func = random.choice(ops)\n            val = input_node if isinstance(input_node, list) else [input_node]\n            return [func] + val\n            \n        elif choice == 'arithmetic':\n            left = self.generate_structured_tree(complexity - 1, input_node)\n            right = self.generate_structured_tree(complexity - 1, input_node)\n            ops = [op for op in ['+', '-', '*'] if op in self.operators]\n            if not ops: return input_node\n            op = random.choice(ops)\n            return [op] + left + right\n            \n        elif choice == 'composition':\n            inner = self.generate_structured_tree(complexity - 1, input_node)\n            outer = self.generate_structured_tree(1, inner)\n            return outer\n            \n        return [input_node]\n\n    def generate_inverse_batch(self, batch_size, point_count=10, x_range=(-5, 5)):\n        \"\"\"\n        Generates complex, structured formulas using the new engine.\n        \"\"\"\n        data = []\n        attempts = 0\n        \n        while len(data) < batch_size and attempts < batch_size * 5:\n            attempts += 1\n            # Random complexity capped by max_depth\n            complexity = random.randint(1, max(1, self.max_depth - 1))\n            \n            try:\n                tokens = self.generate_structured_tree(complexity, 'x')\n                \n                # Convert numeric strings to 'C' placeholders if needed\n                # But here we want the GROUND TRUTH tokens with numbers for checking?\n                # The model predicts tokens. 'C' is for optimization.\n                # If we train \"End-to-End\" (predict 3*x), we keep numbers.\n                # If we train \"Symbolic\" (predict C*x), we swap.\n                # The original code swapped numbers to 'C'. Let's check VOCABULARY.\n                # '1','2','3' are in VOCABULARY. So we can keep small integers.\n                # Large integers -> 'C'.\n                \n                final_tokens = []\n                for t in tokens:\n                    if t in self.vocab:\n                        final_tokens.append(t)\n                    else:\n                        # If it's a number not in vocab, map to C?\n                        # Or just nearest constant?\n                        # For now, simplistic mapping:\n                        try:\n                            val = float(t)\n                            if abs(val - round(val)) < 0.01 and str(int(round(val))) in self.vocab:\n                                final_tokens.append(str(int(round(val))))\n                            else:\n                                final_tokens.append('C')\n                        except:\n                            final_tokens.append('C')\n\n                # --- DATA AUGMENTATION ---\n                if random.random() < 0.3:\n                    final_tokens = augment_formula_tokens(final_tokens)\n                # -------------------------\n                \n                tree = ExpressionTree(final_tokens)\n                if not tree.is_valid:\n                    continue\n                \n                # Ensure 'x' is present (90% of the time)\n                if 'x' not in final_tokens and random.random() < 0.9:\n                    continue\n                    \n                # Check constraints (depth, length)\n                if len(final_tokens) > 30: # Limit length\n                    continue\n\n                # Generate X points\n                # Use safer range for complex funcs\n                # Exp/Pow grow very fast, so we constrain X to avoid float overflow\n                if 'exp' in final_tokens or 'pow' in final_tokens:\n                    x_safe = np.linspace(-2, 2, point_count)\n                elif 'log' in final_tokens or 'sqrt' in final_tokens:\n                    x_safe = np.linspace(0.1, 5, point_count)\n                else:\n                    x_safe = np.linspace(x_range[0], x_range[1], point_count)\n                \n                # Randomize 'C' values if present\n                c_positions = tree.root.get_constant_positions()\n                constant_vals = {}\n                for pos in c_positions:\n                    # Expanded range: -20 to 20\n                    val = random.uniform(-20, 20) if random.random() > 0.1 else 1.0\n                    constant_vals[tuple(pos)] = val\n                \n                y_values = tree.evaluate(x_safe, constants=constant_vals)\n                \n                # Quality Control\n                if np.any(np.isnan(y_values)) or np.any(np.isinf(y_values)):\n                    continue\n                if np.max(np.abs(y_values)) > 1e4: # Relaxed limit\n                    continue\n                if np.std(y_values) < 0.01: # Too flat\n                    continue\n                \n                data.append({\n                    'tokens': final_tokens,\n                    'infix': tree.get_infix(),\n                    'x': x_safe,\n                    'y': y_values\n                })\n            except Exception:\n                continue\n                \n        return data\n\n# Quick test if run directly\nif __name__ == \"__main__\":\n    gen = DataGenerator(max_depth=4)\n    batch = gen.generate_batch(5)\n    for item in batch:\n        print(f\"Formula: {item['infix']}\")\n        print(f\"Tokens: {item['tokens']}\")\n        print(f\"Y sample: {item['y'][:3]}...\")\n        print(\"-\" * 20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/data/benchmark_data.py\n",
        "import numpy as np\n\n# Standard Benchmark Problems\n# Levels: 1 (Easy), 2 (Medium), 3 (Hard)\n\nBENCHMARK_SUITE = [\n    # --- Level 1: Polynomials & Basic Arithmetic ---\n    {\n        'id': 'p1',\n        'name': 'Lineal',\n        'formula_str': '2.5 * x + 1.0',\n        'lambda': lambda x: 2.5 * x + 1.0,\n        'domain': (-10, 10),\n        'points': 20,\n        'level': 1\n    },\n    {\n        'id': 'p2',\n        'name': 'Cuadratica Simple',\n        'formula_str': 'x * x',\n        'lambda': lambda x: x**2,\n        'domain': (-5, 5),\n        'points': 20,\n        'level': 1\n    },\n    {\n        'id': 'p3',\n        'name': 'Polinomio Cubico',\n        'formula_str': 'x**3 + x**2',\n        'lambda': lambda x: x**3 + x**2,\n        'domain': (-3, 3),\n        'points': 20,\n        'level': 1\n    },\n    \n    # --- Level 2: Trigonometric & Transcendental ---\n    {\n        'id': 'p4',\n        'name': 'Seno Basico',\n        'formula_str': 'sin(x)',\n        'lambda': lambda x: np.sin(x),\n        'domain': (-np.pi, np.pi),\n        'points': 30,\n        'level': 2\n    },\n    {\n        'id': 'p5',\n        'name': 'Coseno Desplazado',\n        'formula_str': 'cos(x) + 1',\n        'lambda': lambda x: np.cos(x) + 1,\n        'domain': (-np.pi, np.pi),\n        'points': 30,\n        'level': 2\n    },\n    {\n        'id': 'p6',\n        'name': 'Exponencial Simple',\n        'formula_str': 'exp(x)',\n        'lambda': lambda x: np.exp(x),\n        'domain': (-2, 2), # Small domain to avoid explosion\n        'points': 20,\n        'level': 2\n    },\n    \n    # --- Level 3: Physics / Complex ---\n    {\n        'id': 'p7',\n        'name': 'Damped Oscillation',\n        'formula_str': 'exp(-x) * sin(2*x)',\n        'lambda': lambda x: np.exp(-x) * np.sin(2*x),\n        'domain': (0, 4),\n        'points': 40,\n        'level': 3\n    },\n    {\n        'id': 'p8',\n        'name': 'Gaussian',\n        'formula_str': 'exp(-x**2)',\n        'lambda': lambda x: np.exp(-x**2),\n        'domain': (-3, 3),\n        'points': 30,\n        'level': 3\n    },\n    {\n        'id': 'p9',\n        'name': 'Nguyen-3 (x^3 + x^2 + x)',\n        'formula_str': 'x**3 + x**2 + x',\n        'lambda': lambda x: x**3 + x**2 + x,\n        'domain': (-2, 2),\n        'points': 20,\n        'level': 3\n    },\n    {\n        'id': 'p10',\n        'name': 'Rational Function',\n        'formula_str': 'x / (1 + x**2)',\n        'lambda': lambda x: x / (1 + x**2),\n        'domain': (-4, 4),\n        'points': 30,\n        'level': 3\n    }\n]\n\ndef get_benchmark_data(problem_id):\n    \"\"\"Returns (x, y) for a specific problem ID.\"\"\"\n    for p in BENCHMARK_SUITE:\n        if p['id'] == problem_id:\n            x = np.linspace(p['domain'][0], p['domain'][1], p['points'])\n            y = p['lambda'](x)\n            return x, y, p\n    return None, None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/data/augmentation.py\n",
        "\nimport random\nfrom core.grammar import OPERATORS\n\ndef augment_formula_tokens(tokens):\n    \"\"\"\n    Applies mathematical invariants to generate an equivalent formula structure.\n    Acts as 'Data Augmentation' for symbolic regression.\n    \n    Supported Transformations:\n    1. Commutativity: (+) and (*)\n       e.g. [+ a b] -> [+ b a]\n    2. Identity:\n       e.g. x -> [+ x 0], x -> [* x 1] (Rarely used to avoid bloat, but useful for robustness)\n    3. Inverse operations (Conceptually):\n       Not implemented directly on tokens without tree parsing, \n       so we focus on purely structural swaps that don't change value.\n    \n    Args:\n        tokens (list): List of tokens in Prefix notation.\n    \n    Returns:\n        list: A new list of tokens representing an equivalent formula.\n    \"\"\"\n    if not tokens:\n        return []\n\n    # Helper to parse prefix expression into a tree-like structure (recursive)\n    def parse_prefix(token_list):\n        if not token_list:\n            return None, []\n        \n        root = token_list[0]\n        remaining = token_list[1:]\n        \n        if root in OPERATORS:\n            try:\n                arity = OPERATORS[root]\n                children = []\n                for _ in range(arity):\n                    child, remaining = parse_prefix(remaining)\n                    children.append(child)\n                return {'val': root, 'children': children}, remaining\n            except:\n                 # Fallback for malformed\n                return {'val': root, 'children': []}, remaining\n        else:\n            # Terminal\n            return {'val': root, 'children': []}, remaining\n\n    # Helper to flatten tree back to tokens\n    def flatten(node):\n        res = [node['val']]\n        for child in node['children']:\n            res.extend(flatten(child))\n        return res\n\n    # 1. Parse\n    try:\n        tree, _ = parse_prefix(tokens)\n    except:\n        return list(tokens) # Fail safe\n\n    # 2. Augment Recursive\n    def augment_recursive(node):\n        # First augment children\n        for i in range(len(node['children'])):\n            node['children'][i] = augment_recursive(node['children'][i])\n            \n        val = node['val']\n        children = node['children']\n        \n        # Transformation: Commutativity\n        if val in ['+', '*'] and len(children) == 2:\n            if random.random() < 0.5:\n                # Swap children\n                node['children'] = [children[1], children[0]]\n        \n        # Transformation: (- a b) -> (+ a (- b)) ? Too complex for tokens only without 'neg'\n        # Transformation: (+ x x) -> (* x 2) ?\n        if val == '+' and len(children) == 2:\n            # Check deep equality is hard, but simple check:\n            if flatten(children[0]) == flatten(children[1]):\n                if random.random() < 0.3:\n                    # Convert x + x -> x * 2\n                    return {'val': '*', 'children': [children[0], {'val': '2', 'children': []}]}\n\n        return node\n\n    # 3. Apply\n    augmented_tree = augment_recursive(tree)\n    \n    # 4. Flatten\n    return flatten(augmented_tree)\n\nif __name__ == \"__main__\":\n    # Test\n    # Formula: (+ x y) -> prefix ['+', 'x', 'y']\n    t1 = ['+', 'x', 'y']\n    print(f\"Original: {t1} -> Aug: {augment_formula_tokens(t1)}\")\n    \n    # Formula: (* (+ a b) c)\n    t2 = ['*', '+', 'a', 'b', 'c']\n    print(f\"Original: {t2} -> Aug: {augment_formula_tokens(t2)}\")\n    \n    # Formula: (+ x x)\n    t3 = ['+', 'x', 'x']\n    print(f\"Original: {t3} -> Aug: {augment_formula_tokens(t3)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/data/__init__.py\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/search/mcts.py\n",
        "import math\nimport numpy as np\nimport torch\nimport copy\nfrom core.grammar import VOCABULARY, TOKEN_TO_ID, OPERATORS, ExpressionTree, VARIABLES\nfrom utils.optimize_constants import optimize_constants\n\nclass MCTSNode:\n    def __init__(self, tokens, parent=None, prior=0.0):\n        self.tokens = tokens\n        self.parent = parent\n        self.children = {}\n        self.visit_count = 0\n        self.value_sum = 0.0\n        self.prior = prior\n        self.is_expanded = False\n        \n        # for parallel search\n        self.virtual_loss = 0.0\n        self.virtual_visits = 0\n\n    @property\n    def value(self):\n        count = self.visit_count + self.virtual_visits\n        if count == 0:\n            return 0.0\n        # Combine real value and virtual loss\n        # Virtual loss is SUBTRACTED to discourage visits\n        return (self.value_sum - self.virtual_loss) / count\n\n    def ucb_score(self, c_puct=1.0):\n        count = self.visit_count + self.virtual_visits\n        parent_count = self.parent.visit_count + self.parent.virtual_visits if self.parent else 1\n        \n        if self.parent is None:\n            return 0.0\n            \n        u = c_puct * self.prior * math.sqrt(parent_count) / (1 + count)\n        return self.value + u\n\n    @property\n    def complexity(self):\n        \"\"\"Estimate complexity (length of formula).\"\"\"\n        return len(self.tokens)\n\nclass MCTS:\n    def __init__(self, model, device, grammar=None, c_puct=1.0, n_simulations=100, max_simulations=None, max_depth=50, complexity_lambda=0.1, max_len=200, batch_size=8):\n        self.model = model\n        self.device = device\n        self.grammar = grammar\n        self.c_puct = c_puct\n        \n        # Handle backwards compatibility for max_simulations\n        if max_simulations is not None:\n            self.n_simulations = max_simulations\n        else:\n            self.n_simulations = n_simulations\n            \n        self.max_depth = max_depth\n        self.complexity_lambda = complexity_lambda\n        self.max_len = max_len\n        self.min_value = -float('inf')\n        self.max_value = float('inf')\n        self.vocab_size = len(VOCABULARY)\n        self.sos_id = self.vocab_size\n        self.batch_size = batch_size\n        \n        # Pareto Front: List of {'tokens':, 'rmse':, 'complexity':, 'formula':}\n        self.pareto_front = []\n        \n        # Virtual loss constant usually 1-3\n        self.v_loss_const = 3.0\n        \n    def search(self, x_values, y_values, num_simulations=None):\n        \"\"\"\n        Run MCTS (Parallel/Batched) to find the best formula.\n        \"\"\"\n        self.pareto_front = [] # Reset Pareto Front for new search\n        root = MCTSNode(tokens=[])\n        \n        # Initial expansion (single)\n        self._expand_batch([root], x_values, y_values)\n        \n        best_rmse = float('inf')\n        best_formula = None\n        best_tokens = None\n        \n        limit = num_simulations if num_simulations is not None else self.n_simulations\n        \n        # Loop in batches\n        # Ensure we do at least 1 batch\n        num_batches = max(1, (limit + self.batch_size - 1) // self.batch_size)\n        \n        for _ in range(num_batches): \n            leaves = []\n            \n            # 1. Selection (find N leaves)\n            for _ in range(self.batch_size):\n                node = root\n                depth = 0\n                \n                # Selection loop\n                while node.is_expanded and node.children and depth < self.max_depth:\n                    node = max(node.children.values(), key=lambda n: n.ucb_score(self.c_puct))\n                    \n                    # Apply virtual loss to discourage re-selection in same batch\n                    node.virtual_loss += self.v_loss_const\n                    node.virtual_visits += 1\n                    depth += 1\n                \n                # Check if valid leaf to expand\n                if depth < self.max_depth and not node.is_expanded:\n                    # Avoid duplicates in batch (simple check)\n                    if node not in leaves:\n                        leaves.append(node)\n                else:\n                    pass\n            \n            if not leaves:\n                # If no leaves found (tree fully explored or locked), standard MCTS usually continues or stops.\n                # We can just break or continue backprop of terminals.\n                if root.visit_count > limit: break \n                continue\n                \n            # 2. Batch Expansion & Evaluation\n            values = self._expand_batch(leaves, x_values, y_values)\n            \n            # 3. Backpropagation\n            for node, val in zip(leaves, values):\n                # Check for best solution found\n                if self._is_complete_tree(node.tokens):\n                    # For completed formulas, we calculate REAL RMSE\n                    try:\n                        # Evaluar\n                        # Importar aqu\u00ed para evitar circular imports si es necesario\n                        from utils.optimize_constants import optimize_constants\n                        \n                        # 1. Optimizar constants (Crucial para Accuracy)\n                        # Esto es \"Phase 1\" de TPSR (constantes en las hojas)\n                        # Por simplicidad en esta iteraci\u00f3n, asumimos que 'evaluate_formula' ya hace algo o usamos el string directo.\n                        # Idealmente llamar\u00edamos a BFGS aqu\u00ed.\n                        \n                        # Use existing _evaluate_formula to get RMSE and optimized constants\n                        tree = ExpressionTree(node.tokens)\n                        optimized_constants, real_rmse = optimize_constants(tree, x_values, y_values)\n                        \n                        # Get y_pred using the optimized constants\n                        y_pred = tree.evaluate(x_values, constants=optimized_constants)\n                        \n                        # Check dimensions\n                        if y_pred.shape != y_values.shape:\n                            # If shapes don't match, it's an invalid evaluation\n                            final_val = 0.0\n                        else:\n                            # 2. Calcular Reward TPSR (Hybrid Accuracy + Complexity)\n                            # R = 1 / (1 + NMSE) + lambda * exp(-len/L)\n                            \n                            mse = np.mean((y_pred - y_values)**2)\n                            var_y = np.var(y_values)\n                            if var_y < 1e-9: var_y = 1.0 # Avoid division by zero\n                            \n                            nmse = mse / var_y\n                            \n                            # Evitar NMSE gigantes\n                            if np.isnan(nmse) or np.isinf(nmse):\n                                nmse = 1e9\n                            \n                            r_acc = 1.0 / (1.0 + nmse)\n                            \n                            # Penalizaci\u00f3n por complejidad\n                            token_len = len(node.tokens)\n                            L = self.max_len # Max length del modelo\n                            \n                            r_cplx = self.complexity_lambda * np.exp(-token_len / L)\n                            \n                            # Suma y Normalizaci\u00f3n (para mantener rango 0-1)\n                            # El m\u00e1ximo te\u00f3rico es (1.0 + lambda). Dividimos por eso.\n                            raw_reward = r_acc + r_cplx\n                            final_val = raw_reward / (1.0 + self.complexity_lambda)\n\n                        # Update best formula based on RMSE (for reporting, not for MCTS value)\n                        if real_rmse < best_rmse:\n                            best_rmse = real_rmse\n                            best_tokens = node.tokens\n                            best_formula = ExpressionTree(node.tokens).get_infix()\n                        \n                        # Update Pareto Front\n                        # Complexity = len(tokens) (or could use count_constants + nodes)\n                        complexity = len(node.tokens)\n                        self._update_pareto_front(node.tokens, real_rmse, complexity, ExpressionTree(node.tokens).get_infix())\n\n                    except Exception as e:\n                        # print(f\"Error evaluating formula: {e}\")\n                        final_val = 0.0 # Invalid formula gets 0 reward\n                else:\n                    final_val = val\n                \n                # The following lines were part of the user's instruction but contained syntax errors and undefined variables.\n                # They are commented out to maintain a syntactically correct and functional document.\n                # If these lines were intended to be added, please provide a complete and correct snippet.\n                #\n                # # Construir vector de probabilidades\n                # probs = np.zeros(self.vocab_size, dtype=np.float32)\n                # for token_id, count in counts.items():\n                #     probs[token_id] = count / total_visits_count += 1\n                \n                curr = node\n                while curr is not None:\n                    curr.visit_count += 1\n                    curr.value_sum += final_val\n                    \n                    # Revert virtual loss for parent and above\n                    # Since we added to PARENT's child (which is curr), \n                    # and we traverse Up...\n                    # Wait, logic: We selected CHILD. Virtual loss was added TO CHILD (curr).\n                    # So we must remove it from curr.\n                    if curr.virtual_visits > 0:\n                        curr.virtual_loss -= self.v_loss_const\n                        curr.virtual_visits -= 1\n                            \n                    curr = curr.parent\n        \n        # After search, force cleanup of any residual virtual loss (safety)\n        # (Not strictly needed if logic is perfect, but good practice in complex async MCTS)\n        \n        return {\n            'tokens': best_tokens,\n            'formula': best_formula,\n            'rmse': best_rmse,\n            'root': root,\n            'pareto_front': self.pareto_front\n        }\n\n    def _update_pareto_front(self, tokens, rmse, complexity, formula_str):\n        \"\"\"\n        Update the Pareto Front with a new solution.\n        Keep solutions that are not dominated by any other solution.\n        Solution A dominates B if:\n        A.rmse <= B.rmse AND A.complexity <= B.complexity AND (A.rmse < B.rmse OR A.complexity < B.complexity)\n        \"\"\"\n        # Create candidate\n        candidate = {'tokens': tokens, 'rmse': rmse, 'complexity': complexity, 'formula': formula_str}\n        \n        # Check if dominated by existing\n        is_dominated = False\n        to_remove = []\n        \n        for existing in self.pareto_front:\n            # Check if existing dominates candidate\n            if (existing['rmse'] <= candidate['rmse'] and \n                existing['complexity'] <= candidate['complexity'] and \n                (existing['rmse'] < candidate['rmse'] or existing['complexity'] < candidate['complexity'])):\n                is_dominated = True\n                break\n                \n            # Check if candidate dominates existing\n            if (candidate['rmse'] <= existing['rmse'] and \n                candidate['complexity'] <= existing['complexity'] and \n                (candidate['rmse'] < existing['rmse'] or candidate['complexity'] < existing['complexity'])):\n                to_remove.append(existing)\n        \n        if not is_dominated:\n            # Remove dominated existing solutions\n            for item in to_remove:\n                self.pareto_front.remove(item)\n            \n            # Add candidate\n            self.pareto_front.append(candidate)\n            # Sort by RMSE for easier viewing\n            self.pareto_front.sort(key=lambda x: x['rmse'])\n\n    def _expand_batch(self, nodes, x_values, y_values):\n        \"\"\"\n        Batched expansion. Returns list of values.\n        \"\"\"\n        if not nodes:\n            return []\n            \n        # Prepare inputs\n        x_tensor = torch.tensor(x_values, dtype=torch.float32).unsqueeze(0).to(self.device)\n        y_tensor = torch.tensor(y_values, dtype=torch.float32).unsqueeze(0).to(self.device)\n        \n        # Repeat X/Y for batch\n        batch_size = len(nodes)\n        x_batch = x_tensor.repeat(batch_size, 1, 1).squeeze(1) # [batch, points]\n        y_batch = y_tensor.repeat(batch_size, 1, 1).squeeze(1) # [batch, points]\n        \n        # Prepare sequences\n        # Find max len\n        max_len = 0\n        seqs = []\n        for n in nodes:\n            s = [self.sos_id] + [TOKEN_TO_ID[t] for t in n.tokens]\n            seqs.append(s)\n            max_len = max(max_len, len(s))\n            \n        # Pad and stack\n        input_tensor = torch.full((batch_size, max_len), self.sos_id, dtype=torch.long).to(self.device)\n        for i, s in enumerate(seqs):\n            input_tensor[i, :len(s)] = torch.tensor(s, dtype=torch.long)\n            \n        # Inference\n        with torch.no_grad():\n            logits, value_preds = self.model(x_batch, y_batch, input_tensor)\n            \n        # Process results\n        values = []\n        \n        # To CPU numpy for probability processing\n        probs_batch = torch.softmax(logits[:, -1, :self.vocab_size], dim=1).cpu().numpy()\n        value_preds = value_preds.cpu().numpy() # [batch, 3]\n        \n        for i, node in enumerate(nodes):\n            # 1. Store Value (Median for now)\n            # value_preds is [batch, 3] -> (Pessimistic, Median, Optimistic)\n            # We use Median (index 1) for standard UCB.\n            val_pred = value_preds[i, 1] \n            val = float(np.clip(val_pred, 0.0, 1.0))\n            values.append(val)\n            \n            # 2. Expand children\n            node_probs = probs_batch[i]\n            valid_next = self._get_valid_next_tokens(node.tokens)\n            \n            for idx in valid_next:\n                token = VOCABULARY[idx]\n                prior = node_probs[idx]\n                child = MCTSNode(tokens=node.tokens + [token], parent=node, prior=prior)\n                node.children[token] = child\n            \n            node.is_expanded = True\n            \n        return values\n\n    def _get_valid_next_tokens(self, tokens):\n        \"\"\"Simple grammar check.\"\"\"\n        open_slots = 1\n        for t in tokens:\n            if t in OPERATORS:\n                open_slots += OPERATORS[t] - 1\n            else:\n                open_slots -= 1\n        \n        if open_slots <= 0:\n            return []\n        return list(range(self.vocab_size))\n\n    def _is_complete_tree(self, tokens):\n        if not tokens: return False\n        try:\n            tree = ExpressionTree(tokens)\n            # Basic validation\n            if len(tokens) > self.max_depth * 2: return False\n            return tree.is_valid\n        except:\n            return False\n\n    def _evaluate_formula(self, tokens, x, y):\n        try:\n            tree = ExpressionTree(tokens)\n            _, rmse = optimize_constants(tree, x, y)\n            return rmse\n        except:\n            return 1e9\n\n    def get_training_examples(self, root):\n        \"\"\"\n        Extrae ejemplos de entrenamiento del \u00e1rbol generado.\n        Retorna: lista de (state_tokens, policy_probs, value_target)\n        \"\"\"\n        examples = []\n        queue = [root]\n        \n        while queue:\n            node = queue.pop(0)\n            if node.visit_count < 1: \n                continue\n            \n            # Policy Target (Pi)\n            # Distribuci\u00f3n de visitas de los hijos\n            counts = {}\n            total_visits = 0\n            has_children = False\n            \n            for token_id, child in node.children.items():\n                # child key is token STRING or ID?\n                # In _expand_batch: node.children[token] = child.\n                # token = VOCABULARY[idx] (String).\n                # So keys are strings.\n                # But we need ID for probabilities array index.\n                if token_id in TOKEN_TO_ID:\n                    tid = TOKEN_TO_ID[token_id]\n                    counts[tid] = child.visit_count\n                    total_visits += child.visit_count\n                    queue.append(child)\n                    has_children = True\n            \n            if not has_children or total_visits == 0:\n                continue\n                \n            # Construir vector de probabilidades\n            probs = np.zeros(self.vocab_size, dtype=np.float32)\n            for tid, count in counts.items():\n                probs[tid] = count / total_visits\n            \n            # Value Target (V)\n            # Usamos el Q-value (valor esperado) del nodo como target para el Value Head.\n            # Q = value_sum / visit_count\n            v = node.value_sum / node.visit_count\n            \n            # State: node.tokens (lista de ids?)\n            # node.tokens is list of strings (from VOCABULARY).\n            # self_play.py expects tokens as strings in ReplayBuffer.add.\n            examples.append((node.tokens, probs, v))\n            \n        return examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/search/beam_search.py\n",
        "\"\"\"\nBeam Search for AlphaSymbolic.\nExplores multiple formula candidates in parallel, keeping top-K at each step.\n\"\"\"\nimport torch\nimport numpy as np\nfrom core.grammar import VOCABULARY, OPERATORS, TOKEN_TO_ID, ExpressionTree, OPERATOR_STAGES\nfrom utils.optimize_constants import optimize_constants\n\nclass BeamSearch:\n    def __init__(self, model, device, beam_width=10, max_length=30, curriculum_stage=None):\n        self.model = model\n        self.device = device\n        self.beam_width = beam_width\n        self.max_length = max_length\n        self.vocab_size = len(VOCABULARY)\n        self.sos_id = self.vocab_size  # SOS token ID\n        \n        # Build token mask based on curriculum stage\n        self.token_mask = None\n        if curriculum_stage is not None:\n            allowed_ops = OPERATOR_STAGES.get(curriculum_stage, list(OPERATORS.keys()))\n            allowed_tokens = set(['x', 'C', '0', '1', '2', '3', '5', '10', 'pi', 'e'])\n            allowed_tokens.update(allowed_ops)\n            \n            # Create mask: 0 for allowed, -inf for disallowed\n            mask = torch.full((self.vocab_size,), float('-inf'), device=device)\n            for token in allowed_tokens:\n                if token in TOKEN_TO_ID:\n                    mask[TOKEN_TO_ID[token]] = 0.0\n            self.token_mask = mask\n        \n    def search(self, x_values, y_values, return_partial=False):\n        \"\"\"\n        Beam Search to find the best formula structure.\n        \"\"\"\n        # Prepare data once\n        x_tensor = torch.tensor(x_values, dtype=torch.float32).unsqueeze(0).to(self.device) # [1, points]\n        y_tensor = torch.tensor(y_values, dtype=torch.float32).unsqueeze(0).to(self.device) # [1, points]\n        \n        # Each element in beams is just the sequence of tokens (list of strings)\n        # We track scores and open branches in parallel lists or a list of dicts\n        beams = [{'seq': [], 'log_prob': 0.0, 'open': 1}]\n        \n        completed = []\n        \n        for step in range(self.max_length):\n            if not beams:\n                break\n                \n            # Filter valid beams just in case\n            active_beams = [b for b in beams if b['open'] > 0]\n            if not active_beams:\n                break\n                \n            # Prepare batch for model\n            # Batch size = number of active beams\n            batch_size = len(active_beams)\n            \n            # Expand X and Y to match batch size [batch, points]\n            x_batch = x_tensor.expand(batch_size, -1)\n            y_batch = y_tensor.expand(batch_size, -1)\n            \n            # Prepare input sequences [batch, current_seq_len]\n            # Must prepend SOS token\n            seqs = [[self.sos_id] + [TOKEN_TO_ID[t] for t in b['seq']] for b in active_beams]\n            input_tensor = torch.tensor(seqs, dtype=torch.long).to(self.device)\n            \n            # Single model call for all beams\n            with torch.no_grad():\n                logits, _ = self.model(x_batch, y_batch, input_tensor)\n            \n            # Logits shape: [batch, seq_len, vocab_size]\n            # We want the last token's probabilities\n            last_token_logits = logits[:, -1, :self.vocab_size]\n            \n            # Apply curriculum mask if set\n            if self.token_mask is not None:\n                last_token_logits = last_token_logits + self.token_mask\n            \n            log_probs = torch.log_softmax(last_token_logits, dim=-1) # [batch, vocab]\n            \n            # --- Repetition Penalty (Simple) ---\n            # If the same token was generated recently, penalize it slightly.\n            # This prevents 10 ////////// loops.\n            penalty_factor = 2.0  # Reduce log_prob (which is negative) by absolute amount or multiplier?\n            # Log probs are negative (e.g. -0.1). Making them MORE negative penalizes.\n            # If we multiply by 1.2, -0.1 becomes -0.12 (lower probability).\n            \n            for i, beam in enumerate(active_beams):\n                if beam['seq']:\n                     # Get last token ID\n                    last_token = beam['seq'][-1]\n                    if last_token in TOKEN_TO_ID:\n                        last_id = TOKEN_TO_ID[last_token]\n                        # Penalize current step logits for this token\n                        # If log_prob is close to 0 (high prob), e.g. -0.01 -> -0.012\n                        # If log_prob is -10 (low prob), -> -12\n                        # Check bounds to avoid NaN if -inf\n                        if log_probs[i, last_id] > -1e9:\n                             log_probs[i, last_id] *= 1.5 \n            # -----------------------------------\n            \n            # We need to find the top-K candidates ACROSS current beams?\n            # Standard beam search: expand all, then prune to K\n            \n            all_candidates = []\n            \n            # Get top-K for EACH beam to avoid explosion (e.g. top 2*width)\n            k_per_beam = min(self.beam_width, self.vocab_size)\n            beam_topk_scores, beam_topk_indices = torch.topk(log_probs, k_per_beam, dim=-1)\n            \n            # Move to CPU for processing logic\n            beam_topk_scores = beam_topk_scores.cpu().numpy()\n            beam_topk_indices = beam_topk_indices.cpu().numpy()\n            \n            for i, beam in enumerate(active_beams):\n                for score, idx in zip(beam_topk_scores[i], beam_topk_indices[i]):\n                    token = VOCABULARY[idx]\n                    new_seq = beam['seq'] + [token]\n                    \n                    # Calculate new open branches\n                    if token in OPERATORS:\n                        new_open = beam['open'] + OPERATORS[token] - 1\n                    else:\n                        new_open = beam['open'] - 1\n                    \n                    if new_open < 0:\n                        continue\n                        \n                    all_candidates.append({\n                        'seq': new_seq,\n                        'log_prob': beam['log_prob'] + score,\n                        'open': new_open\n                    })\n            \n            # Global prune: keep top beam_width\n            all_candidates.sort(key=lambda x: x['log_prob'], reverse=True)\n            beams = all_candidates[:self.beam_width]\n            \n            # Check for completions\n            still_active = []\n            for b in beams:\n                if b['open'] == 0:\n                    completed.append(b)\n                else:\n                    still_active.append(b)\n            \n            beams = still_active\n            # If we filled up on completions, we might still want to explore? \n            # Usually we keep exploring until all beams are done or max length\n            if len(completed) >= self.beam_width:\n                 # Optional: early exit if we found enough good candidates\n                 pass\n\n        # Evaluate results\n        scored_results = []\n        for beam in completed:\n            tree = ExpressionTree(beam['seq'])\n            if tree.is_valid:\n                constants, rmse = optimize_constants(tree, x_values, y_values)\n                scored_results.append({\n                    'tokens': beam['seq'],\n                    'log_prob': beam['log_prob'],\n                    'rmse': rmse,\n                    'constants': constants,\n                    'formula': tree.get_infix()\n                })\n        \n        scored_results.sort(key=lambda x: x['rmse'])\n        \n        # If no results and return_partial is requested, return the best incomplete beam\n        if not scored_results and return_partial and beams:\n            # Take the beam with highest probability\n            best_beam = beams[0] \n            # Construct a partial result\n            # We can't optimize constants or get a valid infix easily, but we can show tokens\n            scored_results.append({\n                'tokens': best_beam['seq'],\n                'log_prob': best_beam['log_prob'],\n                'rmse': float('inf'),\n                'constants': {},\n                'formula': \"Partial: \" + \" \".join(best_beam['seq']) + \"...\"\n            })\n            \n        return scored_results\n\n\ndef beam_solve(target_x, target_y, model, device, beam_width=20, max_length=25):\n    \"\"\"\n    Solve symbolic regression using beam search.\n    \"\"\"\n    searcher = BeamSearch(model, device, beam_width=beam_width, max_length=max_length)\n    results = searcher.search(target_x, target_y)\n    \n    if not results:\n        return None\n        \n    return results  # Return all results for Pareto analysis\n\n\nif __name__ == \"__main__\":\n    from core.model import AlphaSymbolicModel\n    \n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    VOCAB_SIZE = len(VOCABULARY)\n    \n    model = AlphaSymbolicModel(vocab_size=VOCAB_SIZE + 1, d_model=64).to(DEVICE)\n    try:\n        model.load_state_dict(torch.load(\"alpha_symbolic_model.pth\", map_location=DEVICE, weights_only=True))\n    except:\n        print(\"Model not found, using random weights\")\n    model.eval()\n    \n    # Test\n    x_test = np.linspace(-5, 5, 20).astype(np.float64)\n    y_test = 2 * x_test + 3\n    \n    print(\"Running Beam Search...\")\n    results = beam_solve(x_test, y_test, model, DEVICE, beam_width=10)\n    \n    print(f\"\\nFound {len(results)} valid formulas:\")\n    for i, r in enumerate(results[:5]):\n        print(f\"  {i+1}. {r['formula']} (RMSE: {r['rmse']:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/search/hybrid_search.py\n",
        "import time\nimport torch\nimport numpy as np\nfrom typing import List, Dict, Any, Optional\n\nfrom core.gp_bridge import GPEngine\nfrom search.beam_search import BeamSearch, beam_solve\n\ndef hybrid_solve(\n    x_values: np.ndarray,\n    y_values: np.ndarray,\n    model: torch.nn.Module,\n    device: torch.device,\n    beam_width: int = 50,\n    gp_timeout: int = 10,\n    gp_binary_path: Optional[str] = None\n) -> Dict[str, Any]:\n    \"\"\"\n    Solves Symbolic Regression using a Hybrid Neuro-Evolutionary approach.\n    \n    Phase 1: Neural Beam Search (The Brain)\n             - Rapidly scans the search space.\n             - Generates diverse, high-likelihood formula skeletons.\n             \n    Phase 2: Genetic Programming Refinement (The Muscle)\n             - Takes the best skeletons from Phase 1.\n             - Uses GPU-accelerated evolution to optimize constants and structure.\n             - Runs for `gp_timeout` seconds.\n             \n    Returns:\n        Best found formula result dict.\n    \"\"\"\n    \n    print(f\"--- Starting Alpha-GP Hybrid Search ---\")\n    start_time = time.time()\n    \n    # 1. Neural Beam Search (Phase 1)\n    print(f\"[Phase 1] Neural Beam Search (Width={beam_width})...\")\n    # We use a larger beam width to ensure diversity for the GP\n    # If the user requests beam_width=X, we might want to multiply it for the \"seeds\"\n    # But let's stick to what is passed.\n    \n    neural_results = beam_solve(x_values, y_values, model, device, beam_width=beam_width)\n    \n    seeds = []\n    if neural_results:\n        print(f\"[Phase 1] Found {len(neural_results)} candidates.\")\n        # Extract formulas tokens/string\n        # neural_results is a list of dicts with 'formula' key (infix string)\n        # GPEngine expects infix strings (e.g. \"((x*x)+2)\")\n        \n        # Filter for uniqueness and validity\n        seen_formulas = set()\n        for res in neural_results:\n            f_str = res['formula']\n            # Basic validation: must verify it's not a Partial result\n            if f_str.startswith(\"Partial\"): continue\n            \n            if f_str not in seen_formulas:\n                seeds.append(f_str)\n                seen_formulas.add(f_str)\n        \n        print(f\"[Phase 1] Generated {len(seeds)} unique seeds for GP.\")\n        if len(seeds) > 0:\n            print(f\"Top Seed: {seeds[0]}\")\n    else:\n        print(\"[Phase 1] No valid candidates found (Beam Search failed).\")\n        print(\"[Phase 1] Falling back to pure GP (Random Initialization).\")\n        seeds = []\n\n    # 2. GP Refinement (Phase 2)\n    print(f\"[Phase 2] GPU Genetic Refinement (Timeout={gp_timeout}s)...\")\n    gp_engine = GPEngine(binary_path=gp_binary_path)\n    \n    # Run GP\n    # We pass the seeds. GP engine handles the rest.\n    # Ensure x_values and y_values are lists for gp_engine\n    x_list = x_values.tolist() if hasattr(x_values, 'tolist') else list(x_values)\n    y_list = y_values.tolist() if hasattr(y_values, 'tolist') else list(y_values)\n    gp_result_str = gp_engine.run(x_list, y_list, seeds, timeout_sec=gp_timeout)\n    \n    total_time = time.time() - start_time\n    \n    if gp_result_str:\n        print(f\"--- Hybrid Search Completed in {total_time:.2f}s ---\")\n        print(f\"Best Formula: {gp_result_str}\")\n        \n        # Construct a result dict similar to Beam Search for consistency\n        # Ideally we would evaluate it here to get RMSE, but GP output doesn't give us RMSE directly in a structured way (only stdout).\n        # We can implement a quick evaluator if needed, or assume the user trusts the string.\n        # For UI display, we probably want RMSE.\n        \n        return {\n            'formula': gp_result_str,\n            'rmse': 0.0, # Placeholder, will be evaluated by UI if needed or we can do it here\n            'source': 'Alpha-GP Hybrid',\n            'time': total_time\n        }\n    else:\n        print(f\"--- Hybrid Search Failed (GP did not return valid result) ---\")\n        return None\n\nif __name__ == \"__main__\":\n    # Test\n    # Mock Model\n    class MockModel(torch.nn.Module):\n        def forward(self, x, y, seq):\n            # Return random logits\n            bs, seq_len = seq.shape\n            vocab = 20\n            return torch.randn(bs, seq_len, vocab), None\n\n    print(\"Testing Hybrid Search...\")\n    x = np.linspace(-5, 5, 10)\n    y = x**2\n    try:\n        res = hybrid_solve(x, y, MockModel(), torch.device(\"cpu\"), beam_width=5)\n        print(res)\n    except Exception as e:\n        print(f\"Test failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/search/pareto.py\n",
        "\"\"\"\nPareto Front Manager for AlphaSymbolic.\nMaintains a set of non-dominated solutions (accuracy vs complexity).\n\"\"\"\nimport numpy as np\nfrom core.grammar import ExpressionTree\n\nclass ParetoSolution:\n    def __init__(self, tokens, rmse, complexity, formula_str, constants=None):\n        self.tokens = tokens\n        self.rmse = rmse  # Lower is better\n        self.complexity = complexity  # Lower is better (number of nodes)\n        self.formula = formula_str\n        self.constants = constants or {}\n        \n    def dominates(self, other):\n        \"\"\"Returns True if self dominates other (better in all objectives).\"\"\"\n        # Self dominates other if:\n        # - Self is at least as good in all objectives\n        # - Self is strictly better in at least one objective\n        at_least_as_good = (self.rmse <= other.rmse) and (self.complexity <= other.complexity)\n        strictly_better = (self.rmse < other.rmse) or (self.complexity < other.complexity)\n        return at_least_as_good and strictly_better\n    \n    def __repr__(self):\n        return f\"ParetoSolution(rmse={self.rmse:.4f}, complexity={self.complexity}, formula='{self.formula}')\"\n\n\nclass ParetoFront:\n    def __init__(self, max_size=50):\n        self.solutions = []\n        self.max_size = max_size\n        \n    def add(self, solution):\n        \"\"\"\n        Attempts to add a solution to the Pareto front.\n        Returns True if added, False if dominated.\n        \"\"\"\n        # Check if new solution is dominated by any existing\n        for existing in self.solutions:\n            if existing.dominates(solution):\n                return False  # New solution is dominated\n        \n        # Remove any solutions dominated by the new one\n        self.solutions = [s for s in self.solutions if not solution.dominates(s)]\n        \n        # Add the new solution\n        self.solutions.append(solution)\n        \n        # Enforce max size by removing worst solutions\n        if len(self.solutions) > self.max_size:\n            # Sort by a combined score and keep top max_size\n            self.solutions.sort(key=lambda s: s.rmse + 0.01 * s.complexity)\n            self.solutions = self.solutions[:self.max_size]\n        \n        return True\n    \n    def add_from_results(self, results_list):\n        \"\"\"\n        Add multiple results from beam search or MCTS.\n        results_list: list of dicts with 'tokens', 'rmse', 'constants', 'formula'\n        \"\"\"\n        added = 0\n        for r in results_list:\n            tree = ExpressionTree(r['tokens'])\n            complexity = len(r['tokens'])  # Simple complexity = token count\n            \n            sol = ParetoSolution(\n                tokens=r['tokens'],\n                rmse=r['rmse'],\n                complexity=complexity,\n                formula_str=r['formula'],\n                constants=r.get('constants', {})\n            )\n            \n            if self.add(sol):\n                added += 1\n        \n        return added\n    \n    def get_best_by_rmse(self):\n        \"\"\"Returns the solution with lowest RMSE.\"\"\"\n        if not self.solutions:\n            return None\n        return min(self.solutions, key=lambda s: s.rmse)\n    \n    def get_simplest(self):\n        \"\"\"Returns the solution with lowest complexity.\"\"\"\n        if not self.solutions:\n            return None\n        return min(self.solutions, key=lambda s: s.complexity)\n    \n    def get_balanced(self, alpha=0.5):\n        \"\"\"\n        Returns a balanced solution.\n        alpha: weight for RMSE (1-alpha for complexity)\n        \"\"\"\n        if not self.solutions:\n            return None\n        \n        # Normalize scores\n        rmse_vals = [s.rmse for s in self.solutions]\n        comp_vals = [s.complexity for s in self.solutions]\n        \n        min_rmse, max_rmse = min(rmse_vals), max(rmse_vals) + 1e-10\n        min_comp, max_comp = min(comp_vals), max(comp_vals) + 1e-10\n        \n        def score(s):\n            norm_rmse = (s.rmse - min_rmse) / (max_rmse - min_rmse)\n            norm_comp = (s.complexity - min_comp) / (max_comp - min_comp)\n            return alpha * norm_rmse + (1 - alpha) * norm_comp\n        \n        return min(self.solutions, key=score)\n    \n    def summary(self):\n        \"\"\"Print a summary of the Pareto front.\"\"\"\n        print(f\"\\n=== Pareto Front ({len(self.solutions)} solutions) ===\")\n        for i, sol in enumerate(sorted(self.solutions, key=lambda s: s.rmse)[:10]):\n            print(f\"  {i+1}. RMSE={sol.rmse:.6f}, Nodes={sol.complexity}, Formula: {sol.formula}\")\n\n\n# Quick test\nif __name__ == \"__main__\":\n    front = ParetoFront()\n    \n    # Add some test solutions\n    solutions = [\n        ParetoSolution(['x'], 10.0, 1, \"x\"),\n        ParetoSolution(['+', 'x', '1'], 5.0, 3, \"(x + 1)\"),\n        ParetoSolution(['*', '2', 'x'], 3.0, 3, \"(2 * x)\"),\n        ParetoSolution(['+', '*', '2', 'x', '3'], 0.5, 5, \"((2 * x) + 3)\"),\n        ParetoSolution(['+', '*', '*', '2', 'x', 'x', '+', 'x', '1'], 0.1, 9, \"complicated\"),\n    ]\n    \n    for sol in solutions:\n        added = front.add(sol)\n        print(f\"Added {sol.formula}: {added}\")\n    \n    front.summary()\n    \n    print(f\"\\nBest by RMSE: {front.get_best_by_rmse()}\")\n    print(f\"Simplest: {front.get_simplest()}\")\n    print(f\"Balanced: {front.get_balanced()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/search/__init__.py\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/ui/app_core.py\n",
        "\"\"\"\nCore state and model management for AlphaSymbolic Gradio App.\n\"\"\"\nimport torch\nimport os\nfrom core.model import AlphaSymbolicModel\nfrom core.grammar import VOCABULARY\n\nfrom collections import deque\nimport time\n\n# Global state\nMODEL = None\nDEVICE = None\nTRAINING_STATUS = {\"running\": False, \"epoch\": 0, \"loss\": 0, \"message\": \"Listo\"}\nSTOP_TRAINING = False  # Flag to request training stop\n\ndef request_stop_training():\n    \"\"\"Request training to stop gracefully.\"\"\"\n    global STOP_TRAINING\n    STOP_TRAINING = True\n    return \"\u23f9\ufe0f Deteniendo entrenamiento...\"\n\ndef should_stop_training():\n    \"\"\"Check if training should stop.\"\"\"\n    return STOP_TRAINING\n\ndef reset_stop_flag():\n    \"\"\"Reset the stop flag (call at start of training).\"\"\"\n    global STOP_TRAINING\n    STOP_TRAINING = False\n\n# Hall of Shame: Rolling buffer of recent failures\n# Format: {'time': str, 'target': str, 'predicted': str, 'loss': float, 'stage': str}\nTRAINING_ERRORS = deque(maxlen=20)\n\ndef add_training_error(target, predicted, loss, stage):\n    \"\"\"Add an error to the Hall of Shame.\"\"\"\n    TRAINING_ERRORS.append({\n        'time': time.strftime(\"%H:%M:%S\"),\n        'target': target,\n        'predicted': predicted,\n        'loss': float(loss),\n        'stage': stage\n    })\n\ndef get_training_errors():\n    \"\"\"Get list of errors for the UI.\"\"\"\n    return list(TRAINING_ERRORS)\n\nMODEL_PRESETS = {\n    'lite': {'d_model': 128, 'nhead': 4, 'num_encoder_layers': 3, 'num_decoder_layers': 3},\n    'pro': {'d_model': 256, 'nhead': 8, 'num_encoder_layers': 6, 'num_decoder_layers': 6}\n}\nCURRENT_PRESET = 'lite'\n\ndef get_device(force_cpu=False):\n    \"\"\"Get the best available device (CUDA > MPS > CPU).\"\"\"\n    if force_cpu:\n        return torch.device(\"cpu\")\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    return torch.device(\"cpu\")\n\ndef set_device(use_gpu=True):\n    \"\"\"Set the device (GPU or CPU).\"\"\"\n    global DEVICE, MODEL\n    new_device = get_device(force_cpu=not use_gpu)\n    \n    if MODEL is not None and DEVICE != new_device:\n        MODEL = MODEL.to(new_device)\n    \n    DEVICE = new_device\n    return get_device_info()\n\ndef get_device_info():\n    \"\"\"Get device info string.\"\"\"\n    global DEVICE\n    if DEVICE is None:\n        DEVICE = get_device()\n    \n    if DEVICE.type == \"cuda\":\n        return f\"CUDA ({torch.cuda.get_device_name(0)})\"\n    elif DEVICE.type == \"mps\":\n        return \"MPS (Apple Silicon)\"\n    else:\n        return \"CPU\"\n\ndef load_model(force_reload=False, preset_name=None):\n    \"\"\"Load or reload the model.\"\"\"\n    global MODEL, DEVICE, CURRENT_PRESET\n    \n    if preset_name:\n        CURRENT_PRESET = preset_name\n    \n    if DEVICE is None:\n        DEVICE = get_device()\n    \n    VOCAB_SIZE = len(VOCABULARY)\n    config = MODEL_PRESETS[CURRENT_PRESET]\n    \n    print(f\"Loading Model [{CURRENT_PRESET.upper()}]...\")\n    MODEL = AlphaSymbolicModel(\n        vocab_size=VOCAB_SIZE + 1, \n        d_model=config['d_model'], \n        nhead=config['nhead'],\n        num_encoder_layers=config['num_encoder_layers'], \n        num_decoder_layers=config['num_decoder_layers']\n    ).to(DEVICE)\n    \n    filename = f\"alpha_symbolic_model_{CURRENT_PRESET}.pth\"\n    status = f\"Nuevo modelo ({CURRENT_PRESET})\" # Default status\n    \n    if os.path.exists(filename):\n        try:\n            state_dict = torch.load(filename, map_location=DEVICE, weights_only=True)\n            \n            # Check for NaNs\n            has_nans = False\n            for k, v in state_dict.items():\n                if torch.isnan(v).any() or torch.isinf(v).any():\n                    has_nans = True\n                    break\n            \n            if has_nans:\n                print(f\"\u26a0\ufe0f Modelo corrupto detectado (NaNs) en {filename}. Eliminando y esperando reinicio.\")\n                try:\n                    os.remove(filename)\n                    print(\"\u2705 Archivo corrupto eliminado.\")\n                except OSError as e:\n                    print(f\"Error al eliminar archivo: {e}\")\n                status = \"\u26a0\ufe0f Modelo corrupto eliminado y reiniciado\"\n            else:\n                MODEL.load_state_dict(state_dict)\n                MODEL.eval()\n                status = f\"Modelo cargado ({CURRENT_PRESET})\"\n                \n        except RuntimeError as e:\n            print(f\"\u26a0\ufe0f Error de compatibilidad ({e}). Iniciando modelo fresco.\")\n            status = f\"Nuevo modelo ({CURRENT_PRESET})\"\n        except Exception as e:\n            print(f\"Error cargando: {e}\")\n            status = \"Sin modelo pre-entrenado\"\n    \n    return status, get_device_info()\n\ndef get_model():\n    \"\"\"Get the current model, loading if needed.\"\"\"\n    global MODEL, DEVICE\n    if MODEL is None:\n        load_model()\n    return MODEL, DEVICE\n\ndef save_model():\n    \"\"\"Save the current model.\"\"\"\n    global MODEL, CURRENT_PRESET\n    if MODEL is not None:\n        filename = f\"alpha_symbolic_model_{CURRENT_PRESET}.pth\"\n        torch.save(MODEL.state_dict(), filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/ui/app_search.py\n",
        "\"\"\"\nSearch/Solve functions for AlphaSymbolic Gradio App.\nSupports both Beam Search and MCTS.\n\"\"\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport gradio as gr\n\nfrom core.grammar import ExpressionTree\nfrom search.beam_search import BeamSearch\nfrom search.mcts import MCTS\nfrom search.hybrid_search import hybrid_solve\nfrom utils.simplify import simplify_tree\nfrom search.pareto import ParetoFront\nfrom utils.detect_pattern import detect_pattern\nfrom utils.optimize_constants import optimize_constants, substitute_constants\nfrom ui.app_core import get_model\n\n\ndef parse_data(x_str, y_str):\n    \"\"\"Parse comma-separated input strings.\"\"\"\n    try:\n        x = np.array([float(v.strip()) for v in x_str.split(',')], dtype=np.float64)\n        y = np.array([float(v.strip()) for v in y_str.split(',')], dtype=np.float64)\n        if len(x) != len(y):\n            return None, None, \"Error: X e Y deben tener igual longitud\"\n        return x, y, None\n    except Exception as e:\n        return None, None, f\"Error: {str(e)}\"\n\n\ndef create_fit_plot(x, y, y_pred, formula):\n    \"\"\"Create a plot showing data vs prediction.\"\"\"\n    fig, ax = plt.subplots(figsize=(8, 5), facecolor='#1a1a2e')\n    ax.set_facecolor('#1a1a2e')\n    \n    ax.scatter(x, y, color='#00d4ff', s=100, label='Datos Reales', zorder=3, edgecolors='white', linewidth=1)\n    \n    sort_idx = np.argsort(x)\n    ax.plot(x[sort_idx], y_pred[sort_idx], color='#ff6b6b', linewidth=3, label='Prediccion', zorder=2)\n    \n    ax.set_xlabel('X', color='white', fontsize=12)\n    ax.set_ylabel('Y', color='white', fontsize=12)\n    ax.set_title('Ajuste de la Formula', color='white', fontsize=14, fontweight='bold')\n    ax.legend(facecolor='#16213e', edgecolor='#00d4ff', labelcolor='white')\n    ax.tick_params(colors='white')\n    ax.grid(True, alpha=0.2, color='white')\n    \n    for spine in ax.spines.values():\n        spine.set_color('#00d4ff')\n    \n    plt.tight_layout()\n    return fig\n\n\ndef solve_formula(x_str, y_str, beam_width, search_method, progress=gr.Progress()):\n    \"\"\"Main solving function with search method selection.\"\"\"\n    x, y, error = parse_data(x_str, y_str)\n    if error:\n        return error, None, \"\", \"\", \"\"\n    \n    MODEL, DEVICE = get_model()\n    \n    progress(0.1, desc=f\"Analizando patron... [{DEVICE.type.upper()}]\")\n    pattern = detect_pattern(x, y)\n    \n    progress(0.3, desc=f\"Buscando formulas ({search_method})... [{DEVICE.type.upper()}]\")\n    start_time = time.time()\n    \n    results = []\n    \n    if search_method == \"Alpha-GP Hybrid\":\n        # Using hybrid search\n        progress(0.4, desc=\"Fase 1: Neural Beam Search...\")\n        # Note: Hybrid search handles its own phases printing, but we want UI updates.\n        # We pass beam_width. gp_timeout is increased to 30s to allow convergence on complex problems.\n        hybrid_res = hybrid_solve(x, y, MODEL, DEVICE, beam_width=int(beam_width), gp_timeout=30)\n        \n        if hybrid_res:\n            progress(0.9, desc=\"Procesando resultados GP...\")\n            # Convert infix string back to tokens for consistency\n            tree = ExpressionTree.from_infix(hybrid_res['formula'])\n            if tree.is_valid:\n                 # Evaluate RMSE roughly (GP result should be good, but let's confirm)\n                 # Optimization is already done by GP, but we might want to fine-tune \n                 # or at least extract constants if they are numbers in the string.\n                 # The string from GP has numbers like 2.345 embedded.\n                 # optimize_constants expects a tree with 'C' placeholders if we want to re-optimize.\n                 # But GP output is fully instantiated.\n                 # So we just evaluate.\n                 \n                 y_pred_check = tree.evaluate(x)\n                 rmse_check = np.sqrt(np.mean((y_pred_check - y)**2))\n                 \n                 results = [{\n                     'tokens': tree.tokens,\n                     'formula': tree.get_infix(),\n                     'rmse': rmse_check,\n                     'constants': {} # Constants are baked into the formula string\n                 }]\n    \n    elif search_method == \"Beam Search\":\n        searcher = BeamSearch(MODEL, DEVICE, beam_width=int(beam_width), max_length=25)\n        results = searcher.search(x, y)\n    else:  # MCTS\n        mcts = MCTS(MODEL, DEVICE, max_simulations=int(beam_width) * 10)\n        result = mcts.search(x, y)\n        if result and result.get('tokens'):\n            tokens = result['tokens']\n            tree = ExpressionTree(tokens)\n            if tree.is_valid:\n                constants, rmse = optimize_constants(tree, x, y)\n                results = [{\n                    'tokens': tokens,\n                    'formula': tree.get_infix(),\n                    'rmse': rmse,\n                    'constants': constants\n                }]\n    \n    search_time = time.time() - start_time\n    \n    if not results:\n        return \"No se encontraron formulas validas\", None, \"\", \"\", \"\"\n    \n    progress(0.7, desc=\"Optimizando constantes...\")\n    pareto = ParetoFront()\n    pareto.add_from_results(results)\n    best = pareto.get_best_by_rmse()\n    \n    if not best:\n        return \"Error en optimizacion\", None, \"\", \"\", \"\"\n    \n    progress(0.9, desc=\"Simplificando...\")\n    tree = ExpressionTree(best.tokens)\n    simplified = simplify_tree(tree)\n    y_pred = tree.evaluate(x, constants=best.constants)\n    \n    # Substitute constants for display\n    substituted_formula = simplified\n    if best.constants:\n        try:\n            positions = tree.root.get_constant_positions()\n            # We use the raw infix for substitution to ensure matching C positions\n            raw_infix = tree.get_infix()\n            substituted_formula = substitute_constants(raw_infix, best.constants, positions)\n        except:\n            substituted_formula = simplified\n    \n    fig = create_fit_plot(x, y, y_pred, simplified)\n    \n    # Format results\n    result_html = f\"\"\"\n    <div style=\"background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); padding: 20px; border-radius: 15px; border: 2px solid #00d4ff;\">\n        <h2 style=\"color: #00d4ff; margin: 0; font-size: 24px;\">Formula Encontrada</h2>\n        <div style=\"background: #0f0f23; padding: 15px; border-radius: 10px; margin: 15px 0; border-left: 4px solid #ff6b6b;\">\n            <code style=\"color: #ff6b6b; font-size: 28px; font-weight: bold;\">{substituted_formula}</code>\n        </div>\n        <div style=\"display: grid; grid-template-columns: repeat(4, 1fr); gap: 10px;\">\n            <div style=\"background: #0f0f23; padding: 10px; border-radius: 8px; text-align: center;\">\n                <span style=\"color: #888;\">RMSE</span><br>\n                <span style=\"color: #00d4ff; font-size: 16px; font-weight: bold;\">{best.rmse:.6f}</span>\n            </div>\n            <div style=\"background: #0f0f23; padding: 10px; border-radius: 8px; text-align: center;\">\n                <span style=\"color: #888;\">Nodos</span><br>\n                <span style=\"color: #00d4ff; font-size: 16px; font-weight: bold;\">{best.complexity}</span>\n            </div>\n            <div style=\"background: #0f0f23; padding: 10px; border-radius: 8px; text-align: center;\">\n                <span style=\"color: #888;\">Tiempo</span><br>\n                <span style=\"color: #00d4ff; font-size: 16px; font-weight: bold;\">{search_time:.2f}s</span>\n            </div>\n            <div style=\"background: #0f0f23; padding: 10px; border-radius: 8px; text-align: center;\">\n                <span style=\"color: #888;\">Metodo</span><br>\n                <span style=\"color: #4ade80; font-size: 16px; font-weight: bold;\">{search_method}</span>\n            </div>\n        </div>\n        <div style=\"margin-top: 15px; padding: 10px; background: #0f0f23; border-radius: 8px;\">\n            <span style=\"color: #888;\">Patron:</span> \n            <span style=\"color: #ffd93d;\">{pattern['type']}</span> \n            <span style=\"color: #666;\">({pattern['confidence']:.0%})</span>\n            <span style=\"color: #888; margin-left: 20px;\">Device:</span>\n            <span style=\"color: #4ade80;\">{DEVICE.type.upper()}</span>\n        </div>\n    \"\"\"\n    \n    # Add constants if any\n    # Add constants if any\n    if best.constants:\n        # Sort and format cleanly\n        sorted_items = sorted(best.constants.items(), key=lambda x: str(x[0]))\n        clean_consts = []\n        for i, (k, v) in enumerate(sorted_items):\n            clean_consts.append(f\"C_{i+1}: {v:.4f}\")\n        const_str = \"  |  \".join(clean_consts)\n        \n        result_html += f\"\"\"\n        <div style=\"margin-top: 10px; padding: 10px; background: #0f0f23; border-radius: 8px; border-left: 3px solid #ffd93d;\">\n            <span style=\"color: #888;\">Constantes:</span>\n            <span style=\"color: #fff; font-family: monospace; margin-left: 10px;\">{const_str}</span>\n        </div>\n        \"\"\"\n        \n    result_html += \"</div>\"\n    \n    # Predictions table\n    pred_html = '<table style=\"width: 100%; border-collapse: collapse; background: #1a1a2e; border-radius: 10px; overflow: hidden;\">'\n    pred_html += '<tr style=\"background: #16213e;\"><th style=\"padding: 10px; color: #00d4ff;\">X</th><th style=\"color: #00d4ff;\">Pred</th><th style=\"color: #00d4ff;\">Real</th><th style=\"color: #00d4ff;\">Delta</th></tr>'\n    for i in range(min(50, len(x))):\n        delta = abs(y_pred[i] - y[i])\n        color = \"#4ade80\" if delta < 0.1 else \"#fbbf24\" if delta < 1 else \"#ef4444\"\n        pred_html += f'<tr style=\"border-bottom: 1px solid #333;\"><td style=\"padding: 8px; color: white; text-align: center;\">{x[i]:.2f}</td><td style=\"color: white; text-align: center;\">{y_pred[i]:.4f}</td><td style=\"color: white; text-align: center;\">{y[i]:.4f}</td><td style=\"color: {color}; text-align: center; font-weight: bold;\">{delta:.4f}</td></tr>'\n    pred_html += '</table>'\n    \n    # Alternatives\n    alt_html = '<div style=\"background: #1a1a2e; padding: 15px; border-radius: 10px;\">'\n    alt_html += '<h4 style=\"color: #00d4ff; margin-top: 0;\">Alternativas</h4>'\n    for i, sol in enumerate(pareto.solutions[:4]):\n        alt_html += f'<div style=\"padding: 5px 10px; margin: 5px 0; background: #0f0f23; border-radius: 5px; border-left: 3px solid {\"#00d4ff\" if i == 0 else \"#666\"};\"><code style=\"color: {\"#ff6b6b\" if i == 0 else \"#888\"};\">{sol.formula}</code> <span style=\"color: #666; font-size: 12px;\">RMSE: {sol.rmse:.4f}</span></div>'\n    alt_html += '</div>'\n    \n    return result_html, fig, pred_html, alt_html, simplified\n\n\ndef generate_example(tipo):\n    \"\"\"Generate example data.\"\"\"\n    if tipo == \"lineal\":\n        x = np.linspace(1, 10, 10)\n        y = 2 * x + 3\n    elif tipo == \"cuadratico\":\n        x = np.linspace(-5, 5, 11)\n        y = x**2 + 1\n    elif tipo == \"trig\":\n        x = np.linspace(0, 6.28, 20)\n        y = np.sin(x)\n    elif tipo == \"exp\":\n        x = np.linspace(0, 5, 15)\n        y = 2 * np.exp(0.5 * x)\n    else:\n        x = np.linspace(1, 10, 10)\n        y = 2 * x + 3\n    \n    return \", \".join([f\"{v:.2f}\" for v in x]), \", \".join([f\"{v:.4f}\" for v in y])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/ui/app_training.py\n",
        "\"\"\"\nTraining functions for AlphaSymbolic Gradio App.\nWith proper data normalization.\n\"\"\"\nimport os\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gradio as gr\nfrom collections import deque\nimport random\nimport time\n\nfrom core.grammar import VOCABULARY, TOKEN_TO_ID, OPERATORS, OPERATOR_STAGES\nfrom data.synthetic_data import DataGenerator\nfrom ui.app_core import get_model, save_model, TRAINING_STATUS, add_training_error, should_stop_training, reset_stop_flag\nfrom core.loss import QuantileLoss\nfrom search.hybrid_search import hybrid_solve\nfrom core.grammar import ExpressionTree, simplify_formula\n\n\ndef get_allowed_token_mask(stage, vocab_size, device):\n    \"\"\"\n    Creates a mask tensor for token logits.\n    Allowed tokens = 1.0, Disallowed = 0.0 (for multiplication mask)\n    Or returns indices of allowed tokens for -inf masking.\n    \"\"\"\n    allowed_ops = OPERATOR_STAGES.get(stage, list(OPERATORS.keys()))\n    \n    # All terminals are always allowed\n    allowed_tokens = set(['x', 'C', '0', '1', '2', '3', '5', '10', 'pi', 'e'])\n    allowed_tokens.update(allowed_ops)\n    \n    # Build mask\n    mask = torch.zeros(vocab_size + 1, device=device)  # +1 for SOS token\n    for token in allowed_tokens:\n        if token in TOKEN_TO_ID:\n            mask[TOKEN_TO_ID[token]] = 1.0\n    mask[vocab_size] = 1.0  # SOS always allowed\n    \n    return mask\n\n\ndef normalize_batch(x_list, y_list):\n    \"\"\"Normalize X and Y values to prevent numerical instability.\"\"\"\n    normalized_x = []\n    normalized_y = []\n    \n    for x, y in zip(x_list, y_list):\n        # Normalize X to [-1, 1]\n        x_min, x_max = x.min(), x.max()\n        if x_max - x_min > 1e-6:\n            x_norm = 2 * (x - x_min) / (x_max - x_min) - 1\n        else:\n            x_norm = np.zeros_like(x)\n        \n        # Normalize Y to [-1, 1] \n        y_min, y_max = y.min(), y.max()\n        if y_max - y_min > 1e-6:\n            y_norm = 2 * (y - y_min) / (y_max - y_min) - 1\n        else:\n            y_norm = np.zeros_like(y)\n        \n        normalized_x.append(x_norm)\n        normalized_y.append(y_norm)\n    \n    return normalized_x, normalized_y\n\n\ndef train_basic(epochs, batch_size, point_count=10, progress=gr.Progress()):\n    \"\"\"Basic training with synthetic data.\"\"\"\n    global TRAINING_STATUS\n    \n    if TRAINING_STATUS[\"running\"]:\n        return \"Entrenamiento ya en progreso\", None\n    \n    TRAINING_STATUS[\"running\"] = True\n    \n    try:\n        MODEL, DEVICE = get_model()\n        \n        MODEL.train()\n        optimizer = torch.optim.AdamW(MODEL.parameters(), lr=1e-4, weight_decay=0.01)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(epochs), eta_min=1e-6)\n        ce_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n        \n        VOCAB_SIZE = len(VOCABULARY)\n        SOS_ID = VOCAB_SIZE\n        \n        data_gen = DataGenerator(max_depth=4)\n        losses = []\n        \n        for epoch in range(int(epochs)):\n            progress((epoch + 1) / epochs, desc=f\"Epoca {epoch+1}/{int(epochs)} [{DEVICE.type.upper()}]\")\n            \n            # Mix of inverse (known formulas) + random data (AlphaTensor-style)\n            half_batch = int(batch_size) // 2\n            batch_inverse = data_gen.generate_inverse_batch(half_batch, point_count=int(point_count))\n            batch_random = data_gen.generate_batch(int(batch_size) - half_batch, point_count=int(point_count))\n            batch = batch_inverse + batch_random\n            if len(batch) < 2:\n                continue\n            \n            x_list = [d['x'] for d in batch]\n            y_list = [d['y'] for d in batch]\n            \n            # Normalize data\n            x_list, y_list = normalize_batch(x_list, y_list)\n            \n            token_lists = [[TOKEN_TO_ID[t] for t in d['tokens']] for d in batch]\n            \n            max_len = max(len(s) for s in token_lists)\n            decoder_input = torch.full((len(batch), max_len + 1), SOS_ID, dtype=torch.long)\n            targets = torch.full((len(batch), max_len + 1), -1, dtype=torch.long)\n            \n            for i, seq in enumerate(token_lists):\n                decoder_input[i, 1:len(seq)+1] = torch.tensor(seq, dtype=torch.long)\n                targets[i, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n            \n            x_tensor = torch.tensor(np.array(x_list), dtype=torch.float32).to(DEVICE)\n            y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n            decoder_input = decoder_input.to(DEVICE)\n            targets = targets.to(DEVICE)\n            \n            # Forward\n            optimizer.zero_grad()\n            logits, _ = MODEL(x_tensor, y_tensor, decoder_input)\n            loss = ce_loss(logits.view(-1, VOCAB_SIZE + 1), targets.view(-1))\n            \n            # Skip if loss is NaN\n            if torch.isnan(loss) or torch.isinf(loss):\n                continue\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(MODEL.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            \n            losses.append(loss.item())\n        \n        save_model()\n        MODEL.eval()\n        TRAINING_STATUS[\"running\"] = False\n        \n        if not losses:\n            return \"Error: No se pudo calcular loss (revisar datos)\", None\n        \n        fig = create_loss_plot(losses, \"Entrenamiento Basico\")\n        \n        result = f\"\"\"\n        <div style=\"background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); padding: 20px; border-radius: 15px; border: 2px solid #4ade80;\">\n            <h2 style=\"color: #4ade80; margin: 0;\">Entrenamiento Completado</h2>\n            <p style=\"color: white;\">Epocas: {int(epochs)} | Loss Final: {losses[-1]:.4f}</p>\n            <p style=\"color: #00d4ff;\">Dispositivo: {DEVICE.type.upper()}</p>\n        </div>\n        \"\"\"\n        return result, fig\n        \n    except Exception as e:\n        TRAINING_STATUS[\"running\"] = False\n        return f\"Error: {str(e)}\", None\n\n\ndef train_curriculum(epochs, batch_size, point_count=10, progress=gr.Progress()):\n    \"\"\"Curriculum Learning - starts simple, increases difficulty gradually.\"\"\"\n    global TRAINING_STATUS\n    \n    if TRAINING_STATUS[\"running\"]:\n        return \"Entrenamiento ya en progreso\", None\n    \n    TRAINING_STATUS[\"running\"] = True\n    \n    try:\n        MODEL, DEVICE = get_model()\n        \n        MODEL.train()\n        optimizer = torch.optim.AdamW(MODEL.parameters(), lr=5e-5, weight_decay=0.01)  # Lower LR\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2)\n        ce_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n        \n        VOCAB_SIZE = len(VOCABULARY)\n        SOS_ID = VOCAB_SIZE\n        losses = []\n        \n        for epoch in range(int(epochs)):\n            # Curriculum: slow progression\n            # Stage 1 (0-50%): depth 2-3, 80% inverse data\n            # Stage 2 (50-80%): depth 3-4, 50% inverse data  \n            # Stage 3 (80-100%): depth 4-5, 20% inverse data\n            progress_pct = epoch / epochs\n            \n            if progress_pct < 0.5:\n                current_depth = 2 + int(progress_pct * 2)  # 2-3\n                inverse_ratio = 0.8\n            elif progress_pct < 0.8:\n                current_depth = 3 + int((progress_pct - 0.5) * 3.3)  # 3-4\n                inverse_ratio = 0.5\n            else:\n                current_depth = 4 + int((progress_pct - 0.8) * 5)  # 4-5\n                inverse_ratio = 0.2\n            \n            progress((epoch + 1) / epochs, desc=f\"Epoca {epoch+1}/{int(epochs)} (prof: {current_depth}, inv: {inverse_ratio:.0%}) [{DEVICE.type.upper()}]\")\n            \n            data_gen = DataGenerator(max_depth=current_depth)\n            \n            # Mix inverse + random based on curriculum stage\n            n_inverse = int(batch_size * inverse_ratio)\n            n_random = int(batch_size) - n_inverse\n            \n            batch_inverse = data_gen.generate_inverse_batch(max(1, n_inverse), point_count=int(point_count)) if n_inverse > 0 else []\n            batch_random = data_gen.generate_batch(max(1, n_random), point_count=int(point_count)) if n_random > 0 else []\n            batch = batch_inverse + batch_random\n            if len(batch) < 2:\n                continue\n            \n            x_list = [d['x'] for d in batch]\n            y_list = [d['y'] for d in batch]\n            x_list, y_list = normalize_batch(x_list, y_list)\n            \n            token_lists = [[TOKEN_TO_ID[t] for t in d['tokens']] for d in batch]\n            \n            max_len = max(len(s) for s in token_lists)\n            decoder_input = torch.full((len(batch), max_len + 1), SOS_ID, dtype=torch.long)\n            targets = torch.full((len(batch), max_len + 1), -1, dtype=torch.long)\n            \n            for i, seq in enumerate(token_lists):\n                decoder_input[i, 1:len(seq)+1] = torch.tensor(seq, dtype=torch.long)\n                targets[i, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n            \n            x_tensor = torch.tensor(np.array(x_list), dtype=torch.float32).to(DEVICE)\n            y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n            decoder_input = decoder_input.to(DEVICE)\n            targets = targets.to(DEVICE)\n            \n            optimizer.zero_grad()\n            logits, value_pred = MODEL(x_tensor, y_tensor, decoder_input)\n            \n            # Policy Loss\n            loss_policy = ce_loss(logits.view(-1, VOCAB_SIZE + 1), targets.view(-1))\n            \n            # Value Loss\n            # For supervised learning, these are \"perfect\" solutions, so Value Target = 1.0\n            value_targets = torch.ones_like(value_pred)\n            loss_value = torch.nn.functional.mse_loss(value_pred, value_targets)\n            \n            # Combined Loss\n            loss = loss_policy + 0.5 * loss_value\n            \n            if torch.isnan(loss) or torch.isinf(loss):\n                continue\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(MODEL.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            \n            losses.append(loss.item())\n        \n        save_model()\n        MODEL.eval()\n        TRAINING_STATUS[\"running\"] = False\n        \n        if not losses:\n            return \"Error: No se pudo calcular loss\", None\n        \n        fig = create_loss_plot(losses, \"Curriculum Learning\")\n        \n        result = f\"\"\"\n        <div style=\"background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); padding: 20px; border-radius: 15px; border: 2px solid #00d4ff;\">\n            <h2 style=\"color: #00d4ff; margin: 0;\">Curriculum Learning Completado</h2>\n            <p style=\"color: white;\">Epocas: {int(epochs)} | Loss Final: {losses[-1]:.4f}</p>\n            <p style=\"color: #888;\">Profundidad maxima: 6 | Dispositivo: {DEVICE.type.upper()}</p>\n        </div>\n        \"\"\"\n        return result, fig\n        \n    except Exception as e:\n        TRAINING_STATUS[\"running\"] = False\n        return f\"Error: {str(e)}\", None\n\n\ndef train_self_play(iterations, problems_per_iter, point_count=10, progress=gr.Progress()):\n    \"\"\"AlphaZero Self-Play loop.\"\"\"\n    global TRAINING_STATUS\n    \n    if TRAINING_STATUS[\"running\"]:\n        return \"Entrenamiento ya en progreso\", None\n    \n    TRAINING_STATUS[\"running\"] = True\n    reset_stop_flag()  # Reset stop flag at start\n    \n    try:\n        MODEL, DEVICE = get_model()\n        \n        from search.mcts import MCTS\n        \n        optimizer = torch.optim.AdamW(MODEL.parameters(), lr=5e-5, weight_decay=0.01)\n        # Scheduler: Reduce LR when plateauing to help convergence\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15, min_lr=1e-6)\n        \n        # Losses for AlphaZero\n        # Policy: KLDiv (comparing distributions)\n        # Value: Quantile Loss (3 Quantiles)\n        kl_loss = torch.nn.KLDivLoss(reduction='batchmean')\n        quantile_loss_fn = QuantileLoss()\n        \n        VOCAB_SIZE = len(VOCABULARY)\n        SOS_ID = VOCAB_SIZE\n        \n        replay_buffer = deque(maxlen=20000)\n        \n        # Adaptive Curriculum State\n        current_depth = 2\n        data_gen = DataGenerator(max_depth=current_depth)\n        \n        # MCTS for A100: Increase batch size and simulations significantly\n        # Adjusted for RTX 3050/i5: Batch 64 is smoother (less CPU wait)\n        searcher = MCTS(MODEL, DEVICE, max_simulations=500, complexity_lambda=0.1, batch_size=64)\n        \n        rmses = []\n        losses = []\n        best_avg_rmse = float('inf')\n        \n        start_time = time.time()\n        \n        for iteration in range(int(iterations)):\n            # Check for stop request\n            if should_stop_training():\n                print(\"\u23f9\ufe0f Training stopped by user\")\n                break\n            # ETA Calculation\n            elapsed = time.time() - start_time\n            if iteration > 0:\n                avg_time_per_iter = elapsed / iteration\n                remaining_iters = int(iterations) - iteration\n                eta_seconds = remaining_iters * avg_time_per_iter\n                \n                # Format ETA\n                if eta_seconds > 3600:\n                    eta_str = f\"{eta_seconds/3600:.1f}h\"\n                elif eta_seconds > 60:\n                    eta_str = f\"{eta_seconds/60:.0f}m\"\n                else:\n                    eta_str = f\"{eta_seconds:.0f}s\"\n            else:\n                eta_str = \"Calculando...\"\n\n            # Adaptive Curriculum Check\n            # Stages: 0=Arithmetic, 1=Poly, 2=Trig, 3=Adv, 4=Complex\n            CURRICULUM_LEVELS = [\n                {'depth': 1, 'ops': ['+', '-', '*', '/']},\n                {'depth': 2, 'ops': ['+', '-', '*', '/']},\n                {'depth': 3, 'ops': ['+', '-', '*', '/', 'pow', 'sqrt']},\n                {'depth': 4, 'ops': ['+', '-', '*', '/', 'pow', 'sqrt', 'sin', 'cos']},\n                {'depth': 5, 'ops': None} # All\n            ]\n            \n            # Initialize state if not present\n            if 'curriculum_stage' not in locals():\n                curriculum_stage = 0\n            \n            recent_rmse = np.mean(rmses[-20:]) if len(rmses) >= 20 else 1.0\n            \n            # Graduation condition: RMSE < 0.1 stable\n            if len(rmses) > 20 and recent_rmse < 0.1 and curriculum_stage < len(CURRICULUM_LEVELS) - 1:\n                curriculum_stage += 1\n                stage_info = CURRICULUM_LEVELS[curriculum_stage]\n                data_gen = DataGenerator(max_depth=stage_info['depth'], allowed_operators=stage_info['ops'])\n                print(f\"*** Curriculum Level Up! Stage {curriculum_stage} ({stage_info['depth']}, {stage_info['ops']}) ***\")\n                # Clear buffer to avoid training on old easy data? Maybe keep some for replay.\n            \n            # Ensure data_gen is initialized at start\n            if iteration == 0:\n                 stage_info = CURRICULUM_LEVELS[0]\n                 data_gen = DataGenerator(max_depth=stage_info['depth'], allowed_operators=stage_info['ops'])\n\n            stage_name = [\"Arithmetic\", \"Polynomials\", \"Trigonometry\", \"Advanced\", \"Complex\"][curriculum_stage]\n            \n            # Safe access to current_lr\n            curr_lr_disp = optimizer.param_groups[0]['lr']\n            msg = f\"Iter {iteration+1}/{int(iterations)} [{stage_name}] RMSE:{recent_rmse:.3f} LR:{curr_lr_disp:.1e} | ETA: {eta_str}\"\n            progress((iteration + 1) / iterations, desc=msg)\n            \n            # Active Learning / Hard Mining Phase\n            MODEL.eval()\n            \n            # Generate a large pool of candidates candidates to find the \"hard\" ones\n            pool_size = problems_per_iter * 3  # Generate 3x more than we need\n            candidates = data_gen.generate_inverse_batch(pool_size, point_count=int(point_count))\n            \n            if not candidates:\n                continue\n                \n            # Quick forward pass to estimate difficulty (Loss)\n            # We want to train on problems where the model currently FAILS (High Loss)\n            hard_problems = []\n            \n            with torch.no_grad():\n                # Process in chunks to avoid OOM\n                chunk_size = 32\n                for i in range(0, len(candidates), chunk_size):\n                    chunk = candidates[i:i+chunk_size]\n                    \n                    x_list = [d['x'] for d in chunk]\n                    y_list = [d['y'] for d in chunk]\n                    x_list, y_list = normalize_batch(x_list, y_list)\n                    \n                    token_lists = [[TOKEN_TO_ID.get(t, TOKEN_TO_ID['C']) for t in d['tokens']] for d in chunk]\n                    max_len = max(len(s) for s in token_lists)\n                    \n                    # Prepare tensors\n                    dec_in = torch.full((len(chunk), max_len + 1), SOS_ID, dtype=torch.long).to(DEVICE)\n                    targets = torch.full((len(chunk), max_len + 1), -1, dtype=torch.long).to(DEVICE)\n                    \n                    for j, seq in enumerate(token_lists):\n                        dec_in[j, 1:len(seq)+1] = torch.tensor(seq, dtype=torch.long)\n                        targets[j, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n                        \n                    x_tensor = torch.tensor(np.array(x_list), dtype=torch.float32).to(DEVICE)\n                    y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n                    \n                    logits, _ = MODEL(x_tensor, y_tensor, dec_in)\n                    \n                    # Calculate loss per item\n                    # CrossEntropy usually aggregates, so we use reduction='none'\n                    loss_f = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='none')\n                    raw_losses = loss_f(logits.view(-1, VOCAB_SIZE + 1), targets.view(-1))\n                    \n                    # Reshape back to [Batch, Seq] to sum/mean per sample\n                    raw_losses = raw_losses.view(len(chunk), -1)\n                    # Average loss per non-padded token\n                    mask = (targets != -1)\n                    sample_losses = (raw_losses * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n                    \n                    for j, loss_val in enumerate(sample_losses):\n                        # Store (Loss, Problem)\n                        hard_problems.append((loss_val.item(), chunk[j]))\n            \n            # Sort by difficulty (Loss descending)\n            hard_problems.sort(key=lambda x: x[0], reverse=True)\n            \n            # Stabilization: Mix Hardest (70%) + Random Examples (30%)\n            # This prevents \"Catastrophic Forgetting\" of simpler patterns\n            n_hard = int(problems_per_iter * 0.7)\n            n_random = int(problems_per_iter) - n_hard\n            \n            # Top K hardest\n            selected_hard = [p[1] for p in hard_problems[:n_hard]]\n            \n            # Random selection from the rest of the pool (to keep variety)\n            remaining_pool = [p[1] for p in hard_problems[n_hard:]]\n            selected_random = random.sample(remaining_pool, min(n_random, len(remaining_pool))) if remaining_pool else []\n            \n            selected_problems = selected_hard + selected_random\n            \n            avg_pool_loss = np.mean([p[0] for p in hard_problems])\n            top_loss = np.mean([p[0] for p in hard_problems[:n_hard]]) if n_hard > 0 else 0\n            \n            print(f\"Active Learning: Pool Loss {avg_pool_loss:.3f} -> Selected Mix (Hard:{top_loss:.3f})\")\n\n            # --- HALL OF SHAME CAPTURE ---\n            # Capture what the model predicts for the top 3 hardest failures\n            try:\n                top_failures = hard_problems[:3]\n                x_fail = [p[1]['x'].astype(np.float64) for p in top_failures]\n                y_fail = [p[1]['y'].astype(np.float64) for p in top_failures]\n                target_formulas = [p[1]['infix'] for p in top_failures]\n                fail_losses = [p[0] for p in top_failures]\n                \n                # Simple Greedy Decode to see what it predicts\n                from search.beam_search import BeamSearch\n                # Use beam search with width 1 (Greedy) for speed, with curriculum mask\n                bs = BeamSearch(MODEL, DEVICE, beam_width=1, max_length=20, curriculum_stage=curriculum_stage)\n                \n                for i in range(len(top_failures)):\n                    try:\n                        # Decode\n                        # Enable return_partial to see what the model is thinking if it fails\n                        res = bs.search(x_fail[i], y_fail[i], return_partial=True)\n                        if not res:\n                            pred_formula = \"Search Empty (No Tokens)\"\n                        else:\n                            pred_formula = res[0]['formula']\n                            \n                        # Detect Looping (e.g. \"10 / / / / / /\")\n                        # Basic heuristic: check if last 10 chars contain > 80% same char or repeating pattern\n                        if len(pred_formula) > 20:\n                            # Check for repeating slashes or other single chars\n                            if pred_formula.count('/') > 10 and pred_formula.endswith('/ .'): \n                                 pred_formula = pred_formula[:20] + \" ... [Loop Detected]\"\n                            elif \" / / / \" in pred_formula:\n                                 pred_formula = pred_formula.split(\" / / / \")[0] + \" ... [Loop Detected]\"\n                        \n                        add_training_error(\n                            target=target_formulas[i],\n                            predicted=pred_formula,\n                            loss=fail_losses[i],\n                            stage=stage_name\n                        )\n                    except Exception as e:\n                        print(f\"HoS Inner Error: {e}\")\n                        add_training_error(\n                            target=target_formulas[i],\n                            predicted=f\"CRASH: {str(e)[:20]}\",\n                            loss=fail_losses[i],\n                            stage=stage_name\n                        )\n            except Exception as e:\n                import traceback\n                print(f\"HoS Outer Error: {e}\")\n                traceback.print_exc()\n\n            # --- MCTS SOLVE ---\n            for prob in selected_problems:\n                x_data = prob['x'].astype(np.float64)\n                y_data = prob['y'].astype(np.float64)\n                \n                try:\n                    # Use MCTS to find the solution (or improve upon it)\n                    # For inverse problems, we KNOW the solution, but MCTS helps explore variations\n                    # and generates the policy distribution we want to learn.\n                    result = searcher.search(x_data, y_data)\n                    \n                    # 1. Store Training Examples\n                    if 'root' in result:\n                        examples = searcher.get_training_examples(result['root'])\n                        for (tokens, policy, value) in examples:\n                            replay_buffer.append({\n                                'x': x_data, 'y': y_data,\n                                'tokens': tokens,\n                                'policy': policy,\n                                'value': value\n                            })\n                    \n                    # 2. Track Metrics\n                    if result.get('tokens'):\n                        rmses.append(result['rmse'])\n                        \n                except Exception as e:\n                    print(f\"Self-play error: {e}\")\n                    continue\n            \n            # Training phase\n            # To saturate GPU: Increase batch size and number of updates\n            if len(replay_buffer) >= 64:\n                MODEL.train()\n                \n                # Dynamic training steps: Train more if we have more data\n                # AlphaZero ratio usually high (e.g. 10 epochs on new data)\n                # Here we sample from buffer.\n                train_batch_size = 128\n                if len(replay_buffer) < train_batch_size:\n                    train_batch_size = 64\n                \n                # Steps: roughly cover 20% of buffer or at least 10 steps\n                steps = max(10, min(50, len(replay_buffer) // train_batch_size))\n                \n                for _ in range(steps):\n                    batch = random.sample(list(replay_buffer), min(train_batch_size, len(replay_buffer)))\n                    \n                    x_list = [exp['x'] for exp in batch]\n                    y_list = [exp['y'] for exp in batch]\n                    x_list, y_list = normalize_batch(x_list, y_list)\n                    \n                    token_lists = [[TOKEN_TO_ID[t] for t in exp['tokens']] for exp in batch]\n                    policy_targets = [exp['policy'] for exp in batch]\n                    value_targets_list = [exp['value'] for exp in batch]\n                    \n                    max_len = max(len(s) for s in token_lists)\n                    decoder_input = torch.full((len(batch), max_len + 1), SOS_ID, dtype=torch.long)\n                    \n                    # Policy targets (for KLDiv) and Value targets\n                    policy_target_tensor = torch.tensor(np.array(policy_targets), dtype=torch.float32).to(DEVICE)\n                    value_target_tensor = torch.tensor(np.array(value_targets_list), dtype=torch.float32).unsqueeze(1).to(DEVICE)\n                    \n                    for i, seq in enumerate(token_lists):\n                        l = len(seq)\n                        decoder_input[i, 1:l+1] = torch.tensor(seq, dtype=torch.long)\n                    \n                    x_tensor = torch.tensor(np.array(x_list), dtype=torch.float32).to(DEVICE)\n                    y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n                    decoder_input = decoder_input.to(DEVICE)\n                    \n                    optimizer.zero_grad()\n                    logits, value_pred = MODEL(x_tensor, y_tensor, decoder_input)\n                    \n                    # Policy Loss (KL Divergence)\n                    # Get logits for the last token position of each sequence\n                    last_logits = []\n                    for i, seq in enumerate(token_lists):\n                        idx = len(seq) # Post-padding index? No, index in padded tensor.\n                        # decoder_input: [SOS, T1, T2]\n                        # logits: [PredSOS, PredT1, PredT2]\n                        # We want prediction AFTER T2? No.\n                        # MCTS Example: State=[T1, T2]. Policy=Dist for T3.\n                        # Model Input: [SOS, T1, T2]. Output Last: Dist for T3.\n                        # Index is len(seq).\n                        last_logits.append(logits[i, idx, :VOCAB_SIZE])\n                    \n                    last_logits = torch.stack(last_logits)\n                    log_probs = torch.nn.functional.log_softmax(last_logits, dim=1)\n                    \n                    loss_policy = kl_loss(log_probs, policy_target_tensor)\n                    \n                    # Value Loss (Quantile)\n                    loss_value = quantile_loss_fn(value_pred, value_target_tensor)\n                    \n                    # Total Loss\n                    loss = loss_policy + loss_value \n                    \n                    if not (torch.isnan(loss) or torch.isinf(loss)):\n                        loss.backward()\n                        torch.nn.utils.clip_grad_norm_(MODEL.parameters(), 1.0)\n                        optimizer.step()\n                        losses.append(loss.item())\n            \n            # Step Scheduler based on recent Loss\n            if losses:\n                current_loss = np.mean(losses[-10:])\n                scheduler.step(current_loss)\n            \n            current_lr = optimizer.param_groups[0]['lr']\n            \n            # Periodic save\n            if (iteration + 1) % 10 == 0:\n                save_model()\n        \n        save_model()\n        MODEL.eval()\n        TRAINING_STATUS[\"running\"] = False\n        \n        fig = create_selfplay_plot(losses, rmses)\n        \n        avg_rmse = np.mean(rmses[-50:]) if rmses else 0\n        result = f\"\"\"\n        <div style=\"background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); padding: 20px; border-radius: 15px; border: 2px solid #ff6b6b;\">\n            <h2 style=\"color: #ff6b6b; margin: 0;\">Self-Play Completado</h2>\n            <p style=\"color: white;\">Iteraciones: {int(iterations)} | Problemas: {len(rmses)}</p>\n            <p style=\"color: #888;\">RMSE Promedio: {avg_rmse:.4f} | Dispositivo: {DEVICE.type.upper()}</p>\n        </div>\n        \"\"\"\n        return result, fig\n        \n    except Exception as e:\n        TRAINING_STATUS[\"running\"] = False\n        return f\"Error: {str(e)}\", None\n\n\ndef create_loss_plot(losses, title):\n    \"\"\"Create a loss plot with dark theme.\"\"\"\n    fig, ax = plt.subplots(figsize=(8, 4), facecolor='#1a1a2e')\n    ax.set_facecolor('#1a1a2e')\n    ax.plot(losses, color='#00d4ff', linewidth=2)\n    ax.set_xlabel('Epoca', color='white')\n    ax.set_ylabel('Loss', color='white')\n    ax.set_title(title, color='white', fontweight='bold')\n    ax.tick_params(colors='white')\n    ax.grid(True, alpha=0.2)\n    for spine in ax.spines.values():\n        spine.set_color('#00d4ff')\n    plt.tight_layout()\n    return fig\n\n\ndef create_selfplay_plot(losses, rmses):\n    \"\"\"Create dual plot for self-play results.\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4), facecolor='#1a1a2e')\n    \n    ax1.set_facecolor('#1a1a2e')\n    if losses:\n        ax1.plot(losses, color='#00d4ff', linewidth=2)\n    ax1.set_xlabel('Step', color='white')\n    ax1.set_ylabel('Loss', color='white')\n    ax1.set_title('Policy Loss', color='white', fontweight='bold')\n    ax1.tick_params(colors='white')\n    ax1.grid(True, alpha=0.2)\n    \n    ax2.set_facecolor('#1a1a2e')\n    if rmses:\n        ax2.plot(rmses, color='#ff6b6b', linewidth=1, alpha=0.5)\n        if len(rmses) > 10:\n            ma = np.convolve(rmses, np.ones(10)/10, mode='valid')\n            ax2.plot(range(9, len(rmses)), ma, color='#ff6b6b', linewidth=2)\n    ax2.set_xlabel('Problema', color='white')\n    ax2.set_ylabel('RMSE', color='white')\n    ax2.set_title('RMSE', color='white', fontweight='bold')\n    ax2.tick_params(colors='white')\n    ax2.grid(True, alpha=0.2)\n    \n    for ax in [ax1, ax2]:\n        for spine in ax.spines.values():\n            spine.set_color('#00d4ff')\n    \n    plt.tight_layout()\n    return fig\n\ndef train_supervised(iterations, batch_size=128, point_count=10, progress=gr.Progress()):\n    \"\"\"\n    Massive Supervised Pre-training (Warmup).\n    Focus: Syntax, Basic Arithmetic, Overcoming \"Collapse to Constant\".\n    Speed: High (No MCTS, just random generation + CrossEntropy).\n    \"\"\"\n    global TRAINING_STATUS\n    \n    if TRAINING_STATUS[\"running\"]:\n        return \"Entrenamiento ya en progreso\", None\n    \n    TRAINING_STATUS[\"running\"] = True\n    reset_stop_flag()  # Reset stop flag at start\n    \n    try:\n        MODEL, DEVICE = get_model()\n        \n        MODEL.train()\n        optimizer = torch.optim.AdamW(MODEL.parameters(), lr=1e-4, weight_decay=0.01)\n        # Slower decay: T_max = iterations * 2 keeps LR higher for longer\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(iterations*2), eta_min=1e-6)\n        ce_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n        \n        VOCAB_SIZE = len(VOCABULARY)\n        SOS_ID = VOCAB_SIZE\n        \n        # Start extremely simple (Depth 1: x+1, x*x, etc.)\n        allowed_ops = OPERATOR_STAGES[0]\n        data_gen = DataGenerator(max_depth=1, allowed_operators=allowed_ops) \n        allowed_mask = get_allowed_token_mask(0, VOCAB_SIZE, DEVICE) # Stage 0 mask\n        losses = []\n        \n        start_time = time.time()\n        \n        for i in range(int(iterations)):\n            # Check for stop request\n            if should_stop_training():\n                print(\"\u23f9\ufe0f Pre-training stopped by user\")\n                break\n            # ETA\n            elapsed = time.time() - start_time\n            if i > 0:\n                iter_per_sec = i / elapsed\n                remaining = int(iterations) - i\n                eta = remaining / iter_per_sec\n                eta_str = f\"{eta:.0f}s\"\n            else:\n                eta_str = \"...\"\n                \n            current_lr = optimizer.param_groups[0]['lr']\n            msg = f\"Iter {i+1}/{int(iterations)} Loss:{np.mean(losses[-50:]) if losses else 0:.3f} LR:{current_lr:.1e} ETA:{eta_str}\"\n            progress((i + 1) / iterations, desc=msg)\n            \n            # Generate Random Batch (High Speed)\n            batch = data_gen.generate_batch(int(batch_size), point_count=int(point_count))\n            \n            if not batch:\n                continue\n            \n            x_list = [d['x'] for d in batch]\n            y_list = [d['y'] for d in batch]\n            x_list, y_list = normalize_batch(x_list, y_list)\n            \n            token_lists = [[TOKEN_TO_ID.get(t, TOKEN_TO_ID['C']) for t in d['tokens']] for d in batch]\n            \n            max_len = max(len(s) for s in token_lists)\n            decoder_input = torch.full((len(batch), max_len + 1), SOS_ID, dtype=torch.long)\n            targets = torch.full((len(batch), max_len + 1), -1, dtype=torch.long)\n            \n            for j, seq in enumerate(token_lists):\n                decoder_input[j, 1:len(seq)+1] = torch.tensor(seq, dtype=torch.long)\n                targets[j, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n                \n            x_tensor = torch.tensor(np.array(x_list), dtype=torch.float32).to(DEVICE)\n            y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n            decoder_input = decoder_input.to(DEVICE)\n            targets = targets.to(DEVICE)\n            \n            optimizer.zero_grad()\n            logits, _ = MODEL(x_tensor, y_tensor, decoder_input)\n            \n            # Apply Stage 0 mask to bridge Pre-training with Curriculum\n            # Use a more stable value (-1e4 instead of -1e9) to avoid overflow\n            logits = logits + (1 - allowed_mask.view(1, 1, -1)) * -1e4\n            \n            loss = ce_loss(logits.view(-1, VOCAB_SIZE + 1), targets.view(-1))\n            \n            if not (torch.isnan(loss) or torch.isinf(loss)):\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(MODEL.parameters(), 1.0)\n                optimizer.step()\n                scheduler.step()\n                losses.append(loss.item())\n                \n            if (i+1) % 100 == 0:\n                save_model()\n                \n        save_model()\n        MODEL.eval()\n        TRAINING_STATUS[\"running\"] = False\n        \n        fig = create_loss_plot(losses, \"Pre-Entrenamiento Supervisado\")\n        \n        result = f\"\"\"\n        <div style=\"background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); padding: 20px; border-radius: 15px; border: 2px solid #ffd93d;\">\n            <h2 style=\"color: #ffd93d; margin: 0;\">Escuela Primaria (Warmup) Completada</h2>\n            <p style=\"color: white;\">Iteraciones: {int(iterations)} | Loss Final: {losses[-1]:.4f}</p>\n            <p style=\"color: #888;\">El modelo ha aprendido sintaxis basica.</p>\n        </div>\n        \"\"\"\n        return result, fig\n        \n    except Exception as e:\n        TRAINING_STATUS[\"running\"] = False\n        return f\"Error: {str(e)}\", None\n\n\ndef train_hybrid_feedback_loop(iterations, problems_per_iter=10, gp_timeout=10, progress=gr.Progress()):\n    \"\"\"\n    Teacher-Student Distillation Loop.\n    1. Find problems where model has high loss.\n    2. Use Hybrid Search (GP) to solve them.\n    3. Train model on GP solutions.\n    \"\"\"\n    global TRAINING_STATUS\n    \n    if TRAINING_STATUS[\"running\"]:\n        return \"Entrenamiento ya en progreso\", None\n    \n    TRAINING_STATUS[\"running\"] = True\n    reset_stop_flag()\n    \n    try:\n        MODEL, DEVICE = get_model()\n        \n        optimizer = torch.optim.AdamW(MODEL.parameters(), lr=5e-5, weight_decay=0.01)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n        \n        VOCAB_SIZE = len(VOCABULARY)\n        SOS_ID = VOCAB_SIZE\n        \n        # Replay buffer for \"Gold Standard\" examples found by GP\n        replay_buffer = deque(maxlen=5000)\n        \n        # Start with simple problems and grow\n        data_gen = DataGenerator(max_depth=3)\n        \n        losses = []\n        gp_successes = 0\n        gp_attempts = 0\n        \n        start_time = time.time()\n        \n        for iteration in range(int(iterations)):\n            if should_stop_training():\n                print(\"\u23f9\ufe0f Feedback Loop stopped\")\n                break\n                \n            elapsed = time.time() - start_time\n            # eta_str = f\"{(int(iterations)-iteration) * (elapsed/(iteration+1) if iteration>0 else 0):.0f}s\"\n            iter_dur = elapsed/(iteration+1) if iteration > 0 else 0\n            eta_seconds = (int(iterations)-iteration) * iter_dur\n            eta_str = f\"{eta_seconds:.0f}s\"\n\n            progress((iteration + 1) / iterations, \n                     desc=f\"Iter {iteration+1}/{int(iterations)} | GP Success: {gp_successes}/{gp_attempts} | Loss: {np.mean(losses[-10:]) if losses else 0:.3f}\")\n            \n            # --- PHASE 1: HARD MINING ---\n            MODEL.eval()\n            \n            # Generate candidates\n            pool_size = 50 \n            candidates = data_gen.generate_inverse_batch(pool_size, point_count=10)\n            \n            hard_problems = []\n            \n            with torch.no_grad():\n                # We want to find problems with HIGH LOSS (model failure)\n                # Quick batch forward\n                x_list = [d['x'] for d in candidates]\n                y_list = [d['y'] for d in candidates]\n                x_list, y_list = normalize_batch(x_list, y_list)\n                \n                token_lists = [[TOKEN_TO_ID.get(t, TOKEN_TO_ID['C']) for t in d['tokens']] for d in candidates]\n                max_len = max(len(s) for s in token_lists)\n                \n                dec_in = torch.full((pool_size, max_len + 1), SOS_ID, dtype=torch.long).to(DEVICE)\n                targets = torch.full((pool_size, max_len + 1), -1, dtype=torch.long).to(DEVICE)\n                \n                for j, seq in enumerate(token_lists):\n                    dec_in[j, 1:len(seq)+1] = torch.tensor(seq, dtype=torch.long)\n                    targets[j, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n                    \n                x_tensor = torch.tensor(np.array(x_list), dtype=torch.float32).to(DEVICE)\n                y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n                \n                logits, value_pred = MODEL(x_tensor, y_tensor, dec_in)\n                \n                loss_f = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='none')\n                raw_losses = loss_f(logits.view(-1, VOCAB_SIZE + 1), targets.view(-1))\n                raw_losses = raw_losses.view(pool_size, -1)\n                \n                mask = (targets != -1)\n                sample_losses = (raw_losses * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n                \n                # Filter: Keep if loss > 1.0 (arbitrary threshold for \"confused\")\n                for j, loss_val in enumerate(sample_losses):\n                    if loss_val.item() > 0.5: # Lower threshold to catch more\n                        hard_problems.append(candidates[j])\n            \n            # Take top K hardest\n            # Limit GP calls per iter to avoid slowness\n            problems_to_solve = hard_problems[:int(problems_per_iter)]\n            \n            if not problems_to_solve:\n                continue\n\n            # --- PHASE 2: TEACHER SOLVES (GP) ---\n            print(f\"Iter {iteration}: Asking Teacher to solve {len(problems_to_solve)} hard problems...\")\n            \n            for prob in problems_to_solve:\n                gp_attempts += 1\n                try:\n                    # Run Hybrid Search (Quick Mode)\n                    # We pass the model so beam search can seed the GP\n                    res = hybrid_solve(\n                        prob['x'], \n                        prob['y'], \n                        MODEL, \n                        DEVICE, \n                        beam_width=10,     # Faster beam\n                        gp_timeout=gp_timeout,\n                        gp_binary_path=None \n                    )\n                    \n                    if res and res.get('formula') and res.get('rmse', 1e6) < 0.01:\n                        # SUCCESS!\n                        gp_successes += 1\n                        \n                        # Parse formula to tokens\n                        try:\n                            # 1. Parse string to tree\n                            tree = ExpressionTree.from_infix(res['formula'])\n                            # 2. Get tokens\n                            tokens = tree.tokens\n                            \n                            replay_buffer.append({\n                                'x': prob['x'],\n                                'y': prob['y'],\n                                'tokens': tokens,\n                                'source': 'GP_Teacher'\n                            })\n                            \n                        except Exception as e:\n                            print(f\"Failed to tokenize GP result: {e}\")\n                            \n                except Exception as e:\n                    print(f\"GP Hybrid Error: {e}\")\n                    \n            # --- PHASE 3: STUDENT TRAINS (NN) ---\n            if len(replay_buffer) > 10:\n                MODEL.train()\n                # Train on batch from buffer\n                batch_size_train = min(len(replay_buffer), 64)\n                \n                # Multiple steps to enforce learning\n                steps = 5\n                \n                for _ in range(steps):\n                    batch = random.sample(list(replay_buffer), batch_size_train)\n                    \n                    x_list = [d['x'] for d in batch]\n                    y_list = [d['y'] for d in batch]\n                    x_list, y_list = normalize_batch(x_list, y_list)\n                    \n                    token_lists = [[TOKEN_TO_ID.get(t, TOKEN_TO_ID['C']) for t in d['tokens']] for d in batch]\n                    max_len = max(len(s) for s in token_lists)\n                    \n                    dec_in = torch.full((batch_size_train, max_len + 1), SOS_ID, dtype=torch.long).to(DEVICE)\n                    targets = torch.full((batch_size_train, max_len + 1), -1, dtype=torch.long).to(DEVICE)\n                    \n                    for j, seq in enumerate(token_lists):\n                        dec_in[j, 1:len(seq)+1] = torch.tensor(seq, dtype=torch.long)\n                        targets[j, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n                        \n                    x_t = torch.tensor(np.array(x_list), dtype=torch.float32).to(DEVICE)\n                    y_t = torch.tensor(np.array(y_list), dtype=torch.float32).to(DEVICE)\n                    dec_in = dec_in.to(DEVICE)\n                    targets = targets.to(DEVICE)\n                    \n                    optimizer.zero_grad()\n                    logits, value_pred = MODEL(x_t, y_t, dec_in)\n                    \n                    # Policy Loss only (Standard Supervised)\n                    # We trust the GP solution is \"Correct\" (Value=1.0)\n                    loss_ce = torch.nn.CrossEntropyLoss(ignore_index=-1)(logits.view(-1, VOCAB_SIZE+1), targets.view(-1))\n                    \n                    # Value Loss\n                    value_targets = torch.ones_like(value_pred) # GP solutions are always valid\n                    loss_val = torch.nn.functional.mse_loss(value_pred, value_targets)\n                    \n                    loss = loss_ce + 0.1 * loss_val\n                    \n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(MODEL.parameters(), 1.0)\n                    optimizer.step()\n                    \n                    losses.append(loss.item())\n                    \n                scheduler.step(np.mean(losses[-10:]))\n                \n            if (iteration + 1) % 5 == 0:\n                save_model()\n                \n        save_model()\n        MODEL.eval()\n        TRAINING_STATUS[\"running\"] = False\n        \n        fig = create_loss_plot(losses, \"Feedback Loop Loss\")\n        \n        result_html = f\"\"\"\n        <div style=\"background: linear-gradient(135deg, #2c3e50 0%, #000000 100%); padding: 20px; border-radius: 15px; border: 2px solid #f1c40f;\">\n            <h2 style=\"color: #f1c40f; margin: 0;\">Feedback Loop Completado</h2>\n            <p style=\"color: white;\">Iteraciones: {iterations} | GP Success: {gp_successes}/{gp_attempts}</p>\n            <p style=\"color: #bbb;\">Nuevos Ejemplos Generados: {len(replay_buffer)}</p>\n        </div>\n        \"\"\"\n        return result_html, fig\n\n    except Exception as e:\n        TRAINING_STATUS[\"running\"] = False\n        import traceback\n        traceback.print_exc()\n        return f\"Error CRITICO: {str(e)}\", None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/ui/app_benchmark.py\n",
        "import gradio as gr\nfrom utils.benchmark_comparison import run_comparison_benchmark\nfrom ui.app_core import get_model, DEVICE\n\ndef get_benchmark_tab():\n    with gr.Tab(\"\ud83e\udd47 Benchmark (IQ Test)\"):\n        gr.Markdown(\"### Evaluar Inteligencia del Modelo (Comparativa)\")\n        gr.Markdown(\"Ejecuta una bater\u00eda de **10 problemas est\u00e1ndar** comparando diferentes m\u00e9todos de b\u00fasqueda.\")\n        \n        with gr.Row():\n            methods_chk = gr.CheckboxGroup(\n                choices=[\"beam\", \"mcts\", \"hybrid\"], \n                value=[\"hybrid\"], \n                label=\"M\u00e9todos a Evaluar\",\n                info=\"Selecciona uno o m\u00e1s m\u00e9todos para comparar.\"\n            )\n            timeout_slider = gr.Slider(\n                minimum=5, \n                maximum=60, \n                value=30, \n                step=5, \n                label=\"Timeout GP (s)\", \n                info=\"Tiempo m\u00e1ximo para Beta-GP por problema.\"\n            )\n        \n        run_btn = gr.Button(\"\ud83d\ude80 Iniciar Benchmark Comparativo\", variant=\"primary\")\n        \n        progress_bar = gr.HTML(\"\")\n        \n        # Area de resultados\n        summary_html = gr.HTML(\"Resultados aparecer\u00e1n aqu\u00ed...\")\n        \n        results_df = gr.Dataframe(\n            headers=[\"Problema\", \"Nivel\", \"M\u00e9todo\", \"Formula\", \"RMSE\", \"Tiempo\", \"Estado\"],\n            label=\"Resultados Detallados\",\n            interactive=False\n        )\n        \n        def run_bench(selected_methods, gp_timeout, progress=gr.Progress()):\n            model_obj, device_obj = get_model()\n            if not model_obj:\n                return \"<div>\u26a0\ufe0f Error: Modelo no cargado. Ve a la pesta\u00f1a 'Config' y carga un modelo.</div>\", None, []\n            \n            if not selected_methods:\n                return \"<div>\u26a0\ufe0f Error: Selecciona al menos un m\u00e9todo.</div>\", None, []\n                \n            progress(0, desc=\"Iniciando Benchmark...\")\n            \n            # Run comparison\n            try:\n                result_data = run_comparison_benchmark(\n                    model_obj, \n                    device_obj, \n                    methods=selected_methods,\n                    gp_timeout=gp_timeout,\n                    beam_width=50,\n                    progress_callback=lambda p, desc: progress(p, desc=desc)\n                )\n            except Exception as e:\n                import traceback\n                traceback.print_exc()\n                return f\"<div>\u274c Error en Benchmark: {e}</div>\", None, []\n            \n            results = result_data['results']\n            summary_dict = result_data['summary']\n            \n            # Format dataframe\n            rows = []\n            for r in results:\n                status_icon = \"\u2705\" if r['success'] else \"\u274c\"\n                rmse_val = f\"{r['rmse']:.5f}\" if r['rmse'] < 1e6 else \"> 10^6\"\n                rows.append([\n                    r['problem_name'],\n                    r['level'],\n                    r['method'].upper(),\n                    r['formula'],\n                    rmse_val,\n                    f\"{r['time']:.2f}s\",\n                    status_icon\n                ])\n            \n            # Generate HTML Summary\n            html_content = \"<div style='display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;'>\"\n            \n            # Determine winner if multiple methods\n            winner_method = None\n            if len(selected_methods) > 1:\n                winner_method = max(summary_dict.items(), key=lambda x: (x[1]['solved'], -x[1]['avg_rmse']))[0]\n            \n            for method, stats in summary_dict.items():\n                is_winner = (method == winner_method)\n                border_color = \"#4CAF50\" if is_winner else (\"#FF9800\" if stats['score'] > 50 else \"#F44336\")\n                bg_color = \"#1e1e2f\"\n                if is_winner:\n                    bg_color = \"#1b3a24\" # Dark green tint for winner\n                    \n                trophy = \"\ud83c\udfc6 GANADOR\" if is_winner else \"\"\n                \n                html_content += f\"\"\"\n                <div style=\"background: {bg_color}; padding: 15px; border-radius: 10px; border: 2px solid {border_color}; min-width: 200px; text-align: center;\">\n                    <h2 style=\"color: {border_color}; margin: 0 0 10px 0;\">{method.upper()} {trophy}</h2>\n                    <div style=\"font-size: 24px; font-weight: bold; margin-bottom: 5px;\">{stats['solved']} / {stats['total']}</div>\n                    <div style=\"color: #ccc; font-size: 14px;\">Resueltos</div>\n                    <hr style=\"border-color: #444; margin: 10px 0;\">\n                    <div style=\"font-size: 14px;\">Nota: <b>{stats['score']:.1f}%</b></div>\n                    <div style=\"font-size: 14px;\">Tiempo Avg: <b>{stats['avg_time']:.2f}s</b></div>\n                </div>\n                \"\"\"\n            html_content += \"</div>\"\n            \n            return html_content, rows\n            \n        run_btn.click(run_bench, inputs=[methods_chk, timeout_slider], outputs=[summary_html, results_df])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/ui/__init__.py\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/utils/optimize_constants.py\n",
        "\"\"\"\nConstant Optimization Module for AlphaSymbolic.\nUses scipy.optimize to find optimal values for 'C' placeholders.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom core.grammar import ExpressionTree\n\ndef optimize_constants(tree, x_data, y_data, method='L-BFGS-B'):\n    \"\"\"\n    Given an ExpressionTree with 'C' placeholders, find optimal constant values.\n    \n    Args:\n        tree: ExpressionTree object\n        x_data: numpy array of x values\n        y_data: numpy array of target y values\n        method: optimization method ('L-BFGS-B', 'SLSQP', 'Nelder-Mead')\n        \n    Returns:\n        dict: mapping of path tuples to optimized constant values\n        float: final RMSE\n    \"\"\"\n    if not tree.is_valid:\n        return {}, float('inf')\n    \n    # Get positions of all constants\n    positions = tree.root.get_constant_positions()\n    n_constants = len(positions)\n    \n    if n_constants == 0:\n        # No constants to optimize, just evaluate\n        y_pred = tree.evaluate(x_data)\n        mse = np.mean((y_pred - y_data)**2)\n        return {}, np.sqrt(mse)\n    \n    def objective(params):\n        \"\"\"Objective function: RMSE given constant values.\"\"\"\n        # Build constants dict\n        constants = {tuple(pos): params[i] for i, pos in enumerate(positions)}\n        \n        # Evaluate\n        y_pred = tree.evaluate(x_data, constants=constants)\n        \n        # Handle invalid predictions\n        if np.any(np.isnan(y_pred)) or np.any(np.isinf(y_pred)):\n            return 1e10\n        \n        if not np.all(np.isfinite(y_pred)):\n            return 1e9\n        \n        # Clip huge values to prevent overflow in MSE\n        y_pred = np.clip(y_pred, -1e9, 1e9)\n        \n        mse = np.mean((y_pred - y_data)**2)\n        return mse\n    \n    # Initial guess: all 1s\n    x0 = np.ones(n_constants)\n    \n    # Bounds: reasonable range for constants\n    bounds = [(-1000, 1000)] * n_constants\n    \n    try:\n        result = minimize(\n            objective,\n            x0,\n            method=method,\n            bounds=bounds if method in ['L-BFGS-B', 'SLSQP'] else None,\n            options={'maxiter': 1000, 'disp': False}\n        )\n        \n        # Build final constants dict\n        optimized_constants = {tuple(pos): result.x[i] for i, pos in enumerate(positions)}\n        final_rmse = np.sqrt(result.fun) if result.fun > 0 else 0.0\n        \n        return optimized_constants, final_rmse\n        \n    except Exception as e:\n        return {}, float('inf')\n\ndef substitute_constants(infix_str, constants_dict, positions):\n    \"\"\"\n    Replace 'C' in the infix string with optimized values.\n    Simple approach: replace each C with optimized value.\n    \"\"\"\n    # For proper substitution, we'd need to track positions properly\n    # This is a simplified version that replaces all C with the first constant\n    result = infix_str\n    for i, pos in enumerate(positions):\n        if tuple(pos) in constants_dict:\n            val = constants_dict[tuple(pos)]\n            # Format nicely\n            if abs(val - round(val)) < 1e-6:\n                val_str = str(int(round(val)))\n            else:\n                val_str = f\"{val:.4f}\"\n            # Replace first occurrence of C\n            result = result.replace('C', val_str, 1)\n    return result\n\n\n# Quick test\nif __name__ == \"__main__\":\n    # Test: C * x + C should be optimized to fit y = 2*x + 3\n    x_test = np.array([1, 2, 3, 4, 5], dtype=np.float64)\n    y_test = 2 * x_test + 3  # y = 2x + 3\n    \n    tokens = ['+', '*', 'C', 'x', 'C']  # C*x + C\n    tree = ExpressionTree(tokens)\n    \n    print(f\"Formula structure: {tree.get_infix()}\")\n    print(f\"Target: y = 2x + 3\")\n    \n    constants, rmse = optimize_constants(tree, x_test, y_test)\n    print(f\"Optimized constants: {constants}\")\n    print(f\"Final RMSE: {rmse:.6f}\")\n    \n    # Verify\n    y_pred = tree.evaluate(x_test, constants=constants)\n    print(f\"Predictions: {y_pred}\")\n    print(f\"Targets: {y_test}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/utils/detect_pattern.py\n",
        "\"\"\"\nTarget Pattern Detection for AlphaSymbolic.\nAnalyzes target Y values to detect patterns (polynomial, exponential, periodic, etc.)\nand suggests initial search biases.\n\"\"\"\nimport numpy as np\nfrom scipy import stats\nfrom scipy.fft import fft\nfrom core.grammar import ExpressionTree\n\ndef detect_pattern(x_values, y_values):\n    \"\"\"\n    Analyze (x, y) data to detect patterns.\n    Returns a dict with pattern type probabilities and suggested operators.\n    \"\"\"\n    x = np.array(x_values, dtype=np.float64)\n    y = np.array(y_values, dtype=np.float64)\n    \n    results = {\n        'type': 'unknown',\n        'confidence': 0.0,\n        'suggested_ops': [],\n        'details': {}\n    }\n    \n    if len(x) < 3:\n        return results\n    \n    scores = {}\n    \n    # 1. Check for linear pattern (y = ax + b)\n    if len(x) >= 2:\n        slope, intercept, r_value, _, _ = stats.linregress(x, y)\n        scores['linear'] = r_value ** 2\n        results['details']['linear'] = {\n            'slope': slope,\n            'intercept': intercept,\n            'r_squared': r_value ** 2\n        }\n    \n    # 2. Check for quadratic pattern (y = ax^2 + bx + c)\n    if len(x) >= 3:\n        try:\n            coeffs = np.polyfit(x, y, 2)\n            y_pred = np.polyval(coeffs, x)\n            ss_res = np.sum((y - y_pred) ** 2)\n            ss_tot = np.sum((y - np.mean(y)) ** 2)\n            r2 = 1 - ss_res / ss_tot if ss_tot > 0 else 0\n            scores['quadratic'] = r2\n            results['details']['quadratic'] = {\n                'coefficients': coeffs.tolist(),\n                'r_squared': r2\n            }\n        except:\n            pass\n    \n    # 3. Check for exponential pattern (y = a * e^(bx))\n    if np.all(y > 0):  # Exponential only for positive y\n        try:\n            log_y = np.log(y)\n            slope, intercept, r_value, _, _ = stats.linregress(x, log_y)\n            scores['exponential'] = r_value ** 2\n            results['details']['exponential'] = {\n                'a': np.exp(intercept),\n                'b': slope,\n                'r_squared': r_value ** 2\n            }\n        except:\n            pass\n    \n    # 4. Check for periodic/sinusoidal pattern\n    if len(y) >= 4:\n        try:\n            # Simple FFT analysis\n            y_centered = y - np.mean(y)\n            fft_vals = np.abs(fft(y_centered))\n            \n            # Check if there's a dominant frequency\n            if len(fft_vals) > 1:\n                max_idx = np.argmax(fft_vals[1:len(fft_vals)//2]) + 1\n                max_power = fft_vals[max_idx]\n                total_power = np.sum(fft_vals[1:len(fft_vals)//2])\n                \n                if total_power > 0:\n                    periodicity = max_power / total_power\n                    scores['periodic'] = periodicity\n                    results['details']['periodic'] = {\n                        'dominant_freq_idx': int(max_idx),\n                        'periodicity_score': periodicity\n                    }\n        except:\n            pass\n    \n    # 5. Check for power law (y = a * x^b)\n    if np.all(x > 0) and np.all(y > 0):\n        try:\n            log_x = np.log(x)\n            log_y = np.log(y)\n            slope, intercept, r_value, _, _ = stats.linregress(log_x, log_y)\n            scores['power'] = r_value ** 2\n            results['details']['power'] = {\n                'a': np.exp(intercept),\n                'b': slope,\n                'r_squared': r_value ** 2\n            }\n        except:\n            pass\n    \n    # 6. Check for factorial/gamma pattern (for integer-like x)\n    if np.all(x > 0) and np.all(x == np.floor(x)):\n        try:\n            from scipy.special import gamma\n            x_int = x.astype(int)\n            y_gamma = gamma(x_int + 1)  # gamma(n+1) = n!\n            \n            # Simple linear fit between y and gamma\n            if not np.any(np.isinf(y_gamma)):\n                slope, intercept, r_value, _, _ = stats.linregress(y_gamma, y)\n                scores['factorial'] = r_value ** 2\n                results['details']['factorial'] = {\n                    'r_squared': r_value ** 2\n                }\n        except:\n            pass\n    \n    # Determine best pattern\n    if scores:\n        best_pattern = max(scores.items(), key=lambda x: x[1])\n        results['type'] = best_pattern[0]\n        results['confidence'] = best_pattern[1]\n        \n        # Suggest operators based on pattern\n        op_suggestions = {\n            'linear': ['+', '-', '*', 'x', 'C'],\n            'quadratic': ['pow', '+', '*', 'x', 'C', '2'],\n            'exponential': ['exp', '*', '+', 'x', 'C'],\n            'periodic': ['sin', 'cos', '*', '+', 'x', 'C'],\n            'power': ['pow', '*', 'x', 'C'],\n            'factorial': ['gamma', '*', '+', 'x', 'C']\n        }\n        results['suggested_ops'] = op_suggestions.get(best_pattern[0], [])\n    \n    return results\n\n\ndef summarize_pattern(result):\n    \"\"\"Pretty-print pattern detection result.\"\"\"\n    print(f\"\\n=== Pattern Detection ===\")\n    print(f\"Detected Type: {result['type']} (confidence: {result['confidence']:.2%})\")\n    print(f\"Suggested Operators: {', '.join(result['suggested_ops'])}\")\n    \n    if result['type'] in result['details']:\n        print(f\"Details: {result['details'][result['type']]}\")\n\n\nif __name__ == \"__main__\":\n    # Test with different patterns\n    \n    # Linear: y = 2x + 3\n    print(\"\\n--- Test: Linear ---\")\n    x1 = np.linspace(0, 10, 20)\n    y1 = 2 * x1 + 3 + np.random.normal(0, 0.1, 20)\n    result1 = detect_pattern(x1, y1)\n    summarize_pattern(result1)\n    \n    # Quadratic: y = x^2 + 1\n    print(\"\\n--- Test: Quadratic ---\")\n    x2 = np.linspace(-5, 5, 20)\n    y2 = x2**2 + 1\n    result2 = detect_pattern(x2, y2)\n    summarize_pattern(result2)\n    \n    # Exponential: y = 2 * e^(0.5x)\n    print(\"\\n--- Test: Exponential ---\")\n    x3 = np.linspace(0, 5, 20)\n    y3 = 2 * np.exp(0.5 * x3)\n    result3 = detect_pattern(x3, y3)\n    summarize_pattern(result3)\n    \n    # Periodic: y = sin(x)\n    print(\"\\n--- Test: Periodic ---\")\n    x4 = np.linspace(0, 4*np.pi, 50)\n    y4 = np.sin(x4)\n    result4 = detect_pattern(x4, y4)\n    summarize_pattern(result4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/utils/benchmark_runner.py\n",
        "import torch\nimport numpy as np\nimport time\nimport traceback\nfrom search.mcts import MCTS\nfrom data.benchmark_data import BENCHMARK_SUITE, get_benchmark_data\nfrom utils.optimize_constants import optimize_constants\n\ndef run_benchmark_suite(model, device, progress_callback=None):\n    \"\"\"\n    Runs the full benchmark suite.\n    Args:\n        model: Loaded AlphaSymbolic model\n        device: Torch device\n        progress_callback: Function(float, string) to update UI\n        \n    Returns:\n        results: List of result dicts\n        summary: Dict with aggregated stats\n    \"\"\"\n    results = []\n    \n    # Configure MCTS for benchmark (balanced speed/accuracy)\n    # 500 simulations is decent for benchmarking\n    mcts = MCTS(model, device, max_simulations=500, batch_size=32)\n    \n    total = len(BENCHMARK_SUITE)\n    solved_count = 0\n    \n    for i, problem in enumerate(BENCHMARK_SUITE):\n        if progress_callback:\n            progress_callback(i / total, f\"Testing: {problem['name']}...\")\n            \n        x, y, _ = get_benchmark_data(problem['id'])\n        \n        start_time = time.time()\n        \n        # Run Search\n        try:\n            search_result = mcts.search(x, y)\n             # Determine success\n            # Success threshold: RMSE < 0.01 (or 1% relative error)\n            rmse = search_result['rmse']\n            is_solved = rmse < 0.05 # Looser threshold for general regression\n            \n            # Special check for exact integer symbolic match? No, RMSE is ground truth.\n            \n            elapsed = time.time() - start_time\n            \n            if is_solved:\n                solved_count += 1\n                status = \"\u2705 SOLVED\"\n            else:\n                status = \"\u274c FAILED\"\n                \n            results.append({\n                'id': problem['id'],\n                'name': problem['name'],\n                'level': problem['level'],\n                'rmse': rmse,\n                'time': elapsed,\n                'status': status,\n                'found_formula': search_result.get('formula', '???'),\n                'is_solved': is_solved\n            })\n            \n        except Exception as e:\n            print(f\"Error in benchmark {problem['name']}:\")\n            traceback.print_exc()\n            results.append({\n                'id': problem['id'],\n                'name': problem['name'],\n                'level': problem['level'],\n                'rmse': 1e9,\n                'time': 0,\n                'status': \"\u26a0\ufe0f ERROR\",\n                'found_formula': \"Error\",\n                'is_solved': False\n            })\n\n    # Summary\n    if progress_callback:\n        progress_callback(1.0, \"Done!\")\n        \n    score = (solved_count / total) * 100\n    summary = {\n        'total': total,\n        'solved': solved_count,\n        'score': score,\n        'avg_time': np.mean([r['time'] for r in results]) if results else 0\n    }\n    \n    return results, summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/utils/benchmark_comparison.py\n",
        "\"\"\"\nComparative Benchmark: Beam Search vs MCTS vs Alpha-GP Hybrid\nRuns all three search methods on the standard benchmark suite and compares performance.\n\"\"\"\nimport torch\nimport numpy as np\nimport time\nimport traceback\nfrom typing import List, Dict, Callable, Optional\n\nfrom search.mcts import MCTS\nfrom search.beam_search import BeamSearch\nfrom search.hybrid_search import hybrid_solve\nfrom data.benchmark_data import BENCHMARK_SUITE, get_benchmark_data\nfrom core.grammar import ExpressionTree\nfrom utils.optimize_constants import optimize_constants\n\n\ndef run_single_problem(\n    x: np.ndarray, \n    y: np.ndarray, \n    method: str, \n    model, \n    device,\n    timeout_sec: int = 30,\n    beam_width: int = 50\n) -> Dict:\n    \"\"\"\n    Runs a single search method on a single problem.\n    \n    Returns:\n        dict with keys: formula, rmse, time, success\n    \"\"\"\n    start_time = time.time()\n    \n    try:\n        if method == \"beam\":\n            searcher = BeamSearch(model, device, beam_width=beam_width)\n            # BeamSearch expects list-like input and returns a list of results sorted by RMSE\n            results_list = searcher.search(x.tolist(), y.tolist())\n            elapsed = time.time() - start_time\n            if results_list and len(results_list) > 0:\n                result = results_list[0]  # Best result (sorted by RMSE)\n                return {\n                    'formula': result.get('formula', 'N/A'),\n                    'rmse': result.get('rmse', 1e9),\n                    'time': elapsed,\n                    'success': result.get('rmse', 1e9) < 0.05\n                }\n            else:\n                return {'formula': 'No Result', 'rmse': 1e9, 'time': elapsed, 'success': False}\n            \n        elif method == \"mcts\":\n            mcts = MCTS(model, device, max_simulations=500, batch_size=32)\n            # MCTS expects list-like input \n            result = mcts.search(x.tolist(), y.tolist())\n            elapsed = time.time() - start_time\n            return {\n                'formula': result.get('formula', 'N/A'),\n                'rmse': result.get('rmse', 1e9),\n                'time': elapsed,\n                'success': result.get('rmse', 1e9) < 0.05\n            }\n            \n        elif method == \"hybrid\":\n            result = hybrid_solve(\n                model=model,\n                device=device,\n                x_values=x.tolist(),\n                y_values=y.tolist(),\n                beam_width=beam_width,\n                gp_timeout=timeout_sec\n            )\n            elapsed = time.time() - start_time\n            \n            if result['formula']:\n                # Evaluate RMSE for hybrid result\n                try:\n                    tree = ExpressionTree.from_infix(result['formula'])\n                    if tree.is_valid:\n                        preds = tree.evaluate(x)\n                        rmse = np.sqrt(np.mean((preds - y) ** 2))\n                    else:\n                        rmse = 1e9\n                except:\n                    rmse = 1e9\n            else:\n                rmse = 1e9\n                \n            return {\n                'formula': result.get('formula', 'N/A') or 'Failed',\n                'rmse': rmse,\n                'time': elapsed,\n                'success': rmse < 0.05\n            }\n        else:\n            return {'formula': 'Unknown Method', 'rmse': 1e9, 'time': 0, 'success': False}\n            \n    except Exception as e:\n        print(f\"[ERROR] Method {method} failed: {e}\")\n        traceback.print_exc()\n        return {'formula': 'Error', 'rmse': 1e9, 'time': time.time() - start_time, 'success': False}\n\n\ndef run_comparison_benchmark(\n    model, \n    device, \n    methods: List[str] = [\"beam\", \"mcts\", \"hybrid\"],\n    gp_timeout: int = 30,\n    beam_width: int = 50,\n    progress_callback: Optional[Callable] = None\n) -> Dict:\n    \"\"\"\n    Runs all methods on all benchmark problems.\n    \n    Returns:\n        Dict with 'results' (per-problem-per-method) and 'summary' (aggregated stats)\n    \"\"\"\n    results = []\n    method_stats = {m: {'solved': 0, 'total_time': 0, 'total_rmse': 0} for m in methods}\n    \n    total_steps = len(BENCHMARK_SUITE) * len(methods)\n    current_step = 0\n    \n    for problem in BENCHMARK_SUITE:\n        x, y, _ = get_benchmark_data(problem['id'])\n        \n        for method in methods:\n            current_step += 1\n            \n            if progress_callback:\n                progress_callback(\n                    current_step / total_steps, \n                    f\"[{method.upper()}] {problem['name']}...\"\n                )\n            \n            result = run_single_problem(x, y, method, model, device, gp_timeout, beam_width)\n            \n            results.append({\n                'problem_id': problem['id'],\n                'problem_name': problem['name'],\n                'level': problem['level'],\n                'method': method,\n                'formula': result['formula'],\n                'rmse': result['rmse'],\n                'time': result['time'],\n                'success': result['success']\n            })\n            \n            # Update stats\n            method_stats[method]['total_time'] += result['time']\n            method_stats[method]['total_rmse'] += result['rmse'] if result['rmse'] < 1e6 else 0\n            if result['success']:\n                method_stats[method]['solved'] += 1\n    \n    # Compute summary\n    num_problems = len(BENCHMARK_SUITE)\n    summary = {}\n    for method in methods:\n        stats = method_stats[method]\n        summary[method] = {\n            'solved': stats['solved'],\n            'total': num_problems,\n            'score': (stats['solved'] / num_problems) * 100,\n            'avg_time': stats['total_time'] / num_problems,\n            'avg_rmse': stats['total_rmse'] / num_problems\n        }\n    \n    if progress_callback:\n        progress_callback(1.0, \"Benchmark Complete!\")\n    \n    return {'results': results, 'summary': summary}\n\n\ndef format_comparison_table(results: List[Dict]) -> str:\n    \"\"\"\n    Formats the results as a human-readable table.\n    \"\"\"\n    # Group by problem\n    problems = {}\n    for r in results:\n        pid = r['problem_id']\n        if pid not in problems:\n            problems[pid] = {'name': r['problem_name'], 'level': r['level'], 'methods': {}}\n        problems[pid]['methods'][r['method']] = {\n            'rmse': r['rmse'],\n            'time': r['time'],\n            'success': r['success'],\n            'formula': r['formula']\n        }\n    \n    output = []\n    output.append(\"=\" * 100)\n    output.append(f\"{'Problem':<25} | {'Method':<8} | {'RMSE':<12} | {'Time':<8} | {'Status':<10} | Formula\")\n    output.append(\"=\" * 100)\n    \n    for pid, pdata in problems.items():\n        name = pdata['name'][:24]\n        for method, mdata in pdata['methods'].items():\n            rmse_str = f\"{mdata['rmse']:.6f}\" if mdata['rmse'] < 1e6 else \"FAILED\"\n            time_str = f\"{mdata['time']:.2f}s\"\n            status = \"[OK]\" if mdata['success'] else \"[FAIL]\"\n            formula = mdata['formula'][:40] if mdata['formula'] else \"N/A\"\n            output.append(f\"{name:<25} | {method:<8} | {rmse_str:<12} | {time_str:<8} | {status:<10} | {formula}\")\n        output.append(\"-\" * 100)\n    \n    return \"\\n\".join(output)\n\n\ndef print_summary(summary: Dict):\n    \"\"\"\n    Prints a formatted summary comparison.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"BENCHMARK SUMMARY - Method Comparison\")\n    print(\"=\" * 60)\n    print(f\"{'Method':<12} | {'Solved':<10} | {'Score':<10} | {'Avg Time':<10} | {'Avg RMSE':<12}\")\n    print(\"-\" * 60)\n    \n    for method, stats in summary.items():\n        solved_str = f\"{stats['solved']}/{stats['total']}\"\n        score_str = f\"{stats['score']:.1f}%\"\n        time_str = f\"{stats['avg_time']:.2f}s\"\n        rmse_str = f\"{stats['avg_rmse']:.6f}\"\n        print(f\"{method.upper():<12} | {solved_str:<10} | {score_str:<10} | {time_str:<10} | {rmse_str:<12}\")\n    \n    print(\"=\" * 60)\n    \n    # Determine winner\n    best_method = max(summary.items(), key=lambda x: (x[1]['solved'], -x[1]['avg_rmse']))\n    print(f\"\\n*** WINNER: {best_method[0].upper()} with {best_method[1]['solved']}/{best_method[1]['total']} problems solved! ***\")\n\n\nif __name__ == \"__main__\":\n    # Standalone test\n    import sys\n    sys.path.insert(0, '.')\n    \n    from ui.app_core import load_model, get_model\n    \n    print(\"Loading model...\")\n    load_model()\n    model, device = get_model()\n    \n    if model is None:\n        print(\"Error: No model loaded!\")\n        exit(1)\n    \n    print(\"Running comparison benchmark...\")\n    result = run_comparison_benchmark(\n        model, \n        device, \n        methods=[\"beam\", \"mcts\", \"hybrid\"],\n        gp_timeout=30,\n        beam_width=50\n    )\n    \n    print(format_comparison_table(result['results']))\n    print_summary(result['summary'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/utils/simplify.py\n",
        "\"\"\"\nAlgebraic Simplification Module for AlphaSymbolic.\nUses SymPy for symbolic math simplification.\n\"\"\"\nimport sympy as sp\nfrom core.grammar import Node, ExpressionTree, OPERATORS\n\n# SymPy symbol for x\nx_sym = sp.Symbol('x')\n\ndef tree_to_sympy(node):\n    \"\"\"Convert an ExpressionTree Node to a SymPy expression.\"\"\"\n    if node is None:\n        return sp.Integer(0)\n    \n    val = node.value\n    \n    # Terminals\n    if val == 'x':\n        return x_sym\n    if val == 'pi':\n        return sp.pi\n    if val == 'e':\n        return sp.E\n    if val == 'C':\n        # Keep C as symbol for now\n        return sp.Symbol('C')\n    \n    # Try numeric\n    try:\n        return sp.Float(float(val))\n    except:\n        pass\n    \n    # Operators\n    args = [tree_to_sympy(c) for c in node.children]\n    \n    if val == '+': return args[0] + args[1]\n    if val == '-': return args[0] - args[1]\n    if val == '*': return args[0] * args[1]\n    if val == '/': return args[0] / args[1]\n    if val == 'pow': return sp.Pow(args[0], args[1])\n    if val == 'mod': return sp.Mod(args[0], args[1])\n    if val == 'sin': return sp.sin(args[0])\n    if val == 'cos': return sp.cos(args[0])\n    if val == 'tan': return sp.tan(args[0])\n    if val == 'exp': return sp.exp(args[0])\n    if val == 'log': return sp.log(args[0])\n    if val == 'sqrt': return sp.sqrt(args[0])\n    if val == 'abs': return sp.Abs(args[0])\n    if val == 'floor': return sp.floor(args[0])\n    if val == 'ceil': return sp.ceiling(args[0])\n    if val == 'gamma': return sp.gamma(args[0])\n    if val == 'lgamma': return sp.loggamma(args[0])  # SymPy's log-gamma\n    if val == 'neg': return -args[0]\n    \n    return sp.Integer(0)\n\ndef sympy_to_infix(expr):\n    \"\"\"Convert SymPy expression back to a readable string.\"\"\"\n    return str(expr)\n\ndef simplify_tree(tree):\n    \"\"\"\n    Takes an ExpressionTree and returns a simplified infix string.\n    \"\"\"\n    if not tree.is_valid:\n        return \"Invalid\"\n    \n    original_infix = tree.get_infix()\n    \n    try:\n        sympy_expr = tree_to_sympy(tree.root)\n        simplified = sp.simplify(sympy_expr)\n        result_str = str(simplified)\n        \n        # Validate: reject results containing invalid SymPy artifacts\n        # zoo = complex infinity, nan, oo = infinity\n        invalid_terms = ['zoo', 'nan', 'I*']  # I* indicates complex numbers\n        for term in invalid_terms:\n            if term in result_str:\n                return original_infix  # Fall back to original\n        \n        return result_str\n    except Exception as e:\n        # If simplification fails, return original\n        return original_infix\n\ndef simplify_infix(infix_str):\n    \"\"\"\n    Takes an infix string and returns a simplified version.\n    \"\"\"\n    try:\n        expr = sp.sympify(infix_str)\n        simplified = sp.simplify(expr)\n        return str(simplified)\n    except:\n        return infix_str\n\n# Quick test\nif __name__ == \"__main__\":\n    from core.grammar import ExpressionTree\n    \n    # Test: x + 0 should simplify to x\n    tokens = ['+', 'x', '0']\n    tree = ExpressionTree(tokens)\n    print(f\"Original: {tree.get_infix()}\")\n    print(f\"Simplified: {simplify_tree(tree)}\")\n    \n    # Test: x * 1 should simplify to x\n    tokens2 = ['*', 'x', '1']\n    tree2 = ExpressionTree(tokens2)\n    print(f\"Original: {tree2.get_infix()}\")\n    print(f\"Simplified: {simplify_tree(tree2)}\")\n    \n    # Test: x - x should simplify to 0\n    tokens3 = ['-', 'x', 'x']\n    tree3 = ExpressionTree(tokens3)\n    print(f\"Original: {tree3.get_infix()}\")\n    print(f\"Simplified: {simplify_tree(tree3)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/utils/__init__.py\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile AlphaSymbolic/app.py\n",
        "\"\"\"\nAlphaSymbolic - Gradio Web Interface\nWith GPU/CPU toggle and search method selection.\n\"\"\"\nimport gradio as gr\nimport torch\n\nfrom ui.app_core import load_model, get_device, get_device_info, set_device, get_training_errors, request_stop_training\nfrom ui.app_training import train_basic, train_curriculum, train_self_play, train_supervised, train_hybrid_feedback_loop\nfrom ui.app_search import solve_formula, generate_example\nfrom ui.app_benchmark import get_benchmark_tab\n\n\ndef toggle_device(use_gpu):\n    \"\"\"Toggle between GPU and CPU.\"\"\"\n    device_info = set_device(use_gpu)\n    color = \"#4ade80\" if \"CUDA\" in device_info else \"#fbbf24\" if \"MPS\" in device_info else \"#888\"\n    return f'<div style=\"padding: 10px; background: #0f0f23; border-radius: 8px; border-left: 3px solid {color};\"><span style=\"color: {color}; font-weight: bold;\">{device_info}</span></div>'\n\n\ndef create_app():\n    \"\"\"Create the Gradio app.\"\"\"\n    \n    with gr.Blocks(title=\"AlphaSymbolic\") as demo:\n        \n        # Header\n        device_info = get_device_info()\n        device_color = \"#4ade80\" if \"CUDA\" in device_info else \"#fbbf24\" if \"MPS\" in device_info else \"#888\"\n        \n        gr.HTML(f\"\"\"\n        <div style=\"text-align: center; padding: 20px; background: linear-gradient(90deg, #00d4ff22, transparent, #ff6b6b22); border-radius: 15px; margin-bottom: 20px;\">\n            <h1 style=\"color: #00d4ff; font-size: 42px; margin: 0;\">AlphaSymbolic</h1>\n            <p style=\"color: #888; font-size: 18px; margin: 5px 0;\">Deep Reinforcement Learning para Regresion Simbolica</p>\n        </div>\n        \"\"\")\n        \n        # System Controls\n        with gr.Row():\n            with gr.Column(scale=1):\n                model_selector = gr.Dropdown(choices=[\"lite\", \"pro\"], value=\"lite\", label=\"Arquitectura (Cerebro)\", interactive=True)\n            with gr.Column(scale=3):\n                model_status = gr.Textbox(label=\"Estado del Modelo\", value=\"Lite (Laptop Optimized) - Vocabulario Extendido\", interactive=False)\n        \n        def on_model_change(preset):\n            status, _ = load_model(preset_name=preset)\n            return status\n\n        model_selector.change(on_model_change, model_selector, model_status)\n        \n        with gr.Tabs():\n            # TAB 1: Search\n            with gr.Tab(\"Buscar Formula\"):\n                with gr.Row():\n                    with gr.Column(scale=1):\n                        gr.HTML('<h3 style=\"color: #00d4ff;\">Datos de Entrada</h3>')\n                        x_input = gr.Textbox(label=\"Valores X\", placeholder=\"1, 2, 3, 4, 5...\", lines=2)\n                        y_input = gr.Textbox(label=\"Valores Y\", placeholder=\"5, 7, 9, 11, 13...\", lines=2)\n                        \n                        with gr.Row():\n                            search_method = gr.Radio(\n                                choices=[\"Beam Search\", \"MCTS\", \"Alpha-GP Hybrid\"],\n                                value=\"Alpha-GP Hybrid\",\n                                label=\"Metodo de Busqueda\"\n                            )\n                        \n                        beam_slider = gr.Slider(5, 500, value=50, step=5, label=\"Beam Width / Simulaciones\")\n                        \n                        solve_btn = gr.Button(\"Buscar Formula\", variant=\"primary\", size=\"lg\")\n                        \n                        with gr.Row():\n                            gr.Button(\"Lineal\", size=\"sm\").click(lambda: generate_example(\"lineal\"), outputs=[x_input, y_input])\n                            gr.Button(\"Cuadratico\", size=\"sm\").click(lambda: generate_example(\"cuadratico\"), outputs=[x_input, y_input])\n                            gr.Button(\"Seno\", size=\"sm\").click(lambda: generate_example(\"trig\"), outputs=[x_input, y_input])\n                            gr.Button(\"Exponencial\", size=\"sm\").click(lambda: generate_example(\"exp\"), outputs=[x_input, y_input])\n                    \n                    with gr.Column(scale=2):\n                        result_html = gr.HTML(label=\"Resultado\")\n                        plot_output = gr.Plot(label=\"Visualizacion\")\n                \n                with gr.Row():\n                    pred_html = gr.HTML(label=\"Predicciones\")\n                    alt_html = gr.HTML(label=\"Alternativas\")\n                \n                raw_formula = gr.Textbox(visible=False)\n                \n                solve_btn.click(solve_formula, [x_input, y_input, beam_slider, search_method], \n                               [result_html, plot_output, pred_html, alt_html, raw_formula])\n            \n            # TAB 2: Training\n            with gr.Tab(\"Entrenar Modelo\"):\n                with gr.Row():\n                    gr.HTML(\"\"\"\n                    <div style=\"background: #16213e; padding: 20px; border-radius: 10px; flex: 1;\">\n                        <h3 style=\"color: #ffd93d; margin: 0;\">Centro de Entrenamiento</h3>\n                    </div>\n                    \"\"\")\n                    with gr.Column():\n                        use_gpu = gr.Checkbox(label=\"Usar GPU\", value=torch.cuda.is_available())\n                        device_display = gr.HTML(value=f'<div style=\"padding: 10px; background: #0f0f23; border-radius: 8px; border-left: 3px solid {device_color};\"><span style=\"color: {device_color}; font-weight: bold;\">{device_info}</span></div>')\n                        use_gpu.change(toggle_device, [use_gpu], [device_display])\n                    with gr.Column():\n                        delete_model_btn = gr.Button(\"\ud83d\uddd1\ufe0f Borrar Modelo\", variant=\"secondary\", size=\"sm\")\n                        delete_status = gr.HTML()\n                        \n                        def delete_model_action():\n                            import os\n                            from ui.app_core import CURRENT_PRESET\n                            filename = f\"alpha_symbolic_model_{CURRENT_PRESET}.pth\"\n                            if os.path.exists(filename):\n                                os.remove(filename)\n                                return f'<div style=\"color: #4ade80; padding: 5px;\">\u2705 Modelo [{CURRENT_PRESET}] eliminado. Reinicia la app para usar pesos nuevos.</div>'\n                            return f'<div style=\"color: #888; padding: 5px;\">No hay modelo [{CURRENT_PRESET}] guardado.</div>'\n                        \n                        delete_model_btn.click(delete_model_action, outputs=[delete_status])\n                        \n                        stop_train_btn = gr.Button(\"\u23f9\ufe0f Detener Entrenamiento\", variant=\"stop\", size=\"sm\")\n                        stop_status = gr.HTML()\n                        stop_train_btn.click(request_stop_training, outputs=[stop_status])\n                \n                with gr.Tabs():\n                    # Basic\n                    with gr.Tab(\"Basico\"):\n                        gr.HTML('<p style=\"color: #888;\">Entrenamiento rapido con datos sinteticos</p>')\n                        with gr.Row():\n                            with gr.Column():\n                                epochs_basic = gr.Slider(10, 500, value=100, step=10, label=\"Epocas\")\n                                batch_basic = gr.Slider(16, 128, value=32, step=16, label=\"Batch Size\")\n                                points_basic = gr.Slider(10, 100, value=20, step=10, label=\"Puntos por Formula\")\n                                train_basic_btn = gr.Button(\"Entrenar Basico\", variant=\"primary\")\n                            with gr.Column():\n                                result_basic = gr.HTML()\n                                plot_basic = gr.Plot()\n                        train_basic_btn.click(train_basic, [epochs_basic, batch_basic, points_basic], [result_basic, plot_basic])\n                    \n                    # Curriculum\n                    with gr.Tab(\"Curriculum\"):\n                        gr.HTML('''\n                        <div style=\"background: #0f0f23; padding: 15px; border-radius: 8px; margin-bottom: 15px;\">\n                            <p style=\"color: #00d4ff; margin: 0;\"><strong>Curriculum Learning</strong></p>\n                            <p style=\"color: #888; margin: 5px 0 0 0;\">Empieza con formulas simples y aumenta la dificultad.</p>\n                        </div>\n                        ''')\n                        with gr.Row():\n                            with gr.Column():\n                                epochs_curriculum = gr.Slider(50, 2000, value=200, step=50, label=\"Epocas\")\n                                batch_curriculum = gr.Slider(16, 128, value=64, step=16, label=\"Batch Size\")\n                                points_curriculum = gr.Slider(10, 100, value=20, step=10, label=\"Puntos por Formula\")\n                                train_curriculum_btn = gr.Button(\"Entrenar Curriculum\", variant=\"primary\")\n                            with gr.Column():\n                                result_curriculum = gr.HTML()\n                                plot_curriculum = gr.Plot()\n                        train_curriculum_btn.click(train_curriculum, [epochs_curriculum, batch_curriculum, points_curriculum], [result_curriculum, plot_curriculum])\n                    \n                    # Self-Play\n                    with gr.Tab(\"Self-Play\"):\n                        gr.HTML('''\n                        <div style=\"background: #0f0f23; padding: 15px; border-radius: 8px; margin-bottom: 15px; border-left: 3px solid #ff6b6b;\">\n                            <p style=\"color: #ff6b6b; margin: 0;\"><strong>AlphaZero Self-Play</strong></p>\n                            <p style=\"color: #888; margin: 5px 0 0 0;\">El modelo resuelve problemas y aprende de sus exitos.</p>\n                        </div>\n                        ''')\n                        with gr.Row():\n                            with gr.Column():\n                                iterations_sp = gr.Slider(10, 1000, value=100, step=10, label=\"Iteraciones\")\n                                problems_sp = gr.Slider(5, 200, value=10, step=5, label=\"Problemas/Iter\")\n                                points_sp = gr.Slider(10, 100, value=20, step=10, label=\"Puntos por Formula\")\n                                train_sp_btn = gr.Button(\"Iniciar Self-Play\", variant=\"primary\")\n                            with gr.Column():\n                                result_sp = gr.HTML()\n                                plot_sp = gr.Plot()\n                        train_sp_btn.click(train_self_play, [iterations_sp, problems_sp, points_sp], [result_sp, plot_sp])\n                \n                    # Feedback Loop (Teacher-Student)\n                    with gr.Tab(\"Feedback Loop (Hybrid)\"):\n                        gr.HTML('''\n                        <div style=\"background: #0f0f23; padding: 15px; border-radius: 8px; margin-bottom: 15px; border-left: 3px solid #f1c40f;\">\n                            <p style=\"color: #f1c40f; margin: 0;\"><strong>Teacher-Student Feedback Loop</strong></p>\n                            <p style=\"color: #888; margin: 5px 0 0 0;\">El modelo (Estudiante) intenta resolver problemas. Si falla, el Alpha-GP (Maestro) interviene y a\u00f1ade la soluci\u00f3n al dataset.</p>\n                        </div>\n                        ''')\n                        with gr.Row():\n                            with gr.Column():\n                                iterations_fb = gr.Slider(5, 500, value=20, step=5, label=\"Ciclos\")\n                                problems_fb = gr.Slider(5, 50, value=10, step=5, label=\"Problemas Dif\u00edciles / Ciclo\")\n                                timeout_fb = gr.Slider(5, 30, value=10, step=5, label=\"Timeout Maestro (s)\")\n                                train_fb_btn = gr.Button(\"Iniciar Feedback Loop\", variant=\"primary\")\n                            with gr.Column():\n                                result_fb = gr.HTML()\n                                plot_fb = gr.Plot()\n                        train_fb_btn.click(train_hybrid_feedback_loop, [iterations_fb, problems_fb, timeout_fb], [result_fb, plot_fb])\n                \n                # --- PRE-TRAINING (Warmup) ---\n                with gr.Accordion(\"\ud83c\udf93 Escuela Primaria (Pre-Entrenamiento)\", open=False):\n                    gr.Markdown(\"Entrenamiento masivo supervisado de alta velocidad para aprender sintaxis basica. **Recomendado al inicio.**\")\n                    with gr.Row():\n                        with gr.Column():\n                            epochs_pre = gr.Slider(100, 10000, value=2000, step=100, label=\"Iteraciones R\u00e1pidas\")\n                            train_pre_btn = gr.Button(\"Iniciar Pre-Entrenamiento\", variant=\"primary\")\n                        with gr.Column():\n                            result_pre = gr.HTML()\n                            plot_pre = gr.Plot()\n                    train_pre_btn.click(train_supervised, [epochs_pre], [result_pre, plot_pre])\n\n                # --- HALL OF SHAME (Error Analysis) ---\n                with gr.Accordion(\"\ud83d\udd75\ufe0f\u200d\u2642\ufe0f Hall of Shame (Analisis de Errores)\", open=False):\n                    gr.Markdown(\"Aqu\u00ed se muestran los problemas donde el modelo fall\u00f3 dr\u00e1sticamente hoy.\")\n                    error_table = gr.DataFrame(\n                        headers=[\"Time\", \"Target Formula\", \"Predicted\", \"Loss\", \"Stage\"],\n                        datatype=[\"str\", \"str\", \"str\", \"number\", \"str\"],\n                        interactive=False\n                    )\n                    refresh_errors_btn = gr.Button(\"\ud83d\udd04 Actualizar Errores\", size=\"sm\")\n                    \n                    def update_errors():\n                        errors = get_training_errors()\n                        # Reverse to show newest first\n                        data = [[\n                            e['time'], e['target'], e['predicted'], round(e['loss'], 2), e['stage']\n                        ] for e in reversed(errors)]\n                        return data\n                    \n                    refresh_errors_btn.click(update_errors, outputs=[error_table])\n            \n            # TAB 4: Benchmark\n            get_benchmark_tab()\n\n            # TAB 5: Info\n            with gr.Tab(\"Informacion\"):\n                device_info_current = get_device_info()\n                device_color_current = \"#4ade80\" if \"CUDA\" in device_info_current else \"#fbbf24\" if \"MPS\" in device_info_current else \"#888\"\n                \n                gr.HTML(f\"\"\"\n                <div style=\"background: #1a1a2e; padding: 30px; border-radius: 15px;\">\n                    <h2 style=\"color: #00d4ff;\">Que es AlphaSymbolic?</h2>\n                    <p style=\"color: #ccc; line-height: 1.8;\">\n                        Sistema de <strong style=\"color: #ff6b6b;\">regresion simbolica</strong> \n                        basado en <strong style=\"color: #00d4ff;\">Deep Learning</strong> y \n                        <strong style=\"color: #ffd93d;\">Monte Carlo Tree Search</strong>.\n                    </p>\n                    \n                    <h3 style=\"color: #00d4ff; margin-top: 30px;\">Dispositivo Actual</h3>\n                    <p style=\"color: {device_color_current}; font-size: 20px;\">{device_info_current}</p>\n                    \n                    <h3 style=\"color: #00d4ff; margin-top: 30px;\">Metodos de Busqueda</h3>\n                    <ul style=\"color: #ccc;\">\n                        <li><strong>Beam Search:</strong> Explora multiples candidatos en paralelo (rapido)</li>\n                        <li><strong>MCTS:</strong> Monte Carlo Tree Search (mas preciso, lento)</li>\n                        <li><strong>Alpha-GP Hybrid:</strong> Fusiona Neural Search con Algoritmo Genetico GPU (Extremo)</li>\n                    </ul>\n                    \n                    <h3 style=\"color: #00d4ff; margin-top: 30px;\">Operadores</h3>\n                    <div style=\"display: flex; flex-wrap: wrap; gap: 10px; margin: 15px 0;\">\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #00d4ff;\">+</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #00d4ff;\">-</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #00d4ff;\">*</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #00d4ff;\">/</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #ff6b6b;\">sin</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #ff6b6b;\">cos</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #ffd93d;\">exp</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #ffd93d;\">log</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #4ade80;\">pow</span>\n                        <span style=\"background: #0f0f23; padding: 5px 15px; border-radius: 20px; color: #4ade80;\">sqrt</span>\n                    </div>\n                </div>\n                \"\"\")\n        \n        gr.HTML(\"\"\"\n        <div style=\"text-align: center; padding: 20px; color: #666; margin-top: 30px;\">\n            <p>Powered by PyTorch - SymPy - Scipy - Gradio</p>\n        </div>\n        \"\"\")\n    \n    return demo\n\n\n\n# --- Global Initialization for Hot Reloading ---\nprint(\"Iniciando AlphaSymbolic (Global Init)...\")\n# Load model once at module level so 'gradio app.py' works\nstatus_init, device_info_init = load_model() \nprint(f\"   {status_init} | {device_info_init}\")\n\n# Create the app instance globally\ndemo = create_app()\n\nif __name__ == \"__main__\":\n    print(\"Abriendo navegador...\")\n    # Launch with auto-reload compatibility if run directly (though proper reload needs 'gradio app.py')\n    demo.launch(share=True, inbrowser=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run AlphaSymbolic\n",
        "# The binaries are in ../Code/build/\n",
        "%cd AlphaSymbolic\n",
        "!python app.py\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}